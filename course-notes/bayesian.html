<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Bayesian Analysis and MCMC | MCMCglmm Course Notes</title>
  <meta name="description" content="Extended documentation and course notes for the MCMCglmm R package." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Bayesian Analysis and MCMC | MCMCglmm Course Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Bayesian Analysis and MCMC | MCMCglmm Course Notes" />
  
  <meta name="twitter:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

<meta name="author" content="Jarrod Hadfield" />


<meta name="date" content="2026-01-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="overview.html"/>
<link rel="next" href="glm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script>
<link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-1.3.31/rglClass.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/utils.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/buffer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/subscenes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shaders.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shadersrc.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/textures.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/projection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/mouse.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/init.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pieces.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/draw.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/controls.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/selection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/rglTimer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pretty.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/axes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/animation.src.js"></script>
<script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.src.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#outline"><i class="fa fa-check"></i><b>1.1</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian Analysis and MCMC</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian.html"><a href="bayesian.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>2.2</b> Likelihood</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-ml"><i class="fa fa-check"></i><b>2.2.1</b> Maximum Likelihood (ML)</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian.html"><a href="bayesian.html#restricted-maximum-likelihood-reml"><i class="fa fa-check"></i><b>2.2.2</b> Restricted Maximum Likelihood (REML)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian.html"><a href="bayesian.html#prior-distribution"><i class="fa fa-check"></i><b>2.3</b> Prior Distribution</a></li>
<li class="chapter" data-level="2.4" data-path="bayesian.html"><a href="bayesian.html#posterior-distribution"><i class="fa fa-check"></i><b>2.4</b> Posterior Distribution</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian.html"><a href="bayesian.html#marginal-posterior-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Marginal Posterior Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="bayesian.html"><a href="bayesian.html#intervals-sec"><i class="fa fa-check"></i><b>2.4.2</b> Credible Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian.html"><a href="bayesian.html#MCMC"><i class="fa fa-check"></i><b>2.5</b> MCMC</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian.html"><a href="bayesian.html#starting-values"><i class="fa fa-check"></i><b>2.5.1</b> Starting values</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian.html"><a href="bayesian.html#metropolis-hastings-updates"><i class="fa fa-check"></i><b>2.5.2</b> Metropolis-Hastings updates</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian.html"><a href="bayesian.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.5.3</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian.html"><a href="bayesian.html#diagnostics-sec"><i class="fa fa-check"></i><b>2.5.4</b> MCMC Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian.html"><a href="bayesian.html#Vprior-sec"><i class="fa fa-check"></i><b>2.6</b> Priors for Residual Variances</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian.html"><a href="bayesian.html#IP-sec"><i class="fa fa-check"></i><b>2.6.1</b> Improper Priors</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian.html"><a href="bayesian.html#transform-sec"><i class="fa fa-check"></i><b>2.7</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>3</b> Linear and Generalised Linear Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="glm.html"><a href="glm.html#linear-model-lm"><i class="fa fa-check"></i><b>3.1</b> Linear Model (LM)</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="glm.html"><a href="glm.html#lm-sec"><i class="fa fa-check"></i><b>3.1.1</b> Linear Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="glm.html"><a href="glm.html#generalised-linear-model-glm"><i class="fa fa-check"></i><b>3.2</b> Generalised Linear Model (GLM)</a></li>
<li class="chapter" data-level="3.3" data-path="glm.html"><a href="glm.html#poisson-glm"><i class="fa fa-check"></i><b>3.3</b> Poisson GLM</a></li>
<li class="chapter" data-level="3.4" data-path="glm.html"><a href="glm.html#overdispersion"><i class="fa fa-check"></i><b>3.4</b> Overdispersion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="glm.html"><a href="glm.html#multiplicative-overdispersion"><i class="fa fa-check"></i><b>3.4.1</b> Multiplicative Overdispersion</a></li>
<li class="chapter" data-level="3.4.2" data-path="glm.html"><a href="glm.html#addod-sec"><i class="fa fa-check"></i><b>3.4.2</b> Additive Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="glm.html"><a href="glm.html#prediction-in-glm"><i class="fa fa-check"></i><b>3.5</b> Prediction in GLM</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="glm.html"><a href="glm.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="glm.html"><a href="glm.html#binom-sec"><i class="fa fa-check"></i><b>3.6</b> Binomial and Bernoulli GLM</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="glm.html"><a href="glm.html#overdispersion-1"><i class="fa fa-check"></i><b>3.6.1</b> Overdispersion</a></li>
<li class="chapter" data-level="3.6.2" data-path="glm.html"><a href="glm.html#binom-pred-sec"><i class="fa fa-check"></i><b>3.6.2</b> Prediction</a></li>
<li class="chapter" data-level="3.6.3" data-path="glm.html"><a href="glm.html#bernoulli-sec"><i class="fa fa-check"></i><b>3.6.3</b> Bernoulli GLM</a></li>
<li class="chapter" data-level="3.6.4" data-path="glm.html"><a href="glm.html#probit-link"><i class="fa fa-check"></i><b>3.6.4</b> Probit link</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="glm.html"><a href="glm.html#ordinal-data"><i class="fa fa-check"></i><b>3.7</b> Ordinal Data</a></li>
<li class="chapter" data-level="3.8" data-path="glm.html"><a href="glm.html#non-zero-binomial-data"><i class="fa fa-check"></i><b>3.8</b> Non-zero Binomial Data</a></li>
<li class="chapter" data-level="3.9" data-path="glm.html"><a href="glm.html#complete-separation"><i class="fa fa-check"></i><b>3.9</b> Complete Separation</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="glm.html"><a href="glm.html#gelman-prior-sec"><i class="fa fa-check"></i><b>3.9.1</b> The <span class="citation">Gelman, Jakulin, et al. (2008)</span> prior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ranef.html"><a href="ranef.html"><i class="fa fa-check"></i><b>4</b> Random effects</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ranef.html"><a href="ranef.html#GLMM"><i class="fa fa-check"></i><b>4.1</b> Generalised Linear Mixed Model (GLMM)</a></li>
<li class="chapter" data-level="4.2" data-path="ranef.html"><a href="ranef.html#ranpred-sec"><i class="fa fa-check"></i><b>4.2</b> Prediction with Random Effects</a></li>
<li class="chapter" data-level="4.3" data-path="ranef.html"><a href="ranef.html#overdispersed-binomial-as-a-bernoulli-glmm"><i class="fa fa-check"></i><b>4.3</b> Overdispersed Binomial as a Bernoulli GLMM</a></li>
<li class="chapter" data-level="4.4" data-path="ranef.html"><a href="ranef.html#ICC"><i class="fa fa-check"></i><b>4.4</b> Intra-class Correlations</a></li>
<li class="chapter" data-level="4.5" data-path="ranef.html"><a href="ranef.html#PXprior-sec"><i class="fa fa-check"></i><b>4.5</b> Priors for Random Effect Variances</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="ranef.html"><a href="ranef.html#f-and-folded-t-priors"><i class="fa fa-check"></i><b>4.5.1</b> <span class="math inline">\(F\)</span> and folded-<span class="math inline">\(t\)</span> priors</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="ranef.html"><a href="ranef.html#Vprior-gen-sec"><i class="fa fa-check"></i><b>4.6</b> Prior Generators</a></li>
<li class="chapter" data-level="4.7" data-path="ranef.html"><a href="ranef.html#priors-on-functions-of-variances"><i class="fa fa-check"></i><b>4.7</b> Priors on Functions of Variances</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="ranef.html"><a href="ranef.html#intra-class-correlation"><i class="fa fa-check"></i><b>4.7.1</b> Intra-class Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="ranef.html"><a href="ranef.html#fix-or-rand"><i class="fa fa-check"></i><b>4.8</b> Fixed or Random?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cat-int.html"><a href="cat-int.html"><i class="fa fa-check"></i><b>5</b> Categorical Random Interactions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="cat-int.html"><a href="cat-int.html#vstruct-sec"><i class="fa fa-check"></i><b>5.1</b> Variance Structures</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="cat-int.html"><a href="cat-int.html#textttidh-variance-structure"><i class="fa fa-check"></i><b>5.1.1</b> <span class="math inline">\(\texttt{idh}\)</span> Variance Structure</a></li>
<li class="chapter" data-level="5.1.2" data-path="cat-int.html"><a href="cat-int.html#us-sec"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(\texttt{us}\)</span> Variance Structure</a></li>
<li class="chapter" data-level="5.1.3" data-path="cat-int.html"><a href="cat-int.html#other-variance-structures"><i class="fa fa-check"></i><b>5.1.3</b> Other Variance Structures</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="cat-int.html"><a href="cat-int.html#linking-functions"><i class="fa fa-check"></i><b>5.2</b> Linking Functions</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="cat-int.html"><a href="cat-int.html#textttstr-covariances-between-random-terms"><i class="fa fa-check"></i><b>5.2.1</b> <span class="math inline">\(\texttt{str}\)</span>: covariances between random terms</a></li>
<li class="chapter" data-level="5.2.2" data-path="cat-int.html"><a href="cat-int.html#multim-sec"><i class="fa fa-check"></i><b>5.2.2</b> <span class="math inline">\(\texttt{mm}\)</span>: multi-membership models</a></li>
<li class="chapter" data-level="5.2.3" data-path="cat-int.html"><a href="cat-int.html#textttcovu-covariances-between-random-and-residual-terms"><i class="fa fa-check"></i><b>5.2.3</b> <span class="math inline">\(\texttt{covu}\)</span>: covariances between random and residual terms</a></li>
<li class="chapter" data-level="5.2.4" data-path="cat-int.html"><a href="cat-int.html#texttttheta_scale-scaled-linear-predictor"><i class="fa fa-check"></i><b>5.2.4</b> <span class="math inline">\(\texttt{theta_scale}\)</span>: scaled linear predictor</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="cat-int.html"><a href="cat-int.html#VCVprior-sec"><i class="fa fa-check"></i><b>5.3</b> Priors for Covariance Matrices</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="cat-int.html"><a href="cat-int.html#marginal-priors-for-variances"><i class="fa fa-check"></i><b>5.3.1</b> Marginal Priors for Variances</a></li>
<li class="chapter" data-level="5.3.2" data-path="cat-int.html"><a href="cat-int.html#VCVprior-r-sec"><i class="fa fa-check"></i><b>5.3.2</b> Marginal Priors for Covariances and Correlations</a></li>
<li class="chapter" data-level="5.3.3" data-path="cat-int.html"><a href="cat-int.html#full-joint-prior"><i class="fa fa-check"></i><b>5.3.3</b> Full joint prior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cont-int.html"><a href="cont-int.html"><i class="fa fa-check"></i><b>6</b> Continuous Random Interactions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="cont-int.html"><a href="cont-int.html#random-regression"><i class="fa fa-check"></i><b>6.1</b> Random Regression</a></li>
<li class="chapter" data-level="6.2" data-path="cont-int.html"><a href="cont-int.html#het-res"><i class="fa fa-check"></i><b>6.2</b> Heterogeneous (Residual) Variances</a></li>
<li class="chapter" data-level="6.3" data-path="cont-int.html"><a href="cont-int.html#autoc-sec"><i class="fa fa-check"></i><b>6.3</b> Autocorrelation</a></li>
<li class="chapter" data-level="6.4" data-path="cont-int.html"><a href="cont-int.html#vstab-sec"><i class="fa fa-check"></i><b>6.4</b> Variance stabilisation</a></li>
<li class="chapter" data-level="6.5" data-path="cont-int.html"><a href="cont-int.html#antedependence-and-autoregressive-models"><i class="fa fa-check"></i><b>6.5</b> Antedependence and Autoregressive Models</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="cont-int.html"><a href="cont-int.html#autoregressive-models"><i class="fa fa-check"></i><b>6.5.1</b> Autoregressive Models</a></li>
<li class="chapter" data-level="6.5.2" data-path="cont-int.html"><a href="cont-int.html#prior-ante-sec"><i class="fa fa-check"></i><b>6.5.2</b> Priors in Antedependence models</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="cont-int.html"><a href="cont-int.html#user-defined-sec"><i class="fa fa-check"></i><b>6.6</b> User-defined Design Matrices</a></li>
<li class="chapter" data-level="6.7" data-path="cont-int.html"><a href="cont-int.html#splines"><i class="fa fa-check"></i><b>6.7</b> Splines</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multi-response models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="multi.html"><a href="multi.html#relaxing-the-univariate-assumptions-of-causality"><i class="fa fa-check"></i><b>7.1</b> Relaxing the univariate assumptions of causality</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="multi.html"><a href="multi.html#textttcovu-covariances-between-random-and-residual-terms-1"><i class="fa fa-check"></i><b>7.1.1</b> <span class="math inline">\(\texttt{covu}\)</span>: covariances between random and residual terms</a></li>
<li class="chapter" data-level="7.1.2" data-path="multi.html"><a href="multi.html#texttttheta_scale-scaled-linear-predictor-1"><i class="fa fa-check"></i><b>7.1.2</b> <span class="math inline">\(\texttt{theta_scale}\)</span>: scaled linear predictor</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="multi.html"><a href="multi.html#multinomial-models"><i class="fa fa-check"></i><b>7.2</b> Multinomial Models</a></li>
<li class="chapter" data-level="7.3" data-path="multi.html"><a href="multi.html#zero-inflated-models"><i class="fa fa-check"></i><b>7.3</b> Zero-inflated Models</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="multi.html"><a href="multi.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="multi.html"><a href="multi.html#Hurdle"><i class="fa fa-check"></i><b>7.4</b> Hurdle Models</a></li>
<li class="chapter" data-level="7.5" data-path="multi.html"><a href="multi.html#ZAP"><i class="fa fa-check"></i><b>7.5</b> Zero-altered Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pedigree.html"><a href="pedigree.html"><i class="fa fa-check"></i><b>8</b> Pedigrees and Phylogenies</a>
<ul>
<li class="chapter" data-level="8.1" data-path="pedigree.html"><a href="pedigree.html#pedigree-and-phylogeny-formats"><i class="fa fa-check"></i><b>8.1</b> Pedigree and phylogeny formats</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="pedigree.html"><a href="pedigree.html#pedigrees"><i class="fa fa-check"></i><b>8.1.1</b> Pedigrees</a></li>
<li class="chapter" data-level="8.1.2" data-path="pedigree.html"><a href="pedigree.html#phylogenies"><i class="fa fa-check"></i><b>8.1.2</b> Phylogenies</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="pedigree.html"><a href="pedigree.html#the-animal-model-and-the-phylogenetic-mixed-model"><i class="fa fa-check"></i><b>8.2</b> The animal model and the phylogenetic mixed model</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="measurement.html"><a href="measurement.html"><i class="fa fa-check"></i><b>9</b> Measurement Error, Meta-analysis an Missing Values</a>
<ul>
<li class="chapter" data-level="9.1" data-path="measurement.html"><a href="measurement.html#error-in-the-response"><i class="fa fa-check"></i><b>9.1</b> Error in the Response</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="measurement.html"><a href="measurement.html#meta-sec"><i class="fa fa-check"></i><b>9.1.1</b> Meta-analysis</a></li>
<li class="chapter" data-level="9.1.2" data-path="measurement.html"><a href="measurement.html#interval-estimation"><i class="fa fa-check"></i><b>9.1.2</b> Interval Estimation</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="measurement.html"><a href="measurement.html#error-in-the-predictors"><i class="fa fa-check"></i><b>9.2</b> Error in the Predictors</a></li>
<li class="chapter" data-level="9.3" data-path="measurement.html"><a href="measurement.html#missing-values"><i class="fa fa-check"></i><b>9.3</b> Missing Values</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="path.html"><a href="path.html"><i class="fa fa-check"></i><b>10</b> Path Analysis &amp; Antedependence Structures</a>
<ul>
<li class="chapter" data-level="10.1" data-path="path.html"><a href="path.html#path-anlaysis"><i class="fa fa-check"></i><b>10.1</b> Path Anlaysis</a></li>
<li class="chapter" data-level="10.2" data-path="path.html"><a href="path.html#ante-sec"><i class="fa fa-check"></i><b>10.2</b> Antedependence</a></li>
<li class="chapter" data-level="10.3" data-path="path.html"><a href="path.html#scaling"><i class="fa fa-check"></i><b>10.3</b> Scaling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="technical-details.html"><a href="technical-details.html"><i class="fa fa-check"></i><b>11</b> Technical Details</a>
<ul>
<li class="chapter" data-level="11.1" data-path="technical-details.html"><a href="technical-details.html#model-form"><i class="fa fa-check"></i><b>11.1</b> Model Form</a></li>
<li class="chapter" data-level="11.2" data-path="technical-details.html"><a href="technical-details.html#MCMC-app"><i class="fa fa-check"></i><b>11.2</b> MCMC Sampling Schemes</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="technical-details.html"><a href="technical-details.html#updating-the-latent-variables-bf-l"><i class="fa fa-check"></i><b>11.2.1</b> Updating the latent variables <span class="math inline">\({\bf l}\)</span></a></li>
<li class="chapter" data-level="11.2.2" data-path="technical-details.html"><a href="technical-details.html#updating-the-location-vector-boldsymboltheta-leftboldsymbolmathbfbeta-bf-uright"><i class="fa fa-check"></i><b>11.2.2</b> Updating the location vector <span class="math inline">\(\boldsymbol{\theta} = \left[{\boldsymbol{\mathbf{\beta}}}^{&#39;}\; {\bf u}^{&#39;}\right]^{&#39;}\)</span></a></li>
<li class="chapter" data-level="11.2.3" data-path="technical-details.html"><a href="technical-details.html#updating-the-variance-structures-bf-g-and-bf-r"><i class="fa fa-check"></i><b>11.2.3</b> Updating the variance structures <span class="math inline">\({\bf G}\)</span> and <span class="math inline">\({\bf R}\)</span></a></li>
<li class="chapter" data-level="11.2.4" data-path="technical-details.html"><a href="technical-details.html#ordinal-models"><i class="fa fa-check"></i><b>11.2.4</b> Ordinal Models</a></li>
<li class="chapter" data-level="11.2.5" data-path="technical-details.html"><a href="technical-details.html#path-analyses"><i class="fa fa-check"></i><b>11.2.5</b> Path Analyses</a></li>
<li class="chapter" data-level="11.2.6" data-path="technical-details.html"><a href="technical-details.html#deviance-and-dic"><i class="fa fa-check"></i><b>11.2.6</b> Deviance and DIC</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="technical-details.html"><a href="technical-details.html#parameter-expansion"><i class="fa fa-check"></i><b>11.3</b> Parameter Expansion</a></li>
<li class="chapter" data-level="11.4" data-path="technical-details.html"><a href="technical-details.html#priors-for-corg-and-corgh-structures"><i class="fa fa-check"></i><b>11.4</b> Priors for <code>corg</code> and <code>corgh</code> structures</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MCMCglmm Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Bayesian Analysis and MCMC<a href="bayesian.html#bayesian" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this Chapter I cover the basics of Bayesian analysis and Markov chain Monte Carlo (MCMC) techniques. Exposure to these ideas, and statistics in general, has increased dramatically since these notes were first written. Many readers may therefore wish to skip straight to later chapters that cover <span class="math inline">\(\texttt{MCMCglmm}\)</span> more specifically.</p>
<div id="introduction" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Introduction<a href="bayesian.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are fundamental differences between frequentist and Bayesian approaches, but for those of us interested in applied statistics the hope is that these differences do not translate into practical differences, and this is often the case. My advice would be <em>if</em> you can fit the same model using different packages and/or methods do so, and if they give very different answers worry. In some cases differences will exist, and it is important to know why, and which method is more appropriate for the data in hand.</p>
<p>In the context of a generalised linear mixed model (GLMM), here are what I see as the pro’s and cons of using (restricted) maximum likelihood (REML) versus Bayesian MCMC methods. REML is fast and easy to use, whereas MCMC can be slow and technically more challenging. Particularly challenging is the specification of a sensible prior, something which is a non-issue in a REML analysis. However, analytical results for non-Gaussian GLMM are generally not available, and REML based procedures use approximate likelihood methods that may not work well. MCMC is also an approximation but the accuracy of the approximation increases the longer the analysis is run for, being exact at the limit. In addition, REML uses large-sample theory to derive approximate confidence intervals that may have very poor coverage, especially for variance components. Again, MCMC measures of confidence are exact, up to Monte Carlo error, and provide an easy and intuitive way of obtaining measures of confidence on derived statistics such as ratios of variances, correlations and predictions.</p>
<p>To illustrate the differences between the approaches let’s imagine we’ve observed several draws (stored in the vector <span class="math inline">\({\bf y}\)</span>) from a standard normal (i.e. <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^{2}=1\)</span>). The likelihood is the probability of the data given the parameters:</p>
<p><span class="math display">\[Pr({\bf y} | \mu, \sigma^{2})\]</span></p>
<p>This is a conditional distribution, where the conditioning is on the model parameters which are taken as fixed and known. In a way this is quite odd because we’ve already observed the data, and we don’t know what the parameter values are. In a Bayesian analysis we evaluate the conditional probability of the model parameters given the observed data:</p>
<p><span class="math display" id="eq:post1-eq">\[Pr(\mu, \sigma^{2} | {\bf y})
\label{post1-eq}   \tag{2.1}\]</span></p>
<p>which seems more reasonable, until we realise that this probability is proportional to</p>
<p><span class="math display">\[Pr({\bf y} | \mu, \sigma^{2})Pr(\mu, \sigma^{2})\]</span></p>
<p>where the first term is the likelihood, and the second term represents our prior belief in the values that the model parameters could take. Because the choice of prior is rarely justified by an objective quantification of the state of knowledge it has come under criticism, and indeed we will see later that the choice of prior can make a difference.</p>
</div>
<div id="likelihood" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Likelihood<a href="bayesian.html#likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can generate 5 observations from this distribution using <code>rnorm</code>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="bayesian.html#cb1-1" tabindex="-1"></a>Ndata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> <span class="fu">rnorm</span>(<span class="dv">5</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>))</span>
<span id="cb1-2"><a href="bayesian.html#cb1-2" tabindex="-1"></a>Ndata<span class="sc">$</span>y</span></code></pre></div>
<pre><code>## [1] -1.01500872 -0.07963674 -0.23298702 -0.81726793  0.77209084</code></pre>
<p>We can plot the probability density function for the standard normal using <code>dnorm</code> and we can then place the 5 data on it:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="bayesian.html#cb3-1" tabindex="-1"></a>possible.y<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="fl">0.1</span>)                    <span class="co"># possible values of y</span></span>
<span id="cb3-2"><a href="bayesian.html#cb3-2" tabindex="-1"></a>Probability<span class="ot">&lt;-</span><span class="fu">dnorm</span>(possible.y, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="dv">1</span>) <span class="co"># density of possible values </span></span>
<span id="cb3-3"><a href="bayesian.html#cb3-3" tabindex="-1"></a><span class="fu">plot</span>(Probability<span class="sc">~</span>possible.y, <span class="at">type=</span><span class="st">&quot;l&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Density&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb3-4"><a href="bayesian.html#cb3-4" tabindex="-1"></a>Probability.y<span class="ot">&lt;-</span><span class="fu">dnorm</span>(Ndata<span class="sc">$</span>y, <span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="dv">1</span>)  <span class="co"># density of actual values</span></span>
<span id="cb3-5"><a href="bayesian.html#cb3-5" tabindex="-1"></a><span class="fu">points</span>(Probability.y<span class="sc">~</span>Ndata<span class="sc">$</span>y)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:dnorm"></span>
<img src="_bookdown_files/fig/dnorm-1.png" alt="Probability density function for the unit normal with the data points overlaid" width="672" />
<p class="caption">
Figure 2.1: Probability density function for the unit normal with the data points overlaid
</p>
</div>
<p>The likelihood of these data, conditioning on <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>, is proportional to the product of the densities (read off the y-axis on Figure <a href="bayesian.html#fig:dnorm">2.1</a>):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="bayesian.html#cb4-1" tabindex="-1"></a><span class="fu">prod</span>(<span class="fu">dnorm</span>(Ndata<span class="sc">$</span>y, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.003113051</code></pre>
<p>Of course we don’t know the true mean and variance and so we may want to ask how probable the data would be if, say, <span class="math inline">\(\mu=0\)</span>, and <span class="math inline">\(\sigma^2=0.5\)</span>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="bayesian.html#cb6-1" tabindex="-1"></a><span class="fu">prod</span>(<span class="fu">dnorm</span>(Ndata<span class="sc">$</span>y, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fl">0.5</span>)))</span></code></pre></div>
<pre><code>## [1] 0.005424967</code></pre>
<p>It would seem that the data are more likely under this set of parameters than the true parameters, which we must expect some of the time just from random sampling. To get some idea as to why this might be the case we can overlay the two densities (Figure <a href="bayesian.html#fig:dnorm1">2.2</a>), and we can see that although some data points (e.g. -1.015) are more likely with the true parameters, in aggregate the new parameters produce a higher likelihood.</p>
<div class="figure"><span style="display:block;" id="fig:dnorm1"></span>
<img src="_bookdown_files/fig/dnorm1-1.png" alt="Two probability density functions for normal distributions with means of zero, and a variance of one (black line) and a variance of 0.5 (red line).  The data points are overlaid." width="672" />
<p class="caption">
Figure 2.2: Two probability density functions for normal distributions with means of zero, and a variance of one (black line) and a variance of 0.5 (red line). The data points are overlaid.
</p>
</div>
<p>The likelihood of the data can be calculated on a grid of possible parameter values to produce a likelihood surface, as in Figure <a href="bayesian.html#fig:Lsurface">2.3</a>. The densities on the contours have been scaled so they are relative to the density of the parameter values that have the highest density (the maximum likelihood estimate of the two parameters). Two things are apparent. First, although the surface is symmetric about the line <span class="math inline">\(\mu = \hat{\mu}\)</span> (where <span class="math inline">\(\hat{}\)</span> stands for maximum likelihood estimate) the surface is far from symmetric about the line <span class="math inline">\(\sigma^{2} = \hat{\sigma}^{2}\)</span>. Second, there are a large range of parameter values for which the data are only 10 times less likely than if the data were generated under the maximum likelihood estimates.</p>
<div class="figure"><span style="display:block;" id="fig:Lsurface"></span>
<img src="_bookdown_files/fig/Lsurface-1.png" alt="Likelihood surface for the likelihood $Pr({\bf y}|\mu, \sigma^{2})$. The likelihood has been normalised so that the maximum likelihood has a value of one." width="672" />
<p class="caption">
Figure 2.3: Likelihood surface for the likelihood <span class="math inline">\(Pr({\bf y}|\mu, \sigma^{2})\)</span>. The likelihood has been normalised so that the maximum likelihood has a value of one.
</p>
</div>
<div id="maximum-likelihood-ml" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Maximum Likelihood (ML)<a href="bayesian.html#maximum-likelihood-ml" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The ML estimator is the combination of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^{2}\)</span> that make the data most likely. Although we could evaluate the density on a grid of parameter values (as we did to produce Figure <a href="bayesian.html#fig:Lsurface">2.3</a>) in order to locate the maximum, for such a simple problem the ML estimator can be derived analytically. However, so we don’t have to meet some nasty maths later, I’ll introduce and use one of R’s generic optimising routines that can be used to maximise the likelihood function (in practice, the log-likelihood is maximised to avoid numerical problems):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="bayesian.html#cb8-1" tabindex="-1"></a>lik <span class="ot">&lt;-</span> <span class="cf">function</span>(par, y, <span class="at">log =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb8-2"><a href="bayesian.html#cb8-2" tabindex="-1"></a>    <span class="cf">if</span> (log) {</span>
<span id="cb8-3"><a href="bayesian.html#cb8-3" tabindex="-1"></a>        l <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dnorm</span>(y, <span class="at">mean =</span> par[<span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(par[<span class="dv">2</span>]), <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb8-4"><a href="bayesian.html#cb8-4" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb8-5"><a href="bayesian.html#cb8-5" tabindex="-1"></a>        l <span class="ot">&lt;-</span> <span class="fu">prod</span>(<span class="fu">dnorm</span>(y, <span class="at">mean =</span> par[<span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(par[<span class="dv">2</span>])))</span>
<span id="cb8-6"><a href="bayesian.html#cb8-6" tabindex="-1"></a>    }</span>
<span id="cb8-7"><a href="bayesian.html#cb8-7" tabindex="-1"></a>    <span class="fu">return</span>(l)</span>
<span id="cb8-8"><a href="bayesian.html#cb8-8" tabindex="-1"></a>}</span>
<span id="cb8-9"><a href="bayesian.html#cb8-9" tabindex="-1"></a><span class="co"># function which takes the parameter vector (mean and *variance*) and the data</span></span>
<span id="cb8-10"><a href="bayesian.html#cb8-10" tabindex="-1"></a><span class="co"># and returns the (log) likelihood</span></span>
<span id="cb8-11"><a href="bayesian.html#cb8-11" tabindex="-1"></a></span>
<span id="cb8-12"><a href="bayesian.html#cb8-12" tabindex="-1"></a>MLest <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="fu">c</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">var =</span> <span class="dv">1</span>), <span class="at">fn =</span> lik, <span class="at">y =</span> Ndata<span class="sc">$</span>y, <span class="at">log =</span> <span class="cn">TRUE</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">fnscale =</span> <span class="sc">-</span><span class="dv">1</span>,</span>
<span id="cb8-13"><a href="bayesian.html#cb8-13" tabindex="-1"></a>    <span class="at">reltol =</span> <span class="fl">1e-16</span>))<span class="sc">$</span>par</span></code></pre></div>
<p>The first call to <code>optim</code> are starting values for the optimisation algorithm, and the second argument (<code>fn</code>) is the function to be maximised. By default <code>optim</code> will try to minimise the function hence multiplying by -1 (<code>fnscale = -1</code>). The algorithm has successfully found
the mode:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="bayesian.html#cb9-1" tabindex="-1"></a>MLest</span></code></pre></div>
<pre><code>##       mean        var 
## -0.2745619  0.3955995</code></pre>
<p>Alternatively we could also fit the model using <code>glm</code>, which by default assumes the response is normal:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="bayesian.html#cb11-1" tabindex="-1"></a>m1a<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> Ndata)</span>
<span id="cb11-2"><a href="bayesian.html#cb11-2" tabindex="-1"></a><span class="fu">summary</span>(m1a<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ 1, data = Ndata)
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  -0.2746     0.3145  -0.873    0.432
## 
## (Dispersion parameter for gaussian family taken to be 0.4944994)
## 
##     Null deviance: 1.978  on 4  degrees of freedom
## Residual deviance: 1.978  on 4  degrees of freedom
## AIC: 13.553
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>Here we see that although the estimate of the mean (intercept) is the same, the estimate of the variance (the dispersion parameter:
0.494) is higher when fitting the model using <code>glm</code>. In fact the ML estimate is a factor <span class="math inline">\(\frac{n}{n-1}\)</span> smaller because the <code>glm</code> estimate has used Bessel’s correction:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="bayesian.html#cb13-1" tabindex="-1"></a>MLest[<span class="st">&quot;var&quot;</span>] <span class="sc">*</span> (<span class="dv">5</span><span class="sc">/</span><span class="dv">4</span>)</span></code></pre></div>
<pre><code>##       var 
## 0.4944994</code></pre>
</div>
<div id="restricted-maximum-likelihood-reml" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Restricted Maximum Likelihood (REML)<a href="bayesian.html#restricted-maximum-likelihood-reml" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Why do we use Bessel’s correction? Imagine we had only observed the first two values of <span class="math inline">\({\bf y}\)</span> (Figure <a href="bayesian.html#fig:muvar">2.4</a>). The variance is defined as the average squared distance between a random variable and the <em>true</em> mean. However, the ML estimator of the variance is the average squared distance between the random variable and the ML <em>estimate</em> of the mean. Since the ML estimator of the mean is the average of the two numbers (the dashed line) then the average squared distance will always be smaller than if the true mean was used, unless the ML estimate of the mean and the true mean coincide. This is why we use Bessel’s <span class="math inline">\(n-1\)</span> correction when estimating the variance from the sum of squares, or why we divide by <span class="math inline">\(n-n_p\)</span> when estimating the residual variance in a liner model with <span class="math inline">\(n_p\)</span> parameters, or more generally why we use REML in linear mixed models. Why these corrections for uncertainty in the mean, or model parameters, have this form can be understood from a Bayesian perspective (see Section <a href="bayesian.html#Vprior-sec">2.6</a>).</p>
<div class="figure"><span style="display:block;" id="fig:muvar"></span>
<img src="_bookdown_files/fig/muvar-1.png" alt="Probability density function for the unit normal with two realisations overlaid. The solid vertical line is the true mean, whereas the vertical dashed line is the mean of the two realisations (the ML estimator of the mean). The variance is the expected squared distance between the true mean and the realisations. The ML estimator of the variance is the average squared distance between the ML mean and the realisations (horizontal dashed lines), which is always smaller than the average squared distance between the true mean and the realisations (horizontal solid lines)" width="672" />
<p class="caption">
Figure 2.4: Probability density function for the unit normal with two realisations overlaid. The solid vertical line is the true mean, whereas the vertical dashed line is the mean of the two realisations (the ML estimator of the mean). The variance is the expected squared distance between the true mean and the realisations. The ML estimator of the variance is the average squared distance between the ML mean and the realisations (horizontal dashed lines), which is always smaller than the average squared distance between the true mean and the realisations (horizontal solid lines)
</p>
</div>
</div>
</div>
<div id="prior-distribution" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Prior Distribution<a href="bayesian.html#prior-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="math inline">\(\texttt{MCMCglmm}\)</span> uses a normal prior for the fixed effects and an inverse-Wishart prior for the residual variance. In the current model their is a single fixed effect (<span class="math inline">\(\mu\)</span>) and a scalar (residual) variance (<span class="math inline">\(\sigma^2\)</span>). For the mean we will use the default prior - a diffuse normal centred around zero but with very large variance (<span class="math inline">\(10^{8}\)</span>). For the residual variance, the inverse-Wishart prior takes two scalar parameters. In <span class="math inline">\(\texttt{MCMCglmm}\)</span> this is parameterised through the parameters <span class="math inline">\(\texttt{V}\)</span> and <span class="math inline">\(\texttt{nu}\)</span>. The distribution tends to a point mass on <span class="math inline">\(\texttt{V}\)</span> as the degree of belief parameter, <span class="math inline">\(\texttt{nu}\)</span> goes to infinity. We will defer a full discussion of the inverse-Wishart prior to Section <a href="bayesian.html#Vprior-sec">2.6</a> and for now we will use the prior specification <span class="math inline">\(\texttt{V}=1\)</span> and <span class="math inline">\(\texttt{nu}=0.002\)</span> which used to be frequently used for variances. The function <code>dprior</code> can be used to obtain the prior density for a variance (or standard deviation) given a <span class="math inline">\(\texttt{MCMCglmm}\)</span> specification and we can plot this in order to visualise what the distribution looks like (Figure <a href="bayesian.html#fig:dinvgamma1">2.5</a>).</p>
<div class="figure"><span style="display:block;" id="fig:dinvgamma1"></span>
<img src="_bookdown_files/fig/dinvgamma1-1.png" alt="Probability density function for a univariate inverse-Wishart with the variance at the limit set to 1 ($\texttt{V}=1$) and a degree of belief parameter set to 0.002 ($\texttt{nu}=0.002$)." width="672" />
<p class="caption">
Figure 2.5: Probability density function for a univariate inverse-Wishart with the variance at the limit set to 1 (<span class="math inline">\(\texttt{V}=1\)</span>) and a degree of belief parameter set to 0.002 (<span class="math inline">\(\texttt{nu}=0.002\)</span>).
</p>
</div>
<p>As before we can write a function for calculating the (log) prior probability:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="bayesian.html#cb15-1" tabindex="-1"></a>prior.p <span class="ot">&lt;-</span> <span class="cf">function</span>(par, priorB, priorR, <span class="at">log =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb15-2"><a href="bayesian.html#cb15-2" tabindex="-1"></a>    <span class="cf">if</span> (log) {</span>
<span id="cb15-3"><a href="bayesian.html#cb15-3" tabindex="-1"></a>        d <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(par[<span class="dv">1</span>], <span class="at">mean =</span> priorB<span class="sc">$</span>mu, <span class="at">sd =</span> <span class="fu">sqrt</span>(priorB<span class="sc">$</span>V), <span class="at">log =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> <span class="fu">dprior</span>(par[<span class="dv">2</span>],</span>
<span id="cb15-4"><a href="bayesian.html#cb15-4" tabindex="-1"></a>            priorR, <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb15-5"><a href="bayesian.html#cb15-5" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb15-6"><a href="bayesian.html#cb15-6" tabindex="-1"></a>        d <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(par[<span class="dv">1</span>], <span class="at">mean =</span> priorB<span class="sc">$</span>mu, <span class="at">sd =</span> <span class="fu">sqrt</span>(priorB<span class="sc">$</span>V)) <span class="sc">*</span> <span class="fu">dprior</span>(par[<span class="dv">2</span>],</span>
<span id="cb15-7"><a href="bayesian.html#cb15-7" tabindex="-1"></a>            priorR)</span>
<span id="cb15-8"><a href="bayesian.html#cb15-8" tabindex="-1"></a>    }</span>
<span id="cb15-9"><a href="bayesian.html#cb15-9" tabindex="-1"></a>    <span class="fu">return</span>(d)</span>
<span id="cb15-10"><a href="bayesian.html#cb15-10" tabindex="-1"></a>}</span></code></pre></div>
<p>where <code>priorR</code> is a list with elements <code>V</code> and <code>nu</code> specifying the prior for the variance, and <code>priorB</code> is a list with elements <code>mu</code> and <code>V</code> specifying the prior for the mean. <span class="math inline">\(\texttt{MCMCglmm}\)</span> takes these prior specifications as a list:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="bayesian.html#cb16-1" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>), <span class="at">B =</span> <span class="fu">list</span>(<span class="at">mu =</span> <span class="dv">0</span>, <span class="at">V =</span> <span class="fl">1e+08</span>))</span></code></pre></div>
</div>
<div id="posterior-distribution" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Posterior Distribution<a href="bayesian.html#posterior-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By multiplying the likelihood by the prior probability for that set of parameters we can get the posterior probability up to a proportional constant. We can write a function for doing this:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="bayesian.html#cb17-1" tabindex="-1"></a>likprior <span class="ot">&lt;-</span> <span class="cf">function</span>(par, y, priorB, priorR, <span class="at">log =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb17-2"><a href="bayesian.html#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="bayesian.html#cb17-3" tabindex="-1"></a>    <span class="cf">if</span> (log) {</span>
<span id="cb17-4"><a href="bayesian.html#cb17-4" tabindex="-1"></a>        pd <span class="ot">&lt;-</span> <span class="fu">lik</span>(par, y, <span class="at">log =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> <span class="fu">prior.p</span>(par, <span class="at">priorB =</span> priorB, <span class="at">priorR =</span> priorR,</span>
<span id="cb17-5"><a href="bayesian.html#cb17-5" tabindex="-1"></a>            <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb17-6"><a href="bayesian.html#cb17-6" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb17-7"><a href="bayesian.html#cb17-7" tabindex="-1"></a>        pd <span class="ot">&lt;-</span> <span class="fu">lik</span>(par, y) <span class="sc">*</span> <span class="fu">prior.p</span>(par, <span class="at">priorB =</span> priorB, <span class="at">priorR =</span> priorR)</span>
<span id="cb17-8"><a href="bayesian.html#cb17-8" tabindex="-1"></a>    }</span>
<span id="cb17-9"><a href="bayesian.html#cb17-9" tabindex="-1"></a>    <span class="fu">return</span>(pd)</span>
<span id="cb17-10"><a href="bayesian.html#cb17-10" tabindex="-1"></a>}</span></code></pre></div>
<p>and we can overlay the posterior density (scaled by the posterior density at the posterior mode) on the likelihood surface we calculated before (Figure <a href="bayesian.html#fig:Lsurface">2.3</a>).</p>
<div class="figure"><span style="display:block;" id="fig:Psurface"></span>
<img src="_bookdown_files/fig/Psurface-1.png" alt="Likelihood surface for the likelihood $Pr({\bf y}|\mu, \sigma^{2})$ in black, and the posterior distribution $Pr(\mu, \sigma^{2} | {\bf y})$ in red.  The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. The prior distributions  $Pr(\mu)\sim N(0, 10^8)$ and  $Pr(\sigma^{2})\sim IW(\texttt{V}=1, \texttt{nu}=0.002)$ were used." width="672" />
<p class="caption">
Figure 2.6: Likelihood surface for the likelihood <span class="math inline">\(Pr({\bf y}|\mu, \sigma^{2})\)</span> in black, and the posterior distribution <span class="math inline">\(Pr(\mu, \sigma^{2} | {\bf y})\)</span> in red. The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. The prior distributions <span class="math inline">\(Pr(\mu)\sim N(0, 10^8)\)</span> and <span class="math inline">\(Pr(\sigma^{2})\sim IW(\texttt{V}=1, \texttt{nu}=0.002)\)</span> were used.
</p>
</div>
<p>The prior has some influence on the posterior mode of the variance, and we can use an optimisation algorithm again to locate the mode:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="bayesian.html#cb18-1" tabindex="-1"></a>Best <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="fu">c</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">var =</span> <span class="dv">1</span>), <span class="at">fn =</span> likprior, <span class="at">y =</span> Ndata<span class="sc">$</span>y, <span class="at">priorB =</span> prior<span class="sc">$</span>B,</span>
<span id="cb18-2"><a href="bayesian.html#cb18-2" tabindex="-1"></a>    <span class="at">priorR =</span> prior<span class="sc">$</span>R, <span class="at">log =</span> <span class="cn">TRUE</span>, <span class="at">method =</span> <span class="st">&quot;L-BFGS-B&quot;</span>, <span class="at">lower =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1e+05</span>, <span class="fl">1e-05</span>),</span>
<span id="cb18-3"><a href="bayesian.html#cb18-3" tabindex="-1"></a>    <span class="at">upper =</span> <span class="fu">c</span>(<span class="fl">1e+05</span>, <span class="fl">1e+05</span>), <span class="at">control =</span> <span class="fu">list</span>(<span class="at">fnscale =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">factr =</span> <span class="fl">1e-16</span>))<span class="sc">$</span>par</span>
<span id="cb18-4"><a href="bayesian.html#cb18-4" tabindex="-1"></a>Best</span></code></pre></div>
<pre><code>##       mean        var 
## -0.2745613  0.2827783</code></pre>
<p>The posterior mode for the mean is essentially identical to the ML estimate, but the posterior mode for the variance is even less than the ML estimate, which is known to be downwardly biased. The reason that the ML estimate is downwardly biased is because it did not take into account the uncertainty in the mean, as we saw when discussing the motivation behind REML. In a Bayesian analysis we can do this by evaluating the marginal distribution of <span class="math inline">\(\sigma^{2}\)</span> by averaging over the uncertainty in the mean. Before we do this, however, it will be instructive to see why this would be hard using our function that simply multiplies the likelihood by the prior (<code>likprior</code>). This function (the probabilities on the right-hand side) is only <em>proprtional</em> to the posterior density (the left-hand side), not equal to it, implying</p>
<p><span class="math display">\[Pr(\mu, \sigma^{2} | {\bf y}) = \frac{1}{C} Pr({\bf y} | \mu, \sigma^{2})Pr(\mu, \sigma^{2})\]</span></p>
<p>where <span class="math inline">\(C\)</span> is some constant. In some cases this is not an issue - when finding the posterior mode it was not an issue since the parameters that maximise the posterior density would also maximise the posterior density scaled by <span class="math inline">\(C\)</span>. Similarly, if we wanted to make relative statements about posterior probabilities, then <span class="math inline">\(C\)</span> would cancel. For example, we can see how much more likely a variance of a half is versus a variance of one (assuming the mean is zero):</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="bayesian.html#cb20-1" tabindex="-1"></a>p.<span class="fl">0.5</span> <span class="ot">&lt;-</span> <span class="fu">likprior</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="at">y =</span> Ndata<span class="sc">$</span>y, <span class="at">priorB =</span> prior<span class="sc">$</span>B, <span class="at">priorR =</span> prior<span class="sc">$</span>R)</span>
<span id="cb20-2"><a href="bayesian.html#cb20-2" tabindex="-1"></a>p.<span class="fl">1.0</span> <span class="ot">&lt;-</span> <span class="fu">likprior</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">y =</span> Ndata<span class="sc">$</span>y, <span class="at">priorB =</span> prior<span class="sc">$</span>B, <span class="at">priorR =</span> prior<span class="sc">$</span>R)</span>
<span id="cb20-3"><a href="bayesian.html#cb20-3" tabindex="-1"></a></span>
<span id="cb20-4"><a href="bayesian.html#cb20-4" tabindex="-1"></a>p.<span class="fl">0.5</span><span class="sc">/</span>p.<span class="fl">1.0</span></span></code></pre></div>
<pre><code>## [1] 3.484236</code></pre>
<p>However, in many instances we would like to work with the normalised posterior density, so how do we get <span class="math inline">\(C\)</span>? Well we know that if we took the posterior probability of being any combination of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^{2}\)</span> it must be equal to one:</p>
<p><span class="math display">\[\int_{\sigma^2}\int_\mu Pr(\mu, \sigma^{2} | {\bf y})d\mu d\sigma^2=1\]</span></p>
<p>and so because</p>
<p><span class="math display">\[1 =\frac{1}{C}\int_{\sigma^2}\int_\mu Pr({\bf y} | \mu, \sigma^{2})Pr(\mu, \sigma^{2})d\mu d\sigma^2\]</span></p>
<p>we can integrate our parameters over <code>likprior</code> to get <span class="math inline">\(C\)</span>. Not easy, and requires numerical integration:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="bayesian.html#cb22-1" tabindex="-1"></a>C <span class="ot">&lt;-</span> <span class="fu">adaptIntegrate</span>(likprior, <span class="at">y =</span> Ndata<span class="sc">$</span>y, <span class="at">priorR =</span> prior<span class="sc">$</span>R, <span class="at">priorB =</span> prior<span class="sc">$</span>B, <span class="at">lower =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="cn">Inf</span>,</span>
<span id="cb22-2"><a href="bayesian.html#cb22-2" tabindex="-1"></a>    <span class="dv">0</span>), <span class="at">upper =</span> <span class="fu">c</span>(<span class="cn">Inf</span>, <span class="cn">Inf</span>))<span class="sc">$</span>integral</span></code></pre></div>
<p>The posterior density for <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=0.5\)</span> is <code>p.0.5/C=</code>0.937. Neither interesting or particularly interpretable, and often we want to perform additional integration in order to compute quantities of interest. For example, imagine we wanted to know the probability that the parameters lay in the region of parameter space we were plotting, i.e. lay in the square <span class="math inline">\(\mu = (-2,2)\)</span> and <span class="math inline">\(\sigma^{2} = (0,5)\)</span>. To obtain this probability we need to calculate the definite integral</p>
<p><span class="math display">\[\int_{\sigma^{2}=0}^{\sigma^{2}=5} \int_{\mu=-2}^{\mu=2} Pr(\mu, \sigma^{2} | {\bf y})d\mu d\sigma^2\]</span></p>
<p>which requires integrating our <code>likprior</code> over the same limits and rescaling by <span class="math inline">\(C\)</span>:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="bayesian.html#cb23-1" tabindex="-1"></a>p.square <span class="ot">&lt;-</span> <span class="fu">adaptIntegrate</span>(likprior, <span class="at">y =</span> Ndata<span class="sc">$</span>y, <span class="at">priorR =</span> prior<span class="sc">$</span>R, <span class="at">priorB =</span> prior<span class="sc">$</span>B,</span>
<span id="cb23-2"><a href="bayesian.html#cb23-2" tabindex="-1"></a>    <span class="at">lower =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">0</span>), <span class="at">upper =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">5</span>))<span class="sc">$</span>integral</span>
<span id="cb23-3"><a href="bayesian.html#cb23-3" tabindex="-1"></a>p.square<span class="sc">/</span>C</span></code></pre></div>
<pre><code>## [1] 0.9816286</code></pre>
<p>While this is doable for simple problems like this, numerical integration for high-dimensional problems is often not feasible and MCMC provides a viable alternative.</p>
<p>We can fit this model in <span class="math inline">\(\texttt{MCMCglmm}\)</span> pretty much in the same way as we did using <code>glm</code>:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="bayesian.html#cb25-1" tabindex="-1"></a>m1a<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> Ndata, <span class="at">prior =</span> prior, <span class="at">thin =</span> <span class="dv">1</span>)</span></code></pre></div>
<p>The Markov chain is drawing random (but often correlated) samples from the joint posterior distribution (depicted by the red contours in Figure <a href="bayesian.html#fig:Psurface">2.6</a>). The element of the output called <code>Sol</code> contains the posterior samples for the
mean, and the element called <code>VCV</code> contains the posterior samples for the variance. We can produce a scatter plot:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="bayesian.html#cb26-1" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">cbind</span>(m1a<span class="fl">.2</span><span class="sc">$</span>Sol, m1a<span class="fl">.2</span><span class="sc">$</span>VCV))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:PsurfaceMCMC"></span>
<img src="_bookdown_files/fig/PsurfaceMCMC-1.png" alt="The posterior distribution $Pr(\mu, \sigma^{2} | {\bf y})$. The black dots are samples from the posterior using MCMC, and the red contours are calculated by evaluating the posterior density on a grid of parameter values. The contours are normalised so that the posterior mode has a value of one." width="672" />
<p class="caption">
Figure 2.7: The posterior distribution <span class="math inline">\(Pr(\mu, \sigma^{2} | {\bf y})\)</span>. The black dots are samples from the posterior using MCMC, and the red contours are calculated by evaluating the posterior density on a grid of parameter values. The contours are normalised so that the posterior mode has a value of one.
</p>
</div>
<p>and we see that MCMCglmm is sampling the same distribution as the posterior distribution calculated on a grid of possible parameter values (Figure <a href="bayesian.html#fig:PsurfaceMCMC">2.7</a>).</p>
<p>A very nice property of MCMC is that we can calculate probabilities from the output without having to explicitly perform integration. Earlier we calculated the probability that the mean lay between <span class="math inline">\(\pm2\)</span> and the variance was less than 5 (<span class="math inline">\(\texttt{p.square/C}=\)</span> 0.982). Because MCMC has sampled the posterior distribution randomly, this probability will be equal to the expected probability that we have drawn an MCMC sample from the region. We can obtain an estimate of this by seeing what proportion of our actual samples lie in this square:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="bayesian.html#cb27-1" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(m1a<span class="fl">.2</span><span class="sc">$</span>Sol <span class="sc">&gt;</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">&amp;</span> m1a<span class="fl">.2</span><span class="sc">$</span>Sol <span class="sc">&lt;</span> <span class="dv">2</span> <span class="sc">&amp;</span> m1a<span class="fl">.2</span><span class="sc">$</span>VCV <span class="sc">&lt;</span> <span class="dv">5</span>))</span></code></pre></div>
<pre><code>## 
##  FALSE   TRUE 
## 0.0196 0.9804</code></pre>
<p>There is Monte Carlo error in the answer (0.980) but if we collect a large number of samples then this can be minimised.</p>
<div id="marginal-posterior-distribution" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Marginal Posterior Distribution<a href="bayesian.html#marginal-posterior-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The marginal distribution is often of primary interest in statistical inference, because it represents our knowledge about a parameter given the data:</p>
<p><span class="math display" id="eq:marg-eq">\[Pr(\sigma^{2} | {\bf y}) = \int Pr(\mu, \sigma^{2} | {\bf y})d\mu
\label{marg-eq}   \tag{2.2}\]</span></p>
<p>after averaging over any nuisance parameters, such as the mean in this case.</p>
<p>Using MCMC, we can obtain the marginal distribution of the variance by simply evaluating the draws in <code>VCV</code> ignoring (averaging over) the draws in <code>Sol</code>:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="bayesian.html#cb29-1" tabindex="-1"></a><span class="fu">hist</span>(m1a<span class="fl">.2</span><span class="sc">$</span>VCV)</span>
<span id="cb29-2"><a href="bayesian.html#cb29-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> Best[<span class="st">&quot;var&quot;</span>], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:MsurfaceMCMC"></span>
<img src="_bookdown_files/fig/MsurfaceMCMC-1.png" alt="Histogram of samples from the marginal distribution of the variance $Pr(\sigma^{2} | {\bf y})$ using MCMC. The vertical line is the joint posterior mode, which differs slightly from the marginal posterior mode (the peak of the marginal distribution)." width="672" />
<p class="caption">
Figure 2.8: Histogram of samples from the marginal distribution of the variance <span class="math inline">\(Pr(\sigma^{2} | {\bf y})\)</span> using MCMC. The vertical line is the joint posterior mode, which differs slightly from the marginal posterior mode (the peak of the marginal distribution).
</p>
</div>
<p>In this example the marginal mode and the joint mode are very similar, although this is not necessarily the case and can depend on both the data and the prior. Section <a href="bayesian.html#Vprior-sec">2.6</a> covers properties of the inverse-Wishart prior in detail.</p>
</div>
<div id="intervals-sec" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Credible Intervals<a href="bayesian.html#intervals-sec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="MCMC" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> MCMC<a href="bayesian.html#MCMC" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In order to be confident that <span class="math inline">\(\texttt{MCMCglmm}\)</span> has successfully sampled the posterior distribution it will be necessary to have a basic understanding of how MCMC works. The aim of MCMC is to sample parameter values from their posterior distribution, shown exactly (up to proportionality) in Figure <a href="bayesian.html#fig:Psurface">2.6</a>. In all but the very simplest cases this distribution is not of a known form and we cannot sample from it directly by using functions such as <code>rnorm</code>. However we can set up a random walk in parameter space such that the chance a walker visits a particular set of parameter values is proportional to their posterior density. Importantly, this can done without needing to normalise the posterior density by <span class="math inline">\(C\)</span>.</p>
<div id="starting-values" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Starting values<a href="bayesian.html#starting-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First we need to initialise the chain and specify a set of parameter values from which the chain can start moving through parameter space. In practice, it’s generally a good idea to start the chain in region of relatively high probability. Although starting configurations can be set by the user using the <code>start</code> argument, in general the heuristic techniques used by <span class="math inline">\(\texttt{MCMCglmm}\)</span> seem to work quite well. We will denote the parameter values of the starting configuration (time <span class="math inline">\(t=0\)</span>) as <span class="math inline">\(\mu_{t=0}\)</span> and <span class="math inline">\({\sigma^{2}}_{t=0}\)</span>. There are several ways in which we can get the chain to move in parameter space, and the main techniques used in <span class="math inline">\(\texttt{MCMCglmm}\)</span> are Gibbs sampling and Metropolis-Hastings updates. To illustrate, it will be easier to turn the contour plot of the posterior distribution into a perspective plot (Figure <a href="bayesian.html#fig:Psurface-persp">2.9</a>).</p>
<div class="figure"><span style="display:block;" id="fig:Psurface-persp"></span>
<img src="_bookdown_files/fig/Psurface-persp-1.png" alt="The posterior distribution $Pr(\mu, \sigma^{2} | {\bf y})$. This perspective plot is equivalent to the contour plot in Figure \@ref(fig:Psurface) but it has been normalised by $C$ and is equal to, not just proportional to, the posterior density." width="672" />
<p class="caption">
Figure 2.9: The posterior distribution <span class="math inline">\(Pr(\mu, \sigma^{2} | {\bf y})\)</span>. This perspective plot is equivalent to the contour plot in Figure <a href="bayesian.html#fig:Psurface">2.6</a> but it has been normalised by <span class="math inline">\(C\)</span> and is equal to, not just proportional to, the posterior density.
</p>
</div>
</div>
<div id="metropolis-hastings-updates" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Metropolis-Hastings updates<a href="bayesian.html#metropolis-hastings-updates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After initialising the chain we need to decide where to go next, and this decision is based on two rules. First we have to generate a candidate destination, and then we need to decide whether to go there or stay where we are. There are many ways in which we could generate candidate parameter values, and <span class="math inline">\(\texttt{MCMCglmm}\)</span> uses a well tested and simple method. A random set of coordinates are picked from a multivariate normal distribution that is centred on the initial coordinates <span class="math inline">\(\mu_{t=0}\)</span> and <span class="math inline">\(\sigma^{2}_{t=0}\)</span>. We will denote this new set of parameter values as <span class="math inline">\(\mu_{new}\)</span> and <span class="math inline">\(\sigma^{2}_{new}\)</span>. The question then remains whether to move to this new set of parameter values or remain at our current parameter values now designated as old <span class="math inline">\(\mu_{old}=\mu_{t=0}\)</span> and <span class="math inline">\(\sigma^{2}_{old}=\sigma^{2}_{t=0}\)</span>. If the posterior probability for the new set of parameter values is greater, then the chain moves to this new set of parameters and the chain has successfully completed an iteration: (<span class="math inline">\(\mu_{t=1} = \mu_{new}\)</span> and <span class="math inline">\(\sigma^{2}_{t=1}=\sigma^{2}_{new}\)</span>). If the new set of parameter values has a lower posterior probability then the chain may move there, but not all the time. The probability that the chain moves to low lying areas, is determined by the relative difference between the old and new posterior probabilities. If the posterior probability for <span class="math inline">\(\mu_{new}\)</span> and <span class="math inline">\(\sigma^{2}_{new}\)</span> is 5 times less than the posterior probability for <span class="math inline">\(\mu_{old}\)</span> and <span class="math inline">\(\sigma^{2}_{old}\)</span>, then the chain would move to the new set of parameter values 1 in 5 times. If the move is successful then we set <span class="math inline">\(\mu_{t=1} = \mu_{new}\)</span> and <span class="math inline">\(\sigma^{2}_{t=1}=\sigma^{2}_{new}\)</span> as before, and if the move is unsuccessful then the chain stays where it is (<span class="math inline">\(\mu_{t=1} = \mu_{old}\)</span> and <span class="math inline">\(\sigma^{2}_{t=1}=\sigma^{2}_{old}\)</span>). Note that we only need to know the posterior density up to proportionality to make these calculations (i.e we could use <code>likprior</code> directly without knowing <span class="math inline">\(C\)</span>). Using these rules we can record where the chain has travelled and generate an approximation of the posterior distribution. Basically, a histogram of Figure <a href="bayesian.html#fig:Psurface-persp">2.9</a>.</p>
<p>Why Metropolis-Hastings updates work can perhaps be more easily understood in terms of a simpler toy example. Imagine we had a strong prior such that only two sets of parameter values had positive posterior probability: Set A (<span class="math inline">\(\mu_A\)</span> an <span class="math inline">\(\sigma^2_A\)</span>) with a posterior probability 5 times that of Set B. If the chain is currently at Set A then the candidate parameter values will be Set B and vice-versa. Since we move from Set A to Set B 20% of the time and we move from Set B to Set A 100% of the time, the odds of being in Set A versus Set B is 1:0.2 which is exactly equal to their posterior odds of 5:1.</p>
</div>
<div id="gibbs-sampling" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Gibbs Sampling<a href="bayesian.html#gibbs-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Gibbs sampling is a special case of Metropolis-Hastings updating, and <span class="math inline">\(\texttt{MCMCglmm}\)</span> uses Gibbs sampling to update most parameters. In the Metropolis-Hastings example above, the Markov Chain was allowed to move in both directions of parameter space simultaneously. An equally valid approach would have been to set up two Metropolis-Hastings schemes where the chain was first allowed to move along the <span class="math inline">\(\mu\)</span> axis, and then along the <span class="math inline">\(\sigma^{2}\)</span> axis. In Figure <a href="bayesian.html#fig:Psurface-persp2">2.10</a> I have cut the posterior distribution of Figure <a href="bayesian.html#fig:Psurface-persp">2.9</a> in half, and the edge of the surface facing left is the conditional distribution of <span class="math inline">\(\mu\)</span> given that <span class="math inline">\(\sigma^{2}=1\)</span>:</p>
<p><span class="math display">\[Pr(\mu |\sigma^{2}=1, \boldsymbol{\mathbf{y}}).\]</span></p>
<div class="figure"><span style="display:block;" id="fig:Psurface-persp2"></span>
<img src="_bookdown_files/fig/Psurface-persp2-1.png" alt="The posterior distribution $Pr(\mu, \sigma^{2} | {\bf y})$, but only for values of $\sigma^{2}$ between 1 and 5, rather than 0 to 5 (Figure \@ref(fig:Psurface-persp). The edge of the surface facing left is the conditional distribution of the mean when $\sigma^{2}=1$ ($Pr(\mu | {\bf y}, \sigma^{2}=1)$). This conditional distribution follows a normal distribution." width="672" />
<p class="caption">
Figure 2.10: The posterior distribution <span class="math inline">\(Pr(\mu, \sigma^{2} | {\bf y})\)</span>, but only for values of <span class="math inline">\(\sigma^{2}\)</span> between 1 and 5, rather than 0 to 5 (Figure <a href="bayesian.html#fig:Psurface-persp">2.9</a>. The edge of the surface facing left is the conditional distribution of the mean when <span class="math inline">\(\sigma^{2}=1\)</span> (<span class="math inline">\(Pr(\mu | {\bf y}, \sigma^{2}=1)\)</span>). This conditional distribution follows a normal distribution.
</p>
</div>
<p>If we were using Metropolis-Hastings updates to sample <span class="math inline">\(\mu\)</span> we would need to evaluate this density, which may be much simpler than evaluating the full density. In fact for some cases, the equation that describes this conditional distribution can be derived despite the equation for the complete joint distribution of Figure <a href="bayesian.html#fig:Psurface-persp">2.9</a> remaining unknown. When the conditional distribution of <span class="math inline">\(\mu\)</span> is known we can use Gibbs sampling. Lets say the chain at a particular iteration is located at <span class="math inline">\(\sigma^{2}=1\)</span>. If we updated <span class="math inline">\(\mu\)</span> using a Metropolis-Hastings algorithm we would generate a candidate value and evaluate its relative probability compared to the old value. This procedure would take place in the slice of posterior facing left in Figure <a href="bayesian.html#fig:Psurface-persp2">2.10</a>. However, because we know the actual equation for this slice we can just generate a new value of <span class="math inline">\(\mu\)</span> directly. This is Gibbs sampling. The slice of the posterior that we can see in Figure <a href="bayesian.html#fig:Psurface-persp2">2.10</a> actually has a normal distribution. Because of the weak prior this normal distribution has a mean close to the mean of <span class="math inline">\(\bf{y}\)</span> and a variance close to <span class="math inline">\(\frac{\sigma^{2}}{n} = \frac{1}{n}\)</span>. Gibbs sampling can be much more efficient than Metropolis-Hastings updates, especially when high dimensional conditional distributions are known, as is typical in GLMMs. A technical description of the sampling schemes used by <span class="math inline">\(\texttt{MCMCglmm}\)</span> is given in the Chapter @ref(#technical), but is perhaps not important to know.</p>
</div>
<div id="diagnostics-sec" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> MCMC Diagnostics<a href="bayesian.html#diagnostics-sec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When fitting a model using <span class="math inline">\(\texttt{MCMCglmm}\)</span> the parameter values through which the Markov chain has travelled are stored and returned. The length of the chain (the number of iterations) can be specified using the <code>nitt</code> argument<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> (the default is 13,000), and should be long enough so that the posterior approximation is valid. If we had known the joint posterior distribution in Figure <a href="bayesian.html#fig:Psurface-persp">2.9</a> we could have sampled directly from the posterior. If this had been the case, each successive value in the Markov chain would be independent of the previous value after conditioning on the data, <span class="math inline">\({\bf y}\)</span>, and a thousand iterations of the chain would have produced a histogram that resembled Figure <a href="bayesian.html#fig:Psurface-persp">2.9</a> very closely. However, generally we do not know the joint posterior distribution of the parameters, and for this reason the parameter values of the Markov chain at successive iterations are usually not independent and care needs to be taken regarding the validity of the approximation. <span class="math inline">\(\texttt{MCMCglmm}\)</span> returns the Markov chain as <code>mcmc</code> objects, which can be analysed using the <code>coda</code> package. The function <code>autocorr</code> from the package <span class="math inline">\(\texttt{coda}\)</span> reports the level of non-independence between successive samples in the chain:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="bayesian.html#cb30-1" tabindex="-1"></a><span class="fu">autocorr</span>(m1a<span class="fl">.2</span><span class="sc">$</span>Sol)</span></code></pre></div>
<pre><code>## , , (Intercept)
## 
##         (Intercept)
## Lag 0  1.0000000000
## Lag 1  0.0295393869
## Lag 5  0.0069888310
## Lag 10 0.0200379700
## Lag 50 0.0009886787</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="bayesian.html#cb32-1" tabindex="-1"></a><span class="fu">autocorr</span>(m1a<span class="fl">.2</span><span class="sc">$</span>VCV)</span></code></pre></div>
<pre><code>## , , units
## 
##                units
## Lag 0   1.0000000000
## Lag 1   0.2473917848
## Lag 5   0.0082937460
## Lag 10 -0.0132076992
## Lag 50 -0.0007171886</code></pre>
<p>The correlation between successive samples is low for the mean (0.030) but a bit high for the variance (0.247). When auto-correlation is high we effectively have fewer samples from the posterior than we have saved. The function <code>effectiveSize</code>, also from <span class="math inline">\(\texttt{coda}\)</span>, reports the <em>effective</em> number of samples saved - if the autocorrelation could be made zero this would be the number of samples required to give the same precision on the posterior mean. For the variance, the effective sample size is 6248 quite a bit less than the number of stored posterior samples 10000. When the effective sample size is low the chain needs to be run for longer, and this can lead to storage problems for high dimensional problems. The argument <code>thin</code> can be passed to <span class="math inline">\(\texttt{MCMCglmm}\)</span> specifying the intervals at which the Markov chain is stored. In model <code>m1a.2</code> we specified <code>thin=1</code> meaning we stored every iteration (the default is <code>thin=10</code>). I usually aim to store 1,000-2,000 effective posterior samples with the autocorrelation between successive <em>stored</em> samples less than 0.1.</p>
<p>The approximation obtained from the Markov chain is conditional on the set of parameter values that were used to initialise the chain. In many cases the first iterations show a strong dependence on the starting parametrisation, but as the chain progresses this dependence may be lost. As the dependence on the starting parametrisation diminishes the chain is said to converge and the argument <code>burnin</code> can be passed to <code>MCMCped</code> specifying the number of iterations which must pass before samples are stored. The default burn-in period is 3,000 iterations. Assessing convergence of the chain is notoriously difficult, but visual inspection and diagnostic tools such as <code>gelman.diag</code> often suffice. For difficult models, running several chains from different starting values and ensuring they have all converged on the same distribution is a good idea.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="bayesian.html#cb34-1" tabindex="-1"></a><span class="fu">plot</span>(m1a<span class="fl">.2</span><span class="sc">$</span>Sol)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:time-series"></span>
<img src="_bookdown_files/fig/time-series-1.png" alt="Summary plot of the Markov Chain for the intercept.  The left plot is a trace of the sampled posterior, and can be thought of as a time-series.  The right plot is a density estimate, and can be thought of a smoothed histogram approximating the posterior." width="672" />
<p class="caption">
Figure 2.11: Summary plot of the Markov Chain for the intercept. The left plot is a trace of the sampled posterior, and can be thought of as a time-series. The right plot is a density estimate, and can be thought of a smoothed histogram approximating the posterior.
</p>
</div>
<p>On the left of Figure <a href="bayesian.html#fig:time-series">2.11</a> is a time-series of the parameter as the MCMC iterates, and on the right is a posterior density estimate of the parameter (a smoothed histogram of the output). If the model has converged there should be no trend in the time-series. The equivalent plot for the variance is a little hard to see on the original scale, but on the log scale the chain looks good (Figure <a href="bayesian.html#fig:time-series2">2.12</a>:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="bayesian.html#cb35-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">log</span>(m1a<span class="fl">.2</span><span class="sc">$</span>VCV))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:time-series2"></span>
<img src="_bookdown_files/fig/time-series2-1.png" alt="Summary plot of the Markov Chain for the logged variance. The logged variance was plotted rather than the variance because it was easier to visualise. The left plot is a trace of the sampled posterior, and can be thought of as a time-series.  The right plot is a density estimate, and can be thought of a smoothed histogram approximating the posterior." width="672" />
<p class="caption">
Figure 2.12: Summary plot of the Markov Chain for the logged variance. The logged variance was plotted rather than the variance because it was easier to visualise. The left plot is a trace of the sampled posterior, and can be thought of as a time-series. The right plot is a density estimate, and can be thought of a smoothed histogram approximating the posterior.
</p>
</div>
</div>
</div>
<div id="Vprior-sec" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Priors for Residual Variances<a href="bayesian.html#Vprior-sec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="math inline">\(\texttt{MCMCglmm}\)</span> uses an inverse-Wishart prior for the residual variance and so here will cover the properties of the scalar inverse-Wishart distribution and prior. Section <a href="cat-int.html#VCVprior-sec">5.3</a> can be consulted for inverse-Wishart covariance matrices. For random effect (co)variances it is possible to use scaled non-central <span class="math inline">\(F\)</span>-distribution priors which is strongly recommended (see Section <a href="ranef.html#PXprior-sec">4.5</a>).</p>
<p>For a single variance, the inverse-Wishart prior is parameterised through the parameters <span class="math inline">\(\texttt{V}\)</span> and <span class="math inline">\(\texttt{nu}\)</span> in MCMCglmm. The MCMCglmm parameterisation of the inverse-Wishart is not standard but I find it intuitive: the prior information is equivalent to observing <span class="math inline">\(\texttt{nu}\)</span> residuals with variance <span class="math inline">\(\texttt{V}\)</span>. Consequently, the distribution concentrates on <span class="math inline">\(\texttt{V}\)</span> as the degree of belief parameter, <span class="math inline">\(\texttt{nu}\)</span> increases. The distribution tends to be right skewed when <span class="math inline">\(\texttt{nu}\)</span> is not very large, with a mode of <span class="math inline">\(\texttt{V}\frac{\texttt{nu}}{\texttt{nu}+2}\)</span> but a mean of <span class="math inline">\(\texttt{V}\frac{\texttt{nu}}{\texttt{nu}-2}\)</span> (which is not defined for <span class="math inline">\(\texttt{nu}&lt;2\)</span>). Figure <a href="bayesian.html#fig:dinvgamma">2.13</a> plots the probability density functions holding <span class="math inline">\(\texttt{V}\)</span> equal to one but with <span class="math inline">\(\texttt{nu}\)</span> varying.</p>
<div class="figure"><span style="display:block;" id="fig:dinvgamma"></span>
<img src="_bookdown_files/fig/dinvgamma-1.png" alt="Probability density function for a univariate inverse-Wishart with the variance at the limit set to 1 ($\texttt{V}=1$) and varying degree of belief parameter ($\texttt{nu}$). With $\texttt{V}=1$ these distributions are equivalent to inverse gamma distributions with shape and scale parameters set to $\texttt{nu}$/2." width="672" />
<p class="caption">
Figure 2.13: Probability density function for a univariate inverse-Wishart with the variance at the limit set to 1 (<span class="math inline">\(\texttt{V}=1\)</span>) and varying degree of belief parameter (<span class="math inline">\(\texttt{nu}\)</span>). With <span class="math inline">\(\texttt{V}=1\)</span> these distributions are equivalent to inverse gamma distributions with shape and scale parameters set to <span class="math inline">\(\texttt{nu}\)</span>/2.
</p>
</div>
<p>For single variances the inverse-gamma is a special case of the inverse-Wishart<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, and with <span class="math inline">\(\texttt{V}=1\)</span>, the shape and scale of the inverse-gamma are both equal to <span class="math inline">\(\texttt{nu}/2\)</span>. The inverse-gamma with shape and scale equal to 0.001 used to be commonly used. The motivation behind the prior was that making <span class="math inline">\(\texttt{nu}\)</span> small would result in a less influential prior because is was equivalent to only observing 0.2% (<span class="math inline">\(\texttt{nu}=0.002\)</span>) of a residual <em>a priori</em>. Setting <span class="math inline">\(\texttt{nu}=0\)</span> was avoided because this does not define a valid distribution. A probability distribution must integrate to one because a variable must have some value, and this condition is not met when setting <span class="math inline">\(\texttt{nu}=0\)</span>. The prior distribution is said to be improper. In the example here, where <span class="math inline">\(\texttt{V}\)</span> is a single variance, the prior is only proper when <span class="math inline">\(\texttt{V}&gt;0\)</span> and <span class="math inline">\(\texttt{nu}&gt;0\)</span>. Although improper priors do not specify valid prior distributions and therefore the Bayesian credentials of any model may be questionable, <span class="math inline">\(\texttt{MCMCglmm}\)</span> does allow them as they have some useful properties.</p>
<div id="IP-sec" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Improper Priors<a href="bayesian.html#IP-sec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When improper priors are used their are two potential problems that may be encountered. The first is that if the data do not contain enough information, the posterior distribution itself may be improper, and any results obtained from <span class="math inline">\(\texttt{MCMCglmm}\)</span> will be meaningless. In addition, with proper priors there is a zero probability of a variance component being exactly zero but this is not necessarily the case with improper priors. This can produce numerical problems (trying to divide through by zero) and can also result in a reducible chain. A reducible chain is one which gets ‘stuck’ at some parameter value(s) and cannot escape. This is usually obvious from the <code>mcmc</code> plots but <span class="math inline">\(\texttt{MCMCglmm}\)</span> will often terminate before the analysis has finished with an error message of the form:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="bayesian.html#cb36-1" tabindex="-1"></a>ill<span class="sc">-</span>conditioned G<span class="sc">/</span>R structure<span class="sc">:</span> use proper priors ...</span></code></pre></div>
<p>However, improper priors do have some useful properties and in fact the default prior for the variances in has <span class="math inline">\(\texttt{nu}=0\)</span> (the value of <span class="math inline">\(\texttt{V}\)</span> is irrelevant). It is tempting to think that this prior is flat for the variance<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>, but it is in fact flat for the log-variance and therefore puts more weight on small values than a prior that is flat on the variance (Figure <a href="bayesian.html#fig:Psurface-nu0">2.14</a>) - see Section <a href="bayesian.html#transform-sec">2.7</a>.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="bayesian.html#cb37-1" tabindex="-1"></a>prior.m1a<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="dv">0</span>))</span>
<span id="cb37-2"><a href="bayesian.html#cb37-2" tabindex="-1"></a>m1a<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> Ndata, <span class="at">thin =</span> <span class="dv">1</span>, <span class="at">prior =</span> prior.m1a<span class="fl">.3</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:Psurface-nu0"></span>
<img src="_bookdown_files/fig/Psurface-nu0-1.png" alt="Likelihood surface for the likelihood $Pr({\bf y}|\mu, \sigma^{2})$ in black, and an MCMC approximation for the posterior distribution $Pr(\mu, \sigma^{2} | {\bf y})$ in red.  The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. An almost flat prior was used for the mean $Pr(\mu)\sim N(0, 10^8)$ and a flat prior was used for the log-variance $Pr(\sigma^{2})\sim IW(\texttt{V}=1, \texttt{nu}=0)$)." width="672" />
<p class="caption">
Figure 2.14: Likelihood surface for the likelihood <span class="math inline">\(Pr({\bf y}|\mu, \sigma^{2})\)</span> in black, and an MCMC approximation for the posterior distribution <span class="math inline">\(Pr(\mu, \sigma^{2} | {\bf y})\)</span> in red. The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. An almost flat prior was used for the mean <span class="math inline">\(Pr(\mu)\sim N(0, 10^8)\)</span> and a flat prior was used for the log-variance <span class="math inline">\(Pr(\sigma^{2})\sim IW(\texttt{V}=1, \texttt{nu}=0)\)</span>).
</p>
</div>
<p>Consequently, the mode of the marginal posterior distribution lies even below the ML estimate <a href="bayesian.html#fig:Pmarg-nu0">2.15</a>.</p>
<div class="figure"><span style="display:block;" id="fig:Pmarg-nu0"></span>
<img src="_bookdown_files/fig/Pmarg-nu0-1.png" alt="An MCMC approximation for the marginal posterior distribution of the variance $Pr(\sigma^{2} | {\bf y})$.  A non-informative prior specification was used ($Pr(\mu)\sim N(0, 10^8)$ and  $Pr(\sigma^{2})\sim IW(\texttt{V}=0, \texttt{nu}=0)$). The ML and REML estimates are plotted in blue and red,m respectively." width="672" />
<p class="caption">
Figure 2.15: An MCMC approximation for the marginal posterior distribution of the variance <span class="math inline">\(Pr(\sigma^{2} | {\bf y})\)</span>. A non-informative prior specification was used (<span class="math inline">\(Pr(\mu)\sim N(0, 10^8)\)</span> and <span class="math inline">\(Pr(\sigma^{2})\sim IW(\texttt{V}=0, \texttt{nu}=0)\)</span>). The ML and REML estimates are plotted in blue and red,m respectively.
</p>
</div>
<p>Although inverse-Wishart distributions with negative degree of belief parameters are not defined, the resulting posterior distribution can be defined and proper if there is sufficient replication. When <span class="math inline">\(\texttt{V}=0\)</span> and <span class="math inline">\(\texttt{nu}=-1\)</span> we have a flat prior on the standard deviation over the interval <span class="math inline">\((0,\infty]\)</span>. When <span class="math inline">\(\texttt{V}=0\)</span> and <span class="math inline">\(\texttt{nu}=-2\)</span> we have a flat prior on the variance. Since the default prior for the mean is normal with a very large variance (<span class="math inline">\(10^8\)</span>) the prior for the mean is also essentially flat, resulting in a prior probability that is proportional to some constant for all possible parameter values. The posterior density in such cases is equal to the likelihood:</p>
<p><span class="math display" id="eq:fprior-eq">\[Pr(\mu, \sigma^{2} | {\bf y}) \propto Pr({\bf y} | \mu, \sigma^{2})
\label{fprior-eq}   \tag{2.3}\]</span></p>
<p>We can overlay the joint posterior distribution on the likelihood surface (Figure <a href="bayesian.html#fig:Psurface-flat">2.16</a>) and see that the two things are in close agreement, up to Monte Carlo error.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="bayesian.html#cb38-1" tabindex="-1"></a>prior.m1a<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fl">1e-16</span>, <span class="at">nu =</span> <span class="sc">-</span><span class="dv">2</span>))</span>
<span id="cb38-2"><a href="bayesian.html#cb38-2" tabindex="-1"></a>m1a<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> Ndata, <span class="at">thin =</span> <span class="dv">1</span>, <span class="at">prior =</span> prior.m1a<span class="fl">.4</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:Psurface-flat"></span>
<img src="_bookdown_files/fig/Psurface-flat-1.png" alt="Likelihood surface for the likelihood $Pr({\bf y}|\mu, \sigma^{2})$ in black, and an MCMC approximation for the posterior distribution $Pr(\mu, \sigma^{2} | {\bf y})$ in red.  The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. Almost flat priors were used for the mean ($Pr(\mu)\sim N(0, 10^8)$ and the variance $Pr(\sigma^{2})\sim IW(\texttt{V}=10^{-16}, \texttt{nu}=-2)$) and so the posterior distribution is equivalent to the likelihood." width="672" />
<p class="caption">
Figure 2.16: Likelihood surface for the likelihood <span class="math inline">\(Pr({\bf y}|\mu, \sigma^{2})\)</span> in black, and an MCMC approximation for the posterior distribution <span class="math inline">\(Pr(\mu, \sigma^{2} | {\bf y})\)</span> in red. The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. Almost flat priors were used for the mean (<span class="math inline">\(Pr(\mu)\sim N(0, 10^8)\)</span> and the variance <span class="math inline">\(Pr(\sigma^{2})\sim IW(\texttt{V}=10^{-16}, \texttt{nu}=-2)\)</span>) and so the posterior distribution is equivalent to the likelihood.
</p>
</div>
<p>Here, the joint posterior mode coincides with the ML estimates, as expected (Figure <a href="bayesian.html#fig:Psurface-flat">2.16</a>). In addition, the mode of the marginal distribution for the variance is equivalent to the REML estimator (See Figure <a href="bayesian.html#fig:Pmarg-flat">2.17</a>). Indeed, the REML estimator can be seen as marginalising the mean under a flat prior, and for this reason is sometimes referred to as the marginal likelihood rather than the restricted likelihood (REML).</p>
<div class="figure"><span style="display:block;" id="fig:Pmarg-flat"></span>
<img src="_bookdown_files/fig/Pmarg-flat-1.png" alt="An MCMC approximation for the marginal posterior distribution of the variance $Pr(\sigma^{2} | {\bf y})$.  An almost flat prior specification was used ($Pr(\mu)\sim N(0, 10^8)$ and  $Pr(\sigma^{2})\sim IW(\texttt{V}=10^{-16}, \texttt{nu}=-2)$) and the REML estimator of the variance (red line) coincides with the marginal posterior mode." width="672" />
<p class="caption">
Figure 2.17: An MCMC approximation for the marginal posterior distribution of the variance <span class="math inline">\(Pr(\sigma^{2} | {\bf y})\)</span>. An almost flat prior specification was used (<span class="math inline">\(Pr(\mu)\sim N(0, 10^8)\)</span> and <span class="math inline">\(Pr(\sigma^{2})\sim IW(\texttt{V}=10^{-16}, \texttt{nu}=-2)\)</span>) and the REML estimator of the variance (red line) coincides with the marginal posterior mode.
</p>
</div>
</div>
</div>
<div id="transform-sec" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Transformations<a href="bayesian.html#transform-sec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sometimes we would like to know the posterior distribution for some transformation of the parameters. For example, we may wish to obtain the posterior distribution for <span class="math inline">\(\log(\sigma^2)\)</span>. With MCMC this is easy - we can just transform the posterior samples (i.e. <code>log(m1a.2$VCV)</code>) and treat these as we would any other set of posterior samples. However, it is useful to understand how probabilities are expected to behave under these transformations. The key idea is that the probability of some function of <span class="math inline">\(x\)</span>, <span class="math inline">\(f(x)\)</span>, is equal to the probability of <span class="math inline">\(x\)</span> divided by the Jacobian. When the transform only involves single parameters the Jacobian is <span class="math inline">\(|df(x)/dx|\)</span> which for the log-transform is <span class="math inline">\(1/x\)</span>. For model <code>m1a.3</code> we used an inverse-Wishart prior with <span class="math inline">\(\texttt{V=1}\)</span> and <span class="math inline">\(\texttt{nu=0}\)</span>. The posterior density is plotted in Figure <a href="bayesian.html#fig:Psurface-nu0">2.14</a> and does not appear to coincide with the likelihood - there is more posterior density at small values of <span class="math inline">\(\sigma^2\)</span></p>
<p>However, this is because the posterior density is expressed for <span class="math inline">\(\sigma^2\)</span> and the prior is only flat in <span class="math inline">\(log(\sigma^2)\)</span>. By multiplying the posterior density for <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(\sigma^2\)</span> (i.e. dividing by <span class="math inline">\(1/x\)</span> where <span class="math inline">\(x\)</span> is <span class="math inline">\(\sigma^2\)</span>) we would obtain the joint density of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(log(\sigma^2)\)</span>. This would reduce the density for small values of <span class="math inline">\(\sigma^2\)</span> and increase the density for large values (since the density is scaled by <span class="math inline">\(\sigma^2\)</span>). Indeed, plotting the posterior distribution of <span class="math inline">\(log(\sigma^2)\)</span> shows this effect and the posterior is in agreement with the likelihood, as expected (Figure <a href="bayesian.html#fig:Psurface2-nu0">2.18</a>).</p>
<div class="figure"><span style="display:block;" id="fig:Psurface2-nu0"></span>
<img src="_bookdown_files/fig/Psurface2-nu0-1.png" alt="Likelihood surface for the likelihood $Pr({\bf y}|\mu, \log(\sigma^{2}))$ in black, and an MCMC approximation for the posterior distribution $Pr(\mu, \log(\sigma^{2}) | {\bf y})$ in red.  An almost flat prior was used for the mean $Pr(\mu)\sim N(0, 10^8)$ and a flat prior was used for the log-variance $Pr(\sigma^{2})\sim IW(\texttt{V}=1, \texttt{nu}=0)$." width="672" />
<p class="caption">
Figure 2.18: Likelihood surface for the likelihood <span class="math inline">\(Pr({\bf y}|\mu, \log(\sigma^{2}))\)</span> in black, and an MCMC approximation for the posterior distribution <span class="math inline">\(Pr(\mu, \log(\sigma^{2}) | {\bf y})\)</span> in red. An almost flat prior was used for the mean <span class="math inline">\(Pr(\mu)\sim N(0, 10^8)\)</span> and a flat prior was used for the log-variance <span class="math inline">\(Pr(\sigma^{2})\sim IW(\texttt{V}=1, \texttt{nu}=0)\)</span>.
</p>
</div>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The double <code>t</code> is because I cannot spell.<a href="bayesian.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>The inverse gamma is a special case of the inverse-Wishart, although it is parametrised using <span class="math inline">\(\texttt{shape}\)</span> and <span class="math inline">\(\texttt{scale}\)</span>, where <span class="math inline">\(\texttt{nu}=2\ast\texttt{shape}\)</span> and <span class="math inline">\(\texttt{V} = \frac{\texttt{scale}}{\texttt{shape}}\)</span> (or <span class="math inline">\(\texttt{shape} = \frac{\texttt{nu}}{2}\)</span> and <span class="math inline">\(\texttt{scale} = \texttt{V}\frac{\texttt{nu}}{2}\)</span>). There is no density function for the inverse-gamma in base R. However, the prior specification can be passed to the function <span class="math inline">\(\texttt{dprior}\)</span> to obtain the density: for example, <code>dprior(1, prior=list(V=1, nu=0.002))</code>.<a href="bayesian.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Embarrassingly, I claimed that <span class="math inline">\(\texttt{V=1}\)</span> and <span class="math inline">\(\texttt{nu=0}\)</span> was flat in the original CourseNotes and I had forgotten to rescale the posterior density by the Jacobian when generating the equivalent contour plot to Figure <a href="bayesian.html#fig:Psurface-nu0">2.14</a> (which is computed on values of <span class="math inline">\(\textrm{log}(\sigma^2)\)</span> for better results, and then transformed).<a href="bayesian.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="overview.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="glm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["MCMCglmm-course-notes.pdf", "MCMCglmm-course-notes.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
