<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Random effects | MCMCglmm Course Notes</title>
  <meta name="description" content="Extended documentation and course notes for the MCMCglmm R package." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Random effects | MCMCglmm Course Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Random effects | MCMCglmm Course Notes" />
  
  <meta name="twitter:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

<meta name="author" content="Jarrod Hadfield" />


<meta name="date" content="2026-01-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="glm.html"/>
<link rel="next" href="cat-int.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script>
<link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-1.3.31/rglClass.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/utils.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/buffer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/subscenes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shaders.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shadersrc.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/textures.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/projection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/mouse.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/init.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pieces.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/draw.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/controls.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/selection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/rglTimer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pretty.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/axes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/animation.src.js"></script>
<script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.src.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#outline"><i class="fa fa-check"></i><b>1.1</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian Analysis and MCMC</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian.html"><a href="bayesian.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>2.2</b> Likelihood</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-ml"><i class="fa fa-check"></i><b>2.2.1</b> Maximum Likelihood (ML)</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian.html"><a href="bayesian.html#restricted-maximum-likelihood-reml"><i class="fa fa-check"></i><b>2.2.2</b> Restricted Maximum Likelihood (REML)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian.html"><a href="bayesian.html#prior-distribution"><i class="fa fa-check"></i><b>2.3</b> Prior Distribution</a></li>
<li class="chapter" data-level="2.4" data-path="bayesian.html"><a href="bayesian.html#posterior-distribution"><i class="fa fa-check"></i><b>2.4</b> Posterior Distribution</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian.html"><a href="bayesian.html#marginal-posterior-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Marginal Posterior Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="bayesian.html"><a href="bayesian.html#intervals-sec"><i class="fa fa-check"></i><b>2.4.2</b> Credible Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian.html"><a href="bayesian.html#MCMC"><i class="fa fa-check"></i><b>2.5</b> MCMC</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian.html"><a href="bayesian.html#starting-values"><i class="fa fa-check"></i><b>2.5.1</b> Starting values</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian.html"><a href="bayesian.html#metropolis-hastings-updates"><i class="fa fa-check"></i><b>2.5.2</b> Metropolis-Hastings updates</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian.html"><a href="bayesian.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.5.3</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian.html"><a href="bayesian.html#diagnostics-sec"><i class="fa fa-check"></i><b>2.5.4</b> MCMC Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian.html"><a href="bayesian.html#Vprior-sec"><i class="fa fa-check"></i><b>2.6</b> Priors for Residual Variances</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian.html"><a href="bayesian.html#IP-sec"><i class="fa fa-check"></i><b>2.6.1</b> Improper Priors</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian.html"><a href="bayesian.html#transform-sec"><i class="fa fa-check"></i><b>2.7</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>3</b> Linear and Generalised Linear Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="glm.html"><a href="glm.html#linear-model-lm"><i class="fa fa-check"></i><b>3.1</b> Linear Model (LM)</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="glm.html"><a href="glm.html#lm-sec"><i class="fa fa-check"></i><b>3.1.1</b> Linear Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="glm.html"><a href="glm.html#generalised-linear-model-glm"><i class="fa fa-check"></i><b>3.2</b> Generalised Linear Model (GLM)</a></li>
<li class="chapter" data-level="3.3" data-path="glm.html"><a href="glm.html#poisson-glm"><i class="fa fa-check"></i><b>3.3</b> Poisson GLM</a></li>
<li class="chapter" data-level="3.4" data-path="glm.html"><a href="glm.html#overdispersion"><i class="fa fa-check"></i><b>3.4</b> Overdispersion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="glm.html"><a href="glm.html#multiplicative-overdispersion"><i class="fa fa-check"></i><b>3.4.1</b> Multiplicative Overdispersion</a></li>
<li class="chapter" data-level="3.4.2" data-path="glm.html"><a href="glm.html#addod-sec"><i class="fa fa-check"></i><b>3.4.2</b> Additive Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="glm.html"><a href="glm.html#prediction-in-glm"><i class="fa fa-check"></i><b>3.5</b> Prediction in GLM</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="glm.html"><a href="glm.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="glm.html"><a href="glm.html#binom-sec"><i class="fa fa-check"></i><b>3.6</b> Binomial and Bernoulli GLM</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="glm.html"><a href="glm.html#overdispersion-1"><i class="fa fa-check"></i><b>3.6.1</b> Overdispersion</a></li>
<li class="chapter" data-level="3.6.2" data-path="glm.html"><a href="glm.html#prediction"><i class="fa fa-check"></i><b>3.6.2</b> Prediction</a></li>
<li class="chapter" data-level="3.6.3" data-path="glm.html"><a href="glm.html#bernoulli-sec"><i class="fa fa-check"></i><b>3.6.3</b> Bernoulli GLM</a></li>
<li class="chapter" data-level="3.6.4" data-path="glm.html"><a href="glm.html#probit-link"><i class="fa fa-check"></i><b>3.6.4</b> Probit link</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="glm.html"><a href="glm.html#ordinal-data"><i class="fa fa-check"></i><b>3.7</b> Ordinal Data</a></li>
<li class="chapter" data-level="3.8" data-path="glm.html"><a href="glm.html#non-zero-binomial-data"><i class="fa fa-check"></i><b>3.8</b> Non-zero Binomial Data</a></li>
<li class="chapter" data-level="3.9" data-path="glm.html"><a href="glm.html#complete-separation"><i class="fa fa-check"></i><b>3.9</b> Complete Separation</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="glm.html"><a href="glm.html#gelman-prior-sec"><i class="fa fa-check"></i><b>3.9.1</b> The <span class="citation">Gelman, Jakulin, et al. (2008)</span> prior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ranef.html"><a href="ranef.html"><i class="fa fa-check"></i><b>4</b> Random effects</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ranef.html"><a href="ranef.html#GLMM"><i class="fa fa-check"></i><b>4.1</b> Generalised Linear Mixed Model (GLMM)</a></li>
<li class="chapter" data-level="4.2" data-path="ranef.html"><a href="ranef.html#ranpred-sec"><i class="fa fa-check"></i><b>4.2</b> Prediction with Random Effects</a></li>
<li class="chapter" data-level="4.3" data-path="ranef.html"><a href="ranef.html#overdispersed-binomial-as-a-bernoulli-glmm"><i class="fa fa-check"></i><b>4.3</b> Overdispersed Binomial as a Bernoulli GLMM</a></li>
<li class="chapter" data-level="4.4" data-path="ranef.html"><a href="ranef.html#intra-class-correlations"><i class="fa fa-check"></i><b>4.4</b> Intra-class Correlations</a></li>
<li class="chapter" data-level="4.5" data-path="ranef.html"><a href="ranef.html#PXprior-sec"><i class="fa fa-check"></i><b>4.5</b> Priors for Random Effect Variances</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="ranef.html"><a href="ranef.html#f-and-half-t-priors"><i class="fa fa-check"></i><b>4.5.1</b> <span class="math inline">\(F\)</span> and half-<span class="math inline">\(t\)</span> priors</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="ranef.html"><a href="ranef.html#fixed-or-random"><i class="fa fa-check"></i><b>4.6</b> Fixed or Random?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cat-int.html"><a href="cat-int.html"><i class="fa fa-check"></i><b>5</b> Categorical Random Interactions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="cat-int.html"><a href="cat-int.html#idh-variance-structure"><i class="fa fa-check"></i><b>5.1</b> <code>idh</code> Variance Structure</a></li>
<li class="chapter" data-level="5.2" data-path="cat-int.html"><a href="cat-int.html#us-variance-structure"><i class="fa fa-check"></i><b>5.2</b> <code>us</code> Variance Structure</a></li>
<li class="chapter" data-level="5.3" data-path="cat-int.html"><a href="cat-int.html#compound-variance-structures"><i class="fa fa-check"></i><b>5.3</b> Compound Variance Structures</a></li>
<li class="chapter" data-level="5.4" data-path="cat-int.html"><a href="cat-int.html#heter-sec"><i class="fa fa-check"></i><b>5.4</b> Heterogenous Residual Variance</a></li>
<li class="chapter" data-level="5.5" data-path="cat-int.html"><a href="cat-int.html#contrasts-and-covariances"><i class="fa fa-check"></i><b>5.5</b> Contrasts and Covariances</a></li>
<li class="chapter" data-level="5.6" data-path="cat-int.html"><a href="cat-int.html#VCVprior-sec"><i class="fa fa-check"></i><b>5.6</b> Priors for Covariance Matrices</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="cat-int.html"><a href="cat-int.html#priors-for-us-structures"><i class="fa fa-check"></i><b>5.6.1</b> Priors for <code>us</code> structures</a></li>
<li class="chapter" data-level="5.6.2" data-path="cat-int.html"><a href="cat-int.html#priors-for-idh-structures"><i class="fa fa-check"></i><b>5.6.2</b> Priors for <code>idh</code> structures</a></li>
<li class="chapter" data-level="5.6.3" data-path="cat-int.html"><a href="cat-int.html#priors-for-corg-and-corgh-structures"><i class="fa fa-check"></i><b>5.6.3</b> Priors for <code>corg</code> and <code>corgh</code> structures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cont-int.html"><a href="cont-int.html"><i class="fa fa-check"></i><b>6</b> Continuous Random Interactions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="cont-int.html"><a href="cont-int.html#random-regression"><i class="fa fa-check"></i><b>6.1</b> Random Regression</a></li>
<li class="chapter" data-level="6.2" data-path="cont-int.html"><a href="cont-int.html#expected-variances-and-covariances"><i class="fa fa-check"></i><b>6.2</b> Expected Variances and Covariances</a></li>
<li class="chapter" data-level="6.3" data-path="cont-int.html"><a href="cont-int.html#RRcentering"><i class="fa fa-check"></i><b>6.3</b> <code>us</code> versus <code>idh</code> and mean centering</a></li>
<li class="chapter" data-level="6.4" data-path="cont-int.html"><a href="cont-int.html#meta-sec"><i class="fa fa-check"></i><b>6.4</b> Meta-analysis</a></li>
<li class="chapter" data-level="6.5" data-path="cont-int.html"><a href="cont-int.html#splines"><i class="fa fa-check"></i><b>6.5</b> Splines</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multi-response models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="multi.html"><a href="multi.html#relaxing-the-univariate-assumptions-of-causality"><i class="fa fa-check"></i><b>7.1</b> Relaxing the univariate assumptions of causality</a></li>
<li class="chapter" data-level="7.2" data-path="multi.html"><a href="multi.html#multinomial-models"><i class="fa fa-check"></i><b>7.2</b> Multinomial Models</a></li>
<li class="chapter" data-level="7.3" data-path="multi.html"><a href="multi.html#zero-inflated-models"><i class="fa fa-check"></i><b>7.3</b> Zero-inflated Models</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="multi.html"><a href="multi.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="multi.html"><a href="multi.html#Hurdle"><i class="fa fa-check"></i><b>7.4</b> Hurdle Models</a></li>
<li class="chapter" data-level="7.5" data-path="multi.html"><a href="multi.html#ZAP"><i class="fa fa-check"></i><b>7.5</b> Zero-altered Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pedigree.html"><a href="pedigree.html"><i class="fa fa-check"></i><b>8</b> Pedigrees and Phylogenies</a>
<ul>
<li class="chapter" data-level="8.1" data-path="pedigree.html"><a href="pedigree.html#pedigree-and-phylogeny-formats"><i class="fa fa-check"></i><b>8.1</b> Pedigree and phylogeny formats</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="pedigree.html"><a href="pedigree.html#pedigrees"><i class="fa fa-check"></i><b>8.1.1</b> Pedigrees</a></li>
<li class="chapter" data-level="8.1.2" data-path="pedigree.html"><a href="pedigree.html#phylogenies"><i class="fa fa-check"></i><b>8.1.2</b> Phylogenies</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="pedigree.html"><a href="pedigree.html#the-animal-model-and-the-phylogenetic-mixed-model"><i class="fa fa-check"></i><b>8.2</b> The animal model and the phylogenetic mixed model</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="parameter-expansion.html"><a href="parameter-expansion.html"><i class="fa fa-check"></i><b>9</b> Parameter Expansion</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="parameter-expansion.html"><a href="parameter-expansion.html#variances-close-to-zero"><i class="fa fa-check"></i><b>9.0.1</b> Variances close to zero</a></li>
<li class="chapter" data-level="9.0.2" data-path="parameter-expansion.html"><a href="parameter-expansion.html#secPX-p"><i class="fa fa-check"></i><b>9.0.2</b> Parameter expanded priors</a></li>
<li class="chapter" data-level="9.0.3" data-path="parameter-expansion.html"><a href="parameter-expansion.html#binary-response-models"><i class="fa fa-check"></i><b>9.0.3</b> Binary response models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="path.html"><a href="path.html"><i class="fa fa-check"></i><b>10</b> Path Analysis &amp; Antedependence Structures</a>
<ul>
<li class="chapter" data-level="10.1" data-path="path.html"><a href="path.html#path-anlaysis"><i class="fa fa-check"></i><b>10.1</b> Path Anlaysis</a></li>
<li class="chapter" data-level="10.2" data-path="path.html"><a href="path.html#ante-sec"><i class="fa fa-check"></i><b>10.2</b> Antedependence</a></li>
<li class="chapter" data-level="10.3" data-path="path.html"><a href="path.html#scaling"><i class="fa fa-check"></i><b>10.3</b> Scaling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="technical-details.html"><a href="technical-details.html"><i class="fa fa-check"></i><b>11</b> Technical Details</a>
<ul>
<li class="chapter" data-level="11.1" data-path="technical-details.html"><a href="technical-details.html#model-form"><i class="fa fa-check"></i><b>11.1</b> Model Form</a></li>
<li class="chapter" data-level="11.2" data-path="technical-details.html"><a href="technical-details.html#MCMC-app"><i class="fa fa-check"></i><b>11.2</b> MCMC Sampling Schemes</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="technical-details.html"><a href="technical-details.html#updating-the-latent-variables-bf-l"><i class="fa fa-check"></i><b>11.2.1</b> Updating the latent variables <span class="math inline">\({\bf l}\)</span></a></li>
<li class="chapter" data-level="11.2.2" data-path="technical-details.html"><a href="technical-details.html#updating-the-location-vector-boldsymboltheta-leftboldsymbolmathbfbeta-bf-uright"><i class="fa fa-check"></i><b>11.2.2</b> Updating the location vector <span class="math inline">\(\boldsymbol{\theta} = \left[{\boldsymbol{\mathbf{\beta}}}^{&#39;}\; {\bf u}^{&#39;}\right]^{&#39;}\)</span></a></li>
<li class="chapter" data-level="11.2.3" data-path="technical-details.html"><a href="technical-details.html#updating-the-variance-structures-bf-g-and-bf-r"><i class="fa fa-check"></i><b>11.2.3</b> Updating the variance structures <span class="math inline">\({\bf G}\)</span> and <span class="math inline">\({\bf R}\)</span></a></li>
<li class="chapter" data-level="11.2.4" data-path="technical-details.html"><a href="technical-details.html#ordinal-models"><i class="fa fa-check"></i><b>11.2.4</b> Ordinal Models</a></li>
<li class="chapter" data-level="11.2.5" data-path="technical-details.html"><a href="technical-details.html#path-analyses"><i class="fa fa-check"></i><b>11.2.5</b> Path Analyses</a></li>
<li class="chapter" data-level="11.2.6" data-path="technical-details.html"><a href="technical-details.html#deviance-and-dic"><i class="fa fa-check"></i><b>11.2.6</b> Deviance and DIC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MCMCglmm Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ranef" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Random effects<a href="ranef.html#ranef" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In some cases we may have measured variables whose effects we would like to treat as random. Often the distinction between fixed and random is given by example: things like city, species, individual and vial are random, but sex, treatment and age are not. Or the distinction is made using rules of thumb: if there are few factor levels and they are interesting to other people they are fixed. However, this doesn’t really confer any understanding about what it means to treat something as fixed or random, and doesn’t really allow judgements to be made for variables in which the rules of thumb seem to contradict each other. Similarly, these ‘explanations’ don’t give any insight into the fact that all effects are technically random in a Bayesian analysis.</p>
<p>Random effect models are often expressed as an extension of Equation <a href="glm.html#eq:lm">(3.2)</a>:</p>
<p><span class="math display" id="eq:MM">\[E[{\bf y}] = {\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf Z}{\bf u}
\label{MM}   \tag{4.1}\]</span></p>
<p>where <span class="math inline">\({\bf Z}\)</span> is a design matrix like <span class="math inline">\({\bf X}\)</span>, and <span class="math inline">\({\bf u}\)</span> is a vector of parameters like <span class="math inline">\({\boldsymbol{\mathbf{\beta}}}\)</span>. However, at this stage there is simply no distinction between fixed and random effects. We could combine the design matrices (<span class="math inline">\({\bf W} = [{\bf X}, {\bf Z}]\)</span>) and combine the vectors of parameters (<span class="math inline">\(\boldsymbol{\theta} = [{\boldsymbol{\mathbf{\beta}}}^{&#39;}, {\bf u}^{&#39;}]^{&#39;}\)</span>) to get:</p>
<p><span class="math display" id="eq:MM2">\[E[{\bf y}] = {\bf W}\boldsymbol{\theta}
\label{MM2}   \tag{4.2}\]</span></p>
<p>which is <strong>identical</strong> to Equation <a href="ranef.html#eq:MM">(4.1)</a>. So if we don’t need to distinguish between fixed and random effects at this stage, when should we distinguish between them, and what distinguishes them?</p>
<p>When we treat an effect as random we believe that the coefficients have some distribution around a mean of zero; often we assume they are normal<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> and that they are independent (represented by an identity matrix) and identically distributed with variance <span class="math inline">\(\sigma^{2}_{u}\)</span>:</p>
<p><span class="math display">\[{\bf u} \sim N({\bf 0}, {\bf I}\sigma^{2}_{u})\]</span></p>
<p><span class="math inline">\(\sigma^{2}_{u}\)</span> is a parameter of the model which we estimate, in addition to <span class="math inline">\({\bf u}\)</span>. In a Bayesian analysis we would also assign <span class="math inline">\(\sigma^{2}_{u}\)</span> a prior, and <span class="math inline">\(\sigma^{2}_{u}\)</span> is often called a hyper-parameter with an associated hyper-prior.</p>
<p>Fixed effects in a frequentist analysis are not assigned a distribution, but we can understand this in terms of the limit to the normal distribution</p>
<p><span class="math display">\[\boldsymbol{\beta} \sim N({\bf 0}, {\bf I}\sigma^{2}_{\beta})\]</span></p>
<p>as <span class="math inline">\(\sigma^{2}_{\beta}\)</span> tends to infinity. In a Bayesian setting we would call this a flat improper prior. In practice, we often use diffuse proper priors in Bayesian analyses. For example, the default in <span class="math inline">\(\texttt{MCMCglmm}\)</span> is to set <span class="math inline">\(\sigma^{2}_{\beta}=10^8\)</span>. Then, <span class="math inline">\(\boldsymbol{\beta}\)</span> are technically random - they are assigned a distribution - but I find it useful to retain the frequentist terminology ‘fixed’. The only difference then is that the ‘fixed’ effects are assigned a prior distribution with a variance that is defined by the user-specified prior (<span class="math inline">\(\sigma^{2}_{\beta}\)</span> - which is often set to be large) and the ‘random’ effects are assigned a prior distribution with a variance that is estimated (<span class="math inline">\(\sigma^{2}_{u}\)</span> - which could be large, but also zero).</p>
<p>That is the distinction between fixed and random effects. The difference really is that simple, but it takes a long time and a lot of practice to understand what this means in practical terms, and why working with random effects can be a very powerful way of modelling data. To get a feel for why we might want to fit an effect as random or not, lets work through an example before moving on to model fitting. In Section <a href="glm.html#binom-sec">3.6</a> we analysed binomial data where 122 respondents had looked at 44 photographs of people and given them a ‘grumpy score’ of more than five (a success) or less than five (a failure). If, instead of 122 respondents, there had been a zillion respondents, we could use the average proportion of success for each photo as a nearly perfect estimates of their probabilities of success. The variance of these near-perfect estimates could serve as a reasonable estimate of the variance in photo effects. If the probabilities were all clustered tightly around 0.5: 0.505, 0.501, 0.499 and so on, then variance would be estimated to be small. Let’s then imagine that we obtained a <span class="math inline">\(45^\textrm{th}\)</span> photograph but by this point the respondents were so bored I managed to only recruit a single person who gave the photo a score greater than five - a success. Since we only have one observation for this photo the average proportion of success would be one. Do you think the best estimate of the probability of success for the <span class="math inline">\(45^\textrm{th}\)</span> photograph is then 1.000? I think you wouldn’t: you would use the knowledge that you have gained from the other photos and say that it is more likely that if you had managed to recruit more respondents you would have got a roughly even split of success and failures. You have used common sense, treated the photo effects as random, and <em>shrunk</em> photo 45’s effect towards the average because the variance (<span class="math inline">\(\sigma^2_u\)</span>) was small and we have a strong prior. If we had treated the photo effects as fixed, we believe that the only information regarding a photo’s value comes from data associated with that particular photo, and the estimate of photo 45’s probability would have been one. When we treat an effect as random, we also use the information that comes from data associated with that particular photo (obviously), but we weight that information by what the data associated with other photos tell us about the likely values that the effect could take - through the parameter <span class="math inline">\(\sigma^2_u\)</span>. What if the probabilities weren’t all clustered tightly around 0.5, but took on values 0.500, 0.998, 0.002, 0.327 …? The variance <span class="math inline">\(\sigma^2_u\)</span> would be larger and the prior information for our <span class="math inline">\(45^\textrm{th}\)</span> photo would be weaker: perhaps we got a success because the underlying probability was 0.998, but a single success would also not be very surprising if the underlying probability was 0.500, or even 0.327. We might then be happy that our best estimate of the probability of success for the <span class="math inline">\(45^\textrm{th}\)</span> photograph was close to one, although with such weak prior information (large <span class="math inline">\(\sigma^2_u\)</span>) the uncertainty would remain large.</p>
<p>When the motivation for treating an effect as random is explained this way, it is hard to come up with a reason why you wouldn’t treat all effects as random. However, you have to consider how much information is in a given data set to estimate <span class="math inline">\(\sigma^2_u\)</span>, which we will cover in Section <a href="#fixed-or-random-sec"><strong>??</strong></a></p>
<div id="GLMM" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Generalised Linear Mixed Model (GLMM)<a href="ranef.html#GLMM" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Section <a href="glm.html#binom-sec">3.6</a>, the binomial model we fitted only contained fixed effects, as specified in the <code>fixed</code> argument to <code>MCMCglmm</code> (<code>fixed=cbind(g5,l5)~type+ypub</code>). No random effects were fitted, although ‘residuals’ were fitted as default to absorb any overdispersion. Residuals are random effects for which we estimate a variance - the hyperparameter, <span class="math inline">\(\sigma^2_e\)</span> - and when used with Binomial or Poisson responses are commonly referred to as observation-level random effects. Since there is a one-to-one correspondence between observation and photo in this data set, <span class="math inline">\(\sigma^2_e\)</span> is equivalent to the <span class="math inline">\(\sigma^2_u\)</span> discussed above (although <span class="math inline">\(\sigma^2_e\)</span> refers to the variance on the logit scale rather the probability scale used implicitly above). We saw that the probability of success varied greatly across photos (model <code>mbinom.1</code>) but we also noted that some of this variation may be due to the person being photographed and we could tease apart the effect of person from the specifics of the photo since each person was photographed twice - once when happy and once when grumpy. <span class="math inline">\(\texttt{person}\)</span> has 22 levels and you are probably not interested in knowing the grumpy score of someone you didn’t know - <span class="math inline">\(\texttt{person}\)</span> effects seem to satisfy the rule of thumb often used to decide that they should be treated as random. The random effect model is specified through the argument <code>random</code> and for simple effects as these we simply put the name of the corresponding column (<span class="math inline">\(\texttt{person}\)</span>) in the model formula. We will also specify inverse-Wishart priors for both the residual variance and the variance of the <span class="math inline">\(\texttt{person}\)</span> effects (see Section <a href="bayesian.html#Vprior-sec">2.6</a>) although scaled non-central <span class="math inline">\(F\)</span>-distribution priors are recommended for random-effect variances (see Section <a href="ranef.html#PXprior-sec">4.5</a>):</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="ranef.html#cb126-1" tabindex="-1"></a>prior.mbinom<span class="fl">.2</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>)))</span>
<span id="cb126-2"><a href="ranef.html#cb126-2" tabindex="-1"></a></span>
<span id="cb126-3"><a href="ranef.html#cb126-3" tabindex="-1"></a>mbinom<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(<span class="fu">cbind</span>(g5, l5) <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">random =</span> <span class="sc">~</span>person, <span class="at">data =</span> Grumpy,</span>
<span id="cb126-4"><a href="ranef.html#cb126-4" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;multinomial2&quot;</span>, <span class="at">pr =</span> <span class="cn">TRUE</span>, <span class="at">prior =</span> prior.mbinom<span class="fl">.2</span>)</span>
<span id="cb126-5"><a href="ranef.html#cb126-5" tabindex="-1"></a><span class="fu">summary</span>(mbinom<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## 
##  Iterations = 3001:12991
##  Thinning interval  = 10
##  Sample size  = 1000 
## 
##  DIC: 5376.535 
## 
##  G-structure:  ~person
## 
##        post.mean l-95% CI u-95% CI eff.samp
## person    0.8571 0.008145    1.644    744.3
## 
##  R-structure:  ~units
## 
##       post.mean l-95% CI u-95% CI eff.samp
## units    0.5662   0.2551     1.03    757.7
## 
##  Location effects: cbind(g5, l5) ~ type + ypub 
## 
##             post.mean l-95% CI u-95% CI eff.samp  pMCMC    
## (Intercept)  -0.69340 -1.71089  0.32701     1000  0.170    
## typehappy    -1.28947 -1.73468 -0.80224     1000 &lt;0.001 ***
## ypub          0.01961 -0.01583  0.05956     1000  0.292    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can see that the between-person variance is comparable to the residual (across-photo within-person) variance although the credible intervals on both variances is wide. <span class="math inline">\(\texttt{MCMCglmm}\)</span> does not store the posterior distribution of the random effects by default, as there may be a lot of them and they are often not of interest. However, since I specified <code>pr=TRUE</code>, the whole of <span class="math inline">\(\boldsymbol{\theta}\)</span> is stored rather than just <span class="math inline">\({\boldsymbol{\mathbf{\beta}}}\)</span>. In Section <a href="glm.html#binom-sec">3.6</a> we saw that photo 4521 and photo 4527, despite having the same fixed effect prediction (<span class="math inline">\(\texttt{type}\)</span> = <span class="math inline">\(\texttt{grumpy}\)</span>, <span class="math inline">\(\texttt{ypub}\)</span> = 16 years), had quite different probabilities of success, with the posterior mean probabilities being 0.862 and 0.400 respectively. What we didn’t know is whether this divergence in probability was due to the person being photographed or some property of the photo.</p>
</div>
<div id="ranpred-sec" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Prediction with Random Effects<a href="ranef.html#ranpred-sec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If we use the <span class="math inline">\(\texttt{predict}\)</span> method on our model the default is to not only marginalise the residuals, but also to marginalise any other random effects. If we predict the probability of success for these two photos they are identical, because we are calculating the expectation based on <span class="math inline">\({\bf X}{\boldsymbol{\beta}}\)</span> only:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="ranef.html#cb128-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.2</span>)[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">25</span>), ]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>##         3        25 
## 0.4268878 0.4268878</code></pre>
<p>The <span class="math inline">\(\texttt{predict}\)</span> method (and <span class="math inline">\(\texttt{simulate}\)</span> method) for <span class="math inline">\(\texttt{MCMCglmm}\)</span> includes the argument <span class="math inline">\(\texttt{marginal}\)</span> which by default takes the <span class="math inline">\(\texttt{random}\)</span> argument used to fit the model. If we want to obtain a prediction that includes (some of) the random effects we can remove the corresponding term from the formula passed to <span class="math inline">\(\texttt{marginal}\)</span>. Since we only have one random term, which we like to include in the prediction, <span class="math inline">\(\texttt{marginal}\)</span> is empty:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="ranef.html#cb130-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.2</span>, <span class="at">marginal =</span> <span class="cn">NULL</span>, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">25</span>), ]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>##          fit       lwr       upr
## 3  0.6305977 0.4239065 0.8337487
## 25 0.3926134 0.2243962 0.5917725</code></pre>
<p>It seems that some of the divergence in probability is due to the person being photographed: our best estimate is that if we had taken many photos of <span class="math inline">\(\texttt{darren_o}\)</span> when grumpy 63.1 % of people would have scored him above five on the grumpy scale, but for <span class="math inline">\(\texttt{craig_w}\)</span> it would be lower (39.3 %). The 95% credible (confidence) intervals on each are wide however, and a formal comparison (on the logit scale) gives a 95% credible interval that overlaps zero:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="ranef.html#cb132-1" tabindex="-1"></a><span class="fu">HPDinterval</span>(mbinom<span class="fl">.2</span><span class="sc">$</span>Sol[, <span class="st">&quot;person.darren_o&quot;</span>] <span class="sc">-</span> mbinom<span class="fl">.2</span><span class="sc">$</span>Sol[, <span class="st">&quot;person.craig_w&quot;</span>])</span></code></pre></div>
<pre><code>##           lower    upper
## var1 -0.1010677 2.502953
## attr(,&quot;Probability&quot;)
## [1] 0.95</code></pre>
</div>
<div id="overdispersed-binomial-as-a-bernoulli-glmm" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Overdispersed Binomial as a Bernoulli GLMM<a href="ranef.html#overdispersed-binomial-as-a-bernoulli-glmm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <span class="math inline">\(\texttt{Grumpy}\)</span> data set aggregates the scores of the 122 respondents into a single binomial response for each photograph. However, we could imagine disaggregating the data such that each respondent for each photograph gets a Bernoulli response with a success if they gave a particular photo a score greater than five. The disaggregated data (<code>FullGrumpy</code>) have <span class="math inline">\(122\times 44 = 5,368\)</span> observations (although a few respondents did not assess all photos).</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="ranef.html#cb134-1" tabindex="-1"></a><span class="fu">data</span>(FullGrumpy)</span>
<span id="cb134-2"><a href="ranef.html#cb134-2" tabindex="-1"></a><span class="fu">head</span>(FullGrumpy, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##   y   type photo person age ypub respondent student
## 1 9 grumpy  4511 ally_p  38   13          1      NO
## 2 8 grumpy  4511 ally_p  38   13          2      NO
## 3 3 grumpy  4511 ally_p  38   13          3      NO</code></pre>
<p><span class="math inline">\(\texttt{y}\)</span> is now the score each respondent <span class="math inline">\(\texttt{respondent}\)</span> gave each <span class="math inline">\(\texttt{photo}\)</span> (rather than the average score for each <span class="math inline">\(\texttt{photo}\)</span> in <code>Grumpy</code>). In addition, we have the respondent-level information <span class="math inline">\(\texttt{student}\)</span> which can be either <span class="math inline">\(\texttt{YES}\)</span> or <span class="math inline">\(\texttt{NO}\)</span>. We will turn each persons score into the Bernoulli response</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="ranef.html#cb136-1" tabindex="-1"></a>FullGrumpy<span class="sc">$</span>g5 <span class="ot">&lt;-</span> FullGrumpy<span class="sc">$</span>y <span class="sc">&gt;</span> <span class="dv">5</span></span></code></pre></div>
<p>and fit the model</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="ranef.html#cb137-1" tabindex="-1"></a>prior.mbinom<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>),</span>
<span id="cb137-2"><a href="ranef.html#cb137-2" tabindex="-1"></a>    <span class="at">G2 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>)))</span>
<span id="cb137-3"><a href="ranef.html#cb137-3" tabindex="-1"></a></span>
<span id="cb137-4"><a href="ranef.html#cb137-4" tabindex="-1"></a>mbinom<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(g5 <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">random =</span> <span class="sc">~</span>person <span class="sc">+</span> photo, <span class="at">data =</span> FullGrumpy,</span>
<span id="cb137-5"><a href="ranef.html#cb137-5" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">prior =</span> prior.mbinom<span class="fl">.3</span>)</span>
<span id="cb137-6"><a href="ranef.html#cb137-6" tabindex="-1"></a></span>
<span id="cb137-7"><a href="ranef.html#cb137-7" tabindex="-1"></a><span class="fu">summary</span>(mbinom<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## 
##  Iterations = 3001:12991
##  Thinning interval  = 10
##  Sample size  = 1000 
## 
##  DIC: 5332.549 
## 
##  G-structure:  ~person
## 
##        post.mean  l-95% CI u-95% CI eff.samp
## person     1.046 0.0005014    2.173    308.1
## 
##                ~photo
## 
##       post.mean l-95% CI u-95% CI eff.samp
## photo    0.8689   0.3223    1.717    210.6
## 
##  R-structure:  ~units
## 
##       post.mean l-95% CI u-95% CI eff.samp
## units         1        1        1        0
## 
##  Location effects: g5 ~ type + ypub 
## 
##             post.mean l-95% CI u-95% CI eff.samp  pMCMC    
## (Intercept)  -0.84769 -1.97064  0.42422    886.5  0.162    
## typehappy    -1.49164 -2.13927 -0.94842    749.9 &lt;0.001 ***
## ypub          0.02431 -0.02036  0.06655   1000.0  0.240    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The random effects deal with any variation in the probability of success across photos and are exactly comparable to the residuals of the binomial model <code>mbinom.2</code>. It is therefore surprising that the posterior distributions for both the variance in <span class="math inline">\(\texttt{person}\)</span> effects and the variance in <span class="math inline">\(\texttt{photo}\)</span>/residual effects appear to be different between the two models, as do the fixed effects. This is a peculiarity of logit-link models in <span class="math inline">\(\texttt{MCMC}\)</span> and wouldn’t be seen in <span class="math inline">\(\texttt{family=&quot;threshold&quot;}\)</span> models that implements the standard probit link. In the binomial model <code>mbinom.2</code> we implicitly assumed that the probability of success did not vary across respondents <em>within</em> photos - this was an assumption, and one that cannot be tested. In the Bernoulli model <code>mbinom.3</code> we explicitly assumed that the probability of success varied across respondents within photos, and the variance on the logit-scale was one. Since Bernoulli data provide no information about observation-level variability either (or most likely, neither) assumption could be true but we have no way of knowing (Section <a href="glm.html#bernoulli-sec">3.6.3</a>). As we saw with fixed effect coefficients in Bernoulli GLM, stating that the variances in photo effects is 0.869 is meaningless in a Bernoulli GLMM without putting it in the context of the assumed residual variance. The standard approach - what I refer to as the standard logit model - is to assume the residual variance is zero. While I think this is a good standard, this is prohibited in <span class="math inline">\(\texttt{MCMCglmm}\)</span> because the chain will not mix. But as we saw with the fixed effects, we can rescale the variances by needs to be multiplied by <span class="math inline">\(1/(1+c^{2}\sigma^{2}_{\texttt{units}})\)</span> where <span class="math inline">\(c=16\sqrt{3}/15\pi\)</span> and <span class="math inline">\(\sigma^{2}_{\texttt{units}}\)</span> is our assumed residual variance, which is one (Figure <a href="ranef.html#fig:bernoulli-rescale2">4.1</a>).</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="ranef.html#cb139-1" tabindex="-1"></a>c2 <span class="ot">&lt;-</span> ((<span class="dv">16</span> <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">3</span>))<span class="sc">/</span>(<span class="dv">15</span> <span class="sc">*</span> pi))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb139-2"><a href="ranef.html#cb139-2" tabindex="-1"></a></span>
<span id="cb139-3"><a href="ranef.html#cb139-3" tabindex="-1"></a>rescale.VCV<span class="fl">.2</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.2</span><span class="sc">$</span>VCV</span>
<span id="cb139-4"><a href="ranef.html#cb139-4" tabindex="-1"></a><span class="fu">colnames</span>(rescale.VCV<span class="fl">.2</span>)[<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="st">&quot;photo&quot;</span></span>
<span id="cb139-5"><a href="ranef.html#cb139-5" tabindex="-1"></a></span>
<span id="cb139-6"><a href="ranef.html#cb139-6" tabindex="-1"></a>rescale.VCV<span class="fl">.3</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.3</span><span class="sc">$</span>VCV[, <span class="fu">c</span>(<span class="st">&quot;person&quot;</span>, <span class="st">&quot;photo&quot;</span>)]<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> c2)</span>
<span id="cb139-7"><a href="ranef.html#cb139-7" tabindex="-1"></a></span>
<span id="cb139-8"><a href="ranef.html#cb139-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(<span class="fu">as.mcmc</span>(rescale.VCV<span class="fl">.2</span>), <span class="fu">as.mcmc</span>(rescale.VCV<span class="fl">.3</span>)), <span class="at">density =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:bernoulli-rescale2"></span>
<img src="_bookdown_files/fig/bernoulli-rescale2-1.png" alt="MCMC traces for the estimated variances in $\texttt{person}$ and $\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`) of individual data (red) and a Binomial GLMM (model `mbinom.2`) where all data for a photo have been aggregated into a single Binomial response (black). The posterior distribution of the variances from the Bernoulli GLMM have been rescaled to what would be observed if the residual variance was zero (rather than one)." width="672" />
<p class="caption">
Figure 4.1: MCMC traces for the estimated variances in <span class="math inline">\(\texttt{person}\)</span> and <span class="math inline">\(\texttt{photo}\)</span> effects from a Bernoulli GLMM (model <code>mbinom.3</code>) of individual data (red) and a Binomial GLMM (model <code>mbinom.2</code>) where all data for a photo have been aggregated into a single Binomial response (black). The posterior distribution of the variances from the Bernoulli GLMM have been rescaled to what would be observed if the residual variance was zero (rather than one).
</p>
</div>
</div>
<div id="intra-class-correlations" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Intra-class Correlations<a href="ranef.html#intra-class-correlations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A more common approach, however, is to express the variances as intra-class correlations where we take the variance of interest and express it as a proportion of the total. For example, for the <span class="math inline">\(\texttt{person}\)</span> effects, the intra-class correlation would be</p>
<p><span class="math display">\[ICC = \frac{\sigma^2_{\texttt{person}}}{\sigma^2_{\texttt{person}}+\sigma^2_{\texttt{photo}}+\sigma^2_{\texttt{units}}+\pi^2/3}\]</span></p>
<p>where the <span class="math inline">\(\pi^2/3\)</span> appears because we have used the logit link and this is the link variance (the variance of the unit logistic).</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="ranef.html#cb140-1" tabindex="-1"></a>ICC<span class="fl">.2</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.2</span><span class="sc">$</span>VCV<span class="sc">/</span>(<span class="fu">rowSums</span>(mbinom<span class="fl">.2</span><span class="sc">$</span>VCV) <span class="sc">+</span> pi<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>)</span>
<span id="cb140-2"><a href="ranef.html#cb140-2" tabindex="-1"></a><span class="fu">colnames</span>(ICC<span class="fl">.2</span>)[<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="st">&quot;photo&quot;</span></span>
<span id="cb140-3"><a href="ranef.html#cb140-3" tabindex="-1"></a></span>
<span id="cb140-4"><a href="ranef.html#cb140-4" tabindex="-1"></a>ICC<span class="fl">.3</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.3</span><span class="sc">$</span>VCV[, <span class="fu">c</span>(<span class="st">&quot;person&quot;</span>, <span class="st">&quot;photo&quot;</span>)]<span class="sc">/</span>(<span class="fu">rowSums</span>(mbinom<span class="fl">.3</span><span class="sc">$</span>VCV) <span class="sc">+</span> pi<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>)</span>
<span id="cb140-5"><a href="ranef.html#cb140-5" tabindex="-1"></a></span>
<span id="cb140-6"><a href="ranef.html#cb140-6" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(<span class="fu">as.mcmc</span>(ICC<span class="fl">.2</span>), <span class="fu">as.mcmc</span>(ICC<span class="fl">.3</span>)), <span class="at">density =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ICC"></span>
<img src="_bookdown_files/fig/ICC-1.png" alt="MCMC traces for the estimated intra-class correlation for $\texttt{person}$ and $\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`) of individual data (red) and a Binomial GLMM (model `mbinom.2`) where all data for a photo have been aggregated into a single Binomial response (black)." width="672" />
<p class="caption">
Figure 4.2: MCMC traces for the estimated intra-class correlation for <span class="math inline">\(\texttt{person}\)</span> and <span class="math inline">\(\texttt{photo}\)</span> effects from a Bernoulli GLMM (model <code>mbinom.3</code>) of individual data (red) and a Binomial GLMM (model <code>mbinom.2</code>) where all data for a photo have been aggregated into a single Binomial response (black).
</p>
</div>
<p>If we had used <span class="math inline">\(\texttt{family=&quot;threshold&quot;}\)</span> this would be omitted because the link variance is zero as we are already working on the … scale<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a> Pierre de Villemereuil’s <a href="https://cran.r-project.org/web/packages/QGglmm/index.html"><span class="math inline">\(\texttt{QGLMM}\)</span></a> package.</p>
<p>The eagle-eyed will have noticed that although the trace plots for the rescaled variance/intra-class correlation for the <span class="math inline">\(\texttt{person}\)</span> effects look identical between the two models, the trace plots for the <span class="math inline">\(\texttt{photo}\)</span> effects look slightly different with the posterior from model <code>mbinom.3</code> (the Bernoulli GLMM in red) appearing to have more density at higher values. However, this difference is due to Monte Carlo error, which is quite high for the variance estimates because the autocorrelation in the chain is moderate. The reported effective sample size in the model summary gives some indication of this - for example in the Bernoulli model the effective sample size for the variance in <span class="math inline">\(\texttt{photo}\)</span> effects is 211, quite a bit less than the 1,000 samples saved (Section <a href="bayesian.html#diagnostics-sec">2.5.4</a>). We can see this more clearly if we just plot the traces for Bernoulli model (Figure <a href="ranef.html#fig:bernoulli-trace">4.3</a>).</p>
<div class="figure"><span style="display:block;" id="fig:bernoulli-trace"></span>
<img src="_bookdown_files/fig/bernoulli-trace-1.png" alt="MCMC trace for the variances in $\texttt{person}$ and $\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`)." width="672" />
<p class="caption">
Figure 4.3: MCMC trace for the variances in <span class="math inline">\(\texttt{person}\)</span> and <span class="math inline">\(\texttt{photo}\)</span> effects from a Bernoulli GLMM (model <code>mbinom.3</code>).
</p>
</div>
<p>Two things are apparent from Figure <a href="ranef.html#fig:bernoulli-trace">4.3</a>. Autocorrelation is present - this is not surprising: for each iteration of the MCMC chain the random effects are Gibbs sampled conditional on their variance in the previous iteration, and then conditional on the updated random effects the variances are then Gibbs sampled (Section <a href="bayesian.html#MCMC">2.5</a>). This will invariably lead to autocorrelation. Second, the trace for the variance in <span class="math inline">\(\texttt{person}\)</span> effects appears to intermittently get ‘stuck’ at values close to zero. In part, this reflects the mechanics of the Gibbs sampling, but it is also a consequence of the inverse-Wishart prior used which has a sharp peak in density at small values (Section <a href="bayesian.html#Vprior-sec">2.6</a>). When the variance in <span class="math inline">\(\texttt{person}\)</span> effects gets ‘stuck’ at zero, the variance in <span class="math inline">\(\texttt{photo}\)</span> effects appears to get ‘stuck’ at high values. This is because the data provide strong support for the combined effect of <span class="math inline">\(\texttt{photo}\)</span> and <span class="math inline">\(\texttt{person}\)</span> being large, but contain less information about their separate effects. Consequently, if the <span class="math inline">\(\texttt{person}\)</span> variance drops to zero, the <span class="math inline">\(\texttt{photo}\)</span> variance increases to compensate. In this example, the effects described above are quite subtle, and simply running the chain for longer would probably suffice. However, a better general strategy would be to employ parameter expansion and use scaled non-central <span class="math inline">\(F\)</span> priors.</p>
</div>
<div id="PXprior-sec" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Priors for Random Effect Variances<a href="ranef.html#PXprior-sec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Parameter expansion is an algorithmic trick for speeding up the mixing and convergence of the MCMC chain. An unintended but - as it turns out - useful side-effect of parameter expansion is that it can allow a wider class of prior distributions that are conjugate and therefore still permit Gibbs sampling (Chapter <a href="bayesian.html#bayesian">2</a>). In order to explore parameter expansion, and the associated <span class="math inline">\(F\)</span> prior for random-effect (co)variances, we will work with a model and data-set where the issues noted for model <code>mbinom.3</code> are much more obvious - the Schools example discussed in <span class="citation">Gelman (<a href="#ref-Gelman.2006">2006</a>)</span>.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="ranef.html#cb141-1" tabindex="-1"></a>schools <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">school =</span> letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>], <span class="at">estimate =</span> <span class="fu">c</span>(<span class="fl">28.39</span>, <span class="fl">7.94</span>, <span class="sc">-</span><span class="fl">2.75</span>, <span class="fl">6.82</span>,</span>
<span id="cb141-2"><a href="ranef.html#cb141-2" tabindex="-1"></a>    <span class="sc">-</span><span class="fl">0.64</span>, <span class="fl">0.63</span>, <span class="fl">18.01</span>, <span class="fl">12.16</span>), <span class="at">ve =</span> <span class="fu">c</span>(<span class="fl">14.9</span>, <span class="fl">10.2</span>, <span class="fl">16.3</span>, <span class="dv">11</span>, <span class="fl">9.4</span>, <span class="fl">11.4</span>, <span class="fl">10.4</span>, <span class="fl">17.6</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb141-3"><a href="ranef.html#cb141-3" tabindex="-1"></a><span class="fu">head</span>(schools)</span></code></pre></div>
<pre><code>##   school estimate     ve
## 1      a    28.39 222.01
## 2      b     7.94 104.04
## 3      c    -2.75 265.69
## 4      d     6.82 121.00
## 5      e    -0.64  88.36
## 6      f     0.63 129.96</code></pre>
<p>The response variable <code>estimate</code> is the relative effect of Scholastic Aptitude Test coaching programs in 8 schools and <span class="citation">Gelman (<a href="#ref-Gelman.2006">2006</a>)</span> focusses on the variance in school effects. Since we only have a single estimate per school there will be a one-to-one mapping between <span class="math inline">\(\texttt{school}\)</span> effects and the residual. In most cases this would result in the variance in school effects being confounded with the residual variance. Here, however, we have been gifted the residual (within school) variance (<span class="math inline">\(\texttt{ve}\)</span>) which varies from school to school. In reality, these residual variances are actually estimates and we might wish to factor in this additional complication, but for now we will ignore this complexity and comeback to it in Chapter <a href="#measurement"><strong>??</strong></a>. First, lets fit the inverse-Wishart prior we have been using up to now with <span class="math inline">\(\texttt{V}=1\)</span> and <span class="math inline">\(\texttt{nu}=0.002\)</span>. This prior is equivalent to an inverse-gamma prior with a shape and scale of 0.001 (Section <a href="bayesian.html#Vprior-sec">2.6</a>):</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="ranef.html#cb143-1" tabindex="-1"></a>prior.mschool<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb143-2"><a href="ranef.html#cb143-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="fl">0.002</span>)))</span>
<span id="cb143-3"><a href="ranef.html#cb143-3" tabindex="-1"></a></span>
<span id="cb143-4"><a href="ranef.html#cb143-4" tabindex="-1"></a>mschool<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb143-5"><a href="ranef.html#cb143-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.1</span>)</span></code></pre></div>
<p>The model contains an argument we haven’t seen before: <span class="math inline">\(\texttt{rcov}\)</span>. In all previous analyses we used the default <code>~units</code> which fits a set of independent and identically distributed residuals with a single variance (<span class="math inline">\(\sigma^2_{\texttt{units}}\)</span>) to be estimated. <span class="math inline">\(\texttt{MCMCglmm}\)</span> allows this assumption to be relaxed, and Chapter <a href="cat-int.html#cat-int">5</a> is dedicated to this subject. Here, we will simply note that we have assigned each school a residual variance (the corresponding element of <span class="math inline">\(\texttt{ve}\)</span>) and fixed it at this value, leaving only the intercept and the variance in school effects (<span class="math inline">\(\sigma^2_{\texttt{school}}\)</span>) to be estimated. The MCMC trace for <span class="math inline">\(\sigma^2_{\texttt{school}}\)</span> looks dreadful (Figure <a href="ranef.html#fig:mschool-1">4.4</a>).</p>
<div class="figure"><span style="display:block;" id="fig:mschool-1"></span>
<img src="_bookdown_files/fig/mschool-1-1.png" alt="MCMC trace for the variance in $\texttt{school}$ effects from model `mschool.1` in which an inverse-Wishart prior was used with $\texttt{V=1}$ and $\texttt{nu=0.002}.$" width="672" />
<p class="caption">
Figure 4.4: MCMC trace for the variance in <span class="math inline">\(\texttt{school}\)</span> effects from model <code>mschool.1</code> in which an inverse-Wishart prior was used with <span class="math inline">\(\texttt{V=1}\)</span> and <span class="math inline">\(\texttt{nu=0.002}.\)</span>
</p>
</div>
<p>The autocorrelation in the chain is evident and there appears to be a lot of posterior density at zero. These are separate issues. Autocorrelation in the chain is due to algorithmic inefficiencies in sampling the posterior distribution, whereas a lot of posterior density near zero reflects the combined information coming from the data and coming from the prior (Chapter <a href="bayesian.html#bayesian">2</a>). Certainly, some posterior distributions are harder to sample from than others, and the efficiency of the MCMC algorithm may decrease when the posterior is close to zero. But if the chain can be run long enough that these inefficiencies are not consequential, situations where the posterior has ‘too much’ density near zero indicates potential problems with the prior, not algorithmic problems.</p>
<div id="f-and-half-t-priors" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> <span class="math inline">\(F\)</span> and half-<span class="math inline">\(t\)</span> priors<a href="ranef.html#f-and-half-t-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the inverse-Wishart prior we specified the parameters <span class="math inline">\(\texttt{V}\)</span> and <span class="math inline">\(\texttt{nu}\)</span>. The parameters <span class="math inline">\(\texttt{alpha.mu}\)</span> and <span class="math inline">\(\texttt{alpha.V}\)</span> can also be specified in the prior, and if <span class="math inline">\(\texttt{alpha.V}\)</span> is non-zero then parameter expansion is used. These additional parameters specify a prior for the variance, <span class="math inline">\(\sigma^2_{\texttt{school}}\)</span>, that is a non-central scaled F-distribution with the numerator degrees-of-freedom set to one. The scale is set by <span class="math inline">\(\texttt{alpha.V}*\texttt{V}\)</span> and the normalised variance <span class="math inline">\(\sigma^2_{\texttt{school}}/(\texttt{alpha.V}*\texttt{V})\)</span> follows a non-central F distribution with <span class="math inline">\(\texttt{df1}=1\)</span>, <span class="math inline">\(\texttt{df2}=\texttt{nu}\)</span> and <span class="math inline">\(\texttt{ncp}=\texttt{alpha.mu}^2/\texttt{alpha.V}\)</span>. The density of <span class="math inline">\(\sigma^2_{\texttt{school}}\)</span> is therefore the density of the <span class="math inline">\(F\)</span> multiplied by <span class="math inline">\(\texttt{alpha.V}*\texttt{V}\)</span> (Section <a href="bayesian.html#transform-sec">2.7</a>).</p>
<p>We can also think about this prior in terms of the standard deviation, <span class="math inline">\(\sigma_{\texttt{school}}\)</span>, which in some ways is more natural, since it is in the same units as the response. The prior distribution for the standard deviation is a scaled non-central half-<span class="math inline">\(t\)</span> (the negative values of the <span class="math inline">\(t\)</span> are ignored) with <span class="math inline">\(\texttt{nu}\)</span> degrees of freedom, non-centrality parameter <span class="math inline">\(\texttt{alpha.mu}/\sqrt{\texttt{alpha.V}}\)</span> and scale <span class="math inline">\(\sqrt{\texttt{alpha.V}*\texttt{V}}\)</span>. The density of <span class="math inline">\(\sigma_{\texttt{school}}\)</span> is therefore the density of the <span class="math inline">\(t\)</span> multiplied by <span class="math inline">\(\sqrt{\texttt{alpha.V}*\texttt{V}}\)</span>.</p>
<p>Before discussing the properties of <span class="math inline">\(F\)</span> and half-<span class="math inline">\(t\)</span> priors on the posterior distributions of variances and standard deviation, respectively, let’s confirm that parameter expansion does indeed increase efficiency independent of the prior being used. Since these distributions only have three effective parameters we will specify <span class="math inline">\(\texttt{V}=1\)</span> throughout, without loss of generality. With <span class="math inline">\(\texttt{nu=1}\)</span> and a non-zero <span class="math inline">\(\texttt{alpha.V}\)</span> the prior on the standard deviation is a scaled half-<span class="math inline">\(t\)</span> distribution with one degree-of-freedom: a half-Cauchy distribution. If we set <span class="math inline">\(\texttt{alpha.mu}=0\)</span> we have a central half-Cauchy distribution and as we up the scale (<span class="math inline">\(\sqrt{\texttt{alpha.V}}\)</span>) of this distribution it tends to a flat prior for the standard deviation. In Section <a href="bayesian.html#Vprior-sec">2.6</a> was also that an improper inverse-Wishart distribution with <span class="math inline">\(\texttt{V}=0\)</span> and <span class="math inline">\(\texttt{nu}=-1\)</span> is also flat for the standard deviation. Let’s fit the improper inverse-Wishart and the central half-Cauchy with a scale that is large (relative to the data) - <span class="math inline">\(\sqrt{\texttt{alpha.V}}=1000\)</span>.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="ranef.html#cb144-1" tabindex="-1"></a>prior.mschool<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fl">1e-16</span>,</span>
<span id="cb144-2"><a href="ranef.html#cb144-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="sc">-</span><span class="dv">1</span>)))</span>
<span id="cb144-3"><a href="ranef.html#cb144-3" tabindex="-1"></a></span>
<span id="cb144-4"><a href="ranef.html#cb144-4" tabindex="-1"></a>mschool<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb144-5"><a href="ranef.html#cb144-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.2</span>)</span>
<span id="cb144-6"><a href="ranef.html#cb144-6" tabindex="-1"></a></span>
<span id="cb144-7"><a href="ranef.html#cb144-7" tabindex="-1"></a></span>
<span id="cb144-8"><a href="ranef.html#cb144-8" tabindex="-1"></a>prior.mschool<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb144-9"><a href="ranef.html#cb144-9" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">1</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="dv">1000</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb144-10"><a href="ranef.html#cb144-10" tabindex="-1"></a></span>
<span id="cb144-11"><a href="ranef.html#cb144-11" tabindex="-1"></a>mschool<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb144-12"><a href="ranef.html#cb144-12" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.3</span>)</span></code></pre></div>
<p>We can see that these two models are sampling from the almost the same posterior (Figure <a href="ranef.html#fig:mschool-2">4.5</a>) but the efficiency of the algorithm is greater when parameter expansion is used with an effective posterior sample size of 1000 rather than 404. Indeed, with 1000 effective samples from 1,000 stored samples we could have ran the chain for less than the default number of iterations and reduced the thinning interval (Section <a href="bayesian.html#diagnostics-sec">2.5.4</a>).</p>
<div class="figure"><span style="display:block;" id="fig:mschool-2"></span>
<img src="_bookdown_files/fig/mschool-2-1.png" alt="MCMC traces for the standard deviation in $\texttt{school}$ effects. In black is the trace for model `mschool.2` in which an improper inverse-Wishart prior was used for the variance with $\texttt{V=1}$ and $\texttt{nu=-1}$. In red is the trace for model `mschool.3` in which a scaled $F_{1,1}$ prior for the variance was used with a scale of $10^6$. The inverse-Wishart prior is flat for the standard deviation. The $F_{1,1}$ prior on the variance places a half-Cauchy prior on the standard deviation with a large scale ($10^3$). This prior is essentially flat for the standard deviation but by switching to the half-Cauchy, parameter expansion is used." width="672" />
<p class="caption">
Figure 4.5: MCMC traces for the standard deviation in <span class="math inline">\(\texttt{school}\)</span> effects. In black is the trace for model <code>mschool.2</code> in which an improper inverse-Wishart prior was used for the variance with <span class="math inline">\(\texttt{V=1}\)</span> and <span class="math inline">\(\texttt{nu=-1}\)</span>. In red is the trace for model <code>mschool.3</code> in which a scaled <span class="math inline">\(F_{1,1}\)</span> prior for the variance was used with a scale of <span class="math inline">\(10^6\)</span>. The inverse-Wishart prior is flat for the standard deviation. The <span class="math inline">\(F_{1,1}\)</span> prior on the variance places a half-Cauchy prior on the standard deviation with a large scale (<span class="math inline">\(10^3\)</span>). This prior is essentially flat for the standard deviation but by switching to the half-Cauchy, parameter expansion is used.
</p>
</div>
<p>We have seen that for closely matched priors, parameter expansion is more efficient at sampling from the posterior. However, to justify its use, it is important that it also allows us to specify priors with sensible properties. Is a flat(ish) prior on the standard deviation more sensible than the inverse-Wishart prior we have been using previously (<span class="math inline">\(\texttt{V}=1\)</span> and <span class="math inline">\(\texttt{nu}\)</span>=0.002) and covered in detail in Section <a href="bayesian.html#Vprior-sec">2.6</a>? The short answer is yes. Setting <span class="math inline">\(\texttt{nu}\)</span> to be small in the inverse-Wishart is motivated by the idea that having few prior degrees of freedom provides a diffuse prior on the variance. However, as <span class="math inline">\(\texttt{nu}\)</span> becomes smaller the prior only becomes flat for the <em>log</em> of the variance, and for the variance there is a nasty spike at small values.</p>
<p>In some cases, as in the Schools example, the amount of information provided by the data may be so small that a flat prior on the standard deviation might be to vague. In such cases, reducing the scale has been advised, and indeed over time the default prior for variances has moved from the inverse-gamma with shape=scale=0.001 (i.e. <span class="math inline">\(\texttt{V}=1\)</span> to <span class="math inline">\(\texttt{nu}=0.002\)</span>) to a Cauchy with a scale of ~5.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="ranef.html#cb145-1" tabindex="-1"></a>prior.mschool<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb145-2"><a href="ranef.html#cb145-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">1</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="dv">5</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb145-3"><a href="ranef.html#cb145-3" tabindex="-1"></a></span>
<span id="cb145-4"><a href="ranef.html#cb145-4" tabindex="-1"></a>mschool<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb145-5"><a href="ranef.html#cb145-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.4</span>)</span></code></pre></div>
<p>However, care has to be taken that these scales are calibrated with the scale of the data, and in the Schools example where the standard deviation ofthe data exceeds one by some margin, a scale of 25 was used.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="ranef.html#cb146-1" tabindex="-1"></a>prior.mschool<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb146-2"><a href="ranef.html#cb146-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">1</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="dv">25</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb146-3"><a href="ranef.html#cb146-3" tabindex="-1"></a></span>
<span id="cb146-4"><a href="ranef.html#cb146-4" tabindex="-1"></a>mschool<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb146-5"><a href="ranef.html#cb146-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.5</span>)</span></code></pre></div>
<p>The default in brms is to use a half-t with 3 degrees of freedom and scale of 2.5 for the standard deviation. This is less ‘tailed’ than the Cauchy:length</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="ranef.html#cb147-1" tabindex="-1"></a>prior.mschool<span class="fl">.6</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb147-2"><a href="ranef.html#cb147-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">3</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="fl">2.5</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb147-3"><a href="ranef.html#cb147-3" tabindex="-1"></a></span>
<span id="cb147-4"><a href="ranef.html#cb147-4" tabindex="-1"></a>mschool<span class="fl">.6</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb147-5"><a href="ranef.html#cb147-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.6</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:prior-compare"></span>
<img src="_bookdown_files/fig/prior-compare-1.png" alt="Kernel density plots for different priors" width="672" />
<p class="caption">
Figure 4.6: Kernel density plots for different priors
</p>
</div>
<p>The default in brms is to use a half-t with 3 degrees of freedom and scale of 2.5 for the standard deviation. This is less ‘tailed’ than the Cauchy:length</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="ranef.html#cb148-1" tabindex="-1"></a>prior.mschool<span class="fl">.6</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb148-2"><a href="ranef.html#cb148-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">3</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="fl">2.5</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb148-3"><a href="ranef.html#cb148-3" tabindex="-1"></a></span>
<span id="cb148-4"><a href="ranef.html#cb148-4" tabindex="-1"></a>mschool<span class="fl">.6</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb148-5"><a href="ranef.html#cb148-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.6</span>)</span></code></pre></div>
<p>student_t(3, 0, 2.5)</p>
</div>
</div>
<div id="fixed-or-random" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Fixed or Random?<a href="ranef.html#fixed-or-random" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
<h3> References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Gelman.2006" class="csl-entry">
Gelman, A. 2006. <span>“Prior Distributions for Variance Parameters in Hierarchical Models.”</span> <em>Bayesian Analysis</em> 1 (3): 515–33.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>If we assumed the distribution was Laplace (back to back exponentials) we have the LASSO. If we assumed the distribution was a mixture of normal and Laplace we have the elastic net.<a href="ranef.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>With the now defunct <span class="math inline">\(\texttt{family=&quot;ordinal&quot;}\)</span> the <span class="math inline">\(\pi^2/3\)</span> in the denominator for the intra-class correlation would have be replaced by a one which is the link variance for the probit (the variance of the unit normal).<a href="ranef.html#fnref12" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="glm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cat-int.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["MCMCglmm-course-notes.pdf", "MCMCglmm-course-notes.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
