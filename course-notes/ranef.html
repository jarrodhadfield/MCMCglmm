<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Random effects | MCMCglmm Course Notes</title>
  <meta name="description" content="Extended documentation and course notes for the MCMCglmm R package." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Random effects | MCMCglmm Course Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Random effects | MCMCglmm Course Notes" />
  
  <meta name="twitter:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

<meta name="author" content="Jarrod Hadfield" />


<meta name="date" content="2026-01-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="glm.html"/>
<link rel="next" href="cat-int.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script>
<link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-1.3.31/rglClass.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/utils.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/buffer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/subscenes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shaders.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shadersrc.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/textures.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/projection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/mouse.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/init.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pieces.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/draw.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/controls.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/selection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/rglTimer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pretty.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/axes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/animation.src.js"></script>
<script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.src.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#outline"><i class="fa fa-check"></i><b>1.1</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian Analysis and MCMC</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian.html"><a href="bayesian.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>2.2</b> Likelihood</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-ml"><i class="fa fa-check"></i><b>2.2.1</b> Maximum Likelihood (ML)</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian.html"><a href="bayesian.html#restricted-maximum-likelihood-reml"><i class="fa fa-check"></i><b>2.2.2</b> Restricted Maximum Likelihood (REML)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian.html"><a href="bayesian.html#prior-distribution"><i class="fa fa-check"></i><b>2.3</b> Prior Distribution</a></li>
<li class="chapter" data-level="2.4" data-path="bayesian.html"><a href="bayesian.html#posterior-distribution"><i class="fa fa-check"></i><b>2.4</b> Posterior Distribution</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian.html"><a href="bayesian.html#marginal-posterior-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Marginal Posterior Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="bayesian.html"><a href="bayesian.html#intervals-sec"><i class="fa fa-check"></i><b>2.4.2</b> Credible Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian.html"><a href="bayesian.html#MCMC"><i class="fa fa-check"></i><b>2.5</b> MCMC</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian.html"><a href="bayesian.html#starting-values"><i class="fa fa-check"></i><b>2.5.1</b> Starting values</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian.html"><a href="bayesian.html#metropolis-hastings-updates"><i class="fa fa-check"></i><b>2.5.2</b> Metropolis-Hastings updates</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian.html"><a href="bayesian.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.5.3</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian.html"><a href="bayesian.html#diagnostics-sec"><i class="fa fa-check"></i><b>2.5.4</b> MCMC Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian.html"><a href="bayesian.html#Vprior-sec"><i class="fa fa-check"></i><b>2.6</b> Priors for Residual Variances</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian.html"><a href="bayesian.html#IP-sec"><i class="fa fa-check"></i><b>2.6.1</b> Improper Priors</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian.html"><a href="bayesian.html#transform-sec"><i class="fa fa-check"></i><b>2.7</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>3</b> Linear and Generalised Linear Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="glm.html"><a href="glm.html#linear-model-lm"><i class="fa fa-check"></i><b>3.1</b> Linear Model (LM)</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="glm.html"><a href="glm.html#lm-sec"><i class="fa fa-check"></i><b>3.1.1</b> Linear Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="glm.html"><a href="glm.html#generalised-linear-model-glm"><i class="fa fa-check"></i><b>3.2</b> Generalised Linear Model (GLM)</a></li>
<li class="chapter" data-level="3.3" data-path="glm.html"><a href="glm.html#poisson-glm"><i class="fa fa-check"></i><b>3.3</b> Poisson GLM</a></li>
<li class="chapter" data-level="3.4" data-path="glm.html"><a href="glm.html#overdispersion"><i class="fa fa-check"></i><b>3.4</b> Overdispersion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="glm.html"><a href="glm.html#multiplicative-overdispersion"><i class="fa fa-check"></i><b>3.4.1</b> Multiplicative Overdispersion</a></li>
<li class="chapter" data-level="3.4.2" data-path="glm.html"><a href="glm.html#addod-sec"><i class="fa fa-check"></i><b>3.4.2</b> Additive Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="glm.html"><a href="glm.html#prediction-in-glm"><i class="fa fa-check"></i><b>3.5</b> Prediction in GLM</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="glm.html"><a href="glm.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="glm.html"><a href="glm.html#binom-sec"><i class="fa fa-check"></i><b>3.6</b> Binomial and Bernoulli GLM</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="glm.html"><a href="glm.html#overdispersion-1"><i class="fa fa-check"></i><b>3.6.1</b> Overdispersion</a></li>
<li class="chapter" data-level="3.6.2" data-path="glm.html"><a href="glm.html#binom-pred-sec"><i class="fa fa-check"></i><b>3.6.2</b> Prediction</a></li>
<li class="chapter" data-level="3.6.3" data-path="glm.html"><a href="glm.html#bernoulli-sec"><i class="fa fa-check"></i><b>3.6.3</b> Bernoulli GLM</a></li>
<li class="chapter" data-level="3.6.4" data-path="glm.html"><a href="glm.html#probit-link"><i class="fa fa-check"></i><b>3.6.4</b> Probit link</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="glm.html"><a href="glm.html#ordinal-data"><i class="fa fa-check"></i><b>3.7</b> Ordinal Data</a></li>
<li class="chapter" data-level="3.8" data-path="glm.html"><a href="glm.html#non-zero-binomial-data"><i class="fa fa-check"></i><b>3.8</b> Non-zero Binomial Data</a></li>
<li class="chapter" data-level="3.9" data-path="glm.html"><a href="glm.html#complete-separation"><i class="fa fa-check"></i><b>3.9</b> Complete Separation</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="glm.html"><a href="glm.html#gelman-prior-sec"><i class="fa fa-check"></i><b>3.9.1</b> The <span class="citation">Gelman, Jakulin, et al. (2008)</span> prior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ranef.html"><a href="ranef.html"><i class="fa fa-check"></i><b>4</b> Random effects</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ranef.html"><a href="ranef.html#GLMM"><i class="fa fa-check"></i><b>4.1</b> Generalised Linear Mixed Model (GLMM)</a></li>
<li class="chapter" data-level="4.2" data-path="ranef.html"><a href="ranef.html#ranpred-sec"><i class="fa fa-check"></i><b>4.2</b> Prediction with Random Effects</a></li>
<li class="chapter" data-level="4.3" data-path="ranef.html"><a href="ranef.html#overdispersed-binomial-as-a-bernoulli-glmm"><i class="fa fa-check"></i><b>4.3</b> Overdispersed Binomial as a Bernoulli GLMM</a></li>
<li class="chapter" data-level="4.4" data-path="ranef.html"><a href="ranef.html#ICC"><i class="fa fa-check"></i><b>4.4</b> Intra-class Correlations</a></li>
<li class="chapter" data-level="4.5" data-path="ranef.html"><a href="ranef.html#PXprior-sec"><i class="fa fa-check"></i><b>4.5</b> Priors for Random Effect Variances</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="ranef.html"><a href="ranef.html#f-and-folded-t-priors"><i class="fa fa-check"></i><b>4.5.1</b> <span class="math inline">\(F\)</span> and folded-<span class="math inline">\(t\)</span> priors</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="ranef.html"><a href="ranef.html#Vprior-gen-sec"><i class="fa fa-check"></i><b>4.6</b> Prior Generators</a></li>
<li class="chapter" data-level="4.7" data-path="ranef.html"><a href="ranef.html#priors-on-functions-of-variances"><i class="fa fa-check"></i><b>4.7</b> Priors on Functions of Variances</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="ranef.html"><a href="ranef.html#intra-class-correlation"><i class="fa fa-check"></i><b>4.7.1</b> Intra-class Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="ranef.html"><a href="ranef.html#fix-or-rand"><i class="fa fa-check"></i><b>4.8</b> Fixed or Random?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cat-int.html"><a href="cat-int.html"><i class="fa fa-check"></i><b>5</b> Categorical Random Interactions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="cat-int.html"><a href="cat-int.html#vstruct-sec"><i class="fa fa-check"></i><b>5.1</b> Variance Structures</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="cat-int.html"><a href="cat-int.html#textttidh-variance-structure"><i class="fa fa-check"></i><b>5.1.1</b> <span class="math inline">\(\texttt{idh}\)</span> Variance Structure</a></li>
<li class="chapter" data-level="5.1.2" data-path="cat-int.html"><a href="cat-int.html#us-sec"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(\texttt{us}\)</span> Variance Structure</a></li>
<li class="chapter" data-level="5.1.3" data-path="cat-int.html"><a href="cat-int.html#other-variance-structures"><i class="fa fa-check"></i><b>5.1.3</b> Other Variance Structures</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="cat-int.html"><a href="cat-int.html#linking-functions"><i class="fa fa-check"></i><b>5.2</b> Linking Functions</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="cat-int.html"><a href="cat-int.html#textttstr-covariances-between-random-terms"><i class="fa fa-check"></i><b>5.2.1</b> <span class="math inline">\(\texttt{str}\)</span>: covariances between random terms</a></li>
<li class="chapter" data-level="5.2.2" data-path="cat-int.html"><a href="cat-int.html#textttmm-multi-membership-models"><i class="fa fa-check"></i><b>5.2.2</b> <span class="math inline">\(\texttt{mm}\)</span>: multi-membership Models</a></li>
<li class="chapter" data-level="5.2.3" data-path="cat-int.html"><a href="cat-int.html#textttcovu-covariances-between-random-and-residual-terms"><i class="fa fa-check"></i><b>5.2.3</b> <span class="math inline">\(\texttt{covu}\)</span>: covariances between random and residual terms</a></li>
<li class="chapter" data-level="5.2.4" data-path="cat-int.html"><a href="cat-int.html#texttttheta_scale-scaled-linear-predictor"><i class="fa fa-check"></i><b>5.2.4</b> <span class="math inline">\(\texttt{theta_scale}\)</span>: scaled linear predictor</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="cat-int.html"><a href="cat-int.html#VCVprior-sec"><i class="fa fa-check"></i><b>5.3</b> Priors for Covariance Matrices</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="cat-int.html"><a href="cat-int.html#marginal-priors-for-variances"><i class="fa fa-check"></i><b>5.3.1</b> Marginal Priors for Variances</a></li>
<li class="chapter" data-level="5.3.2" data-path="cat-int.html"><a href="cat-int.html#marginal-priors-for-covariances"><i class="fa fa-check"></i><b>5.3.2</b> Marginal Priors for Covariances</a></li>
<li class="chapter" data-level="5.3.3" data-path="cat-int.html"><a href="cat-int.html#marginal-priors-for-correlations"><i class="fa fa-check"></i><b>5.3.3</b> Marginal Priors for Correlations</a></li>
<li class="chapter" data-level="5.3.4" data-path="cat-int.html"><a href="cat-int.html#priors-for-corg-and-corgh-structures"><i class="fa fa-check"></i><b>5.3.4</b> Priors for <code>corg</code> and <code>corgh</code> structures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cont-int.html"><a href="cont-int.html"><i class="fa fa-check"></i><b>6</b> Continuous Random Interactions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="cont-int.html"><a href="cont-int.html#random-regression"><i class="fa fa-check"></i><b>6.1</b> Random Regression</a></li>
<li class="chapter" data-level="6.2" data-path="cont-int.html"><a href="cont-int.html#expected-variances-and-covariances"><i class="fa fa-check"></i><b>6.2</b> Expected Variances and Covariances</a></li>
<li class="chapter" data-level="6.3" data-path="cont-int.html"><a href="cont-int.html#RRcentering"><i class="fa fa-check"></i><b>6.3</b> <code>us</code> versus <code>idh</code> and mean centering</a></li>
<li class="chapter" data-level="6.4" data-path="cont-int.html"><a href="cont-int.html#meta-sec"><i class="fa fa-check"></i><b>6.4</b> Meta-analysis</a></li>
<li class="chapter" data-level="6.5" data-path="cont-int.html"><a href="cont-int.html#splines"><i class="fa fa-check"></i><b>6.5</b> Splines</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multi-response models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="multi.html"><a href="multi.html#relaxing-the-univariate-assumptions-of-causality"><i class="fa fa-check"></i><b>7.1</b> Relaxing the univariate assumptions of causality</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="multi.html"><a href="multi.html#textttcovu-covariances-between-random-and-residual-terms-1"><i class="fa fa-check"></i><b>7.1.1</b> <span class="math inline">\(\texttt{covu}\)</span>: covariances between random and residual terms</a></li>
<li class="chapter" data-level="7.1.2" data-path="multi.html"><a href="multi.html#texttttheta_scale-scaled-linear-predictor-1"><i class="fa fa-check"></i><b>7.1.2</b> <span class="math inline">\(\texttt{theta_scale}\)</span>: scaled linear predictor</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="multi.html"><a href="multi.html#multinomial-models"><i class="fa fa-check"></i><b>7.2</b> Multinomial Models</a></li>
<li class="chapter" data-level="7.3" data-path="multi.html"><a href="multi.html#zero-inflated-models"><i class="fa fa-check"></i><b>7.3</b> Zero-inflated Models</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="multi.html"><a href="multi.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="multi.html"><a href="multi.html#Hurdle"><i class="fa fa-check"></i><b>7.4</b> Hurdle Models</a></li>
<li class="chapter" data-level="7.5" data-path="multi.html"><a href="multi.html#ZAP"><i class="fa fa-check"></i><b>7.5</b> Zero-altered Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pedigree.html"><a href="pedigree.html"><i class="fa fa-check"></i><b>8</b> Pedigrees and Phylogenies</a>
<ul>
<li class="chapter" data-level="8.1" data-path="pedigree.html"><a href="pedigree.html#pedigree-and-phylogeny-formats"><i class="fa fa-check"></i><b>8.1</b> Pedigree and phylogeny formats</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="pedigree.html"><a href="pedigree.html#pedigrees"><i class="fa fa-check"></i><b>8.1.1</b> Pedigrees</a></li>
<li class="chapter" data-level="8.1.2" data-path="pedigree.html"><a href="pedigree.html#phylogenies"><i class="fa fa-check"></i><b>8.1.2</b> Phylogenies</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="pedigree.html"><a href="pedigree.html#the-animal-model-and-the-phylogenetic-mixed-model"><i class="fa fa-check"></i><b>8.2</b> The animal model and the phylogenetic mixed model</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="parameter-expansion.html"><a href="parameter-expansion.html"><i class="fa fa-check"></i><b>9</b> Parameter Expansion</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="parameter-expansion.html"><a href="parameter-expansion.html#variances-close-to-zero"><i class="fa fa-check"></i><b>9.0.1</b> Variances close to zero</a></li>
<li class="chapter" data-level="9.0.2" data-path="parameter-expansion.html"><a href="parameter-expansion.html#secPX-p"><i class="fa fa-check"></i><b>9.0.2</b> Parameter expanded priors</a></li>
<li class="chapter" data-level="9.0.3" data-path="parameter-expansion.html"><a href="parameter-expansion.html#binary-response-models"><i class="fa fa-check"></i><b>9.0.3</b> Binary response models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="path.html"><a href="path.html"><i class="fa fa-check"></i><b>10</b> Path Analysis &amp; Antedependence Structures</a>
<ul>
<li class="chapter" data-level="10.1" data-path="path.html"><a href="path.html#path-anlaysis"><i class="fa fa-check"></i><b>10.1</b> Path Anlaysis</a></li>
<li class="chapter" data-level="10.2" data-path="path.html"><a href="path.html#ante-sec"><i class="fa fa-check"></i><b>10.2</b> Antedependence</a></li>
<li class="chapter" data-level="10.3" data-path="path.html"><a href="path.html#scaling"><i class="fa fa-check"></i><b>10.3</b> Scaling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="technical-details.html"><a href="technical-details.html"><i class="fa fa-check"></i><b>11</b> Technical Details</a>
<ul>
<li class="chapter" data-level="11.1" data-path="technical-details.html"><a href="technical-details.html#model-form"><i class="fa fa-check"></i><b>11.1</b> Model Form</a></li>
<li class="chapter" data-level="11.2" data-path="technical-details.html"><a href="technical-details.html#MCMC-app"><i class="fa fa-check"></i><b>11.2</b> MCMC Sampling Schemes</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="technical-details.html"><a href="technical-details.html#updating-the-latent-variables-bf-l"><i class="fa fa-check"></i><b>11.2.1</b> Updating the latent variables <span class="math inline">\({\bf l}\)</span></a></li>
<li class="chapter" data-level="11.2.2" data-path="technical-details.html"><a href="technical-details.html#updating-the-location-vector-boldsymboltheta-leftboldsymbolmathbfbeta-bf-uright"><i class="fa fa-check"></i><b>11.2.2</b> Updating the location vector <span class="math inline">\(\boldsymbol{\theta} = \left[{\boldsymbol{\mathbf{\beta}}}^{&#39;}\; {\bf u}^{&#39;}\right]^{&#39;}\)</span></a></li>
<li class="chapter" data-level="11.2.3" data-path="technical-details.html"><a href="technical-details.html#updating-the-variance-structures-bf-g-and-bf-r"><i class="fa fa-check"></i><b>11.2.3</b> Updating the variance structures <span class="math inline">\({\bf G}\)</span> and <span class="math inline">\({\bf R}\)</span></a></li>
<li class="chapter" data-level="11.2.4" data-path="technical-details.html"><a href="technical-details.html#ordinal-models"><i class="fa fa-check"></i><b>11.2.4</b> Ordinal Models</a></li>
<li class="chapter" data-level="11.2.5" data-path="technical-details.html"><a href="technical-details.html#path-analyses"><i class="fa fa-check"></i><b>11.2.5</b> Path Analyses</a></li>
<li class="chapter" data-level="11.2.6" data-path="technical-details.html"><a href="technical-details.html#deviance-and-dic"><i class="fa fa-check"></i><b>11.2.6</b> Deviance and DIC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MCMCglmm Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ranef" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Random effects<a href="ranef.html#ranef" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In some cases we may have measured variables whose effects we would like to treat as random. Often the distinction between fixed and random is given by example: things like city, species, individual and vial are random, but sex, treatment and age are not. Or the distinction is made using rules of thumb: if there are few factor levels and they are interesting to other people they are fixed. However, this doesn’t really confer any understanding about what it means to treat something as fixed or random, and doesn’t really allow judgements to be made for variables in which the rules of thumb seem to contradict each other. Similarly, these ‘explanations’ don’t give any insight into the fact that all effects are technically random in a Bayesian analysis.</p>
<p>Random effect models are often expressed as an extension of Equation <a href="glm.html#eq:lm">(3.2)</a>:</p>
<p><span class="math display" id="eq:MM">\[E[{\bf y}] = {\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf Z}{\bf u}
\label{MM}   \tag{4.1}\]</span></p>
<p>where <span class="math inline">\({\bf Z}\)</span> is a design matrix like <span class="math inline">\({\bf X}\)</span>, and <span class="math inline">\({\bf u}\)</span> is a vector of parameters like <span class="math inline">\({\boldsymbol{\mathbf{\beta}}}\)</span>. However, at this stage there is simply no distinction between fixed and random effects. We could combine the design matrices (<span class="math inline">\({\bf W} = [{\bf X}, {\bf Z}]\)</span>) and combine the vectors of parameters (<span class="math inline">\(\boldsymbol{\theta} = [{\boldsymbol{\mathbf{\beta}}}^{&#39;}, {\bf u}^{&#39;}]^{&#39;}\)</span>) to get:</p>
<p><span class="math display" id="eq:MM2">\[E[{\bf y}] = {\bf W}\boldsymbol{\theta}
\label{MM2}   \tag{4.2}\]</span></p>
<p>which is <strong>identical</strong> to Equation <a href="ranef.html#eq:MM">(4.1)</a>. So if we don’t need to distinguish between fixed and random effects at this stage, when should we distinguish between them, and what distinguishes them?</p>
<p>When we treat an effect as random we believe that the coefficients have some distribution around a mean of zero; often we assume they are normal<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> and that they are independent (represented by an identity matrix) and identically distributed with variance <span class="math inline">\(\sigma^{2}_{u}\)</span>:</p>
<p><span class="math display">\[{\bf u} \sim N({\bf 0}, {\bf I}\sigma^{2}_{u})\]</span></p>
<p><span class="math inline">\(\sigma^{2}_{u}\)</span> is a parameter of the model which we estimate, in addition to <span class="math inline">\({\bf u}\)</span>. In a Bayesian analysis we would also assign <span class="math inline">\(\sigma^{2}_{u}\)</span> a prior, and <span class="math inline">\(\sigma^{2}_{u}\)</span> is often called a hyper-parameter with an associated hyper-prior.</p>
<p>Fixed effects in a frequentist analysis are not assigned a distribution, but we can understand this in terms of the limit to the normal distribution</p>
<p><span class="math display">\[\boldsymbol{\beta} \sim N({\bf 0}, {\bf I}\sigma^{2}_{\beta})\]</span></p>
<p>as <span class="math inline">\(\sigma^{2}_{\beta}\)</span> tends to infinity. In a Bayesian setting we would call this a flat improper prior. In practice, we often use diffuse proper priors in Bayesian analyses. For example, the default in <span class="math inline">\(\texttt{MCMCglmm}\)</span> is to set <span class="math inline">\(\sigma^{2}_{\beta}=10^8\)</span>. Then, <span class="math inline">\(\boldsymbol{\beta}\)</span> are technically random - they are assigned a distribution - but I find it useful to retain the frequentist terminology ‘fixed’. The only difference then is that the ‘fixed’ effects are assigned a prior distribution with a variance that is defined by the user-specified prior (<span class="math inline">\(\sigma^{2}_{\beta}\)</span> - which is often set to be large) and the ‘random’ effects are assigned a prior distribution with a variance that is estimated (<span class="math inline">\(\sigma^{2}_{u}\)</span> - which could be large, but also zero).</p>
<p>That is the distinction between fixed and random effects. The difference really is that simple, but it takes a long time and a lot of practice to understand what this means in practical terms, and why working with random effects can be a very powerful way of modelling data. To get a feel for why we might want to fit an effect as random or not, lets work through an example before moving on to model fitting. In Section <a href="glm.html#binom-sec">3.6</a> we analysed binomial data where 122 respondents had looked at 44 photographs of people and given them a ‘grumpy score’ of more than five (a success) or less than five (a failure). If, instead of 122 respondents, there had been a zillion respondents, we could use the average proportion of success for each photo as a nearly perfect estimates of their probabilities of success. The variance of these near-perfect estimates could serve as a reasonable estimate of the variance in photo effects. If the probabilities were all clustered tightly around 0.5: 0.505, 0.501, 0.499 and so on, then variance would be estimated to be small. Let’s then imagine that we obtained a <span class="math inline">\(45^\textrm{th}\)</span> photograph but by this point the respondents were so bored I managed to only recruit a single person who gave the photo a score greater than five - a success. Since we only have one observation for this photo the average proportion of success would be one. Do you think the best estimate of the probability of success for the <span class="math inline">\(45^\textrm{th}\)</span> photograph is then 1.000? I think you wouldn’t: you would use the knowledge that you have gained from the other photos and say that it is more likely that if you had managed to recruit more respondents you would have got a roughly even split of success and failures. You have used common sense, treated the photo effects as random, and <em>shrunk</em> photo 45’s effect towards the average because the variance (<span class="math inline">\(\sigma^2_u\)</span>) was small and we have a strong prior. If we had treated the photo effects as fixed, we believe that the only information regarding a photo’s value comes from data associated with that particular photo, and the estimate of photo 45’s probability would have been one. When we treat an effect as random, we also use the information that comes from data associated with that particular photo (obviously), but we weight that information by what the data associated with other photos tell us about the likely values that the effect could take - through the parameter <span class="math inline">\(\sigma^2_u\)</span>. What if the probabilities weren’t all clustered tightly around 0.5, but took on values 0.500, 0.998, 0.002, 0.327 …? The variance <span class="math inline">\(\sigma^2_u\)</span> would be larger and the prior information for our <span class="math inline">\(45^\textrm{th}\)</span> photo would be weaker: perhaps we got a success because the underlying probability was 0.998, but a single success would also not be very surprising if the underlying probability was 0.500, or even 0.327. We might then be happy that our best estimate of the probability of success for the <span class="math inline">\(45^\textrm{th}\)</span> photograph was close to one, although with such weak prior information (large <span class="math inline">\(\sigma^2_u\)</span>) the uncertainty would remain large.</p>
<p>When the motivation for treating an effect as random is explained this way, it is hard to come up with a reason why you wouldn’t treat all effects as random. However, you have to consider how much information is in a given data set to estimate <span class="math inline">\(\sigma^2_u\)</span>, which we will cover in Section <a href="ranef.html#fix-or-rand">4.8</a>.</p>
<div id="GLMM" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Generalised Linear Mixed Model (GLMM)<a href="ranef.html#GLMM" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Section <a href="glm.html#binom-sec">3.6</a>, the binomial model we fitted only contained fixed effects, as specified in the <code>fixed</code> argument to <code>MCMCglmm</code> (<code>fixed=cbind(g5,l5)~type+ypub</code>). No random effects were fitted, although ‘residuals’ were fitted as default to absorb any overdispersion. Residuals are random effects for which we estimate a variance - the hyperparameter, <span class="math inline">\(\sigma^2_e\)</span> - and when used with Binomial or Poisson responses are commonly referred to as observation-level random effects. Since there is a one-to-one correspondence between observation and photo in this data set, <span class="math inline">\(\sigma^2_e\)</span> is equivalent to the <span class="math inline">\(\sigma^2_u\)</span> discussed above (although <span class="math inline">\(\sigma^2_e\)</span> refers to the variance on the logit scale rather the probability scale used implicitly above). We saw that the probability of success varied greatly across photos (model <code>mbinom.1</code>) but we also noted that some of this variation may be due to the person being photographed and we could tease apart the effect of person from the specifics of the photo since each person was photographed twice - once when happy and once when grumpy. <span class="math inline">\(\texttt{person}\)</span> has 22 levels and you are probably not interested in knowing the grumpy score of someone you didn’t know - <span class="math inline">\(\texttt{person}\)</span> effects seem to satisfy the rule of thumb often used to decide that they should be treated as random. The random effect model is specified through the argument <code>random</code> and for simple effects as these we simply put the name of the corresponding column (<span class="math inline">\(\texttt{person}\)</span>) in the model formula. We will also specify inverse-Wishart priors for both the residual variance and the variance of the <span class="math inline">\(\texttt{person}\)</span> effects (see Section <a href="bayesian.html#Vprior-sec">2.6</a>) although scaled non-central <span class="math inline">\(F\)</span>-distribution priors are recommended for random-effect variances (see Section <a href="ranef.html#PXprior-sec">4.5</a>):</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="ranef.html#cb126-1" tabindex="-1"></a>prior.mbinom<span class="fl">.2</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>)))</span>
<span id="cb126-2"><a href="ranef.html#cb126-2" tabindex="-1"></a></span>
<span id="cb126-3"><a href="ranef.html#cb126-3" tabindex="-1"></a>mbinom<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(<span class="fu">cbind</span>(g5, l5) <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">random =</span> <span class="sc">~</span>person, <span class="at">data =</span> Grumpy,</span>
<span id="cb126-4"><a href="ranef.html#cb126-4" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;multinomial2&quot;</span>, <span class="at">pr =</span> <span class="cn">TRUE</span>, <span class="at">prior =</span> prior.mbinom<span class="fl">.2</span>)</span>
<span id="cb126-5"><a href="ranef.html#cb126-5" tabindex="-1"></a><span class="fu">summary</span>(mbinom<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## 
##  Iterations = 3001:12991
##  Thinning interval  = 10
##  Sample size  = 1000 
## 
##  DIC: 5376.535 
## 
##  G-structure:  ~person
## 
##        post.mean l-95% CI u-95% CI eff.samp
## person    0.8571 0.008145    1.644    744.3
## 
##  R-structure:  ~units
## 
##       post.mean l-95% CI u-95% CI eff.samp
## units    0.5662   0.2551     1.03    757.7
## 
##  Location effects: cbind(g5, l5) ~ type + ypub 
## 
##             post.mean l-95% CI u-95% CI eff.samp  pMCMC    
## (Intercept)  -0.69340 -1.71089  0.32701     1000  0.170    
## typehappy    -1.28947 -1.73468 -0.80224     1000 &lt;0.001 ***
## ypub          0.01961 -0.01583  0.05956     1000  0.292    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can see that the between-person variance is comparable to the residual (across-photo within-person) variance although the credible intervals on both variances is wide. <span class="math inline">\(\texttt{MCMCglmm}\)</span> does not store the posterior distribution of the random effects by default, as there may be a lot of them and they are often not of interest. However, since I specified <code>pr=TRUE</code>, the whole of <span class="math inline">\(\boldsymbol{\theta}\)</span> is stored rather than just <span class="math inline">\({\boldsymbol{\mathbf{\beta}}}\)</span>. In Section <a href="glm.html#binom-sec">3.6</a> we saw that photo 4521 and photo 4527, despite having the same fixed effect prediction (<span class="math inline">\(\texttt{type}\)</span> = <span class="math inline">\(\texttt{grumpy}\)</span>, <span class="math inline">\(\texttt{ypub}\)</span> = 16 years), had quite different probabilities of success, with the posterior mean probabilities being 0.862 and 0.400 respectively. What we didn’t know is whether this divergence in probability was due to the person being photographed or some property of the photo.</p>
</div>
<div id="ranpred-sec" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Prediction with Random Effects<a href="ranef.html#ranpred-sec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If we use the <span class="math inline">\(\texttt{predict}\)</span> method on our model the default is to not only marginalise the residuals, but also to marginalise any other random effects. If we predict the probability of success for these two photos they are identical, because we are calculating the expectation based on <span class="math inline">\({\bf X}{\boldsymbol{\beta}}\)</span> only:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="ranef.html#cb128-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.2</span>)[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">25</span>), ]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>##         3        25 
## 0.4268878 0.4268878</code></pre>
<p>The <span class="math inline">\(\texttt{predict}\)</span> method (and <span class="math inline">\(\texttt{simulate}\)</span> method) for <span class="math inline">\(\texttt{MCMCglmm}\)</span> includes the argument <span class="math inline">\(\texttt{marginal}\)</span> which by default takes the <span class="math inline">\(\texttt{random}\)</span> argument used to fit the model. If we want to obtain a prediction that includes (some of) the random effects we can remove the corresponding term from the formula passed to <span class="math inline">\(\texttt{marginal}\)</span>. Since we only have one random term, which we like to include in the prediction, <span class="math inline">\(\texttt{marginal}\)</span> is empty:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="ranef.html#cb130-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.2</span>, <span class="at">marginal =</span> <span class="cn">NULL</span>, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">25</span>), ]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>##          fit       lwr       upr
## 3  0.6305977 0.4239065 0.8337487
## 25 0.3926134 0.2243962 0.5917725</code></pre>
<p>It seems that some of the divergence in probability is due to the person being photographed: our best estimate is that if we had taken many photos of <span class="math inline">\(\texttt{darren_o}\)</span> when grumpy 63.1 % of people would have scored him above five on the grumpy scale, but for <span class="math inline">\(\texttt{craig_w}\)</span> it would be lower (39.3 %). The 95% credible (confidence) intervals on each are wide however, and a formal comparison (on the logit scale) gives a 95% credible interval that overlaps zero:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="ranef.html#cb132-1" tabindex="-1"></a><span class="fu">HPDinterval</span>(mbinom<span class="fl">.2</span><span class="sc">$</span>Sol[, <span class="st">&quot;person.darren_o&quot;</span>] <span class="sc">-</span> mbinom<span class="fl">.2</span><span class="sc">$</span>Sol[, <span class="st">&quot;person.craig_w&quot;</span>])</span></code></pre></div>
<pre><code>##           lower    upper
## var1 -0.1010677 2.502953
## attr(,&quot;Probability&quot;)
## [1] 0.95</code></pre>
</div>
<div id="overdispersed-binomial-as-a-bernoulli-glmm" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Overdispersed Binomial as a Bernoulli GLMM<a href="ranef.html#overdispersed-binomial-as-a-bernoulli-glmm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <span class="math inline">\(\texttt{Grumpy}\)</span> data set aggregates the scores of the 122 respondents into a single binomial response for each photograph. However, we could imagine disaggregating the data such that each respondent for each photograph gets a Bernoulli response with a success if they gave a particular photo a score greater than five. The disaggregated data (<code>FullGrumpy</code>) have <span class="math inline">\(122\times 44 = 5,368\)</span> observations (although a few respondents did not assess all photos).</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="ranef.html#cb134-1" tabindex="-1"></a><span class="fu">data</span>(FullGrumpy)</span>
<span id="cb134-2"><a href="ranef.html#cb134-2" tabindex="-1"></a><span class="fu">head</span>(FullGrumpy, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##   y   type photo person age ypub respondent student
## 1 9 grumpy  4511 ally_p  38   13          1      NO
## 2 8 grumpy  4511 ally_p  38   13          2      NO
## 3 3 grumpy  4511 ally_p  38   13          3      NO</code></pre>
<p><span class="math inline">\(\texttt{y}\)</span> is now the score each respondent <span class="math inline">\(\texttt{respondent}\)</span> gave each <span class="math inline">\(\texttt{photo}\)</span> (rather than the average score for each <span class="math inline">\(\texttt{photo}\)</span> in <code>Grumpy</code>). In addition, we have the respondent-level information <span class="math inline">\(\texttt{student}\)</span> which can be either <span class="math inline">\(\texttt{YES}\)</span> or <span class="math inline">\(\texttt{NO}\)</span>. We will turn each persons score into the Bernoulli response</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="ranef.html#cb136-1" tabindex="-1"></a>FullGrumpy<span class="sc">$</span>g5 <span class="ot">&lt;-</span> FullGrumpy<span class="sc">$</span>y <span class="sc">&gt;</span> <span class="dv">5</span></span></code></pre></div>
<p>and fit the model</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="ranef.html#cb137-1" tabindex="-1"></a>prior.mbinom<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>),</span>
<span id="cb137-2"><a href="ranef.html#cb137-2" tabindex="-1"></a>    <span class="at">G2 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>)))</span>
<span id="cb137-3"><a href="ranef.html#cb137-3" tabindex="-1"></a></span>
<span id="cb137-4"><a href="ranef.html#cb137-4" tabindex="-1"></a>mbinom<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(g5 <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">random =</span> <span class="sc">~</span>person <span class="sc">+</span> photo, <span class="at">data =</span> FullGrumpy,</span>
<span id="cb137-5"><a href="ranef.html#cb137-5" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">prior =</span> prior.mbinom<span class="fl">.3</span>)</span>
<span id="cb137-6"><a href="ranef.html#cb137-6" tabindex="-1"></a></span>
<span id="cb137-7"><a href="ranef.html#cb137-7" tabindex="-1"></a><span class="fu">summary</span>(mbinom<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## 
##  Iterations = 3001:12991
##  Thinning interval  = 10
##  Sample size  = 1000 
## 
##  DIC: 5332.549 
## 
##  G-structure:  ~person
## 
##        post.mean  l-95% CI u-95% CI eff.samp
## person     1.046 0.0005014    2.173    308.1
## 
##                ~photo
## 
##       post.mean l-95% CI u-95% CI eff.samp
## photo    0.8689   0.3223    1.717    210.6
## 
##  R-structure:  ~units
## 
##       post.mean l-95% CI u-95% CI eff.samp
## units         1        1        1        0
## 
##  Location effects: g5 ~ type + ypub 
## 
##             post.mean l-95% CI u-95% CI eff.samp  pMCMC    
## (Intercept)  -0.84769 -1.97064  0.42422    886.5  0.162    
## typehappy    -1.49164 -2.13927 -0.94842    749.9 &lt;0.001 ***
## ypub          0.02431 -0.02036  0.06655   1000.0  0.240    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The random effects deal with any variation in the probability of success across photos and are exactly comparable to the residuals of the binomial model <code>mbinom.2</code>. It is therefore surprising that the posterior distributions for both the variance in <span class="math inline">\(\texttt{person}\)</span> effects and the variance in <span class="math inline">\(\texttt{photo}\)</span>/residual effects appear to be different between the two models, as do the fixed effects. This is a peculiarity of logit-link models in <span class="math inline">\(\texttt{MCMC}\)</span> and wouldn’t be seen in <span class="math inline">\(\texttt{family=&quot;threshold&quot;}\)</span> models that implements the standard probit link. In the binomial model <code>mbinom.2</code> we implicitly assumed that the probability of success did not vary across respondents <em>within</em> photos - this was an assumption, and one that cannot be tested. In the Bernoulli model <code>mbinom.3</code> we explicitly assumed that the probability of success varied across respondents within photos, and the variance on the logit-scale was one. Since Bernoulli data provide no information about observation-level variability either (or most likely, neither) assumption could be true but we have no way of knowing (Section <a href="glm.html#bernoulli-sec">3.6.3</a>). As we saw with fixed effect coefficients in Bernoulli GLM, stating that the variances in photo effects is 0.869 is meaningless in a Bernoulli GLMM without putting it in the context of the assumed residual variance. The standard approach - what I refer to as the standard logit model - is to assume the residual variance is zero. While I think this is a good standard, this is prohibited in <span class="math inline">\(\texttt{MCMCglmm}\)</span> because the chain will not mix. But as we saw with the fixed effects, we can rescale the variances by needs to be multiplied by <span class="math inline">\(1/(1+c^{2}\sigma^{2}_{\texttt{units}})\)</span> where <span class="math inline">\(c=16\sqrt{3}/15\pi\)</span> and <span class="math inline">\(\sigma^{2}_{\texttt{units}}\)</span> is our assumed residual variance, which is one (Figure <a href="ranef.html#fig:bernoulli-rescale2">4.1</a>).</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="ranef.html#cb139-1" tabindex="-1"></a>c2 <span class="ot">&lt;-</span> ((<span class="dv">16</span> <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">3</span>))<span class="sc">/</span>(<span class="dv">15</span> <span class="sc">*</span> pi))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb139-2"><a href="ranef.html#cb139-2" tabindex="-1"></a></span>
<span id="cb139-3"><a href="ranef.html#cb139-3" tabindex="-1"></a>rescale.VCV<span class="fl">.2</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.2</span><span class="sc">$</span>VCV</span>
<span id="cb139-4"><a href="ranef.html#cb139-4" tabindex="-1"></a><span class="fu">colnames</span>(rescale.VCV<span class="fl">.2</span>)[<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="st">&quot;photo&quot;</span></span>
<span id="cb139-5"><a href="ranef.html#cb139-5" tabindex="-1"></a></span>
<span id="cb139-6"><a href="ranef.html#cb139-6" tabindex="-1"></a>rescale.VCV<span class="fl">.3</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.3</span><span class="sc">$</span>VCV[, <span class="fu">c</span>(<span class="st">&quot;person&quot;</span>, <span class="st">&quot;photo&quot;</span>)]<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> c2)</span>
<span id="cb139-7"><a href="ranef.html#cb139-7" tabindex="-1"></a></span>
<span id="cb139-8"><a href="ranef.html#cb139-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(<span class="fu">as.mcmc</span>(rescale.VCV<span class="fl">.2</span>), <span class="fu">as.mcmc</span>(rescale.VCV<span class="fl">.3</span>)), <span class="at">density =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:bernoulli-rescale2"></span>
<img src="_bookdown_files/fig/bernoulli-rescale2-1.png" alt="MCMC traces for the estimated variances in $\texttt{person}$ and $\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`) of individual data (red) and a Binomial GLMM (model `mbinom.2`) where all data for a photo have been aggregated into a single Binomial response (black). The posterior distribution of the variances from the Bernoulli GLMM have been rescaled to what would be observed if the residual variance was zero (rather than one)." width="672" />
<p class="caption">
Figure 4.1: MCMC traces for the estimated variances in <span class="math inline">\(\texttt{person}\)</span> and <span class="math inline">\(\texttt{photo}\)</span> effects from a Bernoulli GLMM (model <code>mbinom.3</code>) of individual data (red) and a Binomial GLMM (model <code>mbinom.2</code>) where all data for a photo have been aggregated into a single Binomial response (black). The posterior distribution of the variances from the Bernoulli GLMM have been rescaled to what would be observed if the residual variance was zero (rather than one).
</p>
</div>
</div>
<div id="ICC" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Intra-class Correlations<a href="ranef.html#ICC" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A more common approach, however, is to express the variances as intra-class correlations where we take the variance of interest and express it as a proportion of the total. For example, for the <span class="math inline">\(\texttt{person}\)</span> effects, the intra-class correlation would be</p>
<p><span class="math display">\[ICC = \frac{\sigma^2_{\texttt{person}}}{\sigma^2_{\texttt{person}}+\sigma^2_{\texttt{photo}}+\sigma^2_{\texttt{units}}+\pi^2/3}\]</span></p>
<p>where the <span class="math inline">\(\pi^2/3\)</span> appears because we have used the logit link and this is the link variance (the variance of the unit logistic).</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="ranef.html#cb140-1" tabindex="-1"></a>ICC<span class="fl">.2</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.2</span><span class="sc">$</span>VCV<span class="sc">/</span>(<span class="fu">rowSums</span>(mbinom<span class="fl">.2</span><span class="sc">$</span>VCV) <span class="sc">+</span> pi<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>)</span>
<span id="cb140-2"><a href="ranef.html#cb140-2" tabindex="-1"></a><span class="fu">colnames</span>(ICC<span class="fl">.2</span>)[<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="st">&quot;photo&quot;</span></span>
<span id="cb140-3"><a href="ranef.html#cb140-3" tabindex="-1"></a></span>
<span id="cb140-4"><a href="ranef.html#cb140-4" tabindex="-1"></a>ICC<span class="fl">.3</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.3</span><span class="sc">$</span>VCV[, <span class="fu">c</span>(<span class="st">&quot;person&quot;</span>, <span class="st">&quot;photo&quot;</span>)]<span class="sc">/</span>(<span class="fu">rowSums</span>(mbinom<span class="fl">.3</span><span class="sc">$</span>VCV) <span class="sc">+</span> pi<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>)</span>
<span id="cb140-5"><a href="ranef.html#cb140-5" tabindex="-1"></a></span>
<span id="cb140-6"><a href="ranef.html#cb140-6" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(<span class="fu">as.mcmc</span>(ICC<span class="fl">.2</span>), <span class="fu">as.mcmc</span>(ICC<span class="fl">.3</span>)), <span class="at">density =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ICC"></span>
<img src="_bookdown_files/fig/ICC-1.png" alt="MCMC traces for the estimated intra-class correlation for $\texttt{person}$ and $\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`) of individual data (red) and a Binomial GLMM (model `mbinom.2`) where all data for a photo have been aggregated into a single Binomial response (black)." width="672" />
<p class="caption">
Figure 4.2: MCMC traces for the estimated intra-class correlation for <span class="math inline">\(\texttt{person}\)</span> and <span class="math inline">\(\texttt{photo}\)</span> effects from a Bernoulli GLMM (model <code>mbinom.3</code>) of individual data (red) and a Binomial GLMM (model <code>mbinom.2</code>) where all data for a photo have been aggregated into a single Binomial response (black).
</p>
</div>
<p>If we had used <span class="math inline">\(\texttt{family=&quot;threshold&quot;}\)</span> this would be omitted because the link variance is zero as we are already working on the … scale<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a> Pierre de Villemereuil’s <a href="https://cran.r-project.org/web/packages/QGglmm/index.html"><span class="math inline">\(\texttt{QGLMM}\)</span></a> package.</p>
<p>The eagle-eyed will have noticed that although the trace plots for the rescaled variance/intra-class correlation for the <span class="math inline">\(\texttt{person}\)</span> effects look identical between the two models, the trace plots for the <span class="math inline">\(\texttt{photo}\)</span> effects look slightly different with the posterior from model <code>mbinom.3</code> (the Bernoulli GLMM in red) appearing to have more density at higher values. However, this difference is due to Monte Carlo error, which is quite high for the variance estimates because the autocorrelation in the chain is moderate. The reported effective sample size in the model summary gives some indication of this - for example in the Bernoulli model the effective sample size for the variance in <span class="math inline">\(\texttt{photo}\)</span> effects is 211, quite a bit less than the 1,000 samples saved (Section <a href="bayesian.html#diagnostics-sec">2.5.4</a>). We can see this more clearly if we just plot the traces for Bernoulli model (Figure <a href="ranef.html#fig:bernoulli-trace">4.3</a>).</p>
<div class="figure"><span style="display:block;" id="fig:bernoulli-trace"></span>
<img src="_bookdown_files/fig/bernoulli-trace-1.png" alt="MCMC trace for the variances in $\texttt{person}$ and $\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`)." width="672" />
<p class="caption">
Figure 4.3: MCMC trace for the variances in <span class="math inline">\(\texttt{person}\)</span> and <span class="math inline">\(\texttt{photo}\)</span> effects from a Bernoulli GLMM (model <code>mbinom.3</code>).
</p>
</div>
<p>Two things are apparent from Figure <a href="ranef.html#fig:bernoulli-trace">4.3</a>. Autocorrelation is present - this is not surprising: for each iteration of the MCMC chain the random effects are Gibbs sampled conditional on their variance in the previous iteration, and then conditional on the updated random effects the variances are then Gibbs sampled (Section <a href="bayesian.html#MCMC">2.5</a>). This will invariably lead to autocorrelation. Second, the trace for the variance in <span class="math inline">\(\texttt{person}\)</span> effects appears to intermittently get ‘stuck’ at values close to zero. In part, this reflects the mechanics of the Gibbs sampling, but it is also a consequence of the inverse-Wishart prior used which has a sharp peak in density at small values (Section <a href="bayesian.html#Vprior-sec">2.6</a>). When the variance in <span class="math inline">\(\texttt{person}\)</span> effects gets ‘stuck’ at zero, the variance in <span class="math inline">\(\texttt{photo}\)</span> effects appears to get ‘stuck’ at high values. This is because the data provide strong support for the combined effect of <span class="math inline">\(\texttt{photo}\)</span> and <span class="math inline">\(\texttt{person}\)</span> being large, but contain less information about their separate effects. Consequently, if the <span class="math inline">\(\texttt{person}\)</span> variance drops to zero, the <span class="math inline">\(\texttt{photo}\)</span> variance increases to compensate. In this example, the effects described above are quite subtle, and simply running the chain for longer would probably suffice. However, a better general strategy would be to employ parameter expansion and use scaled non-central <span class="math inline">\(F\)</span> priors.</p>
</div>
<div id="PXprior-sec" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Priors for Random Effect Variances<a href="ranef.html#PXprior-sec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Parameter expansion is an algorithmic trick for speeding up the mixing and convergence of the MCMC chain. An unintended but useful side-effect of parameter expansion is that it can allow a wider class of prior distributions while still permitting Gibbs sampling (Chapter <a href="bayesian.html#bayesian">2</a>). In order to explore parameter expansion, and the associated <span class="math inline">\(F\)</span> prior for random-effect variances, we will work with a model and data-set where the issues noted for model <code>mbinom.3</code> are much more obvious - the Schools example discussed in <span class="citation">Gelman (<a href="#ref-Gelman.2006">2006</a>)</span>.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="ranef.html#cb141-1" tabindex="-1"></a>schools <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">school =</span> letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>], <span class="at">estimate =</span> <span class="fu">c</span>(<span class="fl">28.39</span>, <span class="fl">7.94</span>, <span class="sc">-</span><span class="fl">2.75</span>, <span class="fl">6.82</span>,</span>
<span id="cb141-2"><a href="ranef.html#cb141-2" tabindex="-1"></a>    <span class="sc">-</span><span class="fl">0.64</span>, <span class="fl">0.63</span>, <span class="fl">18.01</span>, <span class="fl">12.16</span>), <span class="at">ve =</span> <span class="fu">c</span>(<span class="fl">14.9</span>, <span class="fl">10.2</span>, <span class="fl">16.3</span>, <span class="dv">11</span>, <span class="fl">9.4</span>, <span class="fl">11.4</span>, <span class="fl">10.4</span>, <span class="fl">17.6</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb141-3"><a href="ranef.html#cb141-3" tabindex="-1"></a><span class="fu">head</span>(schools)</span></code></pre></div>
<pre><code>##   school estimate     ve
## 1      a    28.39 222.01
## 2      b     7.94 104.04
## 3      c    -2.75 265.69
## 4      d     6.82 121.00
## 5      e    -0.64  88.36
## 6      f     0.63 129.96</code></pre>
<p>The response variable <code>estimate</code> is the relative effect of Scholastic Aptitude Test coaching programs in 8 schools and <span class="citation">Gelman (<a href="#ref-Gelman.2006">2006</a>)</span> focusses on the variance in school effects. Since we only have a single estimate per school there will be a one-to-one mapping between <span class="math inline">\(\texttt{school}\)</span> effects and the residual. In most cases this would result in the variance in school effects being confounded with the residual variance. Here, however, we have been gifted the residual (within school) variance (<span class="math inline">\(\texttt{ve}\)</span>) which varies from school to school. In reality, these residual variances are actually estimates and we might wish to factor in this additional complication, but for now we will ignore this complexity and come back to it in Chapter <a href="#measurement"><strong>??</strong></a>. First, lets fit the inverse-Wishart prior we have been using up to now with <span class="math inline">\(\texttt{V}=1\)</span> and <span class="math inline">\(\texttt{nu}=0.002\)</span>. This prior is equivalent to an inverse-gamma prior with a shape and scale of 0.001 (Section <a href="bayesian.html#Vprior-sec">2.6</a>):</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="ranef.html#cb143-1" tabindex="-1"></a>prior.mschool<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb143-2"><a href="ranef.html#cb143-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="fl">0.002</span>)))</span>
<span id="cb143-3"><a href="ranef.html#cb143-3" tabindex="-1"></a></span>
<span id="cb143-4"><a href="ranef.html#cb143-4" tabindex="-1"></a>mschool<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb143-5"><a href="ranef.html#cb143-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.1</span>)</span></code></pre></div>
<p>The model contains an argument we haven’t seen before: <span class="math inline">\(\texttt{rcov}\)</span>. In all previous analyses we used the default <code>~units</code> which fits a set of independent and identically distributed residuals with a single variance (<span class="math inline">\(\sigma^2_{\texttt{units}}\)</span>) to be estimated. <span class="math inline">\(\texttt{MCMCglmm}\)</span> allows this assumption to be relaxed, and Chapter <a href="cat-int.html#cat-int">5</a> is dedicated to this subject. Here, we will simply note that we have assigned each school a residual variance (the corresponding element of <span class="math inline">\(\texttt{ve}\)</span>) and fixed it at this value, leaving only the intercept and the variance in school effects (<span class="math inline">\(\sigma^2_{\texttt{school}}\)</span>) to be estimated. The MCMC trace for <span class="math inline">\(\sigma^2_{\texttt{school}}\)</span> looks dreadful (Figure <a href="ranef.html#fig:mschool-1">4.4</a>).</p>
<div class="figure"><span style="display:block;" id="fig:mschool-1"></span>
<img src="_bookdown_files/fig/mschool-1-1.png" alt="MCMC trace for the variance in $\texttt{school}$ effects from model `mschool.1` in which an inverse-Wishart prior was used with $\texttt{V=1}$ and $\texttt{nu=0.002}.$" width="672" />
<p class="caption">
Figure 4.4: MCMC trace for the variance in <span class="math inline">\(\texttt{school}\)</span> effects from model <code>mschool.1</code> in which an inverse-Wishart prior was used with <span class="math inline">\(\texttt{V=1}\)</span> and <span class="math inline">\(\texttt{nu=0.002}.\)</span>
</p>
</div>
<p>The autocorrelation in the chain is evident and there appears to be a lot of posterior density at zero. These are separate issues. Autocorrelation in the chain is due to algorithmic inefficiencies in sampling the posterior distribution, whereas a lot of posterior density near zero reflects the combined information coming from the data and coming from the prior (Chapter <a href="bayesian.html#bayesian">2</a>). Certainly, some posterior distributions are harder to sample from than others, and the efficiency of the MCMC algorithm may decrease when the posterior is close to zero. But if the chain can be run long enough that these inefficiencies are not consequential, situations where the posterior has ‘too much’ density near zero indicate potential problems with the prior, not algorithmic problems.</p>
<div id="f-and-folded-t-priors" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> <span class="math inline">\(F\)</span> and folded-<span class="math inline">\(t\)</span> priors<a href="ranef.html#f-and-folded-t-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the inverse-Wishart prior we specified the parameters <span class="math inline">\(\texttt{V}\)</span> and <span class="math inline">\(\texttt{nu}\)</span>. The parameters <span class="math inline">\(\texttt{alpha.mu}\)</span> and <span class="math inline">\(\texttt{alpha.V}\)</span> can also be specified in the prior, and if <span class="math inline">\(\texttt{alpha.V}\)</span> is non-zero then parameter expansion is used. These additional parameters specify a prior for the variance, <span class="math inline">\(\sigma^2_{\texttt{school}}\)</span>, that is a non-central scaled F-distribution with the numerator degrees-of-freedom set to one. We can also think about this prior in terms of the standard deviation, <span class="math inline">\(\sigma_{\texttt{school}}\)</span>, which in some ways is more natural, since it is in the same units as the response. The prior distribution for the standard deviation is a folded scaled non-central <span class="math inline">\(t\)</span>. As the length of their names suggest, these two distributions are quite complicated. Regrettably, this complication is exacerbated in <span class="math inline">\(\texttt{MCMCglmm}\)</span> by specifying the prior through the distribution of the parameter expansion working parameters (See Section <a href="#px-sec"><strong>??</strong></a>). I will leave the full relationship between the prior specification and the <span class="math inline">\(F\)</span> and <span class="math inline">\(t\)</span> distributions to the footnote<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>, and introduce two simplifications here. First, these distributions have three free parameters, yet the prior specification has four. Without loss of generality we will use <span class="math inline">\(\texttt{V}=1\)</span> throughout. Then, <span class="math inline">\(\texttt{alpha.V}\)</span> specifies the scale of the <span class="math inline">\(F\)</span> prior for the variance and <span class="math inline">\(\sqrt{\texttt{alpha.V}}\)</span> specifies the scale of the <span class="math inline">\(t\)</span> prior for the standard deviation. Second, since the non-central forms of these distributions are rarely - if ever - used as priors, we will set the non-centrality parameter to zero via <span class="math inline">\(\texttt{alpha.mu}=0\)</span>. For the standard deviation prior this results in the added simplification that the folded-<span class="math inline">\(t\)</span> becomes the half-<span class="math inline">\(t\)</span> (essentially a <span class="math inline">\(t\)</span> with the negative values missing). We are then left with two-parameter distributions with the scale set by <span class="math inline">\(\texttt{alpha.V}\)</span> and the (denominator) degrees-of-freedom, <span class="math inline">\(\texttt{nu}\)</span>.</p>
<p>Before discussing the properties of the <span class="math inline">\(F\)</span> and half-<span class="math inline">\(t\)</span> priors on the posterior distributions of variances and standard deviations, respectively, let’s confirm that parameter expansion does indeed increase efficiency independent of the prior being used. In Section <a href="bayesian.html#Vprior-sec">2.6</a> we saw that an improper inverse-Wishart distribution with <span class="math inline">\(\texttt{V}=0\)</span> and <span class="math inline">\(\texttt{nu}=-1\)</span> is flat for the standard deviation. A half-<span class="math inline">\(t\)</span> with <span class="math inline">\(\texttt{nu}=-1\)</span> is also an improper flat prior on the standard deviation irrespective of what is specified for the other parameters. Let’s fit these flat improper priors with and without parameter expansion:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="ranef.html#cb144-1" tabindex="-1"></a>prior.nopx <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fl">1e-16</span>,</span>
<span id="cb144-2"><a href="ranef.html#cb144-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="sc">-</span><span class="dv">1</span>)))</span>
<span id="cb144-3"><a href="ranef.html#cb144-3" tabindex="-1"></a></span>
<span id="cb144-4"><a href="ranef.html#cb144-4" tabindex="-1"></a>m.nopx <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units, <span class="at">data =</span> schools,</span>
<span id="cb144-5"><a href="ranef.html#cb144-5" tabindex="-1"></a>    <span class="at">prior =</span> prior.nopx)</span>
<span id="cb144-6"><a href="ranef.html#cb144-6" tabindex="-1"></a><span class="co"># flat prior on the school standard deviation - no parameter expansion</span></span>
<span id="cb144-7"><a href="ranef.html#cb144-7" tabindex="-1"></a></span>
<span id="cb144-8"><a href="ranef.html#cb144-8" tabindex="-1"></a>prior.px <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fl">1e-16</span>,</span>
<span id="cb144-9"><a href="ranef.html#cb144-9" tabindex="-1"></a>    <span class="at">nu =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="dv">1000</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb144-10"><a href="ranef.html#cb144-10" tabindex="-1"></a><span class="co"># note I have set V=1e-16 rather than V=1: with nu=-1, V does not influence the</span></span>
<span id="cb144-11"><a href="ranef.html#cb144-11" tabindex="-1"></a><span class="co"># prior</span></span>
<span id="cb144-12"><a href="ranef.html#cb144-12" tabindex="-1"></a></span>
<span id="cb144-13"><a href="ranef.html#cb144-13" tabindex="-1"></a>m.px <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units, <span class="at">data =</span> schools,</span>
<span id="cb144-14"><a href="ranef.html#cb144-14" tabindex="-1"></a>    <span class="at">prior =</span> prior.px)</span>
<span id="cb144-15"><a href="ranef.html#cb144-15" tabindex="-1"></a><span class="co"># flat prior on the school standard deviation - with parameter expansion</span></span></code></pre></div>
<p>We can see that these two models are sampling from the same posterior (Figure <a href="ranef.html#fig:mschool-2">4.5</a>) but the efficiency of the algorithm is greater when parameter expansion is used, with an effective posterior sample size of 877 rather than 404.</p>
<div class="figure"><span style="display:block;" id="fig:mschool-2"></span>
<img src="_bookdown_files/fig/mschool-2-1.png" alt="MCMC traces for the standard deviation in $\texttt{school}$ effects. In black is the trace for model `m.nopx` in which an improper inverse-Wishart prior was used for the variance with $\texttt{V=0}$ and $\texttt{nu=-1}$. In red is the trace for model `m.px` in which an improper $F_{1,-1}$ prior was used with $\texttt{nu=-1}$ and $$\texttt{alpha.V}=1000^2$. Both priors are flat for the standard deviation but `m.px` employs parameter expansion." width="672" />
<p class="caption">
Figure 4.5: MCMC traces for the standard deviation in <span class="math inline">\(\texttt{school}\)</span> effects. In black is the trace for model <code>m.nopx</code> in which an improper inverse-Wishart prior was used for the variance with <span class="math inline">\(\texttt{V=0}\)</span> and <span class="math inline">\(\texttt{nu=-1}\)</span>. In red is the trace for model <code>m.px</code> in which an improper <span class="math inline">\(F_{1,-1}\)</span> prior was used with <span class="math inline">\(\texttt{nu=-1}\)</span> and $<span class="math inline">\(\texttt{alpha.V}=1000^2\)</span>. Both priors are flat for the standard deviation but <code>m.px</code> employs parameter expansion.
</p>
</div>
<p>While parameter expansion usually results in more efficient sampling of the posterior, to justify its use, it is important that it also allows us to specify priors with sensible properties. Those with Bayesian scruple may baulk at the use of an improper prior. However, if we specify <span class="math inline">\(\texttt{nu=1}\)</span> (a half-<span class="math inline">\(t_1\)</span> or half-Cauchy) we can specify a proper prior that becomes flat for the standard deviation as we increase the scale. Let’s fit a proper half-Cauchy with a scale that is large (relative to the data) - <span class="math inline">\(\sqrt{\texttt{alpha.V}}=1000\)</span>.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="ranef.html#cb145-1" tabindex="-1"></a>prior.mschool<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb145-2"><a href="ranef.html#cb145-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">1</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="dv">1000</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb145-3"><a href="ranef.html#cb145-3" tabindex="-1"></a><span class="co"># half-Cauchy prior on the standard-deviation with scale 1000</span></span>
<span id="cb145-4"><a href="ranef.html#cb145-4" tabindex="-1"></a></span>
<span id="cb145-5"><a href="ranef.html#cb145-5" tabindex="-1"></a>mschool<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb145-6"><a href="ranef.html#cb145-6" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.px)</span>
<span id="cb145-7"><a href="ranef.html#cb145-7" tabindex="-1"></a><span class="co"># flatish prior on the school standard deviation</span></span></code></pre></div>
<p>Overlaying the trace on those obtained under flat improper priors shows that the posterior distributions for <span class="math inline">\(\sigma_\texttt{school}\)</span> are indistinguishable (Figure <a href="ranef.html#fig:mschool-3">4.6</a>).</p>
<div class="figure"><span style="display:block;" id="fig:mschool-3"></span>
<img src="_bookdown_files/fig/mschool-3-1.png" alt="MCMC traces for the standard deviation in $\texttt{school}$ effects. In black is the trace for model `m.nopx` in which an improper inverse-Wishart prior was used for the variance with $\texttt{V=0}$ and $\texttt{nu=-1}$. In red is the trace for model `m.px` in which an improper $t_{-1}$ prior was used for the standard deviation with $\texttt{alpha.V}$ set to $1000^2$. Both priors are flat for the standard deviation but `m.px` employs parameter expansion. In green is the trace for model `mschool.2` in which a proper half-Cauchy ($t_{1}$) with a scale of $1000$ was used. This prior is almost flat for the standard deviation over the range of values that have reasonable support." width="672" />
<p class="caption">
Figure 4.6: MCMC traces for the standard deviation in <span class="math inline">\(\texttt{school}\)</span> effects. In black is the trace for model <code>m.nopx</code> in which an improper inverse-Wishart prior was used for the variance with <span class="math inline">\(\texttt{V=0}\)</span> and <span class="math inline">\(\texttt{nu=-1}\)</span>. In red is the trace for model <code>m.px</code> in which an improper <span class="math inline">\(t_{-1}\)</span> prior was used for the standard deviation with <span class="math inline">\(\texttt{alpha.V}\)</span> set to <span class="math inline">\(1000^2\)</span>. Both priors are flat for the standard deviation but <code>m.px</code> employs parameter expansion. In green is the trace for model <code>mschool.2</code> in which a proper half-Cauchy (<span class="math inline">\(t_{1}\)</span>) with a scale of <span class="math inline">\(1000\)</span> was used. This prior is almost flat for the standard deviation over the range of values that have reasonable support.
</p>
</div>
<p>Is a flat(ish) prior on the standard deviation more sensible than the inverse-Wishart prior on the variance we have been using previously (<span class="math inline">\(\texttt{V}=1\)</span> and <span class="math inline">\(\texttt{nu}\)</span>=0.002)? The short answer is yes. Setting <span class="math inline">\(\texttt{nu}\)</span> to be small in the inverse-Wishart is motivated by the idea that having few prior degrees of freedom provides a diffuse prior on the variance. However, as <span class="math inline">\(\texttt{nu}\)</span> becomes smaller the prior only becomes flat for the <em>log</em> of the variance, and there is a nasty spike at small values for both the variance (Section <a href="bayesian.html#Vprior-sec">2.6</a>) and the standard deviation. However, in some cases, as in the Schools example, the amount of information provided by the data may be so small that a flat prior on the standard deviation might be to vague. In such cases, reducing the scale has been advised, and <span class="citation">Gelman (<a href="#ref-Gelman.2006">2006</a>)</span> used a scale of 25 for this example:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="ranef.html#cb146-1" tabindex="-1"></a>prior.mschool<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb146-2"><a href="ranef.html#cb146-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">1</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="dv">25</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb146-3"><a href="ranef.html#cb146-3" tabindex="-1"></a></span>
<span id="cb146-4"><a href="ranef.html#cb146-4" tabindex="-1"></a>mschool<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb146-5"><a href="ranef.html#cb146-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.3</span>)</span></code></pre></div>
<p>When reducing the scale, some care has to be taken that the scale is aligned with the scale of the data. For example, the common ‘default’ recommendation of setting the scale to 5 might prove problematic in this example where the standard deviation of the data exceeds one by some margin:</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="ranef.html#cb147-1" tabindex="-1"></a>prior.mschool<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb147-2"><a href="ranef.html#cb147-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">1</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="dv">5</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb147-3"><a href="ranef.html#cb147-3" tabindex="-1"></a></span>
<span id="cb147-4"><a href="ranef.html#cb147-4" tabindex="-1"></a>mschool<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb147-5"><a href="ranef.html#cb147-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.4</span>)</span></code></pre></div>
<p>The default in brms <span class="citation">(<a href="#ref-Burkner.2017">Bürkner 2017</a>)</span> is to use a half-<span class="math inline">\(t\)</span> with 3 degrees of freedom and a scale of 2.5. The tails of this distribution are less ‘heavy’ than the Cauchy and in conjunction with the reduced scale penalise large values more strongly.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="ranef.html#cb148-1" tabindex="-1"></a>prior.mschool<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="fu">diag</span>(schools<span class="sc">$</span>ve), <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb148-2"><a href="ranef.html#cb148-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">3</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="fl">2.5</span><span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb148-3"><a href="ranef.html#cb148-3" tabindex="-1"></a></span>
<span id="cb148-4"><a href="ranef.html#cb148-4" tabindex="-1"></a>mschool<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(estimate <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>school, <span class="at">rcov =</span> <span class="sc">~</span><span class="fu">idh</span>(school)<span class="sc">:</span>units,</span>
<span id="cb148-5"><a href="ranef.html#cb148-5" tabindex="-1"></a>    <span class="at">data =</span> schools, <span class="at">prior =</span> prior.mschool<span class="fl">.5</span>)</span></code></pre></div>
<p>When comparing these priors visually we see that they are quite different, despite all having some history as default and/or recommended priors (Figure <a href="ranef.html#fig:prior-compare">4.7</a>).</p>
<div class="figure"><span style="display:block;" id="fig:prior-compare"></span>
<img src="_bookdown_files/fig/prior-compare-1.png" alt="Prior probability densities for the standard deviation of $\texttt{school}$ effects used in the $\texttt{mschool}$ models $\texttt{1-5}$.  The scale of the half-Cauchy and half-t distributions are given after the colon. Note the inverse-gamma is the prior for the variance - not the standard deviation." width="672" />
<p class="caption">
Figure 4.7: Prior probability densities for the standard deviation of <span class="math inline">\(\texttt{school}\)</span> effects used in the <span class="math inline">\(\texttt{mschool}\)</span> models <span class="math inline">\(\texttt{1-5}\)</span>. The scale of the half-Cauchy and half-t distributions are given after the colon. Note the inverse-gamma is the prior for the variance - not the standard deviation.
</p>
</div>
<p>With so little information coming from the data, these priors also have considerable influence on the posterior (Figure <a href="ranef.html#fig:posterior-compare">4.8</a>). When <span class="math inline">\(\texttt{V}\)</span>=1 and <span class="math inline">\(\texttt{nu}\)</span>=0.002, small values are strongly favoured, and this is also true to a lesser extent for the half-<span class="math inline">\(t_3\)</span> prior with low scale. As we up the scale and/or reduce the degree-of-freedom to one (the half-Cauchy) the prior becomes flatter and the resulting posteriors are more similar.</p>
<div class="figure"><span style="display:block;" id="fig:posterior-compare"></span>
<img src="_bookdown_files/fig/posterior-compare-1.png" alt="Posterior probability densities for the standard deviation of $\texttt{school}$ effects used in the $\texttt{mschool}$ models $\texttt{1-5}$.  The priors used in these models are given in the legend with the scale of the half-Cauchy and half-t distributions following the colon. Note the inverse-gamma is the prior for the variance - not the standard deviation." width="672" />
<p class="caption">
Figure 4.8: Posterior probability densities for the standard deviation of <span class="math inline">\(\texttt{school}\)</span> effects used in the <span class="math inline">\(\texttt{mschool}\)</span> models <span class="math inline">\(\texttt{1-5}\)</span>. The priors used in these models are given in the legend with the scale of the half-Cauchy and half-t distributions following the colon. Note the inverse-gamma is the prior for the variance - not the standard deviation.
</p>
</div>
<p>Figure <a href="ranef.html#fig:posterior-compare">4.8</a> is anxiety-inducing - which prior to use? While I am reluctant to give general advise, in my own work I tend to use a half-Cauchy with a large scale: often <span class="math inline">\(\sqrt{1000}\)</span> (<code>alpha.V=1000</code>) depending on the scale of the data. However, I try to avoid collecting and analysing data that contains as little information as seen in this example (but see Section <a href="ranef.html#fix-or-rand">4.8</a>). When this is the case, shifts in the posterior distribution under different priors are often subtle. However, for variances or standard deviations that have support close to zero the spike of the inverse-Wishart/inverse-gamma distribution can cause problems, and for this reason I usually avoid it. The added bonus is faster mixing under parameter expansion. While the prior for the residual variance is restricted to the the inverse-Wishart/inverse-gamma distribution in MCMCglmm (Section <a href="bayesian.html#Vprior-sec">2.6</a>) the data usually contains so much information for the residual variance that prior sensitivity is limited. For models where the random-effect specification defines a (co)variance matrix rather than a scalar variance, see Section <a href="cat-int.html#VCVprior-sec">5.3</a>.</p>
</div>
</div>
<div id="Vprior-gen-sec" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Prior Generators<a href="ranef.html#Vprior-gen-sec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Even when a user knows which prior they would like to use, it can be rather fiddly generating the prior list, especially when there are many random terms, some of which define (co)variance matrices. In order to simplify the process of prior specification a set of prior generator functions are available in versions &gt;4.0. These functions <span class="math inline">\(\texttt{IW}\)</span> (inverse-Wishart), <span class="math inline">\(\texttt{IG}\)</span> (inverse-gamma) and <span class="math inline">\(\texttt{F}\)</span> (central <span class="math inline">\(F\)</span> with 1 numerator degree of freedom) take two arguments specifying the parameters of the prior distribution to be used for the variances. The function <span class="math inline">\(\texttt{tSD}\)</span> can also be used which specifies a half-<span class="math inline">\(t\)</span>-distribution for the standard deviation. The functions come with default values, but these can be overridden (Table <a href="#fig:prior-functions"><strong>??</strong></a>). Note that a set parameters can always be chosen such that <span class="math inline">\(\texttt{IW}=\texttt{IG}\)</span> and <span class="math inline">\(\texttt{F}=\texttt{tSD}\)</span>. The difficult topic of prior specification for (co)variance matrices is covered in Section <a href="cat-int.html#VCVprior-sec">5.3</a> and I’ll leave the documentation of the prior generator functions in this context to then.</p>
<div style="border: 0px;overflow-x: scroll; width:100%; ">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:prior-functions">Table 4.1: </span>Table of prior generator functions for specifying the (marginal) prior of a variance (<span class="math inline">\(\texttt{IW}\)</span>, <span class="math inline">\(\texttt{IG}\)</span> and <span class="math inline">\(\texttt{F}\)</span>) or a standard deviation (<span class="math inline">\(\texttt{tSD}\)</span>). The two parameters for each distribution are given together with their defaults.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Function
</th>
<th style="text-align:left;">
Distribution
</th>
<th style="text-align:left;">
Parameter1
</th>
<th style="text-align:left;">
Parameter2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
IW
</td>
<td style="text-align:left;">
Inverse-Wishart
</td>
<td style="text-align:left;">
V=1
</td>
<td style="text-align:left;">
nu=0.002
</td>
</tr>
<tr>
<td style="text-align:left;">
IG
</td>
<td style="text-align:left;">
Inverse-gamma
</td>
<td style="text-align:left;">
shape=0.001
</td>
<td style="text-align:left;">
scale=0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
F
</td>
<td style="text-align:left;">
F with df1=1
</td>
<td style="text-align:left;">
df2=1
</td>
<td style="text-align:left;">
scale=1000
</td>
</tr>
<tr>
<td style="text-align:left;">
tSD
</td>
<td style="text-align:left;">
Half-t (for SD)
</td>
<td style="text-align:left;">
df=1
</td>
<td style="text-align:left;">
scale=<span class="math inline">\(\sqrt{1000}\)</span>
</td>
</tr>
</tbody>
</table>
</div>
<p>The prior generator functions can be used to specify the prior for a particular term, or all <span class="math inline">\(\texttt{G}\)</span> and <span class="math inline">\(\texttt{R}\)</span> terms. For example, for model <code>mbinom.3</code> we specified priors for the single residual variance and the two random effect variances:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="ranef.html#cb149-1" tabindex="-1"></a>prior.mbinom<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>),</span>
<span id="cb149-2"><a href="ranef.html#cb149-2" tabindex="-1"></a>    <span class="at">G2 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>)))</span></code></pre></div>
<p>If we wished to use a central <span class="math inline">\(F_{1,1}\)</span> prior with a scale of a <span class="math inline">\(1000\)</span> for the variance in the first random term, the long-hand prior would look like:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="ranef.html#cb150-1" tabindex="-1"></a>prior.mbinom<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="dv">1</span>,</span>
<span id="cb150-2"><a href="ranef.html#cb150-2" tabindex="-1"></a>    <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="dv">1000</span>), <span class="at">G2 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>)))</span></code></pre></div>
<p>However, we can also specify this prior as</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="ranef.html#cb151-1" tabindex="-1"></a>prior.mbinom<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">F</span>(<span class="dv">1</span>, <span class="dv">1000</span>), <span class="at">G2 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb151-2"><a href="ranef.html#cb151-2" tabindex="-1"></a>    <span class="at">nu =</span> <span class="fl">0.002</span>)))</span></code></pre></div>
<p>If we wished to also use the <span class="math inline">\(F\)</span> prior for the second random term we could either use</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="ranef.html#cb152-1" tabindex="-1"></a>prior.mbinom<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">F</span>(<span class="dv">1</span>, <span class="dv">1000</span>), <span class="at">G2 =</span> <span class="fu">F</span>(<span class="dv">1</span>,</span>
<span id="cb152-2"><a href="ranef.html#cb152-2" tabindex="-1"></a>    <span class="dv">1000</span>)))</span></code></pre></div>
<p>or more compactly as:</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="ranef.html#cb153-1" tabindex="-1"></a>prior.mbinom<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">F</span>(<span class="dv">1</span>, <span class="dv">1000</span>))</span></code></pre></div>
<p>The prior generators can also be used for the residual variance(s) but because we need to specify additional arguments (<span class="math inline">\(\texttt{fix}\)</span> in this case) we have to stick with the long-hand version.</p>
</div>
<div id="priors-on-functions-of-variances" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Priors on Functions of Variances<a href="ranef.html#priors-on-functions-of-variances" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="intra-class-correlation" class="section level3 hasAnchor" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Intra-class Correlation<a href="ranef.html#intra-class-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="fix-or-rand" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Fixed or Random?<a href="ranef.html#fix-or-rand" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>At the start of this Chapter I highlighted the central decision that needs to be made when deciding if an effect should be random or fixed: should a vague prior for the effect (or flat prior in a frequentist analysis) be specified entirely by the analyst (fixed) or should the prior be updated using all data that has relevant information (random)? When put this way, treating something as random would always seem preferable. However, sometimes there is so little relevant information that the benefit of treating an effect as random is outweighed by the cost of the increased complexity. In my experience, when there is a lack of relevant information - the information required to estimate the random-effect variance, <span class="math inline">\(\sigma^2_u\)</span> - it is almost always because there are only a few levels of the predictor, not because the replication per level is small. This is why people often use the rule-of-thumb that if there are few factor levels (<span class="math inline">\(&lt;5\)</span>) then we should treat the effects as fixed. In this scenario, even if we had infinite replication at each level, we would still only have fewer than five observations from which to estimate the variance. In such cases a second rule-of-thumb is often satisfied: if the levels are interesting to other people, they are fixed. However, when the two rules-of-thumb are in conflict, people often agonise about the choice. If your data set is of an admirable size, I would argue that it often doesn’t matter what you choose. If you have few levels of a predictor, that probably means you have a lot of replication per level. In these situations, the amount of information coming from observations from that level overwhelm any prior information and so it doesn’t matter whether you say the prior information is vague (fixed) or try and update the prior information from all observations (random).</p>
<p>In my own field, year is often a good example. A field project has ran for less than five years but each year a lot data is collected. Should the year effects be treated as fixed or random? Well there are few levels, suggesting fixed, but the effect of year 2014 (for example) isn’t particularly interesting, suggesting random. People sometimes claim fixed because years haven’t been sampled at random, but this argument shows a deep misunderstanding of what a random effect is. ‘random’ isn’t referring to <em>years</em> being <em>sampled at random</em>, but referring to the fact that we would like to treat year <em>effects</em> as <em>random variables</em> coming from a distribution. If I had observations from many years I would certainly fit year effects as random. However, with few years and a lot of data per year I would treat them as fixed. Let’s consider a data-set I collected and that I use for teaching:</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="ranef.html#cb154-1" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">url</span>(<span class="st">&quot;https://github.com/jarrodhadfield/sda/raw/master/data/BTtarsus.rda&quot;</span>))</span>
<span id="cb154-2"><a href="ranef.html#cb154-2" tabindex="-1"></a><span class="fu">head</span>(BTtarsus)</span></code></pre></div>
<pre><code>##   tarsus_mm bird_id sex year nest_orig nest_rear day_hatch
## 1      17.2 L298904   F 2011     11_A9     11_A9         0
## 2      17.6 L298903   M 2011     11_A9     11_A9         0
## 3      16.2 L298905   F 2011     11_82     11_A9         0
## 4      17.0 L298901   M 2011     11_82     11_A9         0
## 5      17.3 L298900   M 2011     11_A9     11_A9         1
## 6      16.1 L298902   M 2011     11_82     11_A9         1</code></pre>
<p><span class="math inline">\(\texttt{tarsus_mm}\)</span> are the lengths of the tarsus bone in Blue Tit chicks and <span class="math inline">\(\texttt{year}\)</span> is the year (2011-2014) in which the chicks hatched and I measured them. The remaining variables will not be relevant here. The key point is that <span class="math inline">\(\texttt{year}\)</span> only has four levels but the number of birds with tarsus measurements is high in each year (between 593 and 854). Let’s fit two models, one where year effects are fixed and one where they are random:</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="ranef.html#cb156-1" tabindex="-1"></a>BTtarsus<span class="sc">$</span>year <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(BTtarsus<span class="sc">$</span>year)</span>
<span id="cb156-2"><a href="ranef.html#cb156-2" tabindex="-1"></a></span>
<span id="cb156-3"><a href="ranef.html#cb156-3" tabindex="-1"></a>prior.myear.fixed <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>))</span>
<span id="cb156-4"><a href="ranef.html#cb156-4" tabindex="-1"></a>myear.fixed <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(tarsus_mm <span class="sc">~</span> year, <span class="at">data =</span> BTtarsus, <span class="at">prior =</span> prior.myear.fixed)</span>
<span id="cb156-5"><a href="ranef.html#cb156-5" tabindex="-1"></a></span>
<span id="cb156-6"><a href="ranef.html#cb156-6" tabindex="-1"></a>prior.myear.random <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb156-7"><a href="ranef.html#cb156-7" tabindex="-1"></a>    <span class="at">nu =</span> <span class="dv">1</span>, <span class="at">alpha.mu =</span> <span class="dv">0</span>, <span class="at">alpha.V =</span> <span class="dv">1000</span>)))</span>
<span id="cb156-8"><a href="ranef.html#cb156-8" tabindex="-1"></a>myear.random <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(tarsus_mm <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>year, <span class="at">data =</span> BTtarsus, <span class="at">pr =</span> <span class="cn">TRUE</span>,</span>
<span id="cb156-9"><a href="ranef.html#cb156-9" tabindex="-1"></a>    <span class="at">prior =</span> prior.myear.random)</span></code></pre></div>
<p>If we plot the posterior distribution for the estimated year means we can see that they are extremely similar (Figure <a href="ranef.html#fig:year">4.9</a>).</p>
<div class="figure"><span style="display:block;" id="fig:year"></span>
<img src="_bookdown_files/fig/year-1.png" alt="MCMC traces for the estimated year means when year effects are treated as fixed (black) or random (red). Because the amount of replication per year is high the posterior distributions are very similar" width="672" />
<p class="caption">
Figure 4.9: MCMC traces for the estimated year means when year effects are treated as fixed (black) or random (red). Because the amount of replication per year is high the posterior distributions are very similar
</p>
</div>
<p>With a flatish prior on the standard deviation of year effects (half-Cauchy with scale <span class="math inline">\(\sqrt{1000}\)</span>) the posterior distribution is, for those familiar with Blue Tit tarsus lengths, hopeless uncertain (Figure <a href="ranef.html#fig:year-sd">4.10</a>). A standard deviation of 0.5 has some support. This implies that some years we expect some seriously stumpy Blue Tits and in other years we expect some seriously leggy Blue Tits. But a standard deviation of 0.05 also has some support and this implies that tarsus length hardly varies from year-to-year. In such cases we might as well admit that we cannot estimate the between year variability with sufficient precision even though we can infer with precision the effect of the handful of years we have measurements for. When this is the case, we may as well treat the effects as fixed.</p>
<div class="figure"><span style="display:block;" id="fig:year-sd"></span>
<img src="_bookdown_files/fig/year-sd-1.png" alt="Posterior distribution for the standard deviation of year effects from model `prior.myear.random`." width="672" />
<p class="caption">
Figure 4.10: Posterior distribution for the standard deviation of year effects from model <code>prior.myear.random</code>.
</p>
</div>

</div>
</div>
<h3> References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Burkner.2017" class="csl-entry">
Bürkner, Paul-Christian. 2017. <span>“<span class="nocase">brms</span>: An <span>R</span> Package for <span>Bayesian</span> Multilevel Models Using <span>Stan</span>.”</span> <em>Journal of Statistical Software</em> 80 (1): 1–28.
</div>
<div id="ref-Gelman.2006" class="csl-entry">
Gelman, A. 2006. <span>“Prior Distributions for Variance Parameters in Hierarchical Models.”</span> <em>Bayesian Analysis</em> 1 (3): 515–33.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>If we assumed the distribution was Laplace (back to back exponentials) we have the LASSO. If we assumed the distribution was a mixture of normal and Laplace we have the elastic net.<a href="ranef.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>With the now defunct <span class="math inline">\(\texttt{family=&quot;ordinal&quot;}\)</span> the <span class="math inline">\(\pi^2/3\)</span> in the denominator for the intra-class correlation would have be replaced by a one which is the link variance for the probit (the variance of the unit normal).<a href="ranef.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>For the <span class="math inline">\(F\)</span> prior on the variance, the scale is set by <span class="math inline">\(\texttt{alpha.V}*\texttt{V}\)</span> and the normalised variance <span class="math inline">\(\sigma^2_{\texttt{school}}/(\texttt{alpha.V}*\texttt{V})\)</span> follows a non-central F distribution with one numerator degree-of-freedom (<span class="math inline">\(\texttt{df1}\)</span>), <span class="math inline">\(\texttt{nu}\)</span> denominator degrees-of-freedom (<span class="math inline">\(\texttt{df2}\)</span>) and a non-centrality parameter (<span class="math inline">\(\texttt{ncp}\)</span>) equal to <span class="math inline">\(\texttt{alpha.mu}^2/\texttt{alpha.V}\)</span>. The density of <span class="math inline">\(\sigma^2_{\texttt{school}}\)</span> is therefore the density of <span class="math inline">\(\sigma^2_{\texttt{school}}/(\texttt{alpha.V}*\texttt{V})\)</span> in this <span class="math inline">\(F\)</span> distribution divided by the Jacobian, <span class="math inline">\(\texttt{alpha.V}*\texttt{V}\)</span> (Section <a href="bayesian.html#transform-sec">2.7</a>). For the <span class="math inline">\(t\)</span> prior on the standard deviation, the scale is set by <span class="math inline">\(\sqrt{\texttt{alpha.V}*\texttt{V}}\)</span> and the normalised standard deviation <span class="math inline">\(\sigma_{\texttt{school}}/\sqrt{\texttt{alpha.V}*\texttt{V}}\)</span> follows a folded non-central <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\texttt{nu}\)</span> degrees-of-freedom and non-centrality parameter <span class="math inline">\(\texttt{alpha.mu}/\sqrt{\texttt{alpha.V}}\)</span>. The folding is because we are working with the absolute value of the <span class="math inline">\(t\)</span> variable and so the density for negative values is reflected onto positive values. The density of <span class="math inline">\(\sigma_{\texttt{school}}\)</span> is therefore the summed density of <span class="math inline">\(\sigma_{\texttt{school}}/(\texttt{alpha.V}*\texttt{V})\)</span> and <span class="math inline">\(-\sigma_{\texttt{school}}/(\texttt{alpha.V}*\texttt{V})\)</span> in this <span class="math inline">\(t\)</span> distribution divided by the Jacobian, <span class="math inline">\(\sqrt{\texttt{alpha.V}*\texttt{V}}\)</span>. This is all very hard to remember! The density of these distributions can be obtained using the function <code>dprior</code> which evaluates <span class="math inline">\(\texttt{x}\)</span> according to the MCMCglmm specification passed as a list to the argument <span class="math inline">\(\texttt{prior}\)</span>. The density of the standard deviation (<span class="math inline">\(\texttt{sd=TRUE}\)</span>) or the variance (the default - <span class="math inline">\(\texttt{sd=FALSE}\)</span>) can be obtained. For example, <code>dprior(1/2, prior=list(V=1, nu=1, alpha.mu=0, alpha.V=10^2), sd=TRUE)</code>, returns the density of <span class="math inline">\(\sigma_{\texttt{school}}=1/2\)</span> for a half-<span class="math inline">\(t\)</span> (since <span class="math inline">\(\texttt{alpha.mu}=0\)</span>) with one degree-of-freedom and scale 10 (i.e a half-Cauchy with scale 10).<a href="ranef.html#fnref13" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="glm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cat-int.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["MCMCglmm-course-notes.pdf", "MCMCglmm-course-notes.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
