<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Linear and Generalised Linear Models | MCMCglmm Course Notes</title>
  <meta name="description" content="Extended documentation and course notes for the MCMCglmm R package." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Linear and Generalised Linear Models | MCMCglmm Course Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Linear and Generalised Linear Models | MCMCglmm Course Notes" />
  
  <meta name="twitter:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

<meta name="author" content="Jarrod Hadfield" />


<meta name="date" content="2026-01-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian.html"/>
<link rel="next" href="ranef.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script>
<link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-1.3.31/rglClass.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/utils.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/buffer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/subscenes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shaders.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shadersrc.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/textures.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/projection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/mouse.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/init.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pieces.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/draw.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/controls.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/selection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/rglTimer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pretty.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/axes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/animation.src.js"></script>
<script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.src.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#outline"><i class="fa fa-check"></i><b>1.1</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian Analysis and MCMC</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian.html"><a href="bayesian.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>2.2</b> Likelihood</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-ml"><i class="fa fa-check"></i><b>2.2.1</b> Maximum Likelihood (ML)</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian.html"><a href="bayesian.html#restricted-maximum-likelihood-reml"><i class="fa fa-check"></i><b>2.2.2</b> Restricted Maximum Likelihood (REML)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian.html"><a href="bayesian.html#prior-distribution"><i class="fa fa-check"></i><b>2.3</b> Prior Distribution</a></li>
<li class="chapter" data-level="2.4" data-path="bayesian.html"><a href="bayesian.html#posterior-distribution"><i class="fa fa-check"></i><b>2.4</b> Posterior Distribution</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian.html"><a href="bayesian.html#marginal-posterior-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Marginal Posterior Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="bayesian.html"><a href="bayesian.html#intervals-sec"><i class="fa fa-check"></i><b>2.4.2</b> Credible Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian.html"><a href="bayesian.html#MCMC"><i class="fa fa-check"></i><b>2.5</b> MCMC</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian.html"><a href="bayesian.html#starting-values"><i class="fa fa-check"></i><b>2.5.1</b> Starting values</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian.html"><a href="bayesian.html#metropolis-hastings-updates"><i class="fa fa-check"></i><b>2.5.2</b> Metropolis-Hastings updates</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian.html"><a href="bayesian.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.5.3</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian.html"><a href="bayesian.html#diagnostics-sec"><i class="fa fa-check"></i><b>2.5.4</b> MCMC Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian.html"><a href="bayesian.html#Vprior-sec"><i class="fa fa-check"></i><b>2.6</b> Priors for Residual Variances</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian.html"><a href="bayesian.html#IP-sec"><i class="fa fa-check"></i><b>2.6.1</b> Improper Priors</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian.html"><a href="bayesian.html#transform-sec"><i class="fa fa-check"></i><b>2.7</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>3</b> Linear and Generalised Linear Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="glm.html"><a href="glm.html#linear-model-lm"><i class="fa fa-check"></i><b>3.1</b> Linear Model (LM)</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="glm.html"><a href="glm.html#lm-sec"><i class="fa fa-check"></i><b>3.1.1</b> Linear Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="glm.html"><a href="glm.html#generalised-linear-model-glm"><i class="fa fa-check"></i><b>3.2</b> Generalised Linear Model (GLM)</a></li>
<li class="chapter" data-level="3.3" data-path="glm.html"><a href="glm.html#poisson-glm"><i class="fa fa-check"></i><b>3.3</b> Poisson GLM</a></li>
<li class="chapter" data-level="3.4" data-path="glm.html"><a href="glm.html#overdispersion"><i class="fa fa-check"></i><b>3.4</b> Overdispersion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="glm.html"><a href="glm.html#multiplicative-overdispersion"><i class="fa fa-check"></i><b>3.4.1</b> Multiplicative Overdispersion</a></li>
<li class="chapter" data-level="3.4.2" data-path="glm.html"><a href="glm.html#addod-sec"><i class="fa fa-check"></i><b>3.4.2</b> Additive Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="glm.html"><a href="glm.html#prediction-in-glm"><i class="fa fa-check"></i><b>3.5</b> Prediction in GLM</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="glm.html"><a href="glm.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="glm.html"><a href="glm.html#binom-sec"><i class="fa fa-check"></i><b>3.6</b> Binomial and Bernoulli GLM</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="glm.html"><a href="glm.html#overdispersion-1"><i class="fa fa-check"></i><b>3.6.1</b> Overdispersion</a></li>
<li class="chapter" data-level="3.6.2" data-path="glm.html"><a href="glm.html#prediction"><i class="fa fa-check"></i><b>3.6.2</b> Prediction</a></li>
<li class="chapter" data-level="3.6.3" data-path="glm.html"><a href="glm.html#bernoulli-sec"><i class="fa fa-check"></i><b>3.6.3</b> Bernoulli GLM</a></li>
<li class="chapter" data-level="3.6.4" data-path="glm.html"><a href="glm.html#probit-link"><i class="fa fa-check"></i><b>3.6.4</b> Probit link</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="glm.html"><a href="glm.html#ordinal-data"><i class="fa fa-check"></i><b>3.7</b> Ordinal Data</a></li>
<li class="chapter" data-level="3.8" data-path="glm.html"><a href="glm.html#non-zero-binomial-data"><i class="fa fa-check"></i><b>3.8</b> Non-zero Binomial Data</a></li>
<li class="chapter" data-level="3.9" data-path="glm.html"><a href="glm.html#complete-separation"><i class="fa fa-check"></i><b>3.9</b> Complete Separation</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="glm.html"><a href="glm.html#gelman-prior-sec"><i class="fa fa-check"></i><b>3.9.1</b> The <span class="citation">Gelman, Jakulin, et al. (2008)</span> prior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ranef.html"><a href="ranef.html"><i class="fa fa-check"></i><b>4</b> Random effects</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ranef.html"><a href="ranef.html#GLMM"><i class="fa fa-check"></i><b>4.1</b> Generalised Linear Mixed Model (GLMM)</a></li>
<li class="chapter" data-level="4.2" data-path="ranef.html"><a href="ranef.html#ranpred-sec"><i class="fa fa-check"></i><b>4.2</b> Prediction with Random Effects</a></li>
<li class="chapter" data-level="4.3" data-path="ranef.html"><a href="ranef.html#overdispersed-binomial-as-a-bernoulli-glmm"><i class="fa fa-check"></i><b>4.3</b> Overdispersed Binomial as a Bernoulli GLMM</a></li>
<li class="chapter" data-level="4.4" data-path="ranef.html"><a href="ranef.html#ICC"><i class="fa fa-check"></i><b>4.4</b> Intra-class Correlations</a></li>
<li class="chapter" data-level="4.5" data-path="ranef.html"><a href="ranef.html#PXprior-sec"><i class="fa fa-check"></i><b>4.5</b> Priors for Random Effect Variances</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="ranef.html"><a href="ranef.html#f-and-folded-t-priors"><i class="fa fa-check"></i><b>4.5.1</b> <span class="math inline">\(F\)</span> and folded-<span class="math inline">\(t\)</span> priors</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="ranef.html"><a href="ranef.html#priors-on-functions-of-variances"><i class="fa fa-check"></i><b>4.6</b> Priors on Functions of Variances</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="ranef.html"><a href="ranef.html#intra-class-correlation"><i class="fa fa-check"></i><b>4.6.1</b> Intra-class Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="ranef.html"><a href="ranef.html#fix-or-rand"><i class="fa fa-check"></i><b>4.7</b> Fixed or Random?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cat-int.html"><a href="cat-int.html"><i class="fa fa-check"></i><b>5</b> Categorical Random Interactions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="cat-int.html"><a href="cat-int.html#idh-variance-structure"><i class="fa fa-check"></i><b>5.1</b> <code>idh</code> Variance Structure</a></li>
<li class="chapter" data-level="5.2" data-path="cat-int.html"><a href="cat-int.html#us-variance-structure"><i class="fa fa-check"></i><b>5.2</b> <code>us</code> Variance Structure</a></li>
<li class="chapter" data-level="5.3" data-path="cat-int.html"><a href="cat-int.html#compound-variance-structures"><i class="fa fa-check"></i><b>5.3</b> Compound Variance Structures</a></li>
<li class="chapter" data-level="5.4" data-path="cat-int.html"><a href="cat-int.html#heter-sec"><i class="fa fa-check"></i><b>5.4</b> Heterogenous Residual Variance</a></li>
<li class="chapter" data-level="5.5" data-path="cat-int.html"><a href="cat-int.html#contrasts-and-covariances"><i class="fa fa-check"></i><b>5.5</b> Contrasts and Covariances</a></li>
<li class="chapter" data-level="5.6" data-path="cat-int.html"><a href="cat-int.html#VCVprior-sec"><i class="fa fa-check"></i><b>5.6</b> Priors for Covariance Matrices</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="cat-int.html"><a href="cat-int.html#priors-for-us-structures"><i class="fa fa-check"></i><b>5.6.1</b> Priors for <code>us</code> structures</a></li>
<li class="chapter" data-level="5.6.2" data-path="cat-int.html"><a href="cat-int.html#priors-for-idh-structures"><i class="fa fa-check"></i><b>5.6.2</b> Priors for <code>idh</code> structures</a></li>
<li class="chapter" data-level="5.6.3" data-path="cat-int.html"><a href="cat-int.html#priors-for-corg-and-corgh-structures"><i class="fa fa-check"></i><b>5.6.3</b> Priors for <code>corg</code> and <code>corgh</code> structures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cont-int.html"><a href="cont-int.html"><i class="fa fa-check"></i><b>6</b> Continuous Random Interactions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="cont-int.html"><a href="cont-int.html#random-regression"><i class="fa fa-check"></i><b>6.1</b> Random Regression</a></li>
<li class="chapter" data-level="6.2" data-path="cont-int.html"><a href="cont-int.html#expected-variances-and-covariances"><i class="fa fa-check"></i><b>6.2</b> Expected Variances and Covariances</a></li>
<li class="chapter" data-level="6.3" data-path="cont-int.html"><a href="cont-int.html#RRcentering"><i class="fa fa-check"></i><b>6.3</b> <code>us</code> versus <code>idh</code> and mean centering</a></li>
<li class="chapter" data-level="6.4" data-path="cont-int.html"><a href="cont-int.html#meta-sec"><i class="fa fa-check"></i><b>6.4</b> Meta-analysis</a></li>
<li class="chapter" data-level="6.5" data-path="cont-int.html"><a href="cont-int.html#splines"><i class="fa fa-check"></i><b>6.5</b> Splines</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multi-response models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="multi.html"><a href="multi.html#relaxing-the-univariate-assumptions-of-causality"><i class="fa fa-check"></i><b>7.1</b> Relaxing the univariate assumptions of causality</a></li>
<li class="chapter" data-level="7.2" data-path="multi.html"><a href="multi.html#multinomial-models"><i class="fa fa-check"></i><b>7.2</b> Multinomial Models</a></li>
<li class="chapter" data-level="7.3" data-path="multi.html"><a href="multi.html#zero-inflated-models"><i class="fa fa-check"></i><b>7.3</b> Zero-inflated Models</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="multi.html"><a href="multi.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="multi.html"><a href="multi.html#Hurdle"><i class="fa fa-check"></i><b>7.4</b> Hurdle Models</a></li>
<li class="chapter" data-level="7.5" data-path="multi.html"><a href="multi.html#ZAP"><i class="fa fa-check"></i><b>7.5</b> Zero-altered Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pedigree.html"><a href="pedigree.html"><i class="fa fa-check"></i><b>8</b> Pedigrees and Phylogenies</a>
<ul>
<li class="chapter" data-level="8.1" data-path="pedigree.html"><a href="pedigree.html#pedigree-and-phylogeny-formats"><i class="fa fa-check"></i><b>8.1</b> Pedigree and phylogeny formats</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="pedigree.html"><a href="pedigree.html#pedigrees"><i class="fa fa-check"></i><b>8.1.1</b> Pedigrees</a></li>
<li class="chapter" data-level="8.1.2" data-path="pedigree.html"><a href="pedigree.html#phylogenies"><i class="fa fa-check"></i><b>8.1.2</b> Phylogenies</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="pedigree.html"><a href="pedigree.html#the-animal-model-and-the-phylogenetic-mixed-model"><i class="fa fa-check"></i><b>8.2</b> The animal model and the phylogenetic mixed model</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="parameter-expansion.html"><a href="parameter-expansion.html"><i class="fa fa-check"></i><b>9</b> Parameter Expansion</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="parameter-expansion.html"><a href="parameter-expansion.html#variances-close-to-zero"><i class="fa fa-check"></i><b>9.0.1</b> Variances close to zero</a></li>
<li class="chapter" data-level="9.0.2" data-path="parameter-expansion.html"><a href="parameter-expansion.html#secPX-p"><i class="fa fa-check"></i><b>9.0.2</b> Parameter expanded priors</a></li>
<li class="chapter" data-level="9.0.3" data-path="parameter-expansion.html"><a href="parameter-expansion.html#binary-response-models"><i class="fa fa-check"></i><b>9.0.3</b> Binary response models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="path.html"><a href="path.html"><i class="fa fa-check"></i><b>10</b> Path Analysis &amp; Antedependence Structures</a>
<ul>
<li class="chapter" data-level="10.1" data-path="path.html"><a href="path.html#path-anlaysis"><i class="fa fa-check"></i><b>10.1</b> Path Anlaysis</a></li>
<li class="chapter" data-level="10.2" data-path="path.html"><a href="path.html#ante-sec"><i class="fa fa-check"></i><b>10.2</b> Antedependence</a></li>
<li class="chapter" data-level="10.3" data-path="path.html"><a href="path.html#scaling"><i class="fa fa-check"></i><b>10.3</b> Scaling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="technical-details.html"><a href="technical-details.html"><i class="fa fa-check"></i><b>11</b> Technical Details</a>
<ul>
<li class="chapter" data-level="11.1" data-path="technical-details.html"><a href="technical-details.html#model-form"><i class="fa fa-check"></i><b>11.1</b> Model Form</a></li>
<li class="chapter" data-level="11.2" data-path="technical-details.html"><a href="technical-details.html#MCMC-app"><i class="fa fa-check"></i><b>11.2</b> MCMC Sampling Schemes</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="technical-details.html"><a href="technical-details.html#updating-the-latent-variables-bf-l"><i class="fa fa-check"></i><b>11.2.1</b> Updating the latent variables <span class="math inline">\({\bf l}\)</span></a></li>
<li class="chapter" data-level="11.2.2" data-path="technical-details.html"><a href="technical-details.html#updating-the-location-vector-boldsymboltheta-leftboldsymbolmathbfbeta-bf-uright"><i class="fa fa-check"></i><b>11.2.2</b> Updating the location vector <span class="math inline">\(\boldsymbol{\theta} = \left[{\boldsymbol{\mathbf{\beta}}}^{&#39;}\; {\bf u}^{&#39;}\right]^{&#39;}\)</span></a></li>
<li class="chapter" data-level="11.2.3" data-path="technical-details.html"><a href="technical-details.html#updating-the-variance-structures-bf-g-and-bf-r"><i class="fa fa-check"></i><b>11.2.3</b> Updating the variance structures <span class="math inline">\({\bf G}\)</span> and <span class="math inline">\({\bf R}\)</span></a></li>
<li class="chapter" data-level="11.2.4" data-path="technical-details.html"><a href="technical-details.html#ordinal-models"><i class="fa fa-check"></i><b>11.2.4</b> Ordinal Models</a></li>
<li class="chapter" data-level="11.2.5" data-path="technical-details.html"><a href="technical-details.html#path-analyses"><i class="fa fa-check"></i><b>11.2.5</b> Path Analyses</a></li>
<li class="chapter" data-level="11.2.6" data-path="technical-details.html"><a href="technical-details.html#deviance-and-dic"><i class="fa fa-check"></i><b>11.2.6</b> Deviance and DIC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MCMCglmm Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="glm" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Linear and Generalised Linear Models<a href="glm.html#glm" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="linear-model-lm" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Linear Model (LM)<a href="glm.html#linear-model-lm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A linear model is one in which unknown parameters are multiplied by (functions of) observed variables and then added together to give a prediction for the response variable. As an example, lets take the results from a Swedish experiment from the sixties:</p>
<p>The experiment involved enforcing speed limits on Swedish roads on some days, but on other days letting everyone drive as fast as they liked. The response variable (<code>y</code>) is the number of accidents recorded. The experiment was conducted in 1961 and 1962 for 92 days in each year. As a first attempt we could specify the linear model:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="glm.html#cb39-1" tabindex="-1"></a>y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day</span></code></pre></div>
<p>but what does this mean?</p>
<div id="lm-sec" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Linear Predictors<a href="glm.html#lm-sec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The model formula defines a set of simultaneous (linear) equations</p>
<p><span class="math display" id="eq:SE-eq">\[\begin{array}{cl}
E[y\texttt{[1]}] &amp;=\beta_{1}+\beta_{2}(\texttt{limit[1]==&quot;yes&quot;})+\beta_{3}(\texttt{year[1]==&quot;1962&quot;})+\beta_{4}\texttt{day[1]}\\
E[y\texttt{[2]}] &amp;= \beta_{1}+\beta_{2}(\texttt{limit[2]==&quot;yes&quot;})+\beta_{3}(\texttt{year[2]==&quot;1962&quot;})+\beta_{4}\texttt{day[2]}\\
\vdots&amp;=\vdots\\
E[y\texttt{[184]}] &amp;= \beta_{1}+\beta_{2}(\texttt{limit[184]==&quot;yes&quot;})+\beta_{3}(\texttt{year[184]==&quot;1962&quot;})+\beta_{4}\texttt{day[184]}\\
\end{array}
\label{SE-eq}   \tag{3.1}\]</span></p>
<p>where the <span class="math inline">\(\beta\)</span>’s are the unknown coefficients to be estimated, and the variables in <span class="math inline">\(\texttt{this font}\)</span> are observed predictors. Continuous predictors such as <code>day</code> remain unchanged, but categorical predictors are expanded into a series of binary variables of the form ‘<em>do the data come from 1961, yes or no?</em>’, ‘<em>do the data come from 1962, yes or no?</em>’, and so on for as many years for which there are data.</p>
<p>It is cumbersome to write out the equation for each data point in this way, and a more compact way of representing the system of equations is</p>
<p><span class="math display" id="eq:lm">\[
E[{\bf y}] = {\bf X}{\boldsymbol{\mathbf{\beta}}}
\tag{3.2}
\]</span></p>
<p>where <span class="math inline">\({\bf X}\)</span> is called a design matrix and contains the predictor information for all observations, and <span class="math inline">\({\boldsymbol{\mathbf{\beta}}} = [\beta_{1}\ \beta_{2}\ \beta_{3}\ \beta_{4}]^{&#39;}\)</span> is the vector of parameters. Here, <span class="math inline">\(E[{\bf y}]\)</span> is a vector of the 184 expected values.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="glm.html#cb40-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">data =</span> Traffic)</span>
<span id="cb40-2"><a href="glm.html#cb40-2" tabindex="-1"></a>X[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">184</span>), ]</span></code></pre></div>
<pre><code>##     (Intercept) limityes year1962 day
## 1             1        0        0   1
## 2             1        0        0   2
## 184           1        1        1  92</code></pre>
<p>The binary predictors <em>do the data come from 1961, yes or no?</em> and <em>there was no speed limit, yes or no?</em> do not appear. These are the first factor levels of <code>year</code> and <code>limit</code> respectively, and are absorbed into the global intercept (<span class="math inline">\(\beta_{1}\)</span>) which is fitted by default in R. Hence the expected number of accidents for the four combinations (on day zero) are <span class="math inline">\(\beta_{1}\)</span> for 1961 with no speed limit, <span class="math inline">\(\beta_{1}+\beta_{2}\)</span> for 1961 with a speed limit, <span class="math inline">\(\beta_{1}+\beta_{3}\)</span> for 1962 with no speed limit and <span class="math inline">\(\beta_{1}+\beta_{2}+\beta_{3}\)</span> for 1962 with a speed limit.</p>
<p>The simultaneous equations defined by Equation <a href="glm.html#eq:lm">(3.2)</a> cannot be solved directly because we do not know the left-hand side - expected values of <span class="math inline">\(y\)</span>. We only know the observed value, which we assume is distributed around the expected value with some error. In a normal linear model we assume that these errors (residuals) are normally distributed:</p>
<p><span class="math display">\[{\bf y}-{\bf X}{\boldsymbol{\mathbf{\beta}}} = {\bf e} \sim N(0, \sigma^{2}_{e}{\bf I})\]</span></p>
<p><span class="math inline">\({\bf I}\)</span> is a <span class="math inline">\(184\times 184\)</span> identity matrix. It has ones along the diagonal, and zeros in the off-diagonals. The zero off-diagonals imply that the residuals are uncorrelated, and the ones along the diagonal imply that they have the same variance (<span class="math inline">\(\sigma^{2}_{e}\)</span>). Thinking about the distribution of residuals is less helpful when we move on to GLM’s and so I prefer to think about the model in the form:</p>
<p><span class="math display">\[{\bf y}\sim N({\bf X}{\boldsymbol{\mathbf{\beta}}}, \sigma^{2}_{e}{\bf I})\]</span></p>
<p>and say the response is <em>conditionally</em> normal, with the conditioning on the model (<span class="math inline">\({\bf X}{\boldsymbol{\mathbf{\beta}}}\)</span>). It is important to note that this is different from saying the response is normal. If having a speed limit had a very strong effect the (marginal) distribution of the response may be bimodal and far from normal, and yet by including speed-limit as a predictor, conditional normality may be achieved.</p>
<p>We could use <code>MCMCglmm</code> to fit this model, but to connect better with what comes next, let’s use <code>glm</code> to estimate <span class="math inline">\({\bf \beta}\)</span> and <span class="math inline">\(\sigma^{2}_{e}\)</span> assuming that the number of accidents follow a conditional normal distribution (the <code>MCMCglmm</code> syntax is identical):</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="glm.html#cb42-1" tabindex="-1"></a>m2a<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">data =</span> Traffic)</span>
<span id="cb42-2"><a href="glm.html#cb42-2" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ limit + year + day, data = Traffic)
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 21.13111    1.45169  14.556  &lt; 2e-16 ***
## limityes    -3.66427    1.35559  -2.703  0.00753 ** 
## year1962    -1.34853    1.31121  -1.028  0.30511    
## day          0.05304    0.02355   2.252  0.02552 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 71.80587)
## 
##     Null deviance: 14128  on 183  degrees of freedom
## Residual deviance: 12925  on 180  degrees of freedom
## AIC: 1314.5
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>On day zero in 1961 in the absence of a speed limit we expect 21.1 accidents (the intercept). With a speed limit we expect 3.7 fewer accidents and we can quite confidently reject the null-hypothesis of no effect - particularly if we were willing to use a one-tailed test, which seems reasonable. There are 1.3 fewer accidents in 1962, although this could just be due to chance, and for every unit increase in <span class="math inline">\(\texttt{day}\)</span> the number of accidents is predicted to go up by 0.05. The <span class="math inline">\(\texttt{day}\)</span> variable is encoded as integers from 1 to 92 with the same <span class="math inline">\(\texttt{day}\)</span> in different years being comparable (for example, the same day of the week and roughly the same date). If <span class="math inline">\(\texttt{day}\)</span>’s are evenly spaced throughout the year the <span class="math inline">\(\texttt{day}\)</span> effect is roughly the effect of increasing calender date by four (365/92) days. The estimate of the residual variance, <span class="math inline">\(\sigma^2_e\)</span>, is the dispersion parameter (71.8).</p>
<p>Because the number of accidents are count data we might worry about the assumption of conditional normality, and indeed the residuals show the typical right skew:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="glm.html#cb44-1" tabindex="-1"></a><span class="fu">hist</span>(<span class="fu">resid</span>(m2a<span class="fl">.1</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:hist-traffic"></span>
<img src="_bookdown_files/fig/hist-traffic-1.png" alt="Histogram of residuals from model `m2a.1` which assumed they followed a Normal distribution." width="672" />
<p class="caption">
Figure 3.1: Histogram of residuals from model <code>m2a.1</code> which assumed they followed a Normal distribution.
</p>
</div>
<p>It’s not extreme, and the conclusions probably won’t change, but we could assume that the data follow some other distribution.</p>
</div>
</div>
<div id="generalised-linear-model-glm" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Generalised Linear Model (GLM)<a href="glm.html#generalised-linear-model-glm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Generalised linear models extend the linear model to non-normal data. They are essentially the same as the linear model described above, except they differ in two aspects. First, it is not necessarily the mean response that is predicted, but some function of the mean response. This function is called the link function. For example, with a log link we are trying to predict the logged expectation:</p>
<p><span class="math display">\[\textrm{log}(E[{\bf y}]) = {\bf X}{\boldsymbol{\mathbf{\beta}}}\]</span></p>
<p>or alternatively</p>
<p><span class="math display">\[E[{\bf y}] = \textrm{exp}({\bf X}{\boldsymbol{\mathbf{\beta}}})\]</span></p>
<p>where <span class="math inline">\(\textrm{exp}\)</span> is the inverse of the log link function- exponentiating. The second difference is that many distributions are single parameter distributions for which a variance does not need to be estimated because it can be inferred from the mean. For example, we could assume that the number of accidents are Poisson distributed, in which case we also make the assumption that the variance is equal to the expected value. Technically, GLM’s only apply to a restricted set of distributions (those in the exponential family) but <span class="math inline">\(\texttt{MCMCglmm}\)</span> can accommodate a range of GLM-like models for other distributions (see Table <a href="technical-details.html#tab:dist">11.1</a>).</p>
</div>
<div id="poisson-glm" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Poisson GLM<a href="glm.html#poisson-glm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For now we will concentrate on a Poisson GLM with log link (the default link function for the Poisson distribution):</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="glm.html#cb45-1" tabindex="-1"></a>m2a<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">family =</span> poisson, <span class="at">data =</span> Traffic)</span>
<span id="cb45-2"><a href="glm.html#cb45-2" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ limit + year + day, family = poisson, data = Traffic)
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  3.0467406  0.0372985  81.685  &lt; 2e-16 ***
## limityes    -0.1749337  0.0355784  -4.917 8.79e-07 ***
## year1962    -0.0605503  0.0334364  -1.811   0.0702 .  
## day          0.0024164  0.0005964   4.052 5.09e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 625.25  on 183  degrees of freedom
## Residual deviance: 569.25  on 180  degrees of freedom
## AIC: 1467.2
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>While the sign of the effects are comparable to that seen in the linear model, their numerical values are completely different and the significance of all effects has increased dramatically. Should we worry? The model is defined on the log scale and so to get back to the data scale we need to exponentiate. Exponentiating the intercept gives us the predicted number of accidents on day zero in 1961 without a speed limit:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="glm.html#cb47-1" tabindex="-1"></a><span class="fu">exp</span>(m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;(Intercept)&quot;</span>])</span></code></pre></div>
<pre><code>## (Intercept) 
##    21.04663</code></pre>
<p>which is very close to the intercept in the linear model (21.131), which is reassuring.</p>
<p>To get the prediction for the same day with a speed limit we need to add the <span class="math inline">\(\texttt{limityes}\)</span> coefficient</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="glm.html#cb49-1" tabindex="-1"></a><span class="fu">exp</span>(m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;(Intercept)&quot;</span>] <span class="sc">+</span> m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;limityes&quot;</span>])</span></code></pre></div>
<pre><code>## (Intercept) 
##    17.66892</code></pre>
<p>With a speed limit there are expected to be 0.840 as many accidents than if there was no speed limit. This value can be more directly obtained:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="glm.html#cb51-1" tabindex="-1"></a><span class="fu">exp</span>(m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;limityes&quot;</span>])</span></code></pre></div>
<pre><code>##  limityes 
## 0.8395127</code></pre>
<p>and holds true for any given day in either year. The proportional change is identical because the model is <em>linear</em> on the log scale and <span class="math inline">\(exp(\beta+\dots)=exp(\beta)exp(\dots)\)</span>. There is not always a direct relationship with the corresponding coefficients from the linear model but we can reassure ourselves that the parameters have the same qualitative meaning. For example, for <span class="math inline">\(\texttt{day}\)</span> 0 in 1961 the linear model predicts a drop from 21.1 to 17.5 accidents when a speed limit is in place - around 0.83 as many accidents, comparable to that predicted in the log-linear model.</p>
<p>So in terms of the reported coefficients, the linear model and the Poisson log-linear model are roughly consistent with each other. However, in terms of accurately quantifying the uncertainty in those coefficients the Poisson model has a serious problem - it is very over confident.</p>
</div>
<div id="overdispersion" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Overdispersion<a href="glm.html#overdispersion" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Most count data do not conform to a Poisson distribution because the variance in the response exceeds the expectation. In the summary to <code>m2a.2</code> the ratio of the residual deviance to the residual degrees of freedom is 3.162 which means, roughly speaking, there is 3.2 times more variation in our response (after conditioning on the model) than what we expect. This is known as overdispersion and it is easy to see how it arises, and why it is so common.</p>
<p>If the predictor data had not been available to us then the only model we could have fitted was one with just an intercept:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="glm.html#cb53-1" tabindex="-1"></a>m2a<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> Traffic, <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>)</span>
<span id="cb53-2"><a href="glm.html#cb53-2" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ 1, family = &quot;poisson&quot;, data = Traffic)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  3.07033    0.01588   193.3   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 625.25  on 183  degrees of freedom
## Residual deviance: 625.25  on 183  degrees of freedom
## AIC: 1517.2
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>for which the residual variance exceeds that expected by a factor of 3.5. Of course, the variability in the residuals must go up if there are factors that influence the number of accidents, but which we hadn’t measured. It’s likely that in most studies there are things that influence the response that haven’t been measured, and even if each thing has a small effect individually, in aggregate they can cause substantial overdispersion.</p>
<div id="multiplicative-overdispersion" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Multiplicative Overdispersion<a href="glm.html#multiplicative-overdispersion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are two ways of dealing with overdispersion. With <code>glm</code> the distribution name can be prefixed with <code>quasi</code> and a dispersion parameter estimated:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="glm.html#cb55-1" tabindex="-1"></a>m2a<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">family =</span> quasipoisson, <span class="at">data =</span> Traffic)</span>
<span id="cb55-2"><a href="glm.html#cb55-2" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.4</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ limit + year + day, family = quasipoisson, 
##     data = Traffic)
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.046741   0.067843  44.909  &lt; 2e-16 ***
## limityes    -0.174934   0.064714  -2.703  0.00753 ** 
## year1962    -0.060550   0.060818  -0.996  0.32078    
## day          0.002416   0.001085   2.227  0.02716 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 3.308492)
## 
##     Null deviance: 625.25  on 183  degrees of freedom
## Residual deviance: 569.25  on 180  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p><code>glm</code> uses a multiplicative model of overdispersion and so the estimate of the dispersion parameter is roughly equivalent to how many times greater the variance is than expected, after taking into account the predictor variables. You will notice that although the parameter estimates have changed very little, the standard errors have gone up and the significance gone down. Overdispersion, if not dealt with, can result in extreme anti-conservatism. For example, the second lowest number of accidents (8) occurred on <span class="math inline">\(\texttt{day}\)</span> 91 of 1961 without a speed limit. Our model predicts this should have been the second worst day for accidents over the whole two years, and the probability of observing 8 or less accidents on this day is predicted to be approximately 3 in a 100,000:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="glm.html#cb57-1" tabindex="-1"></a><span class="fu">ppois</span>(<span class="dv">8</span>, <span class="fu">exp</span>(m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;(Intercept)&quot;</span>] <span class="sc">+</span> <span class="dv">91</span> <span class="sc">*</span> m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;day&quot;</span>]))</span></code></pre></div>
<pre><code>## [1] 3.195037e-05</code></pre>
<p>If we did not accommodate the overdispersion, anything additional we put in in the model that could potentially explain such an improbable occurrence would come out as significant even if in reality it wasn’t important. This is because there simply isn’t any flexibility in the null model to accommodate such occurrences. For example, if the extreme value happened to be associated with a particular level of a categorical predictor or happened to be associated with an extreme value of some continuous predictor, then the coefficients associated with these predictors may well come out as significant. However, under a more plausible null model the extreme observations may not be too surprising and there may be little support for the predictors having an effect on the response. A more plausible model, and one that we’ve alluded to, would be to allow the number of accidents to vary across sampling points due to unmeasured variables. This would allow the variation in the number of accidents to exceed the predicted mean based on the measured variables (the assumption of the standard Poisson).</p>
</div>
<div id="addod-sec" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Additive Overdispersion<a href="glm.html#addod-sec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I believe that a model assuming all relevant variables have been measured or controlled for, should <strong>not</strong> be the default model, and so when you specify <code>family=poisson</code> in <span class="math inline">\(\texttt{MCMCglmm}\)</span>, overdispersion is always dealt with<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. However, <span class="math inline">\(\texttt{MCMCglmm}\)</span> does not use a multiplicative model, but an additive model.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="glm.html#cb59-1" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>))</span>
<span id="cb59-2"><a href="glm.html#cb59-2" tabindex="-1"></a>m2a<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">data =</span> Traffic, <span class="at">prior =</span> prior,</span>
<span id="cb59-3"><a href="glm.html#cb59-3" tabindex="-1"></a>    <span class="at">pl =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>The element <code>Sol</code> contains the posterior distribution of the coefficients of the linear model, and we can plot their marginal distributions:</p>
<div class="figure"><span style="display:block;" id="fig:mcmc-traffic"></span>
<img src="_bookdown_files/fig/mcmc-traffic-1.png" alt="MCMC summary plot for the coefficients from a Poisson glm (model `m2a.5`)." width="672" />
<p class="caption">
Figure 3.2: MCMC summary plot for the coefficients from a Poisson glm (model <code>m2a.5</code>).
</p>
</div>
<p>Note that the posterior distribution for the <code>year1962</code> spans zero, in agreement with the quasipoisson <code>glm</code> model, and that in general the estimates for the two models (and their uncertainty - see Section <a href="bayesian.html#intervals-sec">2.4.2</a>) are broadly similar:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="glm.html#cb60-1" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.4</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ limit + year + day, family = quasipoisson, 
##     data = Traffic)
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.046741   0.067843  44.909  &lt; 2e-16 ***
## limityes    -0.174934   0.064714  -2.703  0.00753 ** 
## year1962    -0.060550   0.060818  -0.996  0.32078    
## day          0.002416   0.001085   2.227  0.02716 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 3.308492)
## 
##     Null deviance: 625.25  on 183  degrees of freedom
## Residual deviance: 569.25  on 180  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>With additive overdispersion the linear predictor includes a ‘residual’, for which a residual variance is estimated (hence our prior specification).</p>
<p><span class="math display">\[E[{\bf y}] = \textrm{exp}({\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf e})\]</span></p>
<p>At this point it will be handy to represent the linear model in a new way:</p>
<p><span class="math display">\[{\bf l} = {\boldsymbol{\mathbf{\eta}}}+{\bf e}\]</span></p>
<p>where <span class="math inline">\({\bf l}\)</span> is a vector of latent variables (<span class="math inline">\(\textrm{log}(E[{\bf y}])\)</span> in this case) and <span class="math inline">\({\boldsymbol{\mathbf{\eta}}}\)</span> is the usual symbol for the linear predictor (<span class="math inline">\({\bf X}{\boldsymbol{\mathbf{\beta}}}\)</span>). The data we observe are assumed to be Poisson variables with expectation equal to the exponentiated latent variables:</p>
<p><span class="math display">\[{\bf y} \sim Pois(\textrm{exp}({\bf l}))\]</span></p>
<p>Note that the latent variable does not exactly predict <span class="math inline">\(y\)</span>, as it would if the data were Normal, because there is additional variability in the Poisson process<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. In the call to <span class="math inline">\(\texttt{MCMCglmm}\)</span> I specified <code>pl=TRUE</code> to indicate that I wanted to store the posterior distributions of the latent variables (also known as the liabilities). This is not usually necessary and can require a lot of memory (we have 1000 posterior samples for each of the 182 data points). However, as an example we can obtain the posterior mean residual for data point 92 which is the data from <span class="math inline">\(\texttt{day}\)</span> 92 in 1961 when there was no speed limit:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="glm.html#cb62-1" tabindex="-1"></a>lat92 <span class="ot">&lt;-</span> m2a<span class="fl">.5</span><span class="sc">$</span>Liab[, <span class="dv">92</span>]</span>
<span id="cb62-2"><a href="glm.html#cb62-2" tabindex="-1"></a><span class="co"># posterior distribution of the 92nd latent variable (liability)</span></span>
<span id="cb62-3"><a href="glm.html#cb62-3" tabindex="-1"></a></span>
<span id="cb62-4"><a href="glm.html#cb62-4" tabindex="-1"></a>eta92 <span class="ot">&lt;-</span> m2a<span class="fl">.5</span><span class="sc">$</span>Sol[, <span class="st">&quot;(Intercept)&quot;</span>] <span class="sc">+</span> m2a<span class="fl">.5</span><span class="sc">$</span>Sol[, <span class="st">&quot;day&quot;</span>] <span class="sc">*</span> Traffic<span class="sc">$</span>day[<span class="dv">92</span>]</span>
<span id="cb62-5"><a href="glm.html#cb62-5" tabindex="-1"></a><span class="co"># posterior distribution of X\beta for the 92nd observation</span></span>
<span id="cb62-6"><a href="glm.html#cb62-6" tabindex="-1"></a></span>
<span id="cb62-7"><a href="glm.html#cb62-7" tabindex="-1"></a>resid92 <span class="ot">&lt;-</span> lat92 <span class="sc">-</span> eta92</span>
<span id="cb62-8"><a href="glm.html#cb62-8" tabindex="-1"></a><span class="co"># posterior distribution of e for the 92nd observation</span></span>
<span id="cb62-9"><a href="glm.html#cb62-9" tabindex="-1"></a></span>
<span id="cb62-10"><a href="glm.html#cb62-10" tabindex="-1"></a><span class="fu">mean</span>(resid92)</span></code></pre></div>
<pre><code>## [1] -0.1341791</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="glm.html#cb64-1" tabindex="-1"></a><span class="co"># posterior mean of e for the 92nd observation</span></span></code></pre></div>
<p>This particular observation has a negative expected residual indicating that the probability of getting injured was less than expected for this <em>particular</em> realisation of that <span class="math inline">\(\texttt{day}\)</span> in that year without a speed limit. If that combination of predictors (<span class="math inline">\(\texttt{day}\)</span>=92, <span class="math inline">\(\texttt{year}\)</span>=1961 and <span class="math inline">\(\texttt{limit}\)</span>=<span class="math inline">\(\texttt{no}\)</span>) could be repeated it does not necessarily mean that the actual number of accidents would always be less than expected, because it would follow a Poisson distribution with a mean equal to <code>exp(lat92)</code> (21.974).</p>
<p>Like residuals in a standard linear model, the residuals are assumed to be independently and normally distributed with an expectation of zero and an estimated variance. If the residual variance was zero then <span class="math inline">\({\bf e}\)</span> would be a vector of zeros and the model would conform to the standard Poisson GLM. However, the posterior distribution of the residual variance is located well away form zero:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="glm.html#cb65-1" tabindex="-1"></a><span class="fu">plot</span>(m2a<span class="fl">.5</span><span class="sc">$</span>VCV)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:vcv-traffic"></span>
<img src="_bookdown_files/fig/vcv-traffic-1.png" alt="MCMC summary plot for the residual (`units`) variance from a Poisson glm (model `m2a.5`). The residual variance models any overdispersion, and a residual variance of zero would imply that the response conforms to a standard Poisson." width="672" />
<p class="caption">
Figure 3.3: MCMC summary plot for the residual (<code>units</code>) variance from a Poisson glm (model <code>m2a.5</code>). The residual variance models any overdispersion, and a residual variance of zero would imply that the response conforms to a standard Poisson.
</p>
</div>
</div>
</div>
<div id="prediction-in-glm" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Prediction in GLM<a href="glm.html#prediction-in-glm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To get the expected number of accidents for the 92nd observation we simply exponentiated the latent variable: exp(<code>lat92</code>). However, it is important to realise that this is the expected number had the residual been exactly equal to the observed residual for that observation (<code>resid92</code>): we are calculating the expected number conditional on the set of unmeasured variables that affected that <em>particular</em> realisation of <span class="math inline">\(\texttt{day}\)</span> 92 in 1961 without a speed limit. When calculating a prediction we usually aim to average over these residuals (or random effects - see <a href="ranef.html#ranpred-sec">4.2</a>) since we would like to know what the average response would be for observations made on a <span class="math inline">\(\texttt{day}\)</span> of <em>type</em> 92 in 1961 without a speed limit. On the log-scale the expectation is simply the linear predictor (<span class="math inline">\(\eta\)</span>):</p>
<p><span class="math display">\[
log(E_e[y]) = E_e[l] = E_e[\eta+e] =  \eta+E_e[e]=\eta
\]</span></p>
<p>since the residuals have zero expectation (here I have subscripted the expectation with the variable we are averaging over). The <code>predict</code> function can be applied to <code>MCMCglmm</code> objects and if we specify <code>type="terms"</code> we get the prediction of the link scale - the log scale in this case:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="glm.html#cb66-1" tabindex="-1"></a><span class="fu">predict</span>(m2a<span class="fl">.5</span>, <span class="at">type =</span> <span class="st">&quot;terms&quot;</span>)[<span class="dv">92</span>]</span></code></pre></div>
<pre><code>## [1] 3.224037</code></pre>
<p>which is equal to the posterior mean of <code>eta92</code> obtained earlier. We can see this visually in Figure <a href="glm.html#fig:prediction2">3.4</a> where I have plotted the distribution of the latent variable on a <span class="math inline">\(\texttt{day}\)</span> of type 92 in 1961 without a speed limit.</p>
<div class="figure"><span style="display:block;" id="fig:prediction2"></span>
<img src="_bookdown_files/fig/prediction2-1.png" alt="The predicted distribution for the average number of accidents on the log scale for a $\texttt{day}$ of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor ($\eta=$) with a variance of $\sigma^{2}_e$. As a consequence the mean, median and mode of the distribution are equal to the linear predictor on the log scale." width="672" />
<p class="caption">
Figure 3.4: The predicted distribution for the average number of accidents on the log scale for a <span class="math inline">\(\texttt{day}\)</span> of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor (<span class="math inline">\(\eta=\)</span>) with a variance of <span class="math inline">\(\sigma^{2}_e\)</span>. As a consequence the mean, median and mode of the distribution are equal to the linear predictor on the log scale.
</p>
</div>
<p>To get the prediction on the data scale (i.e. in terms of the actual expected number of accidents) it is tempting to think we could just calculate <code>exp(eta92)</code>. However, this is the expected number of accidents had the residual been exactly zero. If we wish to average over the residuals we require:</p>
<p><span class="math display">\[
E_e[y] = E_e[\textrm{exp}(l)] = E_e[\textrm{exp}(\eta+e)]
\]</span></p>
<p>and because exponentiation is a non-linear function this average will deviate from <span class="math inline">\(\textrm{exp}(\eta)\)</span> (Figure <a href="glm.html#fig:prediction3">3.5</a>.</p>
<div class="figure"><span style="display:block;" id="fig:prediction3"></span>
<img src="_bookdown_files/fig/prediction3-1.png" alt="The predicted distribution for the average number of accidents on the data scale for a $\texttt{day}$ of type 92 in 1961 without a speed limit (in red).  On the log scale the distribution is assumed to be normal around the linear predictor ($\eta$) with a variance of $\sigma^{2}_e$ (see \@ref(fig:prediction2)). However when transforming to the data scale (by exponentiating) the symmetry is lost and the different measures of central tendency do not coincide. Since the residuals are normal on the log scale, the distribution on the data scale is log-normal and so analytical solutions exist for the mean, mode and median." width="672" />
<p class="caption">
Figure 3.5: The predicted distribution for the average number of accidents on the data scale for a <span class="math inline">\(\texttt{day}\)</span> of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor (<span class="math inline">\(\eta\)</span>) with a variance of <span class="math inline">\(\sigma^{2}_e\)</span> (see <a href="glm.html#fig:prediction2">3.4</a>). However when transforming to the data scale (by exponentiating) the symmetry is lost and the different measures of central tendency do not coincide. Since the residuals are normal on the log scale, the distribution on the data scale is log-normal and so analytical solutions exist for the mean, mode and median.
</p>
</div>
<p>To obtain predictions on the data scale we can specify <code>type="response"</code> (the default) when using <code>predict</code>:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="glm.html#cb68-1" tabindex="-1"></a><span class="fu">predict</span>(m2a<span class="fl">.5</span>)[<span class="dv">92</span>]</span></code></pre></div>
<pre><code>## [1] 26.48956</code></pre>
<p>which is slightly greater than <code>exp(eta92)</code> (25.183). For all link-functions, the median value on the data scale can be easily calculated by taking the inverse-link transform of the linear predictor. However, obtaining the mean and mode is often more challenging than it is for log-link, and numerical integration or approximations are required.</p>
<p>The <code>predict</code> function is returning a single number for observation 92 yet the model object contains 1,000 samples from the posterior distribution of all model parameters. This is because the <code>predict</code> function returns the posterior mean of the predicted value. Since we have the complete posterior distribution we can also place a 95% credible interval on the prediction (see Section <a href="bayesian.html#intervals-sec">2.4.2</a>):</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="glm.html#cb70-1" tabindex="-1"></a><span class="fu">predict</span>(m2a<span class="fl">.5</span>, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)[<span class="dv">92</span>, ]</span></code></pre></div>
<pre><code>##      fit      lwr      upr 
## 26.48956 23.27675 30.10716</code></pre>
<div id="posterior-predictive-distribution" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Posterior Predictive Distribution<a href="glm.html#posterior-predictive-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In some cases we would like to visualise or summarise aspects of the predictive distribution other than the mean. The complete predictive distribution is hard to work with, but the <code>simulate</code> function allows you to draw samples from the predictive distribution. The default is to generate a sample using a random draw from the posterior distribution, resulting in a draw from what is known as the posterior predictive distribution:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="glm.html#cb72-1" tabindex="-1"></a>ypred <span class="ot">&lt;-</span> <span class="fu">simulate</span>(m2a<span class="fl">.5</span>)</span></code></pre></div>
<p>We can use these simulated values to characterise any aspect of the predictive distribution we want. For example, we can obtain quantiles and compare them to the quantiles of the actual data to see how well the model captures aspects of the observed marginal distribution:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="glm.html#cb73-1" tabindex="-1"></a><span class="fu">qqplot</span>(ypred, Traffic<span class="sc">$</span>y)</span>
<span id="cb73-2"><a href="glm.html#cb73-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:marginal-qq"></span>
<img src="_bookdown_files/fig/marginal-qq-1.png" alt="qq-plot of the posterior predictive distribution and the data distribution" width="672" />
<p class="caption">
Figure 3.6: qq-plot of the posterior predictive distribution and the data distribution
</p>
</div>
<p>Not too bad, although the predictive distribution perhaps has greater support for extreme values than are observed. If we merely wish to know the interval in which some specified percentage of the data are predicted to lie, we can also use the predict function but with <code>interval="prediction"</code>. By default the 95% (highest posterior density) interval is calculated using as many simulated samples as there are saved posterior samples. For the 92nd observation the prediction interval is</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="glm.html#cb74-1" tabindex="-1"></a><span class="fu">predict</span>(m2a<span class="fl">.5</span>, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)[<span class="dv">92</span>, ]</span></code></pre></div>
<pre><code>##    fit    lwr    upr 
## 26.696 10.000 47.000</code></pre>
<p>Note that the reported mean (<code>fit</code>) differs from that returned by <code>interval="confidence"</code> due to Monte Carol error only.</p>
</div>
</div>
<div id="binom-sec" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Binomial and Bernoulli GLM<a href="glm.html#binom-sec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The general concepts introduced for the Poisson GLM extend naturally to Binomial data, albeit with a different link function. However, it is worth spending a little time exploring a Binomial GLM as I think overdispersion, and how we deal with it, is easier to understand with binomial data. However, we’ll also see how the ‘residuals’ defined earlier for capturing overdispersion complicate the analysis of Bernoulli data and result in MCMcglmm using a non-standard parameterisation.</p>
<p>The Binomial distribution has two parameters - the number of trials <span class="math inline">\(n\)</span> and the probability of success, <span class="math inline">\(p\)</span>. In a Binomial GLM the number of trials is assumed known leaving only <span class="math inline">\(p\)</span> to be estimated from the number of trials that are ‘successes’ or ‘failures’. When <code>family="binomial"</code> is specified (or equivalently <code>family="multinomial2"</code>) MCMCglmm uses the standard link function for the Binomial - the logit link - and the logit probability of success is modelled as</p>
<p><span class="math display">\[log\left(\frac{p}{1-p}\right)  = l = \eta+e\]</span></p>
<p>The logit transform takes a probability and turns it into a log odds ratio. If we want to get back to the probability we use the
inverse of the logit transform:</p>
<p><span class="math display">\[p = \frac{exp(l)}{1+exp(l)}\]</span></p>
<p>The logit link is actually the quantile function for the logistic distribution and so is available as the function <code>qlogis</code>. The inverse of a quantile function is a cumulative distribution function and so the inverse-logit transform is <code>plogis</code>.</p>
<p>To introduce the Binomial GLM we will analyse some data I collected on how grumpy my colleagues look. I took two photos (<span class="math inline">\(\texttt{photo}\)</span>) of 22 people (<span class="math inline">\(\texttt{person}\)</span>) working in the Institute of Evolution and Ecology, Edinburgh. In one photo the person was happy and in the other they were grumpy (<span class="math inline">\(\texttt{type}\)</span>). 122 respondents gave a score between 1 and 10 indicating how grumpy they thought each person looked in each photo (with 10 being the most grumpy).</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="glm.html#cb76-1" tabindex="-1"></a><span class="fu">data</span>(Grumpy)</span>
<span id="cb76-2"><a href="glm.html#cb76-2" tabindex="-1"></a>Grumpy[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">44</span>), ]</span></code></pre></div>
<pre><code>##           y  l5  g5   type photo   person age ypub
## 1  4.032787 101  21 grumpy  4511   ally_p  38   13
## 2  3.081967 113   9  happy  4512   ally_p  38   13
## 3  7.885246  15 107 grumpy  4521 darren_o  38   16
## 44 3.798319 105  14  happy  4516  laura_r  34   10</code></pre>
<p><span class="math inline">\(\texttt{y}\)</span> gives the average score given by the 122 respondents. The number of respondents giving a photo a score of five or less (<span class="math inline">\(\texttt{l5}\)</span>) or more than five (<span class="math inline">\(\texttt{g5}\)</span>) is also recorded in addition to the person’s age (<span class="math inline">\(\texttt{age}\)</span>) and a proxy for how long they had been academia - the number of years since they published their first academic paper (<span class="math inline">\(\texttt{ypub}\)</span>). Here, we will model the probability of getting a grumpy score greater than five as a function of whether the person was happy or grumpy and how long they had been in academia. As with <code>glm</code>, successes should be in the first column of the response, and failures in the second:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="glm.html#cb78-1" tabindex="-1"></a>mbinom<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(<span class="fu">cbind</span>(g5, l5) <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">data =</span> Grumpy, <span class="at">family =</span> <span class="st">&quot;multinomial2&quot;</span>,</span>
<span id="cb78-2"><a href="glm.html#cb78-2" tabindex="-1"></a>    <span class="at">pl =</span> <span class="cn">TRUE</span>)</span>
<span id="cb78-3"><a href="glm.html#cb78-3" tabindex="-1"></a><span class="fu">summary</span>(mbinom<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>## 
##  Iterations = 3001:12991
##  Thinning interval  = 10
##  Sample size  = 1000 
## 
##  DIC: 5377.999 
## 
##  R-structure:  ~units
## 
##       post.mean l-95% CI u-95% CI eff.samp
## units     1.348   0.7597    1.947     1000
## 
##  Location effects: cbind(g5, l5) ~ type + ypub 
## 
##             post.mean l-95% CI u-95% CI eff.samp  pMCMC    
## (Intercept)  -0.72025 -1.67538  0.05147     1000  0.094 .  
## typehappy    -1.28167 -1.97027 -0.59820     1000 &lt;0.001 ***
## ypub          0.02064 -0.00613  0.05175     1105  0.156    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The model coefficients are most easily interpreted after exponentiating as they then give the proportional change in the odds ratio. The odds of having a score greater than five is exp(-1.282)=0.278 times lower when happy, as expected. The odds increases by a factor exp(0.021)=1.021 for each year in academia. When the coefficient is small in magnitude like this, you can get a rough estimate by looking directly at the coefficient: 0.021 roughly translates into a 2.1% increase and -0.021 would translate into a 2.1% decrease<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>. The <span class="math inline">\(\texttt{units}\)</span> (residual) variance is also large with credible intervals that are far from zero.</p>
<div id="overdispersion-1" class="section level3 hasAnchor" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Overdispersion<a href="glm.html#overdispersion-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As with overdispersion in the Poisson model, this excess variation can be attributed to predictors that are not included in the model but cause the probability of success to vary over observations (photos in this case). For example, the 3rd and 25th observation have the same values for every predictor</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="glm.html#cb80-1" tabindex="-1"></a>Grumpy[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">25</span>), ]</span></code></pre></div>
<pre><code>##           y l5  g5   type photo   person age ypub
## 3  7.885246 15 107 grumpy  4521 darren_o  38   16
## 25 5.319672 73  49 grumpy  4527  craig_w  38   16</code></pre>
<p>and so in the absence of overdispersion these parameters would result in a predicted probability of success of</p>
<p><span class="math display" id="eq:plogis">\[p = \textrm{plogis}(\beta_{\texttt{(Intercept)}}+\beta_{\texttt{typehappy}}\times0+\beta_{\texttt{ypub}}\times16)=0.404
\label{plogis-eq}   \tag{3.3}\]</span></p>
<p>Given there were 122 respondents (trials) we can calculate the two values between which the number of successes is expected to fall 95 % of the time.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="glm.html#cb82-1" tabindex="-1"></a><span class="fu">qbinom</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="at">size =</span> <span class="dv">122</span>, <span class="at">prob =</span> p)</span></code></pre></div>
<pre><code>## [1] 39 60</code></pre>
<p>While photo <span class="math inline">\(\texttt{4527}\)</span> is well within the range with 49 successes, the number of respondents giving photo <span class="math inline">\(\texttt{4521}\)</span> is substantially higher (107) - it’s an outlier. In Figure <a href="glm.html#fig:DandC">3.7</a> the two photos are shown and it is clear why their underlying probabilities may deviate from that predicted. Most obviously the two photos are of different people and people vary in how grumpy they look. Since we have two photos per person we could (and should) estimate <span class="math inline">\(\texttt{person}\)</span> effects, however this is best done by treating these effects as random, which we will cover later, in Section <a href="ranef.html#GLMM">4.1</a>. Even if <span class="math inline">\(\texttt{person}\)</span> effects were fitted there’s also likely to be a whole host of observation-level (photo-specific) effects that are not captured in the model such as whether the person had their eyes closed or was wearing a dreary grey fleece.
<br>
<br></p>
<div class="figure"><span style="display:block;" id="fig:DandC"></span>
<img src="Figures/IMG_4521.JPG" alt="Photo 4521 (left) and photo 4527 (right). For the predictors fitted in model `mbinom.1`, these photos have the same values ($\texttt{type}=\texttt{grumpy}$ and $\texttt{ypub}=16$ years)" width="45%" style="display:inline-block; vertical-align:middle; margin-right:2%;" /><img src="Figures/IMG_4527.JPG" alt="Photo 4521 (left) and photo 4527 (right). For the predictors fitted in model `mbinom.1`, these photos have the same values ($\texttt{type}=\texttt{grumpy}$ and $\texttt{ypub}=16$ years)" width="45%" style="display:inline-block; vertical-align:middle; margin-right:2%;" />
<p class="caption">
Figure 3.7: Photo 4521 (left) and photo 4527 (right). For the predictors fitted in model <code>mbinom.1</code>, these photos have the same values (<span class="math inline">\(\texttt{type}=\texttt{grumpy}\)</span> and <span class="math inline">\(\texttt{ypub}=16\)</span> years)
</p>
</div>
<p>If we include the ‘residuals’ when calculating the predicted probability for these two photos we can see that indeed their probabilities are quite different:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="glm.html#cb84-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">plogis</span>(mbinom<span class="fl">.1</span><span class="sc">$</span>Liab[, <span class="dv">3</span>]))</span></code></pre></div>
<pre><code>## [1] 0.8621967</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="glm.html#cb86-1" tabindex="-1"></a><span class="co"># predicted probability for photo 4521</span></span>
<span id="cb86-2"><a href="glm.html#cb86-2" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">plogis</span>(mbinom<span class="fl">.1</span><span class="sc">$</span>Liab[, <span class="dv">25</span>]))</span></code></pre></div>
<pre><code>## [1] 0.4001446</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="glm.html#cb88-1" tabindex="-1"></a><span class="co"># predicted probability for photo 4527</span></span></code></pre></div>
</div>
<div id="prediction" class="section level3 hasAnchor" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Prediction<a href="glm.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When calculating the predicted probability in the absence of the residuals in Equation <a href="glm.html#eq:plogis">(3.3)</a> I was careful to say that the prediction assumed an absence of overdispersion. However, when overdispersion is present we need to average over the distribution of the residuals in order to get an average. As we saw in the log-linear Poisson model, because the inverse-link function (<code>plogis</code>) is non-linear the average of <span class="math inline">\(E_e[\texttt{plogis}(\eta+e)]\)</span> is different from <span class="math inline">\(\texttt{plogis}(E_e[\eta+e])=\texttt{plogis}(\eta)\)</span>. Unlike the Poisson log-linear model this expectation cannot be calculated analytically and the predict function by default numerically evaluates the integral:</p>
<p><span class="math display">\[\int_l \texttt{plogis}(l)f_N(l | \eta, \sigma^2_e)dl\]</span></p>
<p>where <span class="math inline">\(f_N\)</span> is the probability density function of the normal. For each of the 44 observations this is done 1,000 times (the number of saved posterior samples) to get the posterior mean prediction and can be very slow (it is actually faster to fit the model). The number reported by <code>predict</code> is <span class="math inline">\(np\)</span> rather than <span class="math inline">\(p\)</span> and so to get the predicted probability for a photo of someone who has grumpy and had been publishing for 16 years we can get the prediction for the 3rd observation and divide by the number of trials (122):</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="glm.html#cb89-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.1</span>)[<span class="dv">3</span>]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>## [1] 0.4243498</code></pre>
<p>A little different from <span class="math inline">\(\texttt{plogis}(\eta)=\)</span> 0.404. For the binomial with logit link, analytical approximations have been developed in <span class="citation">Diggle et al. (<a href="#ref-Diggle.2004">2004</a>)</span> and <span class="citation">McCulloch and Searle (<a href="#ref-McCulloch.2001">2001</a>)</span> which are considerably faster and reasonably accurate:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="glm.html#cb91-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.1</span>, <span class="at">approx =</span> <span class="st">&quot;diggle&quot;</span>)[<span class="dv">3</span>]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>## [1] 0.4210936</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="glm.html#cb93-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.1</span>, <span class="at">approx =</span> <span class="st">&quot;mcculloch&quot;</span>)[<span class="dv">3</span>]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>## [1] 0.4255396</code></pre>
</div>
<div id="bernoulli-sec" class="section level3 hasAnchor" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> Bernoulli GLM<a href="glm.html#bernoulli-sec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bernoulli data are a special case of the Binomial in which the number of trials is equal to one and so either a success or a failure is observed. To explore a Brenoulli model we can take our <em>average</em> Grumpy scores and simply dichotomise them into whether the average score was five or less or more than five:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="glm.html#cb95-1" tabindex="-1"></a>Grumpy<span class="sc">$</span>mean5 <span class="ot">&lt;-</span> Grumpy<span class="sc">$</span>y <span class="sc">&gt;</span> <span class="dv">5</span></span></code></pre></div>
<p>If we fit a binomial model to these data in <code>MCMCglmm</code> it has exactly the same form as before (although a single column of outcomes can be passed). But importantly, for Bernoulli data there is no information to estimate the residual variance. This does not necessarily mean that variation in the probability of success across observations is absent, only that we can’t estimate it. For example, imagine we took 100 people who had been publishing for 16 years and took a photograph of them when they were grumpy. Let’s say the probability that the mean score for such photos exceeded 5 was 0.5. If the probability for all photos was exactly 0.5 (i.e. the probability of success did not vary over observations) then we expect 50 success and 50 failures across our observations. However, imagine the case where the probability of success was 100% for 50 photos and 0% for 50 photos (i.e. the probability of success varies considerably over observations). We would also expect 50 success and 50 failures, and so the distribution of successes with and without variation in the underlying probability would be identical. In the absence of information most software sets the ‘residual’ variance to zero (i.e. the probability of success dose not vary over observations), but it is important to understand that this is a convenient but arbitrary choice. Given this, it is desirable that any conclusions drawn from the model do not depend on this arbitrary choice. Worryingly, both the location effects (fixed and random) and variance components are completely dependent on the magnitude of the residual variance. <code>MCMCglmm</code> allows the user to fix the residual variance at a value of their choice, but unfortunately a value of zero results in a chain that will not mix and so I usually fix the residual variance to one<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>:</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="glm.html#cb96-1" tabindex="-1"></a>prior.mbern<span class="fl">.1</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb96-2"><a href="glm.html#cb96-2" tabindex="-1"></a>mbern<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(mean5 <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">data =</span> Grumpy, <span class="at">prior =</span> prior.mbern<span class="fl">.1</span>)</span></code></pre></div>
<p>However, it would have been equally valid to fixed the residual variance at three:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="glm.html#cb97-1" tabindex="-1"></a>prior.mbern<span class="fl">.2</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">3</span>, <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb97-2"><a href="glm.html#cb97-2" tabindex="-1"></a>mbern<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(mean5 <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">data =</span> Grumpy, <span class="at">prior =</span> prior.mbern<span class="fl">.2</span>)</span></code></pre></div>
<p>and if we compare the MCMC traces for the coefficients we can see that we are sampling different posterior distributions (Figure <a href="glm.html#fig:bernoulli">3.8</a>).</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="glm.html#cb98-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(mbern<span class="fl">.1</span><span class="sc">$</span>Sol, mbern<span class="fl">.2</span><span class="sc">$</span>Sol), <span class="at">density =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:bernoulli"></span>
<img src="_bookdown_files/fig/bernoulli-1.png" alt="MCMC trace for coefficients of a Bernoulli GLM from two models (`mbern.1` in black and `mbern.2` in red). The data and model structure are identical but in `mbern.1` the residual variance was set to one and in `mbern.2` the residual variance was set to three. The data provide no information about the residual variance." width="672" />
<p class="caption">
Figure 3.8: MCMC trace for coefficients of a Bernoulli GLM from two models (<code>mbern.1</code> in black and <code>mbern.2</code> in red). The data and model structure are identical but in <code>mbern.1</code> the residual variance was set to one and in <code>mbern.2</code> the residual variance was set to three. The data provide no information about the residual variance.
</p>
</div>
<p>Should we worry? Not really. The two models give almost identical predictions (Figure <a href="glm.html#fig:bernoulli-pred">3.9</a>).</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="glm.html#cb99-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(mbern<span class="fl">.1</span>), <span class="fu">predict</span>(mbern<span class="fl">.2</span>))</span>
<span id="cb99-2"><a href="glm.html#cb99-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:bernoulli-pred"></span>
<img src="_bookdown_files/fig/bernoulli-pred-1.png" alt="Predicted probabilities from a Bernoulli GLM from two models. The data and model structure are identical but in `mbern.1` the residual variance was set to one and in `mbern.2` the residual variance was set to three. The data provide no information about the residual variance." width="672" />
<p class="caption">
Figure 3.9: Predicted probabilities from a Bernoulli GLM from two models. The data and model structure are identical but in <code>mbern.1</code> the residual variance was set to one and in <code>mbern.2</code> the residual variance was set to three. The data provide no information about the residual variance.
</p>
</div>
<p>We just have to be careful about how we express the results. Stating that the <code>typehappy</code> coefficient is -2.852 (the posterior mean estimate from <code>mbern.1</code>) is meaningless without putting it in the context of the assumed residual variance (one). Although the <span class="citation">Diggle et al. (<a href="#ref-Diggle.2004">2004</a>)</span> approximation is less accurate than that in <span class="citation">McCulloch and Searle (<a href="#ref-McCulloch.2001">2001</a>)</span> we can use it rescale the estimates by the assumed residual variance (we’ll call it <span class="math inline">\(\sigma^{2}_{\texttt{units}}\)</span>) in order to obtain the posterior distributions of the parameters under the assumption that the actual residual variance (we’ll call it <span class="math inline">\(\sigma^{2}_{e}\)</span>) is equal to some other value. For location effects the posterior distribution needs to be multiplied by <span class="math inline">\(\sqrt{\frac{1+c^{2}\sigma^{2}_{e}}{1+c^{2}\sigma^{2}_{\texttt{units}}}}\)</span> where <span class="math inline">\(c=16\sqrt{3}/15\pi\)</span>. If obtain estimates under the assumption that <span class="math inline">\(\sigma^{2}_{e}=0\)</span> and we see the posterior distributions of the coefficients are very similar from the two models (Figure <a href="glm.html#fig:bernoulli-rescale">3.10</a>).</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="glm.html#cb100-1" tabindex="-1"></a>c2 <span class="ot">&lt;-</span> ((<span class="dv">16</span> <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">3</span>))<span class="sc">/</span>(<span class="dv">15</span> <span class="sc">*</span> pi))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb100-2"><a href="glm.html#cb100-2" tabindex="-1"></a>rescale<span class="fl">.2</span> <span class="ot">&lt;-</span> mbern<span class="fl">.1</span><span class="sc">$</span>Sol <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> c2 <span class="sc">*</span> <span class="dv">1</span>))</span>
<span id="cb100-3"><a href="glm.html#cb100-3" tabindex="-1"></a>rescale<span class="fl">.3</span> <span class="ot">&lt;-</span> mbern<span class="fl">.2</span><span class="sc">$</span>Sol <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> c2 <span class="sc">*</span> <span class="dv">3</span>))</span>
<span id="cb100-4"><a href="glm.html#cb100-4" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(<span class="fu">as.mcmc</span>(rescale<span class="fl">.2</span>), <span class="fu">as.mcmc</span>(rescale<span class="fl">.3</span>)), <span class="at">density =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:bernoulli-rescale"></span>
<img src="_bookdown_files/fig/bernoulli-rescale-1.png" alt="MCMC trace for rescaled coefficients of a Bernoulli GLM from two models (`mbern.1` in black and `mbern.2` in red). The data and model structure are identical but in `mbern.1` the residual variance was set to one and in `mbern.2` the residual variance was set to three. However, the coefficients have been rescaled using the @Diggle.2004 approximation such that they represent what the coefficients would be if the residual variance was zero. The data provide no information about the residual variance." width="672" />
<p class="caption">
Figure 3.10: MCMC trace for rescaled coefficients of a Bernoulli GLM from two models (<code>mbern.1</code> in black and <code>mbern.2</code> in red). The data and model structure are identical but in <code>mbern.1</code> the residual variance was set to one and in <code>mbern.2</code> the residual variance was set to three. However, the coefficients have been rescaled using the <span class="citation">Diggle et al. (<a href="#ref-Diggle.2004">2004</a>)</span> approximation such that they represent what the coefficients would be if the residual variance was zero. The data provide no information about the residual variance.
</p>
</div>
<p>In addition, the posterior distributions are centred on the ML estimates obtained by <code>glm</code> which implicitly assumes <span class="math inline">\(\sigma^2_e=0\)</span> (Figure <a href="glm.html#fig:bernoulli-glm">3.11</a>).</p>
<div class="figure"><span style="display:block;" id="fig:bernoulli-glm"></span>
<img src="_bookdown_files/fig/bernoulli-glm-1.png" alt="Posterior distributions for rescaled coefficients of a Bernoulli GLM (`mbern.2`). The rescaling gives approximate (but accurate) posterior distributions had the residual variance been set to zero, rather than three. The red lines indicate estimates from `glm` that implicitly assumes the residual variance is zero and the blue lines indicate the unscaled posterior means." width="672" />
<p class="caption">
Figure 3.11: Posterior distributions for rescaled coefficients of a Bernoulli GLM (<code>mbern.2</code>). The rescaling gives approximate (but accurate) posterior distributions had the residual variance been set to zero, rather than three. The red lines indicate estimates from <code>glm</code> that implicitly assumes the residual variance is zero and the blue lines indicate the unscaled posterior means.
</p>
</div>
</div>
<div id="probit-link" class="section level3 hasAnchor" number="3.6.4">
<h3><span class="header-section-number">3.6.4</span> Probit link<a href="glm.html#probit-link" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The inverse-logit function is the cumulative distribution function for the logistic distribution. It makes sense that a cumulative distribution function for a continuous distribution that can take any value would serve as a good inverse link function for a probability: it takes any value between plus and minus infinity and squeezes it to be between zero and one. While the logit is the most common link function for binomial data, the probit link and complementary log-log (cloglog) link are also widely used. They are the cumulative distribution functions for the unit normal and standard Gumbel distributions respectively. In Figure <a href="glm.html#fig:link">3.12</a> we can see how the probability changes as a function of a covariate (<span class="math inline">\(x\)</span>) for the different links (with intercepts and slopes chosen so that the functions are matched at the origin).</p>
<div class="figure"><span style="display:block;" id="fig:link"></span>
<img src="_bookdown_files/fig/link-1.png" alt="Predicted probabilities as a function of covariate $x$ using the inverse logit (black), inverse probit (red) and inverse complementary log-log link functions. For each link the intercept and slope were chosen such that when $x= 0$ the function equals 0.5 and has a derivative of one." width="672" />
<p class="caption">
Figure 3.12: Predicted probabilities as a function of covariate <span class="math inline">\(x\)</span> using the inverse logit (black), inverse probit (red) and inverse complementary log-log link functions. For each link the intercept and slope were chosen such that when <span class="math inline">\(x= 0\)</span> the function equals 0.5 and has a derivative of one.
</p>
</div>
<p>We can see that the link functions generate rather similar predicted probabilities, particularly the logit and probit links. For Bernoulli data, another way to conceptualise these link functions is in terms of threshold models. Imagine the case where we have managed to set the non-identifiable residual variance to zero and all residuals <span class="math inline">\(e\)</span> are zero and can be omitted. The probability of success is then given by the inverse-link of <span class="math inline">\(\eta\)</span>. For probit link this would be the probability of getting a value less than <span class="math inline">\(\eta\)</span> from the unit normal (the grey area in the left panel of Figure <a href="glm.html#fig:thresh">3.13</a>). Equivalently, it is the probability of getting a value greater than 0 from a normal with a mean equal to <span class="math inline">\(\eta\)</span> and a standard deviation of one (the grey area in the right panel of Figure <a href="glm.html#fig:thresh">3.13</a>).</p>
<div class="figure"><span style="display:block;" id="fig:thresh"></span>
<img src="_bookdown_files/fig/thresh-1.png" alt="Probability density function for $\epsilon$ (left) or  $\eta+\epsilon$ (right) where $\epsilon$ is normal with mean zero and a standard deviation of one. Applying `pnorm` to $\eta$ gives the shaded area on the left and is the probability of success using probit link. The shaded area on the right gives the same probability and is the chance of getting a value greater than zero from a normal with mean $\eta$ and a standard deviation of one." width="768" />
<p class="caption">
Figure 3.13: Probability density function for <span class="math inline">\(\epsilon\)</span> (left) or <span class="math inline">\(\eta+\epsilon\)</span> (right) where <span class="math inline">\(\epsilon\)</span> is normal with mean zero and a standard deviation of one. Applying <code>pnorm</code> to <span class="math inline">\(\eta\)</span> gives the shaded area on the left and is the probability of success using probit link. The shaded area on the right gives the same probability and is the chance of getting a value greater than zero from a normal with mean <span class="math inline">\(\eta\)</span> and a standard deviation of one.
</p>
</div>
<p>If we think explicitly about the normal deviates that underpin these probability calculations (we’ll call them <span class="math inline">\(\epsilon\)</span>), then in the left panel <span class="math inline">\(\epsilon\)</span> falls below the ‘threshold’ <span class="math inline">\(\eta\)</span> with the required probability or in the right panel, <span class="math inline">\(\eta+\epsilon\)</span> falls above the ‘threshold’ zero with the required probability. Since <span class="math inline">\(\epsilon\)</span> is unit-normal we can equate <span class="math inline">\(\eta\)</span> with <span class="math inline">\(e\)</span> and <span class="math inline">\(\eta+\epsilon\)</span> with <span class="math inline">\(l\)</span> and apply the inverse link function <span class="math inline">\(\mathbf{1}_{\{l&gt;0\}}\)</span>. This function outputs 1 (a success) if <span class="math inline">\(l&gt;0\)</span> and a failure otherwise (as in the right panel of Figure <a href="glm.html#fig:thresh">3.13</a>). This is implemented as <code>family=threshold</code> in MCMCglmm, and if the residual variance (i.e. the variance of <span class="math inline">\(\epsilon\)</span> or <span class="math inline">\(e\)</span>) is fixed at one corresponds exactly to standard probit regression<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="glm.html#cb101-1" tabindex="-1"></a>prior.mbinom<span class="fl">.4</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb101-2"><a href="glm.html#cb101-2" tabindex="-1"></a>mbinom<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(mean5 <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">family =</span> <span class="st">&quot;threshold&quot;</span>, <span class="at">data =</span> Grumpy, <span class="at">prior =</span> prior.mbinom<span class="fl">.4</span>)</span></code></pre></div>
<p>Unfortunately the same trick can’t be used for other link functions because the <span class="math inline">\(\epsilon\)</span>’s’ cannot be equated with <span class="math inline">\(e\)</span>’s because they come from different distributions. In some ways the probit link is a natural link function for models that contain random effects, which are also usually assumed to be normal. However, the downside is that the coefficients in a probit model do not have a direct interpretation like they do in a logit model. Nevertheless, both models give very similar predictions and are unlikely to be statistically distinguishable in the vast mean5 of cases (Figure <a href="glm.html#fig:thresh-logit">3.14</a>).</p>
<div class="figure"><span style="display:block;" id="fig:thresh-logit"></span>
<img src="_bookdown_files/fig/thresh-logit-1.png" alt="Predicted probabilities from a Bernoulli GLM from two models with the same model structure. However, `mbern.1` uses a logit link with a residual variance set to one and in `mbinom.4` uses a standard probit link." width="672" />
<p class="caption">
Figure 3.14: Predicted probabilities from a Bernoulli GLM from two models with the same model structure. However, <code>mbern.1</code> uses a logit link with a residual variance set to one and in <code>mbinom.4</code> uses a standard probit link.
</p>
</div>
</div>
</div>
<div id="ordinal-data" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Ordinal Data<a href="glm.html#ordinal-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Thinking about Bernoulli GLM’s in terms of thresholds provides a natural way of thinking about how we could model categorical data that falls into a natural ordering. For example, rather than dichotomising our outcome into those that had a mean score greater than, or less than, five, lets place the observations into three categories: (1, 4], (4, 6], (6, 10].</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="glm.html#cb102-1" tabindex="-1"></a>Grumpy<span class="sc">$</span>categories <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">cut</span>(Grumpy<span class="sc">$</span>y, <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">10</span>)))</span></code></pre></div>
<p>Rather than just having a single threshold at zero, we can imagine adding another threshold that chops the distribution of <span class="math inline">\(\eta+e=l\)</span> into three regions, and hence probabilities. To make things simple we will just fit an intercept only model (so all observations have the same value of <span class="math inline">\(\eta\)</span>) as this will be easier to visualise:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="glm.html#cb103-1" tabindex="-1"></a>prior.mordinal <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb103-2"><a href="glm.html#cb103-2" tabindex="-1"></a>mordinal <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(categories <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> Grumpy, <span class="at">family =</span> <span class="st">&quot;threshold&quot;</span>, <span class="at">prior =</span> prior.mordinal)</span></code></pre></div>
<p>The output of <code>mordinal</code> gives the intercept (<span class="math inline">\(\eta\)</span> in this case) as before but also the additional threshold (cutpoint - stored as <code>CP</code> in the model object):</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="glm.html#cb104-1" tabindex="-1"></a><span class="fu">summary</span>(mordinal)</span></code></pre></div>
<pre><code>## 
##  Iterations = 3001:12991
##  Thinning interval  = 10
##  Sample size  = 1000 
## 
##  DIC: 92.2748 
## 
##  R-structure:  ~units
## 
##       post.mean l-95% CI u-95% CI eff.samp
## units         1        1        1        0
## 
##  Location effects: categories ~ 1 
## 
##             post.mean l-95% CI u-95% CI eff.samp pMCMC  
## (Intercept)    0.4892   0.1356   0.9025      607 0.012 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Cutpoints: 
## 
##                            post.mean l-95% CI u-95% CI eff.samp
## cutpoint.traitcategories.1     1.322   0.8801    1.775    435.8</code></pre>
<p>As we did in the right panel of Figure <a href="glm.html#fig:thresh">3.13</a>, we can draw this model with the estimated threshold (<span class="math inline">\(\gamma\)</span>) included (Figure <a href="glm.html#fig:ordinal">3.15</a>).</p>
<div class="figure"><span style="display:block;" id="fig:ordinal"></span>
<img src="_bookdown_files/fig/ordinal-1.png" alt="Probability density function for  $l=\eta+e$ where $e$ is normal with mean zero and a standard deviation of one. $\eta=$ 0.489 and was obtained from the model `mordinal`. The distribution is 'cut' into three regions by the fixed threshold at zero and the estimated threshold ($\gamma$) at 1.322. The shaded areas correspond to the probabilities of observing the three (ordered) outcomes." width="672" />
<p class="caption">
Figure 3.15: Probability density function for <span class="math inline">\(l=\eta+e\)</span> where <span class="math inline">\(e\)</span> is normal with mean zero and a standard deviation of one. <span class="math inline">\(\eta=\)</span> 0.489 and was obtained from the model <code>mordinal</code>. The distribution is ‘cut’ into three regions by the fixed threshold at zero and the estimated threshold (<span class="math inline">\(\gamma\)</span>) at 1.322. The shaded areas correspond to the probabilities of observing the three (ordered) outcomes.
</p>
</div>
<p>The shaded areas give the probability for each category and the posterior mean probabilities can be easily calculated:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="glm.html#cb106-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">pnorm</span>(<span class="dv">0</span>, mordinal<span class="sc">$</span>Sol))</span></code></pre></div>
<pre><code>## [1] 0.3158146</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="glm.html#cb108-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">pnorm</span>(mordinal<span class="sc">$</span>CP, mordinal<span class="sc">$</span>Sol) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">0</span>, mordinal<span class="sc">$</span>Sol))</span></code></pre></div>
<pre><code>## [1] 0.4768311</code></pre>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="glm.html#cb110-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(mordinal<span class="sc">$</span>CP, mordinal<span class="sc">$</span>Sol))</span></code></pre></div>
<pre><code>## [1] 0.2073543</code></pre>
<p>These correspond closely to the observed frequencies in the data:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="glm.html#cb112-1" tabindex="-1"></a><span class="fu">table</span>(Grumpy<span class="sc">$</span>categories)<span class="sc">/</span><span class="dv">44</span></span></code></pre></div>
<pre><code>## 
##         1         2         3 
## 0.3181818 0.4772727 0.2045455</code></pre>
</div>
<div id="non-zero-binomial-data" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Non-zero Binomial Data<a href="glm.html#non-zero-binomial-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The final distribution that we will cover in this Chapter is something I have called the Non-zero Binomial which can be fitted using <code>family="nzbinom"</code>. It was implemented for a specific application in which <span class="math inline">\(n\)</span> (<span class="math inline">\(\texttt{number}\)</span>) bumblebees were pooled and assayed for the presence of the Acute Bee Paralysis virus <span class="citation">(<a href="#ref-Pascall.2018">Pascall et al. 2018</a>)</span>. If the assay came back positive then at least one bee in the pool was infected and the outcome was recorded as a ‘success’ - otherwise non of the bees in the pool were infected and we have a ‘failure’ (<span class="math inline">\(\texttt{infected}\)</span>).</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="glm.html#cb114-1" tabindex="-1"></a><span class="fu">data</span>(ABPvirus)</span>
<span id="cb114-2"><a href="glm.html#cb114-2" tabindex="-1"></a>ABPvirus[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">98</span><span class="sc">:</span><span class="dv">99</span>), ]</span></code></pre></div>
<pre><code>##        species number infected
## 1  B.pascuorum     10        0
## 2  B.pascuorum     10        0
## 98   B.lucorum      2        1
## 99   B.lucorum     10        0</code></pre>
<p>The observations are essentially censored, and while we cover censoring more generally in Chapter <a href="#measurement"><strong>??</strong></a>, the Non-zero Binomial is perhaps best covered here. The probability of a success for the Non-zero Binomial (which we will designate as <span class="math inline">\(P\)</span>) is <span class="math inline">\(1-(1-p)^n\)</span> where <span class="math inline">\(p\)</span> is the probability that an <em>individual</em> bumblebee was infected. If the number of bumblebees in a pool varies over observations, <code>family="nzbinom"</code> is a useful tool and the linear model is defined for the logit transform of <span class="math inline">\(p\)</span>, as in the standard binomial. In the standard Bernoulli model the residual variance could not be estimated and was fixed at some value. Perhaps surprisingly, there is some information to estimate the residual variance (overdispersion) in a Non-zero Binomial model as long as <span class="math inline">\(n\)</span> varies over pools. However, the amount of information is so small that it is probably safest to fix the residual variance at some non-zero value (I use one as a convention) rather than risk sampling very high values of the residual variance that can cause numerical issues. We will ignore which species of bumblebee the observations were made on, as these are best treated as random effects (Chapter <a href="ranef.html#ranef">4</a>)</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="glm.html#cb116-1" tabindex="-1"></a>mnzbinom <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(<span class="fu">cbind</span>(infected, number) <span class="sc">~</span> <span class="dv">1</span>, <span class="at">family =</span> <span class="st">&quot;nzbinom&quot;</span>, <span class="at">data =</span> ABPvirus,</span>
<span id="cb116-2"><a href="glm.html#cb116-2" tabindex="-1"></a>    <span class="at">prior =</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>)))</span>
<span id="cb116-3"><a href="glm.html#cb116-3" tabindex="-1"></a><span class="fu">summary</span>(mnzbinom)</span></code></pre></div>
<pre><code>## 
##  Iterations = 3001:12991
##  Thinning interval  = 10
##  Sample size  = 1000 
## 
##  DIC: 131.4134 
## 
##  R-structure:  ~units
## 
##       post.mean l-95% CI u-95% CI eff.samp
## units         1        1        1        0
## 
##  Location effects: cbind(infected, number) ~ 1 
## 
##             post.mean l-95% CI u-95% CI eff.samp  pMCMC    
## (Intercept)    -3.088   -3.489   -2.656      510 &lt;0.001 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The median probability of an individual bumblebee being infected is low: <span class="math inline">\(\texttt{plogis}(eta)=\)</span> 0 such that the expected probability of at least one infection is small for the lowest pool-size (<span class="math inline">\(\texttt{number}=\)</span> 2) and is still well below one in the largest pool size (<span class="math inline">\(\texttt{number}=\)</span> 11) - see Figure <a href="glm.html#fig:nzbinom">3.16</a>.</p>
<div class="figure"><span style="display:block;" id="fig:nzbinom"></span>
<img src="_bookdown_files/fig/nzbinom-1.png" alt="Probability of seeing at least one infection ($P$) in pools of varying size when the probability of an individual bumblebee being infected ($p$) is constant. The solid black line is the posterior mean and the shaded area is the 95\% credible interval. The estimated median probability of an individual bumblebee being infected is0.044and the mean probability is0.044." width="672" />
<p class="caption">
Figure 3.16: Probability of seeing at least one infection (<span class="math inline">\(P\)</span>) in pools of varying size when the probability of an individual bumblebee being infected (<span class="math inline">\(p\)</span>) is constant. The solid black line is the posterior mean and the shaded area is the 95% credible interval. The estimated median probability of an individual bumblebee being infected is0.044and the mean probability is0.044.
</p>
</div>
<p>If the data set was larger and/or variation in the pool size was greater, there may be sufficient information to justify trying to estimate the residual variance, rather than fixing it to one. To see where the information comes from first imagine that you have information on samples all with a pool-size of 1. You could jut average you the resulting zeros and ones to get an estimate of <span class="math inline">\(P_1\)</span> where the subscript designates the size of the pool for which the probability of at least one infection is calculated. Even if the probability of infection varied over individuals, the average probability <span class="math inline">\(E[p]\)</span> is equal to <span class="math inline">\(P_1\)</span>, as we saw when we determined why variation in the probability cannot be estimated with standard Bernoulli data. Let’s then imagine we move to pool sizes of 2. <span class="math inline">\(P_2\)</span> (the probability of at least one success in a pool of 2) is then <span class="math inline">\(E[1-(1-p)^2]\)</span>. If there is no variation in <span class="math inline">\(p\)</span> then <span class="math inline">\(P_2=1-(1-P_1)^2\)</span> since <span class="math inline">\(p\)</span> is a constant and equal to <span class="math inline">\(P_1\)</span>. However, when there is variation in <span class="math inline">\(p\)</span> the non-linearity in the function (i.e. <span class="math inline">\((1-p)^2\)</span>) means that <span class="math inline">\(E[1-(1-p)^2]\)</span> will deviate from <span class="math inline">\(1-(1-E[p])^2=1-(1-P_1)^2\)</span>: we expect the the number of successful pools of size 2 to be less than that predicted from estimating <span class="math inline">\(p\)</span> from pools of size 1<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>. Consequently, the rate at which <span class="math inline">\(P\)</span> asymptotes with increasing pool-size (as seen in Figure <a href="glm.html#fig:nzbinom">3.16</a>) provides information about how much variation in <span class="math inline">\(p\)</span> exists.</p>
</div>
<div id="complete-separation" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Complete Separation<a href="glm.html#complete-separation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One potential issue that can occur in GLM is something known as complete separation or the extreme category problem. While it can occur for any distribution which is discrete, it is most commonly seen when the response is Bernoulli. It occurs when the predictors perfectly predict the outcome. For example, in Section <a href="glm.html#bernoulli-sec">3.6.3</a> we generated Bernoulli data for each photo by assessing whether the mean score was greater than or less than five. Photo <span class="math inline">\(\texttt{type}\)</span> is a very good predictor of the outcome, but not perfect:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="glm.html#cb118-1" tabindex="-1"></a><span class="fu">table</span>(<span class="at">mean5 =</span> Grumpy<span class="sc">$</span>mean5, <span class="at">type =</span> Grumpy<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>##        type
## mean5   grumpy happy
##   FALSE      9    19
##   TRUE      13     3</code></pre>
<p>However, if we removed the three observations for which the mean grumpy score was greater than five despite the person being happy, then <span class="math inline">\(\texttt{type}\)</span> perfectly predicts the outcome. If we fit a probit model to these data, but using <code>glm</code>, a very strange thing happens:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="glm.html#cb120-1" tabindex="-1"></a>data.cs <span class="ot">&lt;-</span> <span class="fu">subset</span>(Grumpy, <span class="sc">!</span>(mean5 <span class="sc">==</span> <span class="cn">TRUE</span> <span class="sc">&amp;</span> type <span class="sc">==</span> <span class="st">&quot;happy&quot;</span>))</span>
<span id="cb120-2"><a href="glm.html#cb120-2" tabindex="-1"></a></span>
<span id="cb120-3"><a href="glm.html#cb120-3" tabindex="-1"></a>mcs <span class="ot">&lt;-</span> <span class="fu">glm</span>(mean5 <span class="sc">~</span> type, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> probit), <span class="at">data =</span> data.cs)</span>
<span id="cb120-4"><a href="glm.html#cb120-4" tabindex="-1"></a><span class="fu">summary</span>(mcs)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = mean5 ~ type, family = binomial(link = probit), 
##     data = data.cs)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)   0.2299     0.2698   0.852    0.394
## typehappy    -5.9798   359.8415  -0.017    0.987
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 51.221  on 40  degrees of freedom
## Residual deviance: 29.767  on 39  degrees of freedom
## AIC: 33.767
## 
## Number of Fisher Scoring iterations: 17</code></pre>
<p>The coefficient for <span class="math inline">\(\texttt{typehappy}\)</span> now has a huge standard error and the p-value is close to one, despite being a very good predictor of the outcome. If we assess significance using Fisher’s exact test we get:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="glm.html#cb122-1" tabindex="-1"></a><span class="fu">fisher.test</span>(<span class="fu">table</span>(data.cs<span class="sc">$</span>mean5, data.cs<span class="sc">$</span>type))</span></code></pre></div>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  table(data.cs$mean5, data.cs$type)
## p-value = 2.977e-05
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.0000000 0.2098429
## sample estimates:
## odds ratio 
##          0</code></pre>
<p>A very significant result, as expected.</p>
<p>The model fitted using <span class="math inline">\(\texttt{MCMCglmm}\)</span> also behaves oddly with very poor mixing (Figure <a href="glm.html#fig:separation1">3.17</a>).</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="glm.html#cb124-1" tabindex="-1"></a>prior.mcs<span class="fl">.2</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb124-2"><a href="glm.html#cb124-2" tabindex="-1"></a>mcs<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(mean5 <span class="sc">~</span> type, <span class="at">family =</span> <span class="st">&quot;threshold&quot;</span>, <span class="at">data =</span> data.cs, <span class="at">prior =</span> prior.mcs<span class="fl">.2</span>)</span>
<span id="cb124-3"><a href="glm.html#cb124-3" tabindex="-1"></a><span class="fu">plot</span>(mcs<span class="fl">.2</span><span class="sc">$</span>Sol)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:separation1"></span>
<img src="_bookdown_files/fig/separation1-1.png" alt="MCMC summary plots for the intercept and $\texttt{typehappy}$ effect in a binary GLM (`mcs.2`) with probit link. For $\texttt{happy}$ photos all 19 observations are failures (the mean grumpy score is less than 5) and we have complete separation. A normal prior with large variance was used for the model coefficients." width="672" />
<p class="caption">
Figure 3.17: MCMC summary plots for the intercept and <span class="math inline">\(\texttt{typehappy}\)</span> effect in a binary GLM (<code>mcs.2</code>) with probit link. For <span class="math inline">\(\texttt{happy}\)</span> photos all 19 observations are failures (the mean grumpy score is less than 5) and we have complete separation. A normal prior with large variance was used for the model coefficients.
</p>
</div>
<p>This strange behaviour occurs because the ML estimate for the probability of success is zero for <span class="math inline">\(\texttt{happy}\)</span> photos. On the probit scale this translates into <span class="math inline">\(-\infty\)</span> and so the ML estimate of the difference between <span class="math inline">\(\texttt{happy}\)</span> and <span class="math inline">\(\texttt{grumpy}\)</span> photos (the <span class="math inline">\(\texttt{typehappy}\)</span> effect) will also be <span class="math inline">\(-\infty\)</span>. Consequently, with a flat prior on the <span class="math inline">\(\texttt{typehappy}\)</span> effect the posterior distribution is improper. The default prior for model coefficients in <span class="math inline">\(\texttt{MCMCglmm}\)</span> is not flat - they are normal with a large variance (<span class="math inline">\(10^8\)</span>), but even so this diffuse prior can cause problems if there is complete (or near complete) separation. To see this, think about an intercept-only model. A diffuse prior on the probit scale puts a lot of density on very large positive or negative values and so puts a lot of density close to zero and one on the probability scale (see Figure <a href="glm.html#fig:link">3.12</a>). If the likelihood prevents the posterior from reaching these extremes then the prior has little influence because it is largely flat outside of the extremes (it is U-shaped). However, with complete separation the likelihood is indeed placing a lot of density at these extreme values and the prior holds the posterior at these values. If, on the other hand, we had specified the prior on the intercept to be normal with a mean of zero and a variance of one then the prior on the probability scale would be flat (since the inverse-link function is the cumulative density function for the unit normal). If a logit-link had been used then there is no normal prior that would result in flatness on the probability scale, and having a mean of zero and a variance of <span class="math inline">\(\pi^2/3\)</span> (the variance of a logistic distribution) is a close as you can get if the residual variance had been zero (<a href="glm.html#fig:flat-prob-prior">3.18</a>).</p>
<div class="figure"><span style="display:block;" id="fig:flat-prob-prior"></span>
<img src="_bookdown_files/fig/flat-prob-prior-1.png" alt="Prior density on the probability of success in an intercept-only Bernoulli GLM. The solid black line is for a standard probit link and a normal prior on the intercept with zero mean and a variance of one.  The solid red line is for a standard logit link (with the residual variance set to zero) and a normal prior on the intercept with zero mean and a variance of one $\pi^2/3$. The dashed lines are for when the variance is set to 100 (logit-link) or $g^2 100\approx$ 39.3 (probit link) - see Section \@ref(gelman-prior-sec)." width="672" />
<p class="caption">
Figure 3.18: Prior density on the probability of success in an intercept-only Bernoulli GLM. The solid black line is for a standard probit link and a normal prior on the intercept with zero mean and a variance of one. The solid red line is for a standard logit link (with the residual variance set to zero) and a normal prior on the intercept with zero mean and a variance of one <span class="math inline">\(\pi^2/3\)</span>. The dashed lines are for when the variance is set to 100 (logit-link) or <span class="math inline">\(g^2 100\approx\)</span> 39.3 (probit link) - see Section <a href="glm.html#gelman-prior-sec">3.9.1</a>.
</p>
</div>
<div id="gelman-prior-sec" class="section level3 hasAnchor" number="3.9.1">
<h3><span class="header-section-number">3.9.1</span> The <span class="citation">Gelman, Jakulin, et al. (<a href="#ref-Gelman.2008a">2008</a>)</span> prior<a href="glm.html#gelman-prior-sec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ideally we would like to extend this idea so that we can define priors for coefficients associated with categorical and continuous predictors, not just the intercept. We would also like these priors to work well in the presence of variation caused by the addition of random effects (Chapter <a href="ranef.html#ranef">4</a>) and (depending on the <code>family</code> specified) a non-zero residual variance. What follows is very fiddly and a short summary and workflow can be found at the end.</p>
<p>For accommodating additional variance in the intercept-only model we can use a prior variance that is the sum of the link variance and the model-defined variance <span class="math inline">\(v\)</span>. In the Bernoulli models we’ve fitted <span class="math inline">\(v\)</span> is just the residual variance which has been fixed at one. For the logit-link, the link variance is <span class="math inline">\(\pi^2/3\)</span> which would result in a prior variance of <span class="math inline">\(1+\pi^2/3\)</span>. For the threshold link, the link variance is zero resulting in a prior variance of <span class="math inline">\(1\)</span> (i.e. the standard probit where the residual variance is set to zero and the link variance is one). With random effects <span class="math inline">\(v\)</span> may not be known <em>a priori</em> and a judicious choice about it’s magnitude would need to be made.</p>
<p>For models that contain predictors, <span class="citation">Gelman, Jakulin, et al. (<a href="#ref-Gelman.2008a">2008</a>)</span> suggested a prior for logit-link models where <span class="math inline">\(v=0\)</span>. They suggested that the input variables should be standardised prior to model fitting and that the the associated coefficients should have independent Cauchy priors with a scale of 10 for the (new) intercept and a scale of 2.5 for the remaining coefficients. The motivation behind this prior is that the average probability, defined by the new intercept, can have a wide range (<span class="math inline">\(10^{-9}\)</span> <span class="math inline">\(-\)</span> 1-<span class="math inline">\(10^{-9}\)</span>). This is still rather U-shaped on the probability scale and in many applications a lower scale may perform better (Figure <a href="glm.html#fig:flat-prob-prior">3.18</a>). The prior on the remaining coefficients imply that extremely large changes in probability between categorical predictors or per 0.5 standard deviations of a continuous predictor have low probability: for changes greater than ten on the logit scale, the prior density is 0.84. This combination of standardisation and prior naturally penalises higher order interactions, but it’s properties when random effects were fitted (or the residual variance was non-zero) were not explored.</p>
<p>Personally, I don’t like generic rescaling of inputs - I like my coefficients to have meaningful units. I would like to know how much my pot-belly is expected to grow per pistachio nut, not per standard deviation of pistachio nut consumption in the data set the dietician happened to have analysed. However, rather than standardising the inputs prior to model fitting we can fit them in their original form and place a prior on the coefficients that is equivalent to that recommended by <span class="citation">Gelman, Jakulin, et al. (<a href="#ref-Gelman.2008a">2008</a>)</span> had we rescaled the inputs. Although the Cauchy prior can probably be specified by treating the coefficients as random, here we will use a normal distribution with the same scales. This will penalise large coefficients more than the Cauchy prior.</p>
<p>To accommodate non-zero <span class="math inline">\(v\)</span>, we can multiply the scales recommended by <span class="citation">Gelman, Jakulin, et al. (<a href="#ref-Gelman.2008a">2008</a>)</span> by <span class="math inline">\(\sqrt{1/(1+c^2\texttt{v})}\)</span> such that the same prior interpretation can be given (approximately) to the marginal effects of the predictors (see earlier). A change of one logit is neither equivalent or proportional to a change of one probit. For example, going from -1 to -2 on the logit scale we move from a probability of 0.27 to 0.12 but on the probit scale a change from -1 to -2 results in a change of probability from 0.16 to 0.02. Around zero (i.e a probability of 0.5 in both cases) the change in probability per logit is 0.25 and the change in probability per probit is 0.399. Consequently, we can multiply the scales recommended by <span class="citation">Gelman, Jakulin, et al. (<a href="#ref-Gelman.2008a">2008</a>)</span> by <span class="math inline">\(g=\)</span> 0.25 / 0.399=<code>dlogis(0)/dnorm(0)</code>=<span class="math inline">\(\sqrt{2\pi}/4\)</span> to achieve approximate equivalence. Note this was done when determining the regression coefficients to produce Figure <a href="glm.html#fig:link">3.12</a> and the two prior densities diverge as the probability moves away from 0.5 (see Figure <a href="glm.html#fig:flat-prob-prior">3.18</a> also). When additional variance is present we can multiply the original scales by <span class="math inline">\(\sqrt{g^2/\texttt{v}}\)</span> when <code>family="threshold"</code> is used.</p>
<p>The function <code>gelman.prior</code> takes the (fixed effect) model formula as its first argument and together with the data.frame the model is to be fitted to (passed in the argument <code>data</code>) generates the prior specification ( and ) for the model parameters, had the inputs been scaled as in <span class="citation">Gelman, Jakulin, et al. (<a href="#ref-Gelman.2008a">2008</a>)</span>. The argument <code>intercept.scale</code> specifies the scale of the normal (standard deviation) for the intercept and the argument <code>coef.scale</code> specifies the scale of the normal for the remaining parameters - both defined as if inputs were scaled<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>. By default these scales are set to one.</p>
<p>This is all quite confusing and here are the main points and a practical workflow.</p>
<ol style="list-style-type: decimal">
<li><p><span class="citation">Gelman, Jakulin, et al. (<a href="#ref-Gelman.2008a">2008</a>)</span> recommended standardising inputs prior to logistic regression and using a prior that allows a wide range for the average probability of success but penalises large effects associated with predictors.</p></li>
<li><p>The function <code>gelman.prior</code> helps set up a prior similar to that recommended by <span class="citation">Gelman, Jakulin, et al. (<a href="#ref-Gelman.2008a">2008</a>)</span> but a) using a normal in place of a Cauchy and b) not requiring the inputs to be standardised prior to analysis.</p></li>
<li><p>To <code>gelman.prior</code> you should pass your model <code>formula</code> (<code>fixed</code> in MCMCglmm) and <code>data</code> that will be used in the analysis.</p></li>
<li><p>You also need to decide on a prior scale for the mean (<code>intercept.scale</code>) and the coefficients of the standardised inputs (<code>coef.scale</code>). To achieve a prior as close to that recommended by <span class="citation">Gelman, Jakulin, et al. (<a href="#ref-Gelman.2008a">2008</a>)</span> you can use <span class="math inline">\(10\sqrt{1/(1+c^2\texttt{v})}\)</span> (<code>family="binomial"</code>) or <span class="math inline">\(10\sqrt{g^2/\texttt{v}}\)</span> (<code>family="threshold"</code>) for <code>intercept.scale</code> and <span class="math inline">\(2.5\sqrt{1/(1+c^2\texttt{v})}\)</span> (<code>family="binomial"</code>) or <span class="math inline">\(2.5\sqrt{g^2/\texttt{v}}\)</span> (<code>family="threshold"</code>) for <code>coef.scale</code>. <span class="math inline">\(c=16\sqrt{3}/15\pi\)</span> and g=<span class="math inline">\(\sqrt{2\pi}/4.\)</span></p></li>
<li><p>The output of <code>gelman.prior</code> can be passed directly as the fixed effect prior (i.e. <code>prior=list(B=gelman.prior(...))</code>.</p></li>
</ol>
<p>We can try this out for the model we fitted to the data set with complete separation:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="glm.html#cb125-1" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="dv">2</span> <span class="sc">*</span> pi)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb125-2"><a href="glm.html#cb125-2" tabindex="-1"></a></span>
<span id="cb125-3"><a href="glm.html#cb125-3" tabindex="-1"></a>prior.mcs<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">B =</span> <span class="fu">gelman.prior</span>(<span class="sc">~</span>type, <span class="at">data =</span> data.cs, <span class="at">coef.scale =</span> g <span class="sc">*</span> <span class="fl">2.5</span>,</span>
<span id="cb125-4"><a href="glm.html#cb125-4" tabindex="-1"></a>    <span class="at">intercept.scale =</span> g <span class="sc">*</span> <span class="dv">10</span>), <span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb125-5"><a href="glm.html#cb125-5" tabindex="-1"></a></span>
<span id="cb125-6"><a href="glm.html#cb125-6" tabindex="-1"></a>mcs<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(mean5 <span class="sc">~</span> type, <span class="at">family =</span> <span class="st">&quot;threshold&quot;</span>, <span class="at">data =</span> data.cs, <span class="at">prior =</span> prior.mcs<span class="fl">.3</span>)</span>
<span id="cb125-7"><a href="glm.html#cb125-7" tabindex="-1"></a><span class="fu">plot</span>(mcs<span class="fl">.3</span><span class="sc">$</span>Sol)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:separation2"></span>
<img src="_bookdown_files/fig/separation2-1.png" alt="MCMC summary plots for the intercept and $\texttt{type}$ effect in a binary GLM (`mcs.3`) with probit link. For $\texttt{happy}$ photos all 19 observations are failures (the mean grumpy score is less than 5) and we have complete separation. A prior was set up using `gelman.prior` that penalises very large differences between $\texttt{happy}$ and $\texttt{grumpy}$ photos." width="672" />
<p class="caption">
Figure 3.19: MCMC summary plots for the intercept and <span class="math inline">\(\texttt{type}\)</span> effect in a binary GLM (<code>mcs.3</code>) with probit link. For <span class="math inline">\(\texttt{happy}\)</span> photos all 19 observations are failures (the mean grumpy score is less than 5) and we have complete separation. A prior was set up using <code>gelman.prior</code> that penalises very large differences between <span class="math inline">\(\texttt{happy}\)</span> and <span class="math inline">\(\texttt{grumpy}\)</span> photos.
</p>
</div>
<p>and we can see that the chain now looks well behaved (Figure <a href="glm.html#fig:separation2">3.19</a>). We can calculate the posterior distribution for the probability of success for <span class="math inline">\(\texttt{happy}\)</span> photos, where we had complete separation (19 failures and 0 successes). If we compare this posterior distribution with that from the model that used the diffuse (default) prior, we can see that all the density is not concentrated at very low density <a href="glm.html#fig:separation3">3.20</a>. This seems reasonable given that zero successes and 19 failures has a reasonable probability (0.135)even when the probability of success is 0.1.</p>
<div class="figure"><span style="display:block;" id="fig:separation3"></span>
<img src="_bookdown_files/fig/separation3-1.png" alt="Posterior distribution for the probability of success for $\texttt{happy}$ photos, where there was complete separation (19 failures and 0 successes). For model `mcs.3` (left) a prior was set up using `gelman.prior` that penalised large differences between $\texttt{happy}$ and $\texttt{grumpy}$ photos. For model `mcs.2` (right) the default diffuse prior was used." width="768" />
<p class="caption">
Figure 3.20: Posterior distribution for the probability of success for <span class="math inline">\(\texttt{happy}\)</span> photos, where there was complete separation (19 failures and 0 successes). For model <code>mcs.3</code> (left) a prior was set up using <code>gelman.prior</code> that penalised large differences between <span class="math inline">\(\texttt{happy}\)</span> and <span class="math inline">\(\texttt{grumpy}\)</span> photos. For model <code>mcs.2</code> (right) the default diffuse prior was used.
</p>
</div>

</div>
</div>
</div>
<h3> References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Albert.1993" class="csl-entry">
Albert, James H, and Siddhartha Chib. 1993. <span>“Bayesian Analysis of Binary and Polychotomous Response Data.”</span> <em>Journal of the American Statistical Association</em> 88 (422): 669–79.
</div>
<div id="ref-Diggle.2004" class="csl-entry">
Diggle, P., P. Heagerty, K. Liang, and S. Zeger. 2004. <em>Analysis of Longitudinal Data</em>. II. Oxford University Press.
</div>
<div id="ref-Gelman.2008a" class="csl-entry">
Gelman, A., A. Jakulin, M. G. Pittau, and Y. Su. 2008. <span>“A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.”</span>
</div>
<div id="ref-Hinde.1982" class="csl-entry">
Hinde, John. 1982. <span>“Compound <span>Poisson</span> Regression Models.”</span> In <em>Glim 82: Proceedings of the International Conference on Generalised Linear Models</em>, 109–21. Springer.
</div>
<div id="ref-McCulloch.2001" class="csl-entry">
McCulloch, C. E., and S. R. Searle. 2001. <em>Generalized, Linear and Mixed Models</em>. Wiley Series in Probability and Statistics. New York: John Wiley &amp; Sons.
</div>
<div id="ref-Pascall.2018" class="csl-entry">
Pascall, David J, Matthew C Tinsley, Darren J Obbard, and Lena Wilfert. 2018. <span>“Host Evolutionary History Predicts Virus Prevalence Across Bumblebee Species.”</span> <em>BioRxiv</em>, 498717.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>This is a bit disingenuous. The MCMC algorithm implemented in depends on a non-zero residual variance to ensure mixing - if the residual variance was set to zero (i.e. no overdispersion in GLM(M)) then the Markov chain would be reducible.<a href="glm.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Since the residuals are assumed normal the exponentiated residuals are log-normal, and so this model is often referred to as the Poisson log-normal <span class="citation">(<a href="#ref-Hinde.1982">Hinde 1982</a>)</span>. The Negative Binomial distribution is a commonly used alternative for overdispersed count data. The Negative Binomial is conceptually identical to the Poisson log-normal except the exponentiated residuals are assumed to be gamma distributed. The log-normal and gamma distributions are so similar that for most data sets it would be hard to distinguish between the Negative Binomial and the Poisson log-normal.<a href="glm.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>When <span class="math inline">\(x\)</span> is small <span class="math inline">\(\textrm{exp}(x)\approx 1+x\)</span>.<a href="glm.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>For more complicated models a (co)variance matrix may be estimated for a particular random component rather than just a single variance as here. In such cases, <code>V</code> is a matrix. The value at which (part of) the (co)variance matrix is fixed at is determined by <code>V</code>. Any elements of the covariance matrix in rows and/or columns equal to or greater than <code>fix</code> are fixed. In the case of a single variance <code>fix=1</code> simply fixes the variance (element 1,1 of the (co)variance matrix) at whatever is specified in <code>V</code> (one in this example).<a href="glm.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p><code>family="ordinal"</code> is also available but it is only retained to ensure back compatibility: it is equivalent to <code>family="threshold"</code> with the variance fixed at the specified value plus one. When <code>family="ordinal"</code> was implemented I had misunderstood the paper by <span class="citation">Albert and Chib (<a href="#ref-Albert.1993">1993</a>)</span> and didn’t realise the latent variable could be Gibbs sampled with a threshold link (<span class="math inline">\(\mathbf{1}_{\{l&gt;0\}}\)</span>).<a href="glm.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>I know that <span class="math inline">\(E[1-(1-p)^2]\)</span> will be less than <span class="math inline">\(1-(1-E[p])^2\)</span> because of Jensen’s inequality. Jensen’s inequality tells us that <span class="math inline">\(E[f(x)]\)</span> will be less than <span class="math inline">\(f(E[x])\)</span> if <span class="math inline">\(f\)</span> is convex, or greater than than <span class="math inline">\(f(E[x])\)</span> if <span class="math inline">\(f\)</span> is concave. In the current example, <span class="math inline">\(f(x) = (1-x)^2=1-2x+x^2\)</span>, which is a quadratic with a positive quadratic coefficient and so convex.<a href="glm.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>In MCMCglmm versions &lt;3.0, <code>gelman.prior</code> took the arguments <code>scale</code> and <code>intercept</code> instead of <code>coef.scale</code> and <code>coef.intercept</code>. In addition the old version only returned the prior covariance matrix but now passes a list with the prior means (<code>mu</code>) and the prior covariance matrix (<code>V</code>).<a href="glm.html#fnref10" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ranef.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["MCMCglmm-course-notes.pdf", "MCMCglmm-course-notes.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
