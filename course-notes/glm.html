<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Linear and Generalised Linear Models | MCMCglmm Course Notes</title>
  <meta name="description" content="Extended documentation and course notes for the MCMCglmm R package." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Linear and Generalised Linear Models | MCMCglmm Course Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Linear and Generalised Linear Models | MCMCglmm Course Notes" />
  
  <meta name="twitter:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

<meta name="author" content="Jarrod Hadfield" />


<meta name="date" content="2026-01-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian.html"/>
<link rel="next" href="ranef.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script>
<link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-1.3.31/rglClass.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/utils.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/buffer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/subscenes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shaders.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shadersrc.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/textures.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/projection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/mouse.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/init.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pieces.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/draw.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/controls.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/selection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/rglTimer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pretty.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/axes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/animation.src.js"></script>
<script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.src.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#outline"><i class="fa fa-check"></i><b>1.1</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian Analysis and MCMC</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian.html"><a href="bayesian.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>2.2</b> Likelihood</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-ml"><i class="fa fa-check"></i><b>2.2.1</b> Maximum Likelihood (ML)</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian.html"><a href="bayesian.html#restricted-maximum-likelihood-reml"><i class="fa fa-check"></i><b>2.2.2</b> Restricted Maximum Likelihood (REML)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian.html"><a href="bayesian.html#prior-distribution"><i class="fa fa-check"></i><b>2.3</b> Prior Distribution</a></li>
<li class="chapter" data-level="2.4" data-path="bayesian.html"><a href="bayesian.html#posterior-distribution"><i class="fa fa-check"></i><b>2.4</b> Posterior Distribution</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian.html"><a href="bayesian.html#marginal-posterior-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Marginal Posterior Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="bayesian.html"><a href="bayesian.html#intervals-sec"><i class="fa fa-check"></i><b>2.4.2</b> Credible Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian.html"><a href="bayesian.html#mcmc"><i class="fa fa-check"></i><b>2.5</b> MCMC</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian.html"><a href="bayesian.html#starting-values"><i class="fa fa-check"></i><b>2.5.1</b> Starting values</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian.html"><a href="bayesian.html#metropolis-hastings-updates"><i class="fa fa-check"></i><b>2.5.2</b> Metropolis-Hastings updates</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian.html"><a href="bayesian.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.5.3</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian.html"><a href="bayesian.html#mcmc-diagnostics"><i class="fa fa-check"></i><b>2.5.4</b> MCMC Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian.html"><a href="bayesian.html#Vprior-sec"><i class="fa fa-check"></i><b>2.6</b> Prior for Residual Variances</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian.html"><a href="bayesian.html#IP-sec"><i class="fa fa-check"></i><b>2.6.1</b> Improper Priors</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian.html"><a href="bayesian.html#transform-sec"><i class="fa fa-check"></i><b>2.7</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>3</b> Linear and Generalised Linear Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="glm.html"><a href="glm.html#linear-model-lm"><i class="fa fa-check"></i><b>3.1</b> Linear Model (LM)</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="glm.html"><a href="glm.html#lm-sec"><i class="fa fa-check"></i><b>3.1.1</b> Linear Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="glm.html"><a href="glm.html#generalised-linear-model-glm"><i class="fa fa-check"></i><b>3.2</b> Generalised Linear Model (GLM)</a></li>
<li class="chapter" data-level="3.3" data-path="glm.html"><a href="glm.html#poisson-glm"><i class="fa fa-check"></i><b>3.3</b> Poisson GLM</a></li>
<li class="chapter" data-level="3.4" data-path="glm.html"><a href="glm.html#overdispersion"><i class="fa fa-check"></i><b>3.4</b> Overdispersion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="glm.html"><a href="glm.html#multiplicative-overdispersion"><i class="fa fa-check"></i><b>3.4.1</b> Multiplicative Overdispersion</a></li>
<li class="chapter" data-level="3.4.2" data-path="glm.html"><a href="glm.html#addod-sec"><i class="fa fa-check"></i><b>3.4.2</b> Additive Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="glm.html"><a href="glm.html#prediction-in-glm"><i class="fa fa-check"></i><b>3.5</b> Prediction in GLM</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="glm.html"><a href="glm.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="glm.html"><a href="glm.html#binomial-and-bernoulli-glm"><i class="fa fa-check"></i><b>3.6</b> Binomial and Bernoulli GLM</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="glm.html"><a href="glm.html#overdispersion-1"><i class="fa fa-check"></i><b>3.6.1</b> Overdispersion</a></li>
<li class="chapter" data-level="3.6.2" data-path="glm.html"><a href="glm.html#prediction"><i class="fa fa-check"></i><b>3.6.2</b> Prediction</a></li>
<li class="chapter" data-level="3.6.3" data-path="glm.html"><a href="glm.html#bernoulli-glm"><i class="fa fa-check"></i><b>3.6.3</b> Bernoulli GLM</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="glm.html"><a href="glm.html#complete-separation"><i class="fa fa-check"></i><b>3.7</b> Complete Separation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ranef.html"><a href="ranef.html"><i class="fa fa-check"></i><b>4</b> Random effects</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ranef.html"><a href="ranef.html#pred-sec"><i class="fa fa-check"></i><b>4.1</b> Prediction with Random effects</a></li>
<li class="chapter" data-level="4.2" data-path="ranef.html"><a href="ranef.html#PriorContr-sec"><i class="fa fa-check"></i><b>4.2</b> A note on fixed effect priors and covariances</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cat-int.html"><a href="cat-int.html"><i class="fa fa-check"></i><b>5</b> Categorical Random Interactions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="cat-int.html"><a href="cat-int.html#idh-variance-structure"><i class="fa fa-check"></i><b>5.1</b> <code>idh</code> Variance Structure</a></li>
<li class="chapter" data-level="5.2" data-path="cat-int.html"><a href="cat-int.html#us-variance-structure"><i class="fa fa-check"></i><b>5.2</b> <code>us</code> Variance Structure</a></li>
<li class="chapter" data-level="5.3" data-path="cat-int.html"><a href="cat-int.html#compound-variance-structures"><i class="fa fa-check"></i><b>5.3</b> Compound Variance Structures</a></li>
<li class="chapter" data-level="5.4" data-path="cat-int.html"><a href="cat-int.html#heter-sec"><i class="fa fa-check"></i><b>5.4</b> Heterogenous Residual Variance</a></li>
<li class="chapter" data-level="5.5" data-path="cat-int.html"><a href="cat-int.html#contrasts-and-covariances"><i class="fa fa-check"></i><b>5.5</b> Contrasts and Covariances</a></li>
<li class="chapter" data-level="5.6" data-path="cat-int.html"><a href="cat-int.html#VCVprior-sec"><i class="fa fa-check"></i><b>5.6</b> Priors for Covariance Matrices</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="cat-int.html"><a href="cat-int.html#priors-for-us-structures"><i class="fa fa-check"></i><b>5.6.1</b> Priors for <code>us</code> structures</a></li>
<li class="chapter" data-level="5.6.2" data-path="cat-int.html"><a href="cat-int.html#priors-for-idh-structures"><i class="fa fa-check"></i><b>5.6.2</b> Priors for <code>idh</code> structures</a></li>
<li class="chapter" data-level="5.6.3" data-path="cat-int.html"><a href="cat-int.html#priors-for-corg-and-corgh-structures"><i class="fa fa-check"></i><b>5.6.3</b> Priors for <code>corg</code> and <code>corgh</code> structures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cont-int.html"><a href="cont-int.html"><i class="fa fa-check"></i><b>6</b> Continuous Random Interactions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="cont-int.html"><a href="cont-int.html#random-regression"><i class="fa fa-check"></i><b>6.1</b> Random Regression</a></li>
<li class="chapter" data-level="6.2" data-path="cont-int.html"><a href="cont-int.html#expected-variances-and-covariances"><i class="fa fa-check"></i><b>6.2</b> Expected Variances and Covariances</a></li>
<li class="chapter" data-level="6.3" data-path="cont-int.html"><a href="cont-int.html#RRcentering"><i class="fa fa-check"></i><b>6.3</b> <code>us</code> versus <code>idh</code> and mean centering</a></li>
<li class="chapter" data-level="6.4" data-path="cont-int.html"><a href="cont-int.html#meta-sec"><i class="fa fa-check"></i><b>6.4</b> Meta-analysis</a></li>
<li class="chapter" data-level="6.5" data-path="cont-int.html"><a href="cont-int.html#splines"><i class="fa fa-check"></i><b>6.5</b> Splines</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multi-response models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="multi.html"><a href="multi.html#relaxing-the-univariate-assumptions-of-causality"><i class="fa fa-check"></i><b>7.1</b> Relaxing the univariate assumptions of causality</a></li>
<li class="chapter" data-level="7.2" data-path="multi.html"><a href="multi.html#multinomial-models"><i class="fa fa-check"></i><b>7.2</b> Multinomial Models</a></li>
<li class="chapter" data-level="7.3" data-path="multi.html"><a href="multi.html#zero-inflated-models"><i class="fa fa-check"></i><b>7.3</b> Zero-inflated Models</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="multi.html"><a href="multi.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>7.3.1</b> Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="multi.html"><a href="multi.html#Hurdle"><i class="fa fa-check"></i><b>7.4</b> Hurdle Models</a></li>
<li class="chapter" data-level="7.5" data-path="multi.html"><a href="multi.html#ZAP"><i class="fa fa-check"></i><b>7.5</b> Zero-altered Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pedigree.html"><a href="pedigree.html"><i class="fa fa-check"></i><b>8</b> Pedigrees and Phylogenies</a>
<ul>
<li class="chapter" data-level="8.1" data-path="pedigree.html"><a href="pedigree.html#pedigree-and-phylogeny-formats"><i class="fa fa-check"></i><b>8.1</b> Pedigree and phylogeny formats</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="pedigree.html"><a href="pedigree.html#pedigrees"><i class="fa fa-check"></i><b>8.1.1</b> Pedigrees</a></li>
<li class="chapter" data-level="8.1.2" data-path="pedigree.html"><a href="pedigree.html#phylogenies"><i class="fa fa-check"></i><b>8.1.2</b> Phylogenies</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="pedigree.html"><a href="pedigree.html#the-animal-model-and-the-phylogenetic-mixed-model"><i class="fa fa-check"></i><b>8.2</b> The animal model and the phylogenetic mixed model</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="parameter-expansion.html"><a href="parameter-expansion.html"><i class="fa fa-check"></i><b>9</b> Parameter Expansion</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="parameter-expansion.html"><a href="parameter-expansion.html#variances-close-to-zero"><i class="fa fa-check"></i><b>9.0.1</b> Variances close to zero</a></li>
<li class="chapter" data-level="9.0.2" data-path="parameter-expansion.html"><a href="parameter-expansion.html#secPX-p"><i class="fa fa-check"></i><b>9.0.2</b> Parameter expanded priors</a></li>
<li class="chapter" data-level="9.0.3" data-path="parameter-expansion.html"><a href="parameter-expansion.html#binary-response-models"><i class="fa fa-check"></i><b>9.0.3</b> Binary response models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="path.html"><a href="path.html"><i class="fa fa-check"></i><b>10</b> Path Analysis &amp; Antedependence Structures</a>
<ul>
<li class="chapter" data-level="10.1" data-path="path.html"><a href="path.html#path-anlaysis"><i class="fa fa-check"></i><b>10.1</b> Path Anlaysis</a></li>
<li class="chapter" data-level="10.2" data-path="path.html"><a href="path.html#ante-sec"><i class="fa fa-check"></i><b>10.2</b> Antedependence</a></li>
<li class="chapter" data-level="10.3" data-path="path.html"><a href="path.html#scaling"><i class="fa fa-check"></i><b>10.3</b> Scaling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="technical-details.html"><a href="technical-details.html"><i class="fa fa-check"></i><b>11</b> Technical Details</a>
<ul>
<li class="chapter" data-level="11.1" data-path="technical-details.html"><a href="technical-details.html#model-form"><i class="fa fa-check"></i><b>11.1</b> Model Form</a></li>
<li class="chapter" data-level="11.2" data-path="technical-details.html"><a href="technical-details.html#MCMC-app"><i class="fa fa-check"></i><b>11.2</b> MCMC Sampling Schemes</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="technical-details.html"><a href="technical-details.html#updating-the-latent-variables-bf-l"><i class="fa fa-check"></i><b>11.2.1</b> Updating the latent variables <span class="math inline">\({\bf l}\)</span></a></li>
<li class="chapter" data-level="11.2.2" data-path="technical-details.html"><a href="technical-details.html#updating-the-location-vector-boldsymboltheta-leftboldsymbolmathbfbeta-bf-uright"><i class="fa fa-check"></i><b>11.2.2</b> Updating the location vector <span class="math inline">\(\boldsymbol{\theta} = \left[{\boldsymbol{\mathbf{\beta}}}^{&#39;}\; {\bf u}^{&#39;}\right]^{&#39;}\)</span></a></li>
<li class="chapter" data-level="11.2.3" data-path="technical-details.html"><a href="technical-details.html#updating-the-variance-structures-bf-g-and-bf-r"><i class="fa fa-check"></i><b>11.2.3</b> Updating the variance structures <span class="math inline">\({\bf G}\)</span> and <span class="math inline">\({\bf R}\)</span></a></li>
<li class="chapter" data-level="11.2.4" data-path="technical-details.html"><a href="technical-details.html#ordinal-models"><i class="fa fa-check"></i><b>11.2.4</b> Ordinal Models</a></li>
<li class="chapter" data-level="11.2.5" data-path="technical-details.html"><a href="technical-details.html#path-analyses"><i class="fa fa-check"></i><b>11.2.5</b> Path Analyses</a></li>
<li class="chapter" data-level="11.2.6" data-path="technical-details.html"><a href="technical-details.html#deviance-and-dic"><i class="fa fa-check"></i><b>11.2.6</b> Deviance and DIC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MCMCglmm Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="glm" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Linear and Generalised Linear Models<a href="glm.html#glm" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="linear-model-lm" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Linear Model (LM)<a href="glm.html#linear-model-lm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A linear model is one in which unknown parameters are multiplied by (functions of) observed variables and then added together to give a prediction for the response variable. As an example, lets take the results from a Swedish experiment from the sixties:</p>
<p>The experiment involved enforcing speed limits on Swedish roads on some days, but on other days letting everyone drive as fast as they liked. The response variable (<code>y</code>) is the number of accidents recorded. The experiment was conducted in 1961 and 1962 for 92 days in each year. As a first attempt we could specify the linear model:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="glm.html#cb39-1" tabindex="-1"></a>y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day</span></code></pre></div>
<p>but what does this mean?</p>
<div id="lm-sec" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Linear Predictors<a href="glm.html#lm-sec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The model formula defines a set of simultaneous (linear) equations</p>
<p><span class="math display" id="eq:SE-eq">\[\begin{array}{cl}
E[y\texttt{[1]}] &amp;=\beta_{1}+\beta_{2}(\texttt{limit[1]==&quot;yes&quot;})+\beta_{3}(\texttt{year[1]==&quot;1962&quot;})+\beta_{4}\texttt{day[1]}\\
E[y\texttt{[2]}] &amp;= \beta_{1}+\beta_{2}(\texttt{limit[2]==&quot;yes&quot;})+\beta_{3}(\texttt{year[2]==&quot;1962&quot;})+\beta_{4}\texttt{day[2]}\\
\vdots&amp;=\vdots\\
E[y\texttt{[184]}] &amp;= \beta_{1}+\beta_{2}(\texttt{limit[184]==&quot;yes&quot;})+\beta_{3}(\texttt{year[184]==&quot;1962&quot;})+\beta_{4}\texttt{day[184]}\\
\end{array}
\label{SE-eq}   \tag{3.1}\]</span></p>
<p>where the <span class="math inline">\(\beta\)</span>’s are the unknown coefficients to be estimated, and the variables in <span class="math inline">\(\texttt{this font}\)</span> are observed predictors. Continuous predictors such as <code>day</code> remain unchanged, but categorical predictors are expanded into a series of binary variables of the form ‘<em>do the data come from 1961, yes or no?</em>’, ‘<em>do the data come from 1962, yes or no?</em>’, and so on for as many years for which there are data.</p>
<p>It is cumbersome to write out the equation for each data point in this way, and a more compact way of representing the system of equations is</p>
<p><span class="math display" id="eq:lm">\[
E[{\bf y}] = {\bf X}{\boldsymbol{\mathbf{\beta}}}
\tag{3.2}
\]</span></p>
<p>where <span class="math inline">\({\bf X}\)</span> is called a design matrix and contains the predictor information for all observations, and <span class="math inline">\({\boldsymbol{\mathbf{\beta}}} = [\beta_{1}\ \beta_{2}\ \beta_{3}\ \beta_{4}]^{&#39;}\)</span> is the vector of parameters. Here, <span class="math inline">\(E[{\bf y}]\)</span> is a vector of the 184 expected values.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="glm.html#cb40-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">data =</span> Traffic)</span>
<span id="cb40-2"><a href="glm.html#cb40-2" tabindex="-1"></a>X[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">184</span>), ]</span></code></pre></div>
<pre><code>##     (Intercept) limityes year1962 day
## 1             1        0        0   1
## 2             1        0        0   2
## 184           1        1        1  92</code></pre>
<p>The binary predictors <em>do the data come from 1961, yes or no?</em> and <em>there was no speed limit, yes or no?</em> do not appear. These are the first factor levels of <code>year</code> and <code>limit</code> respectively, and are absorbed into the global intercept (<span class="math inline">\(\beta_{1}\)</span>) which is fitted by default in R. Hence the expected number of accidents for the four combinations (on day zero) are <span class="math inline">\(\beta_{1}\)</span> for 1961 with no speed limit, <span class="math inline">\(\beta_{1}+\beta_{2}\)</span> for 1961 with a speed limit, <span class="math inline">\(\beta_{1}+\beta_{3}\)</span> for 1962 with no speed limit and <span class="math inline">\(\beta_{1}+\beta_{2}+\beta_{3}\)</span> for 1962 with a speed limit.</p>
<p>The simultaneous equations defined by Equation <a href="glm.html#eq:lm">(3.2)</a> cannot be solved directly because we do not know the left-hand side - expected values of <span class="math inline">\(y\)</span>. We only know the observed value, which we assume is distributed around the expected value with some error. In a normal linear model we assume that these errors (residuals) are normally distributed:</p>
<p><span class="math display">\[{\bf y}-{\bf X}{\boldsymbol{\mathbf{\beta}}} = {\bf e} \sim N(0, \sigma^{2}_{e}{\bf I})\]</span></p>
<p><span class="math inline">\({\bf I}\)</span> is a <span class="math inline">\(184\times 184\)</span> identity matrix. It has ones along the diagonal, and zeros in the off-diagonals. The zero off-diagonals imply that the residuals are uncorrelated, and the ones along the diagonal imply that they have the same variance (<span class="math inline">\(\sigma^{2}_{e}\)</span>). Thinking about the distribution of residuals is less helpful when we move on to GLM’s and so I prefer to think about the model in the form:</p>
<p><span class="math display">\[{\bf y}\sim N({\bf X}{\boldsymbol{\mathbf{\beta}}}, \sigma^{2}_{e}{\bf I})\]</span></p>
<p>and say the response is <em>conditionally</em> normal, with the conditioning on the model (<span class="math inline">\({\bf X}{\boldsymbol{\mathbf{\beta}}}\)</span>). It is important to note that this is different from saying the response is normal. If having a speed limit had a very strong effect the (marginal) distribution of the response may be bimodal and far from normal, and yet by including speed-limit as a predictor, conditional normality may be achieved.</p>
<p>We could use <code>MCMCglmm</code> to fit this model, but to connect better with what comes next, let’s use <code>glm</code> to estimate <span class="math inline">\({\bf \beta}\)</span> and <span class="math inline">\(\sigma^{2}_{e}\)</span> assuming that the number of accidents follow a conditional normal distribution (the <code>MCMCglmm</code> syntax is identical):</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="glm.html#cb42-1" tabindex="-1"></a>m2a<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">data =</span> Traffic)</span>
<span id="cb42-2"><a href="glm.html#cb42-2" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ limit + year + day, data = Traffic)
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 21.13111    1.45169  14.556  &lt; 2e-16 ***
## limityes    -3.66427    1.35559  -2.703  0.00753 ** 
## year1962    -1.34853    1.31121  -1.028  0.30511    
## day          0.05304    0.02355   2.252  0.02552 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 71.80587)
## 
##     Null deviance: 14128  on 183  degrees of freedom
## Residual deviance: 12925  on 180  degrees of freedom
## AIC: 1314.5
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>On day zero in 1961 in the absence of a speed limit we expect 21.1 accidents (the intercept). With a speed limit we expect 3.7 fewer accidents and we can quite confidently reject the null-hypothesis of no effect - particularly if we were willing to use a one-tailed test, which seems reasonable. There are 1.3 fewer accidents in 1962, although this could just be due to chance, and for every unit increase in <span class="math inline">\(\texttt{day}\)</span> the number of accidents is predicted to go up by 0.05. The <span class="math inline">\(\texttt{day}\)</span> variable is encoded as integers from 1 to 92 with the same <span class="math inline">\(\texttt{day}\)</span> in different years being comparable (for example, the same day of the week and roughly the same date). If <span class="math inline">\(\texttt{day}\)</span>’s are evenly spaced throughout the year the <span class="math inline">\(\texttt{day}\)</span> effect is roughly the effect of increasing calender date by four (365/92) days. The estimate of the residual variance, <span class="math inline">\(\sigma^2_e\)</span>, is the dispersion parameter (71.8).</p>
<p>Because the number of accidents are count data we might worry about the assumption of conditional normality, and indeed the residuals show the typical right skew:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="glm.html#cb44-1" tabindex="-1"></a><span class="fu">hist</span>(<span class="fu">resid</span>(m2a<span class="fl">.1</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:hist-traffic"></span>
<img src="MCMCglmm-course-notes_files/figure-html/hist-traffic-1.png" alt="Histogram of residuals from model `m2a.1` which assumed they followed a Normal distribution." width="672" />
<p class="caption">
Figure 3.1: Histogram of residuals from model <code>m2a.1</code> which assumed they followed a Normal distribution.
</p>
</div>
<p>It’s not extreme, and the conclusions probably won’t change, but we could assume that the data follow some other distribution.</p>
</div>
</div>
<div id="generalised-linear-model-glm" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Generalised Linear Model (GLM)<a href="glm.html#generalised-linear-model-glm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Generalised linear models extend the linear model to non-normal data. They are essentially the same as the linear model described above, except they differ in two aspects. First, it is not necessarily the mean response that is predicted, but some function of the mean response. This function is called the link function. For example, with a log link we are trying to predict the logged expectation:</p>
<p><span class="math display">\[\textrm{log}(E[{\bf y}]) = {\bf X}{\boldsymbol{\mathbf{\beta}}}\]</span></p>
<p>or alternatively</p>
<p><span class="math display">\[E[{\bf y}] = \textrm{exp}({\bf X}{\boldsymbol{\mathbf{\beta}}})\]</span></p>
<p>where <span class="math inline">\(\textrm{exp}\)</span> is the inverse of the log link function- exponentiating. The second difference is that many distributions are single parameter distributions for which a variance does not need to be estimated because it can be inferred from the mean. For example, we could assume that the number of accidents are Poisson distributed, in which case we also make the assumption that the variance is equal to the expected value. Technically, GLM’s only apply to a restricted set of distributions (those in the exponential family) but <span class="math inline">\(\texttt{MCMCglmm}\)</span> can accommodate a range of GLM-like models for other distributions (see Table <a href="technical-details.html#tab:dist">11.1</a>).</p>
</div>
<div id="poisson-glm" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Poisson GLM<a href="glm.html#poisson-glm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For now we will concentrate on a Poisson GLM with log link (the default link function for the Poisson distribution):</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="glm.html#cb45-1" tabindex="-1"></a>m2a<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">family =</span> poisson, <span class="at">data =</span> Traffic)</span>
<span id="cb45-2"><a href="glm.html#cb45-2" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ limit + year + day, family = poisson, data = Traffic)
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  3.0467406  0.0372985  81.685  &lt; 2e-16 ***
## limityes    -0.1749337  0.0355784  -4.917 8.79e-07 ***
## year1962    -0.0605503  0.0334364  -1.811   0.0702 .  
## day          0.0024164  0.0005964   4.052 5.09e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 625.25  on 183  degrees of freedom
## Residual deviance: 569.25  on 180  degrees of freedom
## AIC: 1467.2
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>While the sign of the effects are comparable to that seen in the linear model, their numerical values are completely different and the significance of all effects has increased dramatically. Should we worry? The model is defined on the log scale and so to get back to the data scale we need to exponentiate. Exponentiating the intercept gives us the predicted number of accidents on day zero in 1961 without a speed limit:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="glm.html#cb47-1" tabindex="-1"></a><span class="fu">exp</span>(m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;(Intercept)&quot;</span>])</span></code></pre></div>
<pre><code>## (Intercept) 
##    21.04663</code></pre>
<p>which is very close to the intercept in the linear model (21.131), which is reassuring.</p>
<p>To get the prediction for the same day with a speed limit we need to add the <span class="math inline">\(\texttt{limityes}\)</span> coefficient</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="glm.html#cb49-1" tabindex="-1"></a><span class="fu">exp</span>(m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;(Intercept)&quot;</span>] <span class="sc">+</span> m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;limityes&quot;</span>])</span></code></pre></div>
<pre><code>## (Intercept) 
##    17.66892</code></pre>
<p>With a speed limit there are expected to be 0.840 as many accidents than if there was no speed limit. This value can be more directly obtained:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="glm.html#cb51-1" tabindex="-1"></a><span class="fu">exp</span>(m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;limityes&quot;</span>])</span></code></pre></div>
<pre><code>##  limityes 
## 0.8395127</code></pre>
<p>and holds true for any given day in either year. The proportional change is identical because the model is <em>linear</em> on the log scale and <span class="math inline">\(exp(\beta+\dots)=exp(\beta)exp(\dots)\)</span>. There is not always a direct relationship with the corresponding coefficients from the linear model but we can reassure ourselves that the parameters have the same qualitative meaning. For example, for <span class="math inline">\(\texttt{day}\)</span> 0 in 1961 the linear model predicts a drop from 21.1 to 17.5 accidents when a speed limit is in place - around 0.83 as many accidents, comparable to that predicted in the log-linear model.</p>
<p>So in terms of the reported coefficients, the linear model and the Poisson log-linear model are roughly consistent with each other. However, in terms of accurately quantifying the uncertainty in those coefficients the Poisson model has a serious problem - it is very over confident.</p>
</div>
<div id="overdispersion" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Overdispersion<a href="glm.html#overdispersion" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Most count data do not conform to a Poisson distribution because the variance in the response exceeds the expectation. In the summary to <code>m2a.2</code> the ratio of the residual deviance to the residual degrees of freedom is 3.162 which means, roughly speaking, there is 3.2 times more variation in our response (after conditioning on the model) than what we expect. This is known as overdispersion and it is easy to see how it arises, and why it is so common.</p>
<p>If the predictor data had not been available to us then the only model we could have fitted was one with just an intercept:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="glm.html#cb53-1" tabindex="-1"></a>m2a<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> Traffic, <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>)</span>
<span id="cb53-2"><a href="glm.html#cb53-2" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ 1, family = &quot;poisson&quot;, data = Traffic)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  3.07033    0.01588   193.3   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 625.25  on 183  degrees of freedom
## Residual deviance: 625.25  on 183  degrees of freedom
## AIC: 1517.2
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>for which the residual variance exceeds that expected by a factor of 3.5. Of course, the variability in the residuals must go up if there are factors that influence the number of accidents, but which we hadn’t measured. It’s likely that in most studies there are things that influence the response that haven’t been measured, and even if each thing has a small effect individually, in aggregate they can cause substantial overdispersion.</p>
<div id="multiplicative-overdispersion" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Multiplicative Overdispersion<a href="glm.html#multiplicative-overdispersion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are two ways of dealing with overdispersion. With <code>glm</code> the distribution name can be prefixed with <code>quasi</code> and a dispersion parameter estimated:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="glm.html#cb55-1" tabindex="-1"></a>m2a<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">family =</span> quasipoisson, <span class="at">data =</span> Traffic)</span>
<span id="cb55-2"><a href="glm.html#cb55-2" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.4</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ limit + year + day, family = quasipoisson, 
##     data = Traffic)
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.046741   0.067843  44.909  &lt; 2e-16 ***
## limityes    -0.174934   0.064714  -2.703  0.00753 ** 
## year1962    -0.060550   0.060818  -0.996  0.32078    
## day          0.002416   0.001085   2.227  0.02716 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 3.308492)
## 
##     Null deviance: 625.25  on 183  degrees of freedom
## Residual deviance: 569.25  on 180  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p><code>glm</code> uses a multiplicative model of overdispersion and so the estimate of the dispersion parameter is roughly equivalent to how many times greater the variance is than expected, after taking into account the predictor variables. You will notice that although the parameter estimates have changed very little, the standard errors have gone up and the significance gone down. Overdispersion, if not dealt with, can result in extreme anti-conservatism. For example, the second lowest number of accidents (8) occurred on <span class="math inline">\(\texttt{day}\)</span> 91 of 1961 without a speed limit. Our model predicts this should have been the second worst day for accidents over the whole two years, and the probability of observing 8 or less accidents on this day is predicted to be approximately 3 in a 100,000:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="glm.html#cb57-1" tabindex="-1"></a><span class="fu">ppois</span>(<span class="dv">8</span>, <span class="fu">exp</span>(m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;(Intercept)&quot;</span>] <span class="sc">+</span> <span class="dv">91</span> <span class="sc">*</span> m2a<span class="fl">.2</span><span class="sc">$</span>coef[<span class="st">&quot;day&quot;</span>]))</span></code></pre></div>
<pre><code>## [1] 3.195037e-05</code></pre>
<p>If we did not accommodate the overdispersion, anything additional we put in in the model that could potentially explain such an improbable occurrence would come out as significant even if in reality it wasn’t important. This is because there simply isn’t any flexibility in the null model to accommodate such occurrences. For example, if the extreme value happened to be associated with a particular level of a categorical predictor or happened to be associated with an extreme value of some continuous predictor, then the coefficients associated with these predictors may well come out as significant. However, under a more plausible null model the extreme observations may not be too surprising and there may be little support for the predictors having an effect on the response. A more plausible model, and one that we’ve alluded to, would be to allow the number of accidents to vary across sampling points due to unmeasured variables. This would allow the variation in the number of accidents to exceed the predicted mean based on the measured variables (the assumption of the standard Poisson).</p>
</div>
<div id="addod-sec" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Additive Overdispersion<a href="glm.html#addod-sec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I believe that a model assuming all relevant variables have been measured or controlled for, should <strong>not</strong> be the default model, and so when you specify <code>family=poisson</code> in <span class="math inline">\(\texttt{MCMCglmm}\)</span>, overdispersion is always dealt with<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. However, <span class="math inline">\(\texttt{MCMCglmm}\)</span> does not use a multiplicative model, but an additive model.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="glm.html#cb59-1" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>))</span>
<span id="cb59-2"><a href="glm.html#cb59-2" tabindex="-1"></a>m2a<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(y <span class="sc">~</span> limit <span class="sc">+</span> year <span class="sc">+</span> day, <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">data =</span> Traffic, <span class="at">prior =</span> prior,</span>
<span id="cb59-3"><a href="glm.html#cb59-3" tabindex="-1"></a>    <span class="at">pl =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>The element <code>Sol</code> contains the posterior distribution of the coefficients of the linear model, and we can plot their marginal distributions:</p>
<div class="figure"><span style="display:block;" id="fig:mcmc-traffic"></span>
<img src="MCMCglmm-course-notes_files/figure-html/mcmc-traffic-1.png" alt="MCMC summary plot for the coefficients from a Poisson glm (model `m2a.5`)." width="672" />
<p class="caption">
Figure 3.2: MCMC summary plot for the coefficients from a Poisson glm (model <code>m2a.5</code>).
</p>
</div>
<p>Note that the posterior distribution for the <code>year1962</code> spans zero, in agreement with the quasipoisson <code>glm</code> model, and that in general the estimates for the two models (and their uncertainty - see Section <a href="bayesian.html#intervals-sec">2.4.2</a>) are broadly similar:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="glm.html#cb60-1" tabindex="-1"></a><span class="fu">summary</span>(m2a<span class="fl">.4</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ limit + year + day, family = quasipoisson, 
##     data = Traffic)
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.046741   0.067843  44.909  &lt; 2e-16 ***
## limityes    -0.174934   0.064714  -2.703  0.00753 ** 
## year1962    -0.060550   0.060818  -0.996  0.32078    
## day          0.002416   0.001085   2.227  0.02716 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 3.308492)
## 
##     Null deviance: 625.25  on 183  degrees of freedom
## Residual deviance: 569.25  on 180  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>With additive overdispersion the linear predictor includes a ‘residual’, for which a residual variance is estimated (hence our prior specification).</p>
<p><span class="math display">\[E[{\bf y}] = \textrm{exp}({\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf e})\]</span></p>
<p>At this point it will be handy to represent the linear model in a new way:</p>
<p><span class="math display">\[{\bf l} = {\boldsymbol{\mathbf{\eta}}}+{\bf e}\]</span></p>
<p>where <span class="math inline">\({\bf l}\)</span> is a vector of latent variables (<span class="math inline">\(\textrm{log}(E[{\bf y}])\)</span> in this case) and <span class="math inline">\({\boldsymbol{\mathbf{\eta}}}\)</span> is the usual symbol for the linear predictor (<span class="math inline">\({\bf X}{\boldsymbol{\mathbf{\beta}}}\)</span>). The data we observe are assumed to be Poisson variables with expectation equal to the exponentiated latent variables:</p>
<p><span class="math display">\[{\bf y} \sim Pois(\textrm{exp}({\bf l}))\]</span></p>
<p>Note that the latent variable does not exactly predict <span class="math inline">\(y\)</span>, as it would if the data were Normal, because there is additional variability in the Poisson process<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. In the call to <span class="math inline">\(\texttt{MCMCglmm}\)</span> I specified <code>pl=TRUE</code> to indicate that I wanted to store the posterior distributions of the latent variables (also known as the liabilities). This is not usually necessary and can require a lot of memory (we have 1000 posterior samples for each of the 182 data points). However, as an example we can obtain the posterior mean residual for data point 92 which is the data from <span class="math inline">\(\texttt{day}\)</span> 92 in 1961 when there was no speed limit:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="glm.html#cb62-1" tabindex="-1"></a>lat92 <span class="ot">&lt;-</span> m2a<span class="fl">.5</span><span class="sc">$</span>Liab[, <span class="dv">92</span>]</span>
<span id="cb62-2"><a href="glm.html#cb62-2" tabindex="-1"></a><span class="co"># posterior distribution of the 92nd latent variable (liability)</span></span>
<span id="cb62-3"><a href="glm.html#cb62-3" tabindex="-1"></a></span>
<span id="cb62-4"><a href="glm.html#cb62-4" tabindex="-1"></a>eta92 <span class="ot">&lt;-</span> m2a<span class="fl">.5</span><span class="sc">$</span>Sol[, <span class="st">&quot;(Intercept)&quot;</span>] <span class="sc">+</span> m2a<span class="fl">.5</span><span class="sc">$</span>Sol[, <span class="st">&quot;day&quot;</span>] <span class="sc">*</span> Traffic<span class="sc">$</span>day[<span class="dv">92</span>]</span>
<span id="cb62-5"><a href="glm.html#cb62-5" tabindex="-1"></a><span class="co"># posterior distribution of X\beta for the 92nd observation</span></span>
<span id="cb62-6"><a href="glm.html#cb62-6" tabindex="-1"></a></span>
<span id="cb62-7"><a href="glm.html#cb62-7" tabindex="-1"></a>resid92 <span class="ot">&lt;-</span> lat92 <span class="sc">-</span> eta92</span>
<span id="cb62-8"><a href="glm.html#cb62-8" tabindex="-1"></a><span class="co"># posterior distribution of e for the 92nd observation</span></span>
<span id="cb62-9"><a href="glm.html#cb62-9" tabindex="-1"></a></span>
<span id="cb62-10"><a href="glm.html#cb62-10" tabindex="-1"></a><span class="fu">mean</span>(resid92)</span></code></pre></div>
<pre><code>## [1] -0.1341791</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="glm.html#cb64-1" tabindex="-1"></a><span class="co"># posterior mean of e for the 92nd observation</span></span></code></pre></div>
<p>This particular observation has a negative expected residual indicating that the probability of getting injured was less than expected for this <em>particular</em> realisation of that <span class="math inline">\(\texttt{day}\)</span> in that year without a speed limit. If that combination of predictors (<span class="math inline">\(\texttt{day}\)</span>=92, <span class="math inline">\(\texttt{year}\)</span>=1961 and <span class="math inline">\(\texttt{limit}\)</span>=<span class="math inline">\(\texttt{no}\)</span>) could be repeated it does not necessarily mean that the actual number of accidents would always be less than expected, because it would follow a Poisson distribution with a mean equal to <code>exp(lat92)</code> (21.974).</p>
<p>Like residuals in a standard linear model, the residuals are assumed to be independently and normally distributed with an expectation of zero and an estimated variance. If the residual variance was zero then <span class="math inline">\({\bf e}\)</span> would be a vector of zeros and the model would conform to the standard Poisson GLM. However, the posterior distribution of the residual variance is located well away form zero:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="glm.html#cb65-1" tabindex="-1"></a><span class="fu">plot</span>(m2a<span class="fl">.5</span><span class="sc">$</span>VCV)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:vcv-traffic"></span>
<img src="MCMCglmm-course-notes_files/figure-html/vcv-traffic-1.png" alt="MCMC summary plot for the residual (`units`) variance from a Poisson glm (model `m2a.5`). The residual variance models any overdispersion, and a residual variance of zero would imply that the response conforms to a standard Poisson." width="672" />
<p class="caption">
Figure 3.3: MCMC summary plot for the residual (<code>units</code>) variance from a Poisson glm (model <code>m2a.5</code>). The residual variance models any overdispersion, and a residual variance of zero would imply that the response conforms to a standard Poisson.
</p>
</div>
</div>
</div>
<div id="prediction-in-glm" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Prediction in GLM<a href="glm.html#prediction-in-glm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To get the expected number of accidents for the 92nd observation we simply exponentiated the latent variable: exp(<code>lat92</code>). However, it is important to realise that this is the expected number had the residual been exactly equal to the observed residual for that observation (<code>resid92</code>): we are calculating the expected number conditional on the set of unmeasured variables that affected that <em>particular</em> realisation of <span class="math inline">\(\texttt{day}\)</span> 92 in 1961 without a speed limit. When calculating a prediction we usually aim to average over these residuals (or random effects - see <a href="#ranpred-sec"><strong>??</strong></a>) since we would like to know what the average response would be for observations made on a <span class="math inline">\(\texttt{day}\)</span> of <em>type</em> 92 in 1961 without a speed limit. On the log-scale the expectation is simply the linear predictor (<span class="math inline">\(\eta\)</span>):</p>
<p><span class="math display">\[
log(E_e[y]) = E_e[l] = E_e[\eta+e] =  \eta+E_e[e]=\eta
\]</span></p>
<p>since the residuals have zero expectation (here I have subscripted the expectation with the variable we are averaging over). The <code>predict</code> function can be applied to <code>MCMCglmm</code> objects and if we specify <code>type="terms"</code> we get the prediction of the link scale - the log scale in this case:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="glm.html#cb66-1" tabindex="-1"></a><span class="fu">predict</span>(m2a<span class="fl">.5</span>, <span class="at">type =</span> <span class="st">&quot;terms&quot;</span>)[<span class="dv">92</span>]</span></code></pre></div>
<pre><code>## [1] 3.224037</code></pre>
<p>which is equal to the posterior mean of <code>eta92</code> obtained earlier. We can see this visually in Figure <a href="glm.html#fig:prediction2">3.4</a> where I have plotted the distribution of the latent variable on a <span class="math inline">\(\texttt{day}\)</span> of type 92 in 1961 without a speed limit.</p>
<div class="figure"><span style="display:block;" id="fig:prediction2"></span>
<img src="MCMCglmm-course-notes_files/figure-html/prediction2-1.png" alt="The predicted distribution for the average number of accidents on the log scale for a $\texttt{day}$ of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor ($\eta=$) with a variance of $\sigma^{2}_e$. As a consequence the mean, median and mode of the distribution are equal to the linear predictor on the log scale." width="672" />
<p class="caption">
Figure 3.4: The predicted distribution for the average number of accidents on the log scale for a <span class="math inline">\(\texttt{day}\)</span> of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor (<span class="math inline">\(\eta=\)</span>) with a variance of <span class="math inline">\(\sigma^{2}_e\)</span>. As a consequence the mean, median and mode of the distribution are equal to the linear predictor on the log scale.
</p>
</div>
<p>To get the prediction on the data scale (i.e. in terms of the actual expected number of accidents) it is tempting to think we could just calculate <code>exp(eta92)</code>. However, this is the expected number of accidents had the residual been exactly zero. If we wish to average over the residuals we require:</p>
<p><span class="math display">\[
E_e[y] = E_e[\textrm{exp}(l)] = E_e[\textrm{exp}(\eta+e)]
\]</span></p>
<p>and because exponentiation is a non-linear function this average will deviate from <span class="math inline">\(\textrm{exp}(\eta)\)</span> (Figure <a href="glm.html#fig:prediction3">3.5</a>.</p>
<div class="figure"><span style="display:block;" id="fig:prediction3"></span>
<img src="MCMCglmm-course-notes_files/figure-html/prediction3-1.png" alt="The predicted distribution for the average number of accidents on the data scale for a $\texttt{day}$ of type 92 in 1961 without a speed limit (in red).  On the log scale the distribution is assumed to be normal around the linear predictor ($\eta$) with a variance of $\sigma^{2}_e$ (see \@ref(fig:prediction2)). However when transforming to the data scale (by exponentiating) the symmetry is lost and the different measures of central tendency do not coincide. Since the residuals are normal on the log scale, the distribution on the data scale is log-normal and so analytical solutions exist for the mean, mode and median." width="672" />
<p class="caption">
Figure 3.5: The predicted distribution for the average number of accidents on the data scale for a <span class="math inline">\(\texttt{day}\)</span> of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor (<span class="math inline">\(\eta\)</span>) with a variance of <span class="math inline">\(\sigma^{2}_e\)</span> (see <a href="glm.html#fig:prediction2">3.4</a>). However when transforming to the data scale (by exponentiating) the symmetry is lost and the different measures of central tendency do not coincide. Since the residuals are normal on the log scale, the distribution on the data scale is log-normal and so analytical solutions exist for the mean, mode and median.
</p>
</div>
<p>To obtain predictions on the data scale we can specify <code>type="response"</code> (the default) when using <code>predict</code>:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="glm.html#cb68-1" tabindex="-1"></a><span class="fu">predict</span>(m2a<span class="fl">.5</span>)[<span class="dv">92</span>]</span></code></pre></div>
<pre><code>## [1] 26.48956</code></pre>
<p>which is slightly greater than <code>exp(eta92)</code> (25.183). For all link-functions, the median value on the data scale can be easily calculated by taking the inverse-link transform of the linear predictor. However, obtaining the mean and mode is often more challenging than it is for log-link, and numerical integration or approximations are required.</p>
<p>The <code>predict</code> function is returning a single number for observation 92 yet the model object contains 1,000 samples from the posterior distribution of all model parameters. This is because the <code>predict</code> function returns the posterior mean of the predicted value. Since we have the complete posterior distribution we can also place a 95% credible interval on the prediction (see Section <a href="bayesian.html#intervals-sec">2.4.2</a>):</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="glm.html#cb70-1" tabindex="-1"></a><span class="fu">predict</span>(m2a<span class="fl">.5</span>, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)[<span class="dv">92</span>, ]</span></code></pre></div>
<pre><code>##      fit      lwr      upr 
## 26.48956 23.27675 30.10716</code></pre>
<div id="posterior-predictive-distribution" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Posterior Predictive Distribution<a href="glm.html#posterior-predictive-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In some cases we would like to visualise or summarise aspects of the predictive distribution other than the mean. The complete predictive distribution is hard to work with, but the <code>simulate</code> function allows you to draw samples from the predictive distribution. The default is to generate a sample using a random draw from the posterior distribution, resulting in a draw from what is known as the posterior predictive distribution:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="glm.html#cb72-1" tabindex="-1"></a>ypred <span class="ot">&lt;-</span> <span class="fu">simulate</span>(m2a<span class="fl">.5</span>)</span></code></pre></div>
<p>We can use these simulated values to characterise any aspect of the predictive distribution we want. For example, we can obtain quantiles and compare them to the quantiles of the actual data to see how well the model captures aspects of the observed marginal distribution:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="glm.html#cb73-1" tabindex="-1"></a><span class="fu">qqplot</span>(ypred, Traffic<span class="sc">$</span>y)</span>
<span id="cb73-2"><a href="glm.html#cb73-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:marginal-qq"></span>
<img src="MCMCglmm-course-notes_files/figure-html/marginal-qq-1.png" alt="qq-plot of the posterior predictive distribution and the data distribution" width="672" />
<p class="caption">
Figure 3.6: qq-plot of the posterior predictive distribution and the data distribution
</p>
</div>
<p>Not too bad, although the predictive distribution perhaps has greater support for extreme values than are observed. If we merely wish to know the interval in which some specified percentage of the data are predicted to lie, we can also use the predict function but with <code>interval="prediction"</code>. By default the 95% (highest posterior density) interval is calculated using as many simulated samples as there are saved posterior samples. For the 92nd observation the prediction interval is</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="glm.html#cb74-1" tabindex="-1"></a><span class="fu">predict</span>(m2a<span class="fl">.5</span>, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)[<span class="dv">92</span>, ]</span></code></pre></div>
<pre><code>##    fit    lwr    upr 
## 26.696 10.000 47.000</code></pre>
<p>Note that the reported mean (<code>fit</code>) differs from that returned by <code>interval="confidence"</code> due to Monte Carol error only.</p>
</div>
</div>
<div id="binomial-and-bernoulli-glm" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Binomial and Bernoulli GLM<a href="glm.html#binomial-and-bernoulli-glm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The general concepts introduced for the Poisson GLM extend naturally to Binomial data, albeit with a different link function. However, it is worth spending a little time exploring a Binomial GLM as I think overdispersion, and how we deal with it, is easier to understand with binomial data. However, we’ll also see how the ‘residuals’ defined earlier for capturing overdispersion complicate the analysis of Bernoulli data and result in MCMcglmm using a non-standard parameterisation.</p>
<p>The Binomial distribution has two parameters - the number of trials <span class="math inline">\(n\)</span> and the probability of success, <span class="math inline">\(p\)</span>. In a Binomial GLM the number of trials is assumed known leaving only <span class="math inline">\(p\)</span> to be estimated from the number of trials that are ‘successes’ or ‘failures’. When <code>family="binomial"</code> is specified (or equivalently <code>family="multinomial2"</code>) MCMCglmm uses the standard link function for the Binomial - the logit link - and the logit probability of success is modelled as</p>
<p><span class="math display">\[log\left(\frac{p}{1-p}\right)  = l = \eta+e\]</span></p>
<p>The logit transform takes a probability and turns it into a log odds ratio. If we want to get back to the probability we use the
inverse of the logit transform:</p>
<p><span class="math display">\[p = \frac{exp(l)}{1+exp(l)}\]</span></p>
<p>The logit link is actually the quantile function for the logistic distribution and so is available as the function <code>qlogis</code>. The inverse of a quantile function is a cumulative distribution function and so the inverse-logit transform is <code>plogis</code>.</p>
<p>To introduce the Binomial GLM we will analyse some data I collected on how grumpy my colleagues look. I took two photos (<span class="math inline">\(\texttt{photo}\)</span>) of 22 people (<span class="math inline">\(\texttt{person}\)</span>) working in the Institute of Evolution and Ecology, Edinburgh. In one photo the person was happy and in the other they were grumpy (<span class="math inline">\(\texttt{type}\)</span>). 122 respondents gave a score between 1 and 10 indicating how grumpy they thought each person looked in each photo (with 10 being the most grumpy).</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="glm.html#cb76-1" tabindex="-1"></a><span class="fu">data</span>(Grumpy)</span>
<span id="cb76-2"><a href="glm.html#cb76-2" tabindex="-1"></a>Grumpy[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">44</span>), ]</span></code></pre></div>
<pre><code>##           y  l5  g5   type photo   person age ypub
## 1  4.032787 101  21 grumpy  4511   ally_p  38   13
## 2  3.081967 113   9  happy  4512   ally_p  38   13
## 3  7.885246  15 107 grumpy  4521 darren_o  38   16
## 44 3.798319 105  14  happy  4516  laura_r  34   10</code></pre>
<p><span class="math inline">\(\texttt{y}\)</span> gives the average score given by the 122 respondents. The number of respondents giving a photo a score of five or less (<span class="math inline">\(\texttt{l5}\)</span>) or more than five (<span class="math inline">\(\texttt{g5}\)</span>) is also recorded in addition to the person’s age (<span class="math inline">\(\texttt{age}\)</span>) and a proxy for how long they had been academia - the number of years since they published their first academic paper (<span class="math inline">\(\texttt{ypub}\)</span>). Here, we will model the probability of getting a grumpy score greater than five as a function of whether the person was happy or grumpy and how long they had been in academia. As with <code>glm</code>, successes should be in the first column of the response, and failures in the second:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="glm.html#cb78-1" tabindex="-1"></a>mbinom<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(<span class="fu">cbind</span>(g5, l5) <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">data =</span> Grumpy, <span class="at">family =</span> <span class="st">&quot;multinomial2&quot;</span>,</span>
<span id="cb78-2"><a href="glm.html#cb78-2" tabindex="-1"></a>    <span class="at">pl =</span> <span class="cn">TRUE</span>)</span>
<span id="cb78-3"><a href="glm.html#cb78-3" tabindex="-1"></a><span class="fu">summary</span>(mbinom<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>## 
##  Iterations = 3001:12991
##  Thinning interval  = 10
##  Sample size  = 1000 
## 
##  DIC: 5377.999 
## 
##  R-structure:  ~units
## 
##       post.mean l-95% CI u-95% CI eff.samp
## units     1.348   0.7597    1.947     1000
## 
##  Location effects: cbind(g5, l5) ~ type + ypub 
## 
##             post.mean l-95% CI u-95% CI eff.samp  pMCMC    
## (Intercept)  -0.72025 -1.67538  0.05147     1000  0.094 .  
## typehappy    -1.28167 -1.97027 -0.59820     1000 &lt;0.001 ***
## ypub          0.02064 -0.00613  0.05175     1105  0.156    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The model coefficients are most easily interpreted after exponentiating as they then give the proportional change in the odds ratio. The odds of having a score greater than five is exp(-1.282)=0.278 times lower when happy, as expected. The odds increases by a factor exp(0.021)=1.021 for each year in academia. When the coefficient is small in magnitude like this, you can get a rough estimate by looking directly at the coefficient: 0.021 roughly translates into a 2.1% increase and -0.021 would translate into a 2.1% decrease<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>. The <span class="math inline">\(\texttt{units}\)</span> (residual) variance is also large with credible intervals that are far from zero.</p>
<div id="overdispersion-1" class="section level3 hasAnchor" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Overdispersion<a href="glm.html#overdispersion-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As with overdispersion in the Poisson model, this excess variation can be attributed to predictors that are not included in the model but cause the probability of success to vary over observations (photos in this case). For example, the 3rd and 25th observation have the same values for every predictor</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="glm.html#cb80-1" tabindex="-1"></a>Grumpy[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">25</span>), ]</span></code></pre></div>
<pre><code>##           y l5  g5   type photo   person age ypub
## 3  7.885246 15 107 grumpy  4521 darren_o  38   16
## 25 5.319672 73  49 grumpy  4527  craig_w  38   16</code></pre>
<p>and so in the absence of overdispersion these parameters would result in a predicted probability of success of</p>
<p><span class="math display" id="eq:plogis">\[p = \textrm{plogis}(\beta_{\texttt{(Intercept)}}+\beta_{\texttt{typehappy}}\times0+\beta_{\texttt{ypub}}\times16)=0.404
\label{plogis-eq}   \tag{3.3}\]</span></p>
<p>Given there were 122 respondents (trials) we can calculate the two values between which the number of successes is expected to fall 95 % of the time.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="glm.html#cb82-1" tabindex="-1"></a><span class="fu">qbinom</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="at">size =</span> <span class="dv">122</span>, <span class="at">prob =</span> p)</span></code></pre></div>
<pre><code>## [1] 39 60</code></pre>
<p>While photo <span class="math inline">\(\texttt{4527}\)</span> is well within the range with 49 successes, the number of respondents giving photo <span class="math inline">\(\texttt{4521}\)</span> is substantially higher (107) - it’s an outlier. In Figure <a href="glm.html#fig:DandC">3.7</a> the two photos are shown and it is clear why their underlying probabilities may deviate from that predicted. Most obviously the two photos are of different people and people vary in how grumpy they look. Since we have two photos per person we could (and should) estimate <span class="math inline">\(\texttt{person}\)</span> effects, however this is best done by treating these effects as random, which we will cover later, in Chapter <a href="ranef.html#ranef">4</a>. Even if <span class="math inline">\(\texttt{person}\)</span> effects were fitted there’s also likely to be a whole host of observation-level (photo-specific) effects that are not captured in the model such as whether the person had their eyes closed or was wearing a dreary grey fleece.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:DandC"></span>
<img src="Figures/IMG_4521.JPG" alt="Photo 4521 (left) and photo 4527 (right). For the predictors fitted in model `mbinom.1`, these photos have the same values ($\texttt{type}=\texttt{grumpy}$ and $\texttt{ypub}=16$ years)" width="40%" style="display:inline-block; vertical-align:middle; margin-right:2%;" /><img src="Figures/IMG_4527.JPG" alt="Photo 4521 (left) and photo 4527 (right). For the predictors fitted in model `mbinom.1`, these photos have the same values ($\texttt{type}=\texttt{grumpy}$ and $\texttt{ypub}=16$ years)" width="40%" style="display:inline-block; vertical-align:middle; margin-right:2%;" />
<p class="caption">
Figure 3.7: Photo 4521 (left) and photo 4527 (right). For the predictors fitted in model <code>mbinom.1</code>, these photos have the same values (<span class="math inline">\(\texttt{type}=\texttt{grumpy}\)</span> and <span class="math inline">\(\texttt{ypub}=16\)</span> years)
</p>
</div>
<p>If we include the ‘residuals’ when calculating the predicted probability for these two photos we can see that indeed their probabilities are quite different:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="glm.html#cb84-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">plogis</span>(mbinom<span class="fl">.1</span><span class="sc">$</span>Liab[, <span class="dv">3</span>]))</span></code></pre></div>
<pre><code>## [1] 0.8621967</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="glm.html#cb86-1" tabindex="-1"></a><span class="co"># predicted probability for photo 4521</span></span>
<span id="cb86-2"><a href="glm.html#cb86-2" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">plogis</span>(mbinom<span class="fl">.1</span><span class="sc">$</span>Liab[, <span class="dv">25</span>]))</span></code></pre></div>
<pre><code>## [1] 0.4001446</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="glm.html#cb88-1" tabindex="-1"></a><span class="co"># predicted probability for photo 4527</span></span></code></pre></div>
</div>
<div id="prediction" class="section level3 hasAnchor" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Prediction<a href="glm.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When calculating the predicted probability in the absence of the residuals in Equation <a href="glm.html#eq:plogis">(3.3)</a> I was careful to say that the prediction assumed an absence of overdispersion. However, when overdispersion is present we need to average over the distribution of the residuals in order to get an average. As we saw in the log-linear Poisson model, because the inverse-link function (<code>plogis</code>) is non-linear the average of <span class="math inline">\(E_e[\texttt{plogis}(\eta+e)]\)</span> is different from <span class="math inline">\(\texttt{plogis}(E_e[\eta+e])=\texttt{plogis}(\eta)\)</span>. Unlike the Poisson log-linear model this expectation cannot be calculated analytically and the predict function by default numerically evaluates the integral:</p>
<p><span class="math display">\[\int_l \texttt{plogis}(l)f_N(l, \eta, \sigma^2_e)dl\]</span></p>
<p>where <span class="math inline">\(f_N\)</span> is the probability density function of the normal. For each of the 44 observations this is done 1,000 times (the number of saved posterior samples) to get the posterior mean prediction and can be very slow (it is actually faster to fit the model). The number reported by <code>predict</code> is <span class="math inline">\(np\)</span> rather than <span class="math inline">\(p\)</span> and so to get the predicted probability for a photo of someone who has grumpy and had been publishing for 16 years we can get the prediction for the 3rd observation and divide by the number of trials (122):</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="glm.html#cb89-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.1</span>)[<span class="dv">3</span>]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>## [1] 0.4243498</code></pre>
<p>A little different from <span class="math inline">\(\texttt{plogis}(\eta)=\)</span> 0.404. For the binomial with logit link, analytical approximations have been developed in <span class="citation">Diggle et al. (<a href="#ref-Diggle.2004">2004</a>)</span> and <span class="citation">McCulloch and Searle (<a href="#ref-McCulloch.2001">2001</a>)</span> which are considerably faster and reasonably accurate:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="glm.html#cb91-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.1</span>, <span class="at">approx =</span> <span class="st">&quot;diggle&quot;</span>)[<span class="dv">3</span>]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>## [1] 0.4210936</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="glm.html#cb93-1" tabindex="-1"></a><span class="fu">predict</span>(mbinom<span class="fl">.1</span>, <span class="at">approx =</span> <span class="st">&quot;mcculloch&quot;</span>)[<span class="dv">3</span>]<span class="sc">/</span><span class="dv">122</span></span></code></pre></div>
<pre><code>## [1] 0.4255396</code></pre>
</div>
<div id="bernoulli-glm" class="section level3 hasAnchor" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> Bernoulli GLM<a href="glm.html#bernoulli-glm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bernoulli data are a special case of the Binomial in which the number of trials is equal to one and so either a success or a failure is observed. To explore a Brenoulli model we can take our <em>average</em> Grumpy scores and simply dichotomise them into whether the average score was five or less or more than five:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="glm.html#cb95-1" tabindex="-1"></a>Grumpy<span class="sc">$</span>majority <span class="ot">&lt;-</span> Grumpy<span class="sc">$</span>y <span class="sc">&gt;</span> <span class="dv">5</span></span></code></pre></div>
<p>If we fit a binomial model to these data in <code>MCMCglmm</code> it has exactly the same form as before (although a single column of outcomes can be passed). But importantly, for Bernoulli data there is no information to estimate the residual variance. This does not necessarily mean that variation in the probability of success across observations is absent, only that we can’t estimate it. For example, imagine we took 100 people who had been publishing for 16 years and took a photograph of them when they were grumpy. Let’s say the probability that the mean score for such photos exceeded 5 was 0.5. If the probability for all photos was exactly 0.5 (i.e. the probability of success did not vary over observations) then we expect 50 success and 50 failures across our observations. However, imagine the case where the probability of success was 100% for 50 photos and 0% for 50 photos (i.e. the probability of success varies considerably over observations). We would also expect 50 success and 50 failures, and so the distribution of successes with and without variation in the underlying probability would be identical. In the absence of information most software sets the ‘residual’ variance to zero (i.e. the probability of success dose not vary over observations), but it is important to understand that this is a convenient but arbitrary choice. Given this, it is desirable that any conclusions drawn from the model do not depend on this arbitrary choice. Worryingly, both the location effects (fixed and random) and variance components are completely dependent on the magnitude of the residual variance. <code>MCMCglmm</code> allows the user to fix the residual variance at a value of their choice, but unfortunately a value of zero results in a chain that will not mix and so I usually fix the residual variance to one<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>:</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="glm.html#cb96-1" tabindex="-1"></a>prior.mbinom<span class="fl">.2</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb96-2"><a href="glm.html#cb96-2" tabindex="-1"></a>mbinom<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(majority <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">data =</span> Grumpy,</span>
<span id="cb96-3"><a href="glm.html#cb96-3" tabindex="-1"></a>    <span class="at">prior =</span> prior.mbinom<span class="fl">.2</span>)</span></code></pre></div>
<p>However, it would have been equally valid to fixed the residual variance at three:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="glm.html#cb97-1" tabindex="-1"></a>prior.mbinom<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">3</span>, <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb97-2"><a href="glm.html#cb97-2" tabindex="-1"></a>mbinom<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(majority <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">data =</span> Grumpy,</span>
<span id="cb97-3"><a href="glm.html#cb97-3" tabindex="-1"></a>    <span class="at">prior =</span> prior.mbinom<span class="fl">.3</span>)</span></code></pre></div>
<p>and if we compare the MCMC traces for the coefficients we can see that we are sampling different posterior distributions (Figure <a href="glm.html#fig:bernoulli">3.8</a>).</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="glm.html#cb98-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(mbinom<span class="fl">.2</span><span class="sc">$</span>Sol, mbinom<span class="fl">.3</span><span class="sc">$</span>Sol), <span class="at">density =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:bernoulli"></span>
<img src="MCMCglmm-course-notes_files/figure-html/bernoulli-1.png" alt="MCMC trace for coefficients of a Bernoulli GLM from two models (`mbinom.2` in black and `mbinom.3` in red). The data and model structure are identical but in `mbinom.2` the residual variance was set to one and in `mbinom.3` the residual variance was set to three. The data provide no information about the residual variance" width="672" />
<p class="caption">
Figure 3.8: MCMC trace for coefficients of a Bernoulli GLM from two models (<code>mbinom.2</code> in black and <code>mbinom.3</code> in red). The data and model structure are identical but in <code>mbinom.2</code> the residual variance was set to one and in <code>mbinom.3</code> the residual variance was set to three. The data provide no information about the residual variance
</p>
</div>
<p>Should we worry? Not really. We just have to be careful about how we express the results. Stating that the <code>typehappy</code> coefficient is -2.852 (the posterior mean estimate from <code>mbinom.2</code>) is meaningless without putting it in the context of the assumed residual variance (one). The two models give almost identical predictions (Figure <a href="#bernoulli-pred"><strong>??</strong></a>).</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="glm.html#cb99-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(mbinom<span class="fl">.2</span>), <span class="fu">predict</span>(mbinom<span class="fl">.3</span>))</span>
<span id="cb99-2"><a href="glm.html#cb99-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:bernoulli-pred"></span>
<img src="MCMCglmm-course-notes_files/figure-html/bernoulli-pred-1.png" alt="Predicted probabilities from a Bernoulli GLM from two models. The data and model structure are identical but in `mbinom.2` the residual variance was set to one and in `mbinom.3` the residual variance was set to three. The data provide no information about the residual variance." width="672" />
<p class="caption">
Figure 3.9: Predicted probabilities from a Bernoulli GLM from two models. The data and model structure are identical but in <code>mbinom.2</code> the residual variance was set to one and in <code>mbinom.3</code> the residual variance was set to three. The data provide no information about the residual variance.
</p>
</div>
<p>Although the <span class="citation">Diggle et al. (<a href="#ref-Diggle.2004">2004</a>)</span> approximation is less accurate than that in <span class="citation">(<a href="#ref-McCulloch.2004"><strong>McCulloch.2004?</strong></a>)</span> we can use it rescale the estimates by the estimated residual variance (<span class="math inline">\(\sigma^{2}_{\texttt{units}}\)</span>) in order to obtain the posterior distributions of the parameters under the assumption that the actual residual variance (<span class="math inline">\(\sigma^{2}_{e}\)</span>) is equal to some other value. For location effects the posterior distribution needs to be multiplied by <span class="math inline">\(\sqrt{\frac{1+c^{2}\sigma^{2}_{e}}{1+c^{2}\sigma^{2}_{\texttt{units}}}}\)</span> where <span class="math inline">\(c=16\sqrt{3}/15\pi\)</span>. We can obtain estimates under the assumption that <span class="math inline">\(\sigma^{2}_{e}=0\)</span> and we see the posterior distributions of the coefficients are very similar from the two models <a href="glm.html#fig:bernoulli-rescale">3.10</a>.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="glm.html#cb100-1" tabindex="-1"></a>c2 <span class="ot">&lt;-</span> ((<span class="dv">16</span> <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">3</span>))<span class="sc">/</span>(<span class="dv">15</span> <span class="sc">*</span> pi))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb100-2"><a href="glm.html#cb100-2" tabindex="-1"></a>rescale<span class="fl">.2</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.2</span><span class="sc">$</span>Sol <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> c2 <span class="sc">*</span> <span class="dv">1</span>))</span>
<span id="cb100-3"><a href="glm.html#cb100-3" tabindex="-1"></a>rescale<span class="fl">.3</span> <span class="ot">&lt;-</span> mbinom<span class="fl">.3</span><span class="sc">$</span>Sol <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> c2 <span class="sc">*</span> <span class="dv">3</span>))</span>
<span id="cb100-4"><a href="glm.html#cb100-4" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(<span class="fu">as.mcmc</span>(rescale<span class="fl">.2</span>), <span class="fu">as.mcmc</span>(rescale<span class="fl">.3</span>)), <span class="at">density =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:bernoulli-rescale"></span>
<img src="MCMCglmm-course-notes_files/figure-html/bernoulli-rescale-1.png" alt="MCMC trace for rescaled coefficients of a Bernoulli GLM from two models (`mbinom.2` in black and `mbinom.3` in red). The data and model structure are identical but in `mbinom.2` the residual variance was set to one and in `mbinom.3` the residual variance was set to three. However, the coefficients have been rescaled using the @Diggle.2004 approximation such that they represent what the coefficients would be if the residual variance was zero. The data provide no information about the residual variance." width="672" />
<p class="caption">
Figure 3.10: MCMC trace for rescaled coefficients of a Bernoulli GLM from two models (<code>mbinom.2</code> in black and <code>mbinom.3</code> in red). The data and model structure are identical but in <code>mbinom.2</code> the residual variance was set to one and in <code>mbinom.3</code> the residual variance was set to three. However, the coefficients have been rescaled using the <span class="citation">Diggle et al. (<a href="#ref-Diggle.2004">2004</a>)</span> approximation such that they represent what the coefficients would be if the residual variance was zero. The data provide no information about the residual variance.
</p>
</div>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="glm.html#cb101-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">1</span>))</span>
<span id="cb101-2"><a href="glm.html#cb101-2" tabindex="-1"></a>m.binom.<span class="fl">2.</span>glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(majority <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="at">data =</span> Grumpy)</span>
<span id="cb101-3"><a href="glm.html#cb101-3" tabindex="-1"></a>mbinom.<span class="fl">3.</span>plot <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(majority <span class="sc">~</span> type <span class="sc">+</span> ypub, <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">data =</span> Grumpy,</span>
<span id="cb101-4"><a href="glm.html#cb101-4" tabindex="-1"></a>    <span class="at">prior =</span> prior.mbinom<span class="fl">.3</span>, <span class="at">nitt =</span> <span class="dv">13000</span> <span class="sc">*</span> <span class="dv">10</span>)</span>
<span id="cb101-5"><a href="glm.html#cb101-5" tabindex="-1"></a>rescale.<span class="fl">3.</span>plot <span class="ot">&lt;-</span> mbinom.<span class="fl">3.</span>plot<span class="sc">$</span>Sol <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> c2 <span class="sc">*</span> <span class="dv">3</span>))</span>
<span id="cb101-6"><a href="glm.html#cb101-6" tabindex="-1"></a></span>
<span id="cb101-7"><a href="glm.html#cb101-7" tabindex="-1"></a><span class="fu">hist</span>(rescale.<span class="fl">3.</span>plot[, <span class="dv">1</span>], <span class="at">breaks =</span> <span class="dv">50</span>, <span class="at">main =</span> <span class="cn">NULL</span>, <span class="at">xlab =</span> <span class="st">&quot;(Intercept)&quot;</span>)</span>
<span id="cb101-8"><a href="glm.html#cb101-8" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">coef</span>(m.binom.<span class="fl">2.</span>glm)[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb101-9"><a href="glm.html#cb101-9" tabindex="-1"></a><span class="fu">hist</span>(rescale.<span class="fl">3.</span>plot[, <span class="dv">2</span>], <span class="at">breaks =</span> <span class="dv">50</span>, <span class="at">main =</span> <span class="cn">NULL</span>, <span class="at">xlab =</span> <span class="st">&quot;typehappy&quot;</span>)</span>
<span id="cb101-10"><a href="glm.html#cb101-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">coef</span>(m.binom.<span class="fl">2.</span>glm)[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb101-11"><a href="glm.html#cb101-11" tabindex="-1"></a><span class="fu">hist</span>(rescale.<span class="fl">3.</span>plot[, <span class="dv">3</span>], <span class="at">breaks =</span> <span class="dv">50</span>, <span class="at">main =</span> <span class="cn">NULL</span>, <span class="at">xlab =</span> <span class="st">&quot;ypub&quot;</span>)</span>
<span id="cb101-12"><a href="glm.html#cb101-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">coef</span>(m.binom.<span class="fl">2.</span>glm)[<span class="dv">3</span>], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="MCMCglmm-course-notes_files/figure-html/bernoulli-glm-1.png" alt="" width="672" /></p>
</div>
</div>
<div id="complete-separation" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Complete Separation<a href="glm.html#complete-separation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It makes sense that a cumulative distribution function for a continuous distribution that can take any value would serve as a good inverse link function:</p>
<p>To demonstrate we will use some data from a pilot study on the Indian meal moth (<em>Plodia interpunctella</em>) and its granulosis virus (PiGV) collected by Hannah Tidbury &amp; Mike Boots at the University of Sheffield.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="glm.html#cb102-1" tabindex="-1"></a><span class="fu">data</span>(PlodiaRB)</span></code></pre></div>
<p>The data are taken from 874 moth pupae for which the <code>Pupated</code> variable is zero if they failed to pupate (because they were infected with the virus) or one if they successfully pupated. The 874 individuals are spread across 49 full-sib families, with family sizes ranging from 6 to 38.</p>
<p>To start we will fix the residual variance at 1:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="glm.html#cb103-1" tabindex="-1"></a>prior.m2b<span class="fl">.1</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>)))</span>
<span id="cb103-2"><a href="glm.html#cb103-2" tabindex="-1"></a>m2b<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(Pupated <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>FSfamily, <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">data =</span> PlodiaRB,</span>
<span id="cb103-3"><a href="glm.html#cb103-3" tabindex="-1"></a>    <span class="at">prior =</span> prior.m2b<span class="fl">.1</span>)</span></code></pre></div>
<p>and then fit a second model where the residual variance is fixed at 2:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="glm.html#cb104-1" tabindex="-1"></a>prior.m2b<span class="fl">.2</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">2</span>, <span class="at">fix =</span> <span class="dv">1</span>), <span class="at">G =</span> <span class="fu">list</span>(<span class="at">G1 =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">nu =</span> <span class="fl">0.002</span>)))</span>
<span id="cb104-2"><a href="glm.html#cb104-2" tabindex="-1"></a>m2b<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(Pupated <span class="sc">~</span> <span class="dv">1</span>, <span class="at">random =</span> <span class="sc">~</span>FSfamily, <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">data =</span> PlodiaRB,</span>
<span id="cb104-3"><a href="glm.html#cb104-3" tabindex="-1"></a>    <span class="at">prior =</span> prior.m2b<span class="fl">.2</span>)</span></code></pre></div>
<p>The posterior distribution for the intercept differs between the two models (see Figure <a href="glm.html#fig:Bin1">3.11</a>):</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="glm.html#cb105-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(m2b<span class="fl">.1</span><span class="sc">$</span>Sol, m2b<span class="fl">.2</span><span class="sc">$</span>Sol))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:Bin1"></span>
<img src="MCMCglmm-course-notes_files/figure-html/Bin1-1.png" alt="MCMC summary plots for the intercept of a binary GLMM where the residual variance was fixed at one (black) and two (red)." width="672" />
<p class="caption">
Figure 3.11: MCMC summary plots for the intercept of a binary GLMM where the residual variance was fixed at one (black) and two (red).
</p>
</div>
<p>as do the variance components (see Figure <a href="glm.html#fig:Bin2">3.12</a>):</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="glm.html#cb106-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(m2b<span class="fl">.1</span><span class="sc">$</span>VCV, m2b<span class="fl">.2</span><span class="sc">$</span>VCV))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:Bin2"></span>
<img src="MCMCglmm-course-notes_files/figure-html/Bin2-1.png" alt="MCMC summary plots for the between family variance component of a binary GLMM where the residual variance was fixed at one (black) and two (red)." width="672" />
<p class="caption">
Figure 3.12: MCMC summary plots for the between family variance component of a binary GLMM where the residual variance was fixed at one (black) and two (red).
</p>
</div>
<p>Should we worry? Not really. We just have to be careful about how we express the results. Stating that the family variance is 0.817 is meaningless without putting it in the context of the assumed residual variance. It is therefore more appropriate to report the intraclass correlation which in this context is the expected correlation between the state Pupated/Not Pupated, for members of the same family. It can be
calculated as:</p>
<p><span class="math display">\[\texttt{IC} =  \frac{\sigma^{2}_{\texttt{FSfamily}}}{\sigma^{2}_{\texttt{FSfamily}}+\sigma^{2}_{\texttt{units}}+\pi^{2}/3}\]</span></p>
<p>for the logit link, which is used when <code>family=categorical</code>, or</p>
<p><span class="math display">\[\texttt{IC} =  \frac{\sigma^{2}_{\texttt{FSfamily}}}{\sigma^{2}_{\texttt{FSfamily}}+\sigma^{2}_{\texttt{units}}+1}\]</span></p>
<p>for the probit link, which is used if <code>family=ordinal</code> was specified.</p>
<p>Obtaining the posterior distribution of the intra-class correlation for each model shows that they are sampling very similar posterior distributions (see Figure <a href="glm.html#fig:IC">3.13</a>)</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="glm.html#cb107-1" tabindex="-1"></a>IC<span class="fl">.1</span> <span class="ot">&lt;-</span> m2b<span class="fl">.1</span><span class="sc">$</span>VCV[, <span class="dv">1</span>]<span class="sc">/</span>(<span class="fu">rowSums</span>(m2b<span class="fl">.1</span><span class="sc">$</span>VCV) <span class="sc">+</span> pi<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>)</span>
<span id="cb107-2"><a href="glm.html#cb107-2" tabindex="-1"></a>IC<span class="fl">.2</span> <span class="ot">&lt;-</span> m2b<span class="fl">.2</span><span class="sc">$</span>VCV[, <span class="dv">1</span>]<span class="sc">/</span>(<span class="fu">rowSums</span>(m2b<span class="fl">.2</span><span class="sc">$</span>VCV) <span class="sc">+</span> pi<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>)</span>
<span id="cb107-3"><a href="glm.html#cb107-3" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(IC<span class="fl">.1</span>, IC<span class="fl">.2</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:IC"></span>
<img src="MCMCglmm-course-notes_files/figure-html/IC-1.png" alt="MCMC summary plots for the intra-family correlation from  a binary GLMM where the residual variance was fixed at one (black) and two (red)." width="672" />
<p class="caption">
Figure 3.13: MCMC summary plots for the intra-family correlation from a binary GLMM where the residual variance was fixed at one (black) and two (red).
</p>
</div>
<p>Using the approximation due to <span class="citation">Diggle et al. (<a href="#ref-Diggle.2004">2004</a>)</span> described earlier we can also rescale the estimates by the estimated residual variance (<span class="math inline">\(\sigma^{2}_{\texttt{units}}\)</span>) in order to obtain the posterior distributions of the parameters under the assumption that the actual residual variance (<span class="math inline">\(\sigma^{2}_{e}\)</span>) is equal to some other value. For location effects the posterior distribution needs to be multiplied by <span class="math inline">\(\sqrt{\frac{1+c^{2}\sigma^{2}_{e}}{1+c^{2}\sigma^{2}_{\texttt{units}}}}\)</span> and for the variance components the posterior distribution needs to be multiplied by <span class="math inline">\(\frac{1+c^{2}\sigma^{2}_{e}}{1+c^{2}\sigma^{2}_{\texttt{units}}}\)</span> where <span class="math inline">\(c\)</span> is some constant that depends on the link function. For the probit <span class="math inline">\(c=1\)</span> and for the logit <span class="math inline">\(c=16\sqrt{3}/15\pi\)</span>. We can obtain estimates under the assumption that <span class="math inline">\(\sigma^{2}_{e}=0\)</span>:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="glm.html#cb108-1" tabindex="-1"></a>c2 <span class="ot">&lt;-</span> ((<span class="dv">16</span> <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">3</span>))<span class="sc">/</span>(<span class="dv">15</span> <span class="sc">*</span> pi))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb108-2"><a href="glm.html#cb108-2" tabindex="-1"></a>Int<span class="fl">.1</span> <span class="ot">&lt;-</span> m2b<span class="fl">.1</span><span class="sc">$</span>Sol<span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">+</span> c2 <span class="sc">*</span> m2b<span class="fl">.1</span><span class="sc">$</span>VCV[, <span class="dv">2</span>])</span>
<span id="cb108-3"><a href="glm.html#cb108-3" tabindex="-1"></a>Int<span class="fl">.2</span> <span class="ot">&lt;-</span> m2b<span class="fl">.2</span><span class="sc">$</span>Sol<span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">+</span> c2 <span class="sc">*</span> m2b<span class="fl">.2</span><span class="sc">$</span>VCV[, <span class="dv">2</span>])</span>
<span id="cb108-4"><a href="glm.html#cb108-4" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">mcmc.list</span>(<span class="fu">as.mcmc</span>(Int<span class="fl">.1</span>), <span class="fu">as.mcmc</span>(Int<span class="fl">.2</span>)))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ICI"></span>
<img src="MCMCglmm-course-notes_files/figure-html/ICI-1.png" alt="MCMC summary plots for the expected proportion of caterpillars pupating from  a binary GLMM where the residual variance was fixed at one (black) and two (red)." width="672" />
<p class="caption">
Figure 3.14: MCMC summary plots for the expected proportion of caterpillars pupating from a binary GLMM where the residual variance was fixed at one (black) and two (red).
</p>
</div>
<p>The posteriors should be virtually identical under a flat prior (See Figure <a href="glm.html#fig:ICI">3.14</a>) although with different priors this is not always the case. Remarkably, <span class="citation">Dyk and Meng (<a href="#ref-vanDyk.2001">2001</a>)</span> show that leaving a diffuse prior on
<span class="math inline">\(\sigma^{2}_{\texttt{units}}\)</span> and rescaling the estimates each iteration, a Markov chain with superior mixing and convergence properties can be obtained (See section <a href="parameter-expansion.html#parameter-expansion">9</a>).</p>
<p>It should also be noted that a diffuse prior on the logit scale is not necessarily weakly informative on the probability scale. For example, the default setting for the prior on the intercept is <span class="math inline">\(N(0, 10^{8})\)</span> on the logit scale, which although relatively flat across most of the probability scale, has a lot of density close to zero and one:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="glm.html#cb109-1" tabindex="-1"></a><span class="fu">hist</span>(<span class="fu">plogis</span>(<span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">1e+08</span>))))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:invlogit"></span>
<img src="MCMCglmm-course-notes_files/figure-html/invlogit-1.png" alt="Histogram of 1000 random deviates from a normal distribution with a mean of zero and a large variance ($10^8$) after undergoing an inverse logit transformation." width="672" />
<p class="caption">
Figure 3.15: Histogram of 1000 random deviates from a normal distribution with a mean of zero and a large variance (<span class="math inline">\(10^8\)</span>) after undergoing an inverse logit transformation.
</p>
</div>
<p>This diffuse prior can cause problems if there is complete (or near complete) separation. Generally this happens when the binary data associated with some level of a categorical predictor are all success or all failures. For example, imagine we had 50 binary observations from an experiment with two treatments, for the first treatment the probability of success is 0.5 but in the second it is only one in a thousand:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="glm.html#cb110-1" tabindex="-1"></a>treatment <span class="ot">&lt;-</span> <span class="fu">gl</span>(<span class="dv">2</span>, <span class="dv">25</span>)</span>
<span id="cb110-2"><a href="glm.html#cb110-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">50</span>, <span class="dv">1</span>, <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.001</span>)[treatment])</span>
<span id="cb110-3"><a href="glm.html#cb110-3" tabindex="-1"></a>data.bin <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">treatment =</span> treatment, <span class="at">y =</span> y)</span>
<span id="cb110-4"><a href="glm.html#cb110-4" tabindex="-1"></a><span class="fu">table</span>(data.bin)</span></code></pre></div>
<pre><code>##          y
## treatment  0  1
##         1 16  9
##         2 25  0</code></pre>
<p>if we analyse using <code>glm</code> we see some odd behaviour:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="glm.html#cb112-1" tabindex="-1"></a>m2c<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> treatment, <span class="at">data =</span> data.bin, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb112-2"><a href="glm.html#cb112-2" tabindex="-1"></a><span class="fu">summary</span>(m2c<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ treatment, family = &quot;binomial&quot;, data = data.bin)
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)   -0.5754     0.4167  -1.381    0.167
## treatment2   -18.9907  2150.8026  -0.009    0.993
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 47.139  on 49  degrees of freedom
## Residual deviance: 32.671  on 48  degrees of freedom
## AIC: 36.671
## 
## Number of Fisher Scoring iterations: 18</code></pre>
<p>the effect of treatment does not appear significant despite the large effect size. This is in direct contrast to an exact binomial test:</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="glm.html#cb114-1" tabindex="-1"></a>m2c<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">binom.test</span>(<span class="fu">table</span>(data.bin)[<span class="dv">2</span>, <span class="dv">2</span>], <span class="dv">25</span>)</span>
<span id="cb114-2"><a href="glm.html#cb114-2" tabindex="-1"></a>m2c<span class="fl">.2</span></span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  table(data.bin)[2, 2] and 25
## number of successes = 0, number of trials = 25, p-value = 5.96e-08
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.0000000 0.1371852
## sample estimates:
## probability of success 
##                      0</code></pre>
<p>where the 95% confidence interval for the probability of success is 0.000 to
0.137.</p>
<p>The default <span class="math inline">\(\texttt{MCMCglmm}\)</span> model also behaves oddly (see Figure <a href="glm.html#fig:separation1">3.16</a>):</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="glm.html#cb116-1" tabindex="-1"></a>prior.m2c<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>, <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb116-2"><a href="glm.html#cb116-2" tabindex="-1"></a>m2c<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(y <span class="sc">~</span> treatment, <span class="at">data =</span> data.bin, <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">prior =</span> prior.m2c<span class="fl">.3</span>)</span>
<span id="cb116-3"><a href="glm.html#cb116-3" tabindex="-1"></a><span class="fu">plot</span>(m2c<span class="fl">.3</span><span class="sc">$</span>Sol)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:separation1"></span>
<img src="MCMCglmm-course-notes_files/figure-html/separation1-1.png" alt="MCMC summary plots for the intercept and treatment effect in a binary GLM. In treatment 2 all 25 observations were failures and so the ML estimator on the probability scale is zero and $-\infty$ on the logit scale. With a flat prior on the treatment effect the posterior distribution is improper, and with a diffuse prior (as used here) the posterior is dominated by the high prior densities at extreme values." width="672" />
<p class="caption">
Figure 3.16: MCMC summary plots for the intercept and treatment effect in a binary GLM. In treatment 2 all 25 observations were failures and so the ML estimator on the probability scale is zero and <span class="math inline">\(-\infty\)</span> on the logit scale. With a flat prior on the treatment effect the posterior distribution is improper, and with a diffuse prior (as used here) the posterior is dominated by the high prior densities at extreme values.
</p>
</div>
<p>For these types of problems, I usually remove the global intercept (<code>-1</code>) and use the prior <span class="math inline">\(N(0, \sigma^{2}_{\texttt{units}}+\pi^2/3)\)</span> because this is reasonably flat on the probability scale when a logit link is used. For example,</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="glm.html#cb117-1" tabindex="-1"></a>prior.m2c<span class="fl">.4</span> <span class="ot">=</span> <span class="fu">list</span>(<span class="at">B =</span> <span class="fu">list</span>(<span class="at">mu =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="at">V =</span> <span class="fu">diag</span>(<span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">+</span> pi<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>)), <span class="at">R =</span> <span class="fu">list</span>(<span class="at">V =</span> <span class="dv">1</span>,</span>
<span id="cb117-2"><a href="glm.html#cb117-2" tabindex="-1"></a>    <span class="at">fix =</span> <span class="dv">1</span>))</span>
<span id="cb117-3"><a href="glm.html#cb117-3" tabindex="-1"></a>m2c<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">MCMCglmm</span>(y <span class="sc">~</span> treatment <span class="sc">-</span> <span class="dv">1</span>, <span class="at">data =</span> data.bin, <span class="at">family =</span> <span class="st">&quot;categorical&quot;</span>, <span class="at">prior =</span> prior.m2c<span class="fl">.4</span>)</span>
<span id="cb117-4"><a href="glm.html#cb117-4" tabindex="-1"></a><span class="fu">plot</span>(m2c<span class="fl">.4</span><span class="sc">$</span>Sol)</span></code></pre></div>
<p>looks a little better (see Figure <a href="glm.html#fig:separation1">3.16</a>), and the posterior distribution for the probability of success in treatment 2 is consistent with the exact binomial test for which the 95% CI were (0.000 -0.137). With such a simple model, the prediction for observation 26 is equal to the treatment 2 effect and so we can get the the credible interval (on the data scale) for treatment 2 using the predict function:</p>
<div class="figure"><span style="display:block;" id="fig:separation2"></span>
<img src="MCMCglmm-course-notes_files/figure-html/separation2-1.png" alt="MCMC summary plots for the intercept and treatment effect in a binary GLM. In treatment 2 all 25 observations were failures and so the ML estimator on the probability scale is zero and $-\infty$ on the logit scale. A flat prior on the probability scale was used and the posterior distribution is better behaved than if a flat prior on the logit scale had been used (see Figure \ref{separation1-fig})." width="672" />
<p class="caption">
Figure 3.17: MCMC summary plots for the intercept and treatment effect in a binary GLM. In treatment 2 all 25 observations were failures and so the ML estimator on the probability scale is zero and <span class="math inline">\(-\infty\)</span> on the logit scale. A flat prior on the probability scale was used and the posterior distribution is better behaved than if a flat prior on the logit scale had been used (see Figure <span class="math inline">\(\ref{separation1-fig}\)</span>).
</p>
</div>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="glm.html#cb118-1" tabindex="-1"></a><span class="fu">predict</span>(m2c<span class="fl">.4</span>, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)[<span class="dv">26</span>, ]</span></code></pre></div>
<pre><code>##         fit         lwr         upr 
## 0.036775726 0.001263126 0.100985872</code></pre>

</div>
</div>
<h3> References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Diggle.2004" class="csl-entry">
Diggle, P., P. Heagerty, K. Liang, and S. Zeger. 2004. <em>Analysis of Longitudinal Data</em>. II. Oxford University Press.
</div>
<div id="ref-vanDyk.2001" class="csl-entry">
Dyk, D. A. van, and X. L. Meng. 2001. <span>“The Art of Data Augmentation.”</span> <em>Journal of Computational and Graphical Statistics</em> 10 (1): 1–50.
</div>
<div id="ref-Hinde.1982" class="csl-entry">
Hinde, John. 1982. <span>“Compound <span>Poisson</span> Regression Models.”</span> In <em>Glim 82: Proceedings of the International Conference on Generalised Linear Models</em>, 109–21. Springer.
</div>
<div id="ref-McCulloch.2001" class="csl-entry">
McCulloch, C. E., and S. R. Searle. 2001. <em>Generalized, Linear and Mixed Models</em>. Wiley Series in Probability and Statistics. New York: John Wiley &amp; Sons.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>This is a bit disingenuous. The MCMC algorithm implemented in depends on a non-zero residual variance to ensure mixing - if the residual variance was set to zero (i.e. no overdispersion in GLM(M)) then the Markov chain would be reducible.<a href="glm.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Since the residuals are assumed normal the exponentiated residuals are log-normal, and so this model is often referred to as the Poisson log-normal <span class="citation">(<a href="#ref-Hinde.1982">Hinde 1982</a>)</span>. The Negative Binomial distribution is a commonly used alternative for overdispersed count data. The Negative Binomial is conceptually identical to the Poisson log-normal except the exponentiated residuals are assumed to be gamma distributed. The log-normal and gamma distributions are so similar that for most data sets it would be hard to distinguish between the Negative Binomial and the Poisson log-normal.<a href="glm.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>When <span class="math inline">\(x\)</span> is small <span class="math inline">\(\textrm{exp}(x)\approx 1+x\)</span>.<a href="glm.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>For more complicated models a (co)variance matrix may be estimated for a particular random component rather than just a single variance as here. In such cases, <code>V</code> is a matrix. The value at which (part of) the (co)variance matrix is fixed at is determined by <code>V</code>. Any elements of the covariance matrix in rows and/or columns equal to or greater than <code>fix</code> are fixed. In the case of a single variance <code>fix=1</code> simply fixes the variance (element 1,1 of the (co)variance matrix) at whatever is specified in <code>V</code> (one in this example).<a href="glm.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ranef.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["MCMCglmm-course-notes.pdf", "MCMCglmm-course-notes.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
