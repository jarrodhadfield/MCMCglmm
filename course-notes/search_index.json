[["overview.html", "MCMCglmm Course Notes 1 Overview 1.1 Outline", " MCMCglmm Course Notes Jarrod Hadfield 2026-01-06 1 Overview These are (incomplete) course notes about generalised linear mixed models (GLMM). Special emphasis is placed on understanding the underlying structure of a GLMM in order to show that slight modifications of this structure can produce a wide range of models. These include familiar models like regression and ANOVA, but also models with intimidating names: animal models, threshold models, meta-analysis, random regression The primary aim of the course is to show that these models are only daunting by name. The secondary aim is to show how these models can be fitted in a Bayesian framework using Markov chain Monte Carlo (MCMC) methods in the R package \\(\\texttt{MCMCglmm}\\) (Hadfield 2010). For those not comfortable using Bayesian methods, many of the models outlined in the course notes can be fitted using the packages \\(\\texttt{asreml-r}\\) (Butler et al. 2017), \\(\\texttt{lme4}\\) (Bates et al. 2015) or \\(\\texttt{glmmTMB}\\) (Brooks et al. 2017) with little extra work. 1.1 Outline Chapter 2 covers the very basics of Bayesian analysis and Markov chain Monte Carlo (MCMC) techniques. Chapter 3 covers simple generalised linear mixed models (GLMM). The response is assumed to be (conditionally) normal or from a single-parameter family such as the Poisson, and the distribution of the random effects can be parameterised through scalar variances. Chapters 5 and 6 cover situations where the distribution of the random effects are parameterised through a (co)variance matrix either because the random terms are interacted with categorical predictors (Chapter 5) or continuos predictors (random-regression - Chapter 6). Chapter 7 covers multi-response models in which multiple response variables, possibly from different distributions, are simultaneously analysed. This chapter also covers distributions that are multi-parameter such as multinomial and zero-inflated distributions. Chapter 8 discusses models where a set of random-effects are correlated through a known structure such as a pedigree or phylogeny. Much of the technical details regarding the implementation are in Chapter 11. Chapter 9 covers parameter-expansion, an algorithm designed to improve the mixing properties of MCMC but which also induces new, and useful, prior distributions. Chapter 10 covers path analysis and related antedependence models. Much of the material is currently taken from the old CourseNotes (c2009) but the hope is that over time these new notes will be better structured and include better/some documentation for more recent functionality. References "],["bayesian.html", "2 Bayesian Analysis and MCMC 2.1 Introduction 2.2 Likelihood 2.3 Prior Distribution 2.4 Posterior Distribution 2.5 MCMC 2.6 Prior for Residual Variances 2.7 Transformations", " 2 Bayesian Analysis and MCMC In this Chapter I cover the basics of Bayesian analysis and Markov chain Monte Carlo (MCMC) techniques. Exposure to these ideas, and statistics in general, has increased dramatically since these notes were first written. Many readers may therefore wish to skip straight to later chapters that cover \\(\\texttt{MCMCglmm}\\) more specifically. 2.1 Introduction There are fundamental differences between frequentist and Bayesian approaches, but for those of us interested in applied statistics the hope is that these differences do not translate into practical differences, and this is often the case. My advice would be if you can fit the same model using different packages and/or methods do so, and if they give very different answers worry. In some cases differences will exist, and it is important to know why, and which method is more appropriate for the data in hand. In the context of a generalised linear mixed model (GLMM), here are what I see as the pro’s and cons of using (restricted) maximum likelihood (REML) versus Bayesian MCMC methods. REML is fast and easy to use, whereas MCMC can be slow and technically more challenging. Particularly challenging is the specification of a sensible prior, something which is a non-issue in a REML analysis. However, analytical results for non-Gaussian GLMM are generally not available, and REML based procedures use approximate likelihood methods that may not work well. MCMC is also an approximation but the accuracy of the approximation increases the longer the analysis is run for, being exact at the limit. In addition, REML uses large-sample theory to derive approximate confidence intervals that may have very poor coverage, especially for variance components. Again, MCMC measures of confidence are exact, up to Monte Carlo error, and provide an easy and intuitive way of obtaining measures of confidence on derived statistics such as ratios of variances, correlations and predictions. To illustrate the differences between the approaches let’s imagine we’ve observed several draws (stored in the vector \\({\\bf y}\\)) from a standard normal (i.e. \\(\\mu=0\\) and \\(\\sigma^{2}=1\\)). The likelihood is the probability of the data given the parameters: \\[Pr({\\bf y} | \\mu, \\sigma^{2})\\] This is a conditional distribution, where the conditioning is on the model parameters which are taken as fixed and known. In a way this is quite odd because we’ve already observed the data, and we don’t know what the parameter values are. In a Bayesian analysis we evaluate the conditional probability of the model parameters given the observed data: \\[Pr(\\mu, \\sigma^{2} | {\\bf y}) \\label{post1-eq} \\tag{2.1}\\] which seems more reasonable, until we realise that this probability is proportional to \\[Pr({\\bf y} | \\mu, \\sigma^{2})Pr(\\mu, \\sigma^{2})\\] where the first term is the likelihood, and the second term represents our prior belief in the values that the model parameters could take. Because the choice of prior is rarely justified by an objective quantification of the state of knowledge it has come under criticism, and indeed we will see later that the choice of prior can make a difference. 2.2 Likelihood We can generate 5 observations from this distribution using rnorm: Ndata &lt;- data.frame(y = rnorm(5, mean = 0, sd = 1)) Ndata$y ## [1] -1.01500872 -0.07963674 -0.23298702 -0.81726793 0.77209084 We can plot the probability density function for the standard normal using dnorm and we can then place the 5 data on it: possible.y&lt;-seq(-3,3,0.1) # possible values of y Probability&lt;-dnorm(possible.y, mean=0, sd=1) # density of possible values plot(Probability~possible.y, type=&quot;l&quot;, ylab=&quot;Density&quot;, xlab=&quot;y&quot;) Probability.y&lt;-dnorm(Ndata$y, mean=0, sd=1) # density of actual values points(Probability.y~Ndata$y) Figure 2.1: Probability density function for the unit normal with the data points overlaid The likelihood of these data, conditioning on \\(\\mu=0\\) and \\(\\sigma^2=1\\), is proportional to the product of the densities (read off the y-axis on Figure 2.1): prod(dnorm(Ndata$y, mean = 0, sd = 1)) ## [1] 0.003113051 Of course we don’t know the true mean and variance and so we may want to ask how probable the data would be if, say, \\(\\mu=0\\), and \\(\\sigma^2=0.5\\): prod(dnorm(Ndata$y, mean = 0, sd = sqrt(0.5))) ## [1] 0.005424967 It would seem that the data are more likely under this set of parameters than the true parameters, which we must expect some of the time just from random sampling. To get some idea as to why this might be the case we can overlay the two densities (Figure 2.2), and we can see that although some data points (e.g. -1.015) are more likely with the true parameters, in aggregate the new parameters produce a higher likelihood. Figure 2.2: Two probability density functions for normal distributions with means of zero, and a variance of one (black line) and a variance of 0.5 (red line). The data points are overlaid. The likelihood of the data can be calculated on a grid of possible parameter values to produce a likelihood surface, as in Figure 2.3. The densities on the contours have been scaled so they are relative to the density of the parameter values that have the highest density (the maximum likelihood estimate of the two parameters). Two things are apparent. First, although the surface is symmetric about the line \\(\\mu = \\hat{\\mu}\\) (where \\(\\hat{}\\) stands for maximum likelihood estimate) the surface is far from symmetric about the line \\(\\sigma^{2} = \\hat{\\sigma}^{2}\\). Second, there are a large range of parameter values for which the data are only 10 times less likely than if the data were generated under the maximum likelihood estimates. Figure 2.3: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\sigma^{2})\\). The likelihood has been normalised so that the maximum likelihood has a value of one. 2.2.1 Maximum Likelihood (ML) The ML estimator is the combination of \\(\\mu\\) and \\(\\sigma^{2}\\) that make the data most likely. Although we could evaluate the density on a grid of parameter values (as we did to produce Figure 2.3) in order to locate the maximum, for such a simple problem the ML estimator can be derived analytically. However, so we don’t have to meet some nasty maths later, I’ll introduce and use one of R’s generic optimising routines that can be used to maximise the likelihood function (in practice, the log-likelihood is maximised to avoid numerical problems): lik &lt;- function(par, y, log = FALSE) { if (log) { l &lt;- sum(dnorm(y, mean = par[1], sd = sqrt(par[2]), log = TRUE)) } else { l &lt;- prod(dnorm(y, mean = par[1], sd = sqrt(par[2]))) } return(l) } # function which takes the parameter vector (mean and *variance*) and the data # and returns the (log) likelihood MLest &lt;- optim(c(mean = 0, var = 1), fn = lik, y = Ndata$y, log = TRUE, control = list(fnscale = -1, reltol = 1e-16))$par The first call to optim are starting values for the optimisation algorithm, and the second argument (fn) is the function to be maximised. By default optim will try to minimise the function hence multiplying by -1 (fnscale = -1). The algorithm has successfully found the mode: MLest ## mean var ## -0.2745619 0.3955995 Alternatively we could also fit the model using glm, which by default assumes the response is normal: m1a.1 &lt;- glm(y ~ 1, data = Ndata) summary(m1a.1) ## ## Call: ## glm(formula = y ~ 1, data = Ndata) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2746 0.3145 -0.873 0.432 ## ## (Dispersion parameter for gaussian family taken to be 0.4944994) ## ## Null deviance: 1.978 on 4 degrees of freedom ## Residual deviance: 1.978 on 4 degrees of freedom ## AIC: 13.553 ## ## Number of Fisher Scoring iterations: 2 Here we see that although the estimate of the mean (intercept) is the same, the estimate of the variance (the dispersion parameter: 0.494) is higher when fitting the model using glm. In fact the ML estimate is a factor \\(\\frac{n}{n-1}\\) smaller because the glm estimate has used Bessel’s correction: MLest[&quot;var&quot;] * (5/4) ## var ## 0.4944994 2.2.2 Restricted Maximum Likelihood (REML) Why do we use Bessel’s correction? Imagine we had only observed the first two values of \\({\\bf y}\\) (Figure 2.4). The variance is defined as the average squared distance between a random variable and the true mean. However, the ML estimator of the variance is the average squared distance between the random variable and the ML estimate of the mean. Since the ML estimator of the mean is the average of the two numbers (the dashed line) then the average squared distance will always be smaller than if the true mean was used, unless the ML estimate of the mean and the true mean coincide. This is why we use Bessel’s \\(n-1\\) correction when estimating the variance from the sum of squares, or why we divide by \\(n-n_p\\) when estimating the residual variance in a liner model with \\(n_p\\) parameters, or more generally why we use REML in linear mixed models. Why these corrections for uncertainty in the mean, or model parameters, have this form can be understood from a Bayesian perspective (see Section 2.6). Figure 2.4: Probability density function for the unit normal with two realisations overlaid. The solid vertical line is the true mean, whereas the vertical dashed line is the mean of the two realisations (the ML estimator of the mean). The variance is the expected squared distance between the true mean and the realisations. The ML estimator of the variance is the average squared distance between the ML mean and the realisations (horizontal dashed lines), which is always smaller than the average squared distance between the true mean and the realisations (horizontal solid lines) 2.3 Prior Distribution \\(\\texttt{MCMCglmm}\\) uses a normal prior for the fixed effects and an inverse-Wishart prior for the residual variance. In the current model their is a single fixed effect (\\(\\mu\\)) and a scalar (residual) variance (\\(\\sigma^2\\)). For the mean we will use the default prior - a diffuse normal centred around zero but with very large variance (\\(10^{8}\\)). For the residual variance, the inverse-Wishart prior takes two scalar parameters. In \\(\\texttt{MCMCglmm}\\) this is parameterised through the parameters \\(\\texttt{V}\\) and \\(\\texttt{nu}\\). The distribution tends to a point mass on \\(\\texttt{V}\\) as the degree of belief parameter, \\(\\texttt{nu}\\) goes to infinity. We will defer a full discussion of the inverse-Wishart prior to Section 2.6 and for now we will use the prior specification \\(\\texttt{V}=1\\) and \\(\\texttt{nu}=0.002\\) which used to be frequently used for variances. As before, we can evaluate and plot the density function in order to visualise what the distribution looks like (Figure 2.5). Figure 2.5: Probability density function for a univariate inverse-Wishart with the variance at the limit set to 1 (\\(\\texttt{V}=1\\)) and a degree of belief parameter set to 0.002 (\\(\\texttt{nu}=0.002\\)). As before we can write a function for calculating the (log) prior probability: prior.p &lt;- function(par, priorB, priorR, log = FALSE) { if (log) { d &lt;- dnorm(par[1], mean = priorB$mu, sd = sqrt(priorB$V), log = TRUE) + dgamma(1/par[2], shape = priorR$nu/2, rate = (priorR$nu * priorR$V)/2, log = TRUE) - 2 * log(par[2]) } else { d &lt;- dnorm(par[1], mean = priorB$mu, sd = sqrt(priorB$V)) * dgamma(1/par[2], shape = priorR$nu/2, rate = (priorR$nu * priorR$V)/2)/par[2]^2 } return(d) } where priorR is a list with elements V and nu specifying the prior for the variance, and priorB is a list with elements mu and V specifying the prior for the mean. \\(\\texttt{MCMCglmm}\\) takes these prior specifications as a list: prior &lt;- list(R = list(V = 1, nu = 0.002), B = list(mu = 0, V = 1e+08)) 2.4 Posterior Distribution By multiplying the likelihood by the prior probability for that set of parameters we can get the posterior probability up to a proportional constant. We can write a function for doing this: likprior &lt;- function(par, y, priorB, priorR, log = FALSE) { if (log) { pd &lt;- lik(par, y, log = TRUE) + prior.p(par, priorB = priorB, priorR = priorR, log = TRUE) } else { pd &lt;- lik(par, y) * prior.p(par, priorB = priorB, priorR = priorR) } return(pd) } and we can overlay the posterior density (scaled by the posterior density at the posterior mode) on the likelihood surface we calculated before (Figure 2.3). Figure 2.6: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\sigma^{2})\\) in black, and the posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\) in red. The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. The prior distributions \\(Pr(\\mu)\\sim N(0, 10^8)\\) and \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=1, \\texttt{nu}=0.002)\\) were used. The prior has some influence on the posterior mode of the variance, and we can use an optimisation algorithm again to locate the mode: Best &lt;- optim(c(mean = 0, var = 1), fn = likprior, y = Ndata$y, priorB = prior$B, priorR = prior$R, log = TRUE, method = &quot;L-BFGS-B&quot;, lower = c(-1e+05, 1e-05), upper = c(1e+05, 1e+05), control = list(fnscale = -1, factr = 1e-16))$par Best ## mean var ## -0.2745613 0.2827783 The posterior mode for the mean is essentially identical to the ML estimate, but the posterior mode for the variance is even less than the ML estimate, which is known to be downwardly biased. The reason that the ML estimate is downwardly biased is because it did not take into account the uncertainty in the mean, as we saw when discussing the motivation behind REML. In a Bayesian analysis we can do this by evaluating the marginal distribution of \\(\\sigma^{2}\\) by averaging over the uncertainty in the mean. Before we do this, however, it will be instructive to see why this would be hard using our function that simply multiplies the likelihood by the prior (likprior). This function (the probabilities on the right-hand side) is only proprtional to the posterior density (the left-hand side), not equal to it, implying \\[Pr(\\mu, \\sigma^{2} | {\\bf y}) = \\frac{1}{C} Pr({\\bf y} | \\mu, \\sigma^{2})Pr(\\mu, \\sigma^{2})\\] where \\(C\\) is some constant. In some cases this is not an issue - when finding the posterior mode it was not an issue since the parameters that maximise the posterior density would also maximise the posterior density scaled by \\(C\\). Similarly, if we wanted to make relative statements about posterior probabilities, then \\(C\\) would cancel. For example, we can see how much more likely a variance of a half is versus a variance of one (assuming the mean is zero): p.0.5 &lt;- likprior(c(0, 0.5), y = Ndata$y, priorB = prior$B, priorR = prior$R) p.1.0 &lt;- likprior(c(0, 1), y = Ndata$y, priorB = prior$B, priorR = prior$R) p.0.5/p.1.0 ## [1] 3.484236 However, in many instances we would like to work with the normalised posterior density, so how do we get \\(C\\)? Well we know that if we took the posterior probability of being any combination of \\(\\mu\\) and \\(\\sigma^{2}\\) it must be equal to one: \\[\\int_{\\sigma^2}\\int_\\mu Pr(\\mu, \\sigma^{2} | {\\bf y})d\\mu d\\sigma^2=1\\] and so because \\[1 =\\frac{1}{C}\\int_{\\sigma^2}\\int_\\mu Pr({\\bf y} | \\mu, \\sigma^{2})Pr(\\mu, \\sigma^{2})d\\mu d\\sigma^2\\] we can integrate our parameters over likprior to get \\(C\\). Not easy, and requires numerical integration: C &lt;- adaptIntegrate(likprior, y = Ndata$y, priorR = prior$R, priorB = prior$B, lower = c(-Inf, 0), upper = c(Inf, Inf))$integral The posterior density for \\(\\mu=0\\) and \\(\\sigma^2=0.5\\) is p.0.5/C=0.937. Neither interesting or particularly interpretable, and often we want to perform additional integration in order to compute quantities of interest. For example, imagine we wanted to know the probability that the parameters lay in the region of parameter space we were plotting, i.e. lay in the square \\(\\mu = (-2,2)\\) and \\(\\sigma^{2} = (0,5)\\). To obtain this probability we need to calculate the definite integral \\[\\int_{\\sigma^{2}=0}^{\\sigma^{2}=5} \\int_{\\mu=-2}^{\\mu=2} Pr(\\mu, \\sigma^{2} | {\\bf y})d\\mu d\\sigma^2\\] which requires integrating our likprior over the same limits and rescaling by \\(C\\): p.square &lt;- adaptIntegrate(likprior, y = Ndata$y, priorR = prior$R, priorB = prior$B, lower = c(-2, 0), upper = c(2, 5))$integral p.square/C ## [1] 0.9816286 While this is doable for simple problems like this, numerical integration for high-dimensional problems is often not feasible and MCMC provides a viable alternative. We can fit this model in \\(\\texttt{MCMCglmm}\\) pretty much in the same way as we did using glm: m1a.2 &lt;- MCMCglmm(y ~ 1, data = Ndata, prior = prior, thin = 1) The Markov chain is drawing random (but often correlated) samples from the joint posterior distribution (depicted by the red contours in Figure 2.6). The element of the output called Sol contains the posterior samples for the mean, and the element called VCV contains the posterior samples for the variance. We can produce a scatter plot: points(cbind(m1a.2$Sol, m1a.2$VCV)) Figure 2.7: The posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\). The black dots are samples from the posterior using MCMC, and the red contours are calculated by evaluating the posterior density on a grid of parameter values. The contours are normalised so that the posterior mode has a value of one. and we see that MCMCglmm is sampling the same distribution as the posterior distribution calculated on a grid of possible parameter values (Figure 2.7). A very nice property of MCMC is that we can calculate probabilities from the output without having to explicitly perform integration. Earlier we calculated the probability that the mean lay between \\(\\pm2\\) and the variance was less than 5 (\\(\\texttt{p.square/C}=\\) 0.982). Because MCMC has sampled the posterior distribution randomly, this probability will be equal to the expected probability that we have drawn an MCMC sample from the region. We can obtain an estimate of this by seeing what proportion of our actual samples lie in this square: prop.table(table(m1a.2$Sol &gt; -2 &amp; m1a.2$Sol &lt; 2 &amp; m1a.2$VCV &lt; 5)) ## ## FALSE TRUE ## 0.0196 0.9804 There is Monte Carlo error in the answer (0.980) but if we collect a large number of samples then this can be minimised. 2.4.1 Marginal Posterior Distribution The marginal distribution is often of primary interest in statistical inference, because it represents our knowledge about a parameter given the data: \\[Pr(\\sigma^{2} | {\\bf y}) = \\int Pr(\\mu, \\sigma^{2} | {\\bf y})d\\mu \\label{marg-eq} \\tag{2.2}\\] after averaging over any nuisance parameters, such as the mean in this case. Using MCMC, we can obtain the marginal distribution of the variance by simply evaluating the draws in VCV ignoring (averaging over) the draws in Sol: hist(m1a.2$VCV) abline(v = Best[&quot;var&quot;], col = &quot;red&quot;) Figure 2.8: Histogram of samples from the marginal distribution of the variance \\(Pr(\\sigma^{2} | {\\bf y})\\) using MCMC. The vertical line is the joint posterior mode, which differs slightly from the marginal posterior mode (the peak of the marginal distribution). In this example the marginal mode and the joint mode are very similar, although this is not necessarily the case and can depend on both the data and the prior. Section 2.6 covers properties of the inverse-Wishart prior in detail. 2.4.2 Credible Intervals 2.5 MCMC In order to be confident that \\(\\texttt{MCMCglmm}\\) has successfully sampled the posterior distribution it will be necessary to have a basic understanding of how MCMC works. The aim of MCMC is to sample parameter values from their posterior distribution, shown exactly (up to proportionality) in Figure 2.6. In all but the very simplest cases this distribution is not of a known form and we cannot sample from it directly by using functions such as rnorm. However we can set up a random walk in parameter space such that the chance a walker visits a particular set of parameter values is proportional to their posterior density. Importantly, this can done without needing to normalise the posterior density by \\(C\\). 2.5.1 Starting values First we need to initialise the chain and specify a set of parameter values from which the chain can start moving through parameter space. In practice, it’s generally a good idea to start the chain in region of relatively high probability. Although starting configurations can be set by the user using the start argument, in general the heuristic techniques used by \\(\\texttt{MCMCglmm}\\) seem to work quite well. We will denote the parameter values of the starting configuration (time \\(t=0\\)) as \\(\\mu_{t=0}\\) and \\({\\sigma^{2}}_{t=0}\\). There are several ways in which we can get the chain to move in parameter space, and the main techniques used in \\(\\texttt{MCMCglmm}\\) are Gibbs sampling and Metropolis-Hastings updates. To illustrate, it will be easier to turn the contour plot of the posterior distribution into a perspective plot (Figure 2.9). Figure 2.9: The posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\). This perspective plot is equivalent to the contour plot in Figure 2.6 but it has been normalised by \\(C\\) and is equal to, not just proportional to, the posterior density. 2.5.2 Metropolis-Hastings updates After initialising the chain we need to decide where to go next, and this decision is based on two rules. First we have to generate a candidate destination, and then we need to decide whether to go there or stay where we are. There are many ways in which we could generate candidate parameter values, and \\(\\texttt{MCMCglmm}\\) uses a well tested and simple method. A random set of coordinates are picked from a multivariate normal distribution that is centred on the initial coordinates \\(\\mu_{t=0}\\) and \\(\\sigma^{2}_{t=0}\\). We will denote this new set of parameter values as \\(\\mu_{new}\\) and \\(\\sigma^{2}_{new}\\). The question then remains whether to move to this new set of parameter values or remain at our current parameter values now designated as old \\(\\mu_{old}=\\mu_{t=0}\\) and \\(\\sigma^{2}_{old}=\\sigma^{2}_{t=0}\\). If the posterior probability for the new set of parameter values is greater, then the chain moves to this new set of parameters and the chain has successfully completed an iteration: (\\(\\mu_{t=1} = \\mu_{new}\\) and \\(\\sigma^{2}_{t=1}=\\sigma^{2}_{new}\\)). If the new set of parameter values has a lower posterior probability then the chain may move there, but not all the time. The probability that the chain moves to low lying areas, is determined by the relative difference between the old and new posterior probabilities. If the posterior probability for \\(\\mu_{new}\\) and \\(\\sigma^{2}_{new}\\) is 5 times less than the posterior probability for \\(\\mu_{old}\\) and \\(\\sigma^{2}_{old}\\), then the chain would move to the new set of parameter values 1 in 5 times. If the move is successful then we set \\(\\mu_{t=1} = \\mu_{new}\\) and \\(\\sigma^{2}_{t=1}=\\sigma^{2}_{new}\\) as before, and if the move is unsuccessful then the chain stays where it is (\\(\\mu_{t=1} = \\mu_{old}\\) and \\(\\sigma^{2}_{t=1}=\\sigma^{2}_{old}\\)). Note that we only need to know the posterior density up to proportionality to make these calculations (i.e we could use likprior directly without knowing \\(C\\)). Using these rules we can record where the chain has travelled and generate an approximation of the posterior distribution. Basically, a histogram of Figure 2.9. Why Metropolis-Hastings updates work can perhaps be more easily understood in terms of a simpler toy example. Imagine we had a strong prior such that only two sets of parameter values had positive posterior probability: Set A (\\(\\mu_A\\) an \\(\\sigma^2_A\\)) with a posterior probability 5 times that of Set B. If the chain is currently at Set A then the candidate parameter values will be Set B and vice-versa. Since we move from Set A to Set B 20% of the time and we move from Set B to Set A 100% of the time, the odds of being in Set A versus Set B is 1:0.2 which is exactly equal to their posterior odds of 5:1. 2.5.3 Gibbs Sampling Gibbs sampling is a special case of Metropolis-Hastings updating, and \\(\\texttt{MCMCglmm}\\) uses Gibbs sampling to update most parameters. In the Metropolis-Hastings example above, the Markov Chain was allowed to move in both directions of parameter space simultaneously. An equally valid approach would have been to set up two Metropolis-Hastings schemes where the chain was first allowed to move along the \\(\\mu\\) axis, and then along the \\(\\sigma^{2}\\) axis. In Figure 2.10 I have cut the posterior distribution of Figure 2.9 in half, and the edge of the surface facing left is the conditional distribution of \\(\\mu\\) given that \\(\\sigma^{2}=1\\): \\[Pr(\\mu |\\sigma^{2}=1, \\boldsymbol{\\mathbf{y}}).\\] Figure 2.10: The posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\), but only for values of \\(\\sigma^{2}\\) between 1 and 5, rather than 0 to 5 (Figure 2.9. The edge of the surface facing left is the conditional distribution of the mean when \\(\\sigma^{2}=1\\) (\\(Pr(\\mu | {\\bf y}, \\sigma^{2}=1)\\)). This conditional distribution follows a normal distribution. If we were using Metropolis-Hastings updates to sample \\(\\mu\\) we would need to evaluate this density, which may be much simpler than evaluating the full density. In fact for some cases, the equation that describes this conditional distribution can be derived despite the equation for the complete joint distribution of Figure 2.9 remaining unknown. When the conditional distribution of \\(\\mu\\) is known we can use Gibbs sampling. Lets say the chain at a particular iteration is located at \\(\\sigma^{2}=1\\). If we updated \\(\\mu\\) using a Metropolis-Hastings algorithm we would generate a candidate value and evaluate its relative probability compared to the old value. This procedure would take place in the slice of posterior facing left in Figure 2.10. However, because we know the actual equation for this slice we can just generate a new value of \\(\\mu\\) directly. This is Gibbs sampling. The slice of the posterior that we can see in Figure 2.10 actually has a normal distribution. Because of the weak prior this normal distribution has a mean close to the mean of \\(\\bf{y}\\) and a variance close to \\(\\frac{\\sigma^{2}}{n} = \\frac{1}{n}\\). Gibbs sampling can be much more efficient than Metropolis-Hastings updates, especially when high dimensional conditional distributions are known, as is typical in GLMMs. A technical description of the sampling schemes used by \\(\\texttt{MCMCglmm}\\) is given in the Chapter @ref(#technical), but is perhaps not important to know. 2.5.4 MCMC Diagnostics When fitting a model using \\(\\texttt{MCMCglmm}\\) the parameter values through which the Markov chain has travelled are stored and returned. The length of the chain (the number of iterations) can be specified using the nitt argument1 (the default is 13,000), and should be long enough so that the posterior approximation is valid. If we had known the joint posterior distribution in Figure 2.9 we could have sampled directly from the posterior. If this had been the case, each successive value in the Markov chain would be independent of the previous value after conditioning on the data, \\({\\bf y}\\), and a thousand iterations of the chain would have produced a histogram that resembled Figure 2.9 very closely. However, generally we do not know the joint posterior distribution of the parameters, and for this reason the parameter values of the Markov chain at successive iterations are usually not independent and care needs to be taken regarding the validity of the approximation. \\(\\texttt{MCMCglmm}\\) returns the Markov chain as mcmc objects, which can be analysed using the coda package. The function autocorr estimates the level of non-independence between successive samples in the chain: autocorr(m1a.2$Sol) ## , , (Intercept) ## ## (Intercept) ## Lag 0 1.0000000000 ## Lag 1 0.0295393869 ## Lag 5 0.0069888310 ## Lag 10 0.0200379700 ## Lag 50 0.0009886787 autocorr(m1a.2$VCV) ## , , units ## ## units ## Lag 0 1.0000000000 ## Lag 1 0.2473917848 ## Lag 5 0.0082937460 ## Lag 10 -0.0132076992 ## Lag 50 -0.0007171886 The correlation between successive samples is low for the mean (0.030) but a bit high for the variance (0.247). When auto-correlation is high the chain needs to be run for longer, and this can lead to storage problems for high dimensional problems. The argument thin can be passed to \\(\\texttt{MCMCglmm}\\) specifying the intervals at which the Markov chain is stored. In model m1a.2 we specified thin=1 meaning we stored every iteration (the default is thin=10). I usually aim to store 1,000-2,000 iterations and have the autocorrelation between successive stored iterations less than 0.1. The approximation obtained from the Markov chain is conditional on the set of parameter values that were used to initialise the chain. In many cases the first iterations show a strong dependence on the starting parametrisation, but as the chain progresses this dependence may be lost. As the dependence on the starting parametrisation diminishes the chain is said to converge and the argument burnin can be passed to MCMCped specifying the number of iterations which must pass before samples are stored. The default burn-in period is 3,000 iterations. Assessing convergence of the chain is notoriously difficult, but visual inspection and diagnostic tools such as gelman.diag often suffice. For difficult models, running several chains from different starting values and ensuring they have all converged on the same distribution is a good idea. plot(m1a.2$Sol) Figure 2.11: Summary plot of the Markov Chain for the intercept. The left plot is a trace of the sampled posterior, and can be thought of as a time-series. The right plot is a density estimate, and can be thought of a smoothed histogram approximating the posterior. On the left of Figure 2.11 is a time-series of the parameter as the MCMC iterates, and on the right is a posterior density estimate of the parameter (a smoothed histogram of the output). If the model has converged there should be no trend in the time-series. The equivalent plot for the variance is a little hard to see on the original scale, but on the log scale the chain looks good (Figure 2.12: plot(log(m1a.2$VCV)) Figure 2.12: Summary plot of the Markov Chain for the logged variance. The logged variance was plotted rather than the variance because it was easier to visualise. The left plot is a trace of the sampled posterior, and can be thought of as a time-series. The right plot is a density estimate, and can be thought of a smoothed histogram approximating the posterior. 2.6 Prior for Residual Variances \\(\\texttt{MCMCglmm}\\) uses an inverse-Wishart prior for the residual variance and so here will cover the properties of the scalar inverse-Wishart distribution and prior. Section 5.6 can be consulted for inverse-Wishart covariance matrices. For random effect (co)variances it is possible to use scaled non-central F-distribution priors which is strongly recommended (see Chapter 9). For a single variance, the inverse-Wishart prior is parameterised through the parameters \\(\\texttt{V}\\) and \\(\\texttt{nu}\\) in MCMCglmm. The MCMCglmm parameterisation of the inverse-Wishart is not standard but I find it intuitive: the prior information is equivalent to observing \\(\\texttt{nu}\\) residuals with variance \\(\\texttt{V}\\). Consequently, the distribution concentrates on \\(\\texttt{V}\\) as the degree of belief parameter, \\(\\texttt{nu}\\) increases. The distribution tends to be right skewed when \\(\\texttt{nu}\\) is not very large, with a mode of \\(\\texttt{V}\\frac{\\texttt{nu}}{\\texttt{nu}+2}\\) but a mean of \\(\\texttt{V}\\frac{\\texttt{nu}}{\\texttt{nu}-2}\\) (which is not defined for \\(\\texttt{nu}&lt;2\\)). Figure 2.13 plots the probability density functions holding \\(\\texttt{V}\\) equal to one but with \\(\\texttt{nu}\\) varying. Figure 2.13: Probability density function for a univariate inverse-Wishart with the variance at the limit set to 1 (\\(\\texttt{V}=1\\)) and varying degree of belief parameter (\\(\\texttt{nu}\\)). With \\(\\texttt{V}=1\\) these distributions are equivalent to inverse gamma distributions with shape and scale parameters set to \\(\\texttt{nu}\\)/2. For single variances the inverse-gamma is a special case of the inverse-Wishart2, and with \\(\\texttt{V}=1\\), the shape and scale of the inverse-gamma are both equal to \\(\\texttt{nu}/2\\). The inverse-gamma with shape and scale equal to 0.001 used to be commonly used. The motivation behind the prior was that making \\(\\texttt{nu}\\) small would result in a less influential prior because is was equivalent to only observing 0.2% (\\(\\texttt{nu}=0.002\\)) of a residual a priori. Setting \\(\\texttt{nu}=0\\) was avoided because this does not define a valid distribution. A probability distribution must integrate to one because a variable must have some value, and this condition is not met when setting \\(\\texttt{nu}=0\\). The prior distribution is said to be improper. In the example here, where \\(\\texttt{V}\\) is a single variance, the prior is only proper when \\(\\texttt{V}&gt;0\\) and \\(\\texttt{nu}&gt;0\\). Although improper priors do not specify valid prior distributions and therefore the Bayesian credentials of any model may be questionable, \\(\\texttt{MCMCglmm}\\) does allow them as they have some useful properties. 2.6.1 Improper Priors When improper priors are used their are two potential problems that may be encountered. The first is that if the data do not contain enough information, the posterior distribution itself may be improper, and any results obtained from \\(\\texttt{MCMCglmm}\\) will be meaningless. In addition, with proper priors there is a zero probability of a variance component being exactly zero but this is not necessarily the case with improper priors. This can produce numerical problems (trying to divide through by zero) and can also result in a reducible chain. A reducible chain is one which gets ‘stuck’ at some parameter value(s) and cannot escape. This is usually obvious from the mcmc plots but \\(\\texttt{MCMCglmm}\\) will often terminate before the analysis has finished with an error message of the form: ill-conditioned G/R structure: use proper priors ... However, improper priors do have some useful properties and in fact the default prior for the variances in has \\(\\texttt{nu}=0\\) (the value of \\(\\texttt{V}\\) is irrelevant). It is tempting to think that this prior is flat for the variance3, but it is in fact flat for the log-variance and therefore puts more weight on small values than a prior that is flat on the variance (Figure 2.14) - see Section 2.7. prior.m1a.3 &lt;- list(R = list(V = 1, nu = 0)) m1a.3 &lt;- MCMCglmm(y ~ 1, data = Ndata, thin = 1, prior = prior.m1a.3) Figure 2.14: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\sigma^{2})\\) in black, and an MCMC approximation for the posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\) in red. The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. An almost flat prior was used for the mean \\(Pr(\\mu)\\sim N(0, 10^8)\\) and a flat prior was used for the log-variance \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=1, \\texttt{nu}=0)\\)). Consequently, the mode of the marginal posterior distribution lies even below the ML estimate 2.15. Figure 2.15: An MCMC approximation for the marginal posterior distribution of the variance \\(Pr(\\sigma^{2} | {\\bf y})\\). A non-informative prior specification was used (\\(Pr(\\mu)\\sim N(0, 10^8)\\) and \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=0, \\texttt{nu}=0)\\)). The ML and REML estimates are plotted in blue and red,m respectively. Although inverse-Wishart distributions with negative degree of belief parameters are not defined, the resulting posterior distribution can be defined and proper if there is sufficient replication. When \\(\\texttt{V}=0\\) and \\(\\texttt{nu}=-1\\) we have a flat prior on the standard deviation over the interval \\((0,\\infty]\\). When \\(\\texttt{V}=0\\) and \\(\\texttt{nu}=-2\\) we have a flat prior on the variance. Since the default prior for the mean is normal with a very large variance (\\(10^8\\)) the prior for the mean is also essentially flat, resulting in a prior probability that is proportional to some constant for all possible parameter values. The posterior density in such cases is equal to the likelihood: \\[Pr(\\mu, \\sigma^{2} | {\\bf y}) \\propto Pr({\\bf y} | \\mu, \\sigma^{2}) \\label{fprior-eq} \\tag{2.3}\\] We can overlay the joint posterior distribution on the likelihood surface (Figure 2.16) and see that the two things are in close agreement, up to Monte Carlo error. prior.m1a.4 &lt;- list(R = list(V = 1e-16, nu = -2)) m1a.4 &lt;- MCMCglmm(y ~ 1, data = Ndata, thin = 1, prior = prior.m1a.4) Figure 2.16: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\sigma^{2})\\) in black, and an MCMC approximation for the posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\) in red. The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. Almost flat priors were used for the mean (\\(Pr(\\mu)\\sim N(0, 10^8)\\) and the variance \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=10^{-16}, \\texttt{nu}=-2)\\)) and so the posterior distribution is equivalent to the likelihood. Here, the joint posterior mode coincides with the ML estimates, as expected (Figure 2.16). In addition, the mode of the marginal distribution for the variance is equivalent to the REML estimator (See Figure 2.17). Indeed, the REML estimator can be seen as marginalising the mean under a flat prior, and for this reason is sometimes referred to as the marginal likelihood rather than the restricted likelihood (REML). Figure 2.17: An MCMC approximation for the marginal posterior distribution of the variance \\(Pr(\\sigma^{2} | {\\bf y})\\). An almost flat prior specification was used (\\(Pr(\\mu)\\sim N(0, 10^8)\\) and \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=10^{-16}, \\texttt{nu}=-2)\\)) and the REML estimator of the variance (red line) coincides with the marginal posterior mode. 2.7 Transformations Sometimes we would like to know the posterior distribution for some transformation of the parameters. For example, we may wish to obtain the posterior distribution for \\(\\log(\\sigma^2)\\). With MCMC this is easy - we can just transform the posterior samples (i.e. log(m1a.2$VCV)) and treat these as we would any other set of posterior samples. However, it is useful to understand how probabilities are expected to behave under these transformations. The key idea is that the probability of some function of \\(x\\), \\(f(x)\\), is equal to the probability of \\(x\\) divided by the Jacobian. When the transform only involves single parameters the Jacobian is \\(|df(x)/dx|\\) which for the log-transform is \\(1/x\\). For model m1a.3 we used an inverse-Wishart prior with \\(\\texttt{V=1}\\) and \\(\\texttt{nu=0}\\). The posterior density is plotted in Figure 2.14 and does not appear to coincide with the likelihood - there is more posterior density at small values of \\(\\sigma^2\\) However, this is because the posterior density is expressed for \\(\\sigma^2\\) and the prior is only flat in \\(log(\\sigma^2)\\). By multiplying the posterior density for \\(\\sigma^2\\) by \\(\\sigma^2\\) (i.e. dividing by \\(1/x\\) where \\(x\\) is \\(\\sigma^2\\)) we would obtain the joint density of \\(\\mu\\) and \\(log(\\sigma^2)\\). This would reduce the density for small values of \\(\\sigma^2\\) and increase the density for large values (since the density is scaled by \\(\\sigma^2\\)). Indeed, plotting the posterior distribution of \\(log(\\sigma^2)\\) shows this effect and the posterior is in agreement with the likelihood, as expected (Figure 2.18). Figure 2.18: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\log(\\sigma^{2}))\\) in black, and an MCMC approximation for the posterior distribution \\(Pr(\\mu, \\log(\\sigma^{2}) | {\\bf y})\\) in red. An almost flat prior was used for the mean \\(Pr(\\mu)\\sim N(0, 10^8)\\) and a flat prior was used for the log-variance \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=1, \\texttt{nu}=0)\\). The double t is because I cannot spell.↩︎ The inverse gamma is a special case of the inverse-Wishart, although it is parametrised using \\(\\texttt{shape}\\) and \\(\\texttt{scale}\\), where \\(\\texttt{nu}=2\\ast\\texttt{shape}\\) and \\(\\texttt{V} = \\frac{\\texttt{scale}}{\\texttt{shape}}\\) (or \\(\\texttt{shape} = \\frac{\\texttt{nu}}{2}\\) and \\(\\texttt{scale} = \\texttt{V}\\frac{\\texttt{nu}}{2}\\)). There is no density function for the inverse-gamma in base R. However, it can be obtained using the density function for the gamma. Using \\(\\texttt{shape_g}\\) and \\(\\texttt{scale_g}\\) to designate the shape and scale parameters of a gamma distribution then the density of \\(\\texttt{x}\\) under the inverse-gamma is the density of \\(1/\\texttt{x}\\) under a gamma distribution with \\(\\texttt{shape_g}=\\texttt{shape}\\) and \\(\\texttt{scale_g}=1/\\texttt{scale}\\) multiplied by the Jacobian, \\(1/\\texttt{x}^{2}\\). Consequently, we can implement the function dinvgamma&lt;-function(x, shape, scale, ...){dgamma(1/x, shape=shape, rate=scale, ...)/(x^2)}. Note \\(\\texttt{rate}=1/\\texttt{scale}\\)↩︎ Embarrassingly, I claimed that \\(\\texttt{V=1}\\) and \\(\\texttt{nu=0}\\) was flat in the original CourseNotes and I had forgotten to rescale the posterior density by the Jacobian when generating the equivalent contour plot to Figure 2.14 (which is computed on values of \\(\\textrm{log}(\\sigma^2)\\) for better results, and then transformed).↩︎ "],["glm.html", "3 Linear and Generalised Linear Models 3.1 Linear Model (LM) 3.2 Generalised Linear Model (GLM) 3.3 Poisson GLM 3.4 Overdispersion 3.5 Prediction in GLM 3.6 Binomial and Bernoulli GLM 3.7 Ordinal Data 3.8 Complete Separation", " 3 Linear and Generalised Linear Models 3.1 Linear Model (LM) A linear model is one in which unknown parameters are multiplied by (functions of) observed variables and then added together to give a prediction for the response variable. As an example, lets take the results from a Swedish experiment from the sixties: The experiment involved enforcing speed limits on Swedish roads on some days, but on other days letting everyone drive as fast as they liked. The response variable (y) is the number of accidents recorded. The experiment was conducted in 1961 and 1962 for 92 days in each year. As a first attempt we could specify the linear model: y ~ limit + year + day but what does this mean? 3.1.1 Linear Predictors The model formula defines a set of simultaneous (linear) equations \\[\\begin{array}{cl} E[y\\texttt{[1]}] &amp;=\\beta_{1}+\\beta_{2}(\\texttt{limit[1]==&quot;yes&quot;})+\\beta_{3}(\\texttt{year[1]==&quot;1962&quot;})+\\beta_{4}\\texttt{day[1]}\\\\ E[y\\texttt{[2]}] &amp;= \\beta_{1}+\\beta_{2}(\\texttt{limit[2]==&quot;yes&quot;})+\\beta_{3}(\\texttt{year[2]==&quot;1962&quot;})+\\beta_{4}\\texttt{day[2]}\\\\ \\vdots&amp;=\\vdots\\\\ E[y\\texttt{[184]}] &amp;= \\beta_{1}+\\beta_{2}(\\texttt{limit[184]==&quot;yes&quot;})+\\beta_{3}(\\texttt{year[184]==&quot;1962&quot;})+\\beta_{4}\\texttt{day[184]}\\\\ \\end{array} \\label{SE-eq} \\tag{3.1}\\] where the \\(\\beta\\)’s are the unknown coefficients to be estimated, and the variables in \\(\\texttt{this font}\\) are observed predictors. Continuous predictors such as day remain unchanged, but categorical predictors are expanded into a series of binary variables of the form ‘do the data come from 1961, yes or no?’, ‘do the data come from 1962, yes or no?’, and so on for as many years for which there are data. It is cumbersome to write out the equation for each data point in this way, and a more compact way of representing the system of equations is \\[ E[{\\bf y}] = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}} \\tag{3.2} \\] where \\({\\bf X}\\) is called a design matrix and contains the predictor information for all observations, and \\({\\boldsymbol{\\mathbf{\\beta}}} = [\\beta_{1}\\ \\beta_{2}\\ \\beta_{3}\\ \\beta_{4}]^{&#39;}\\) is the vector of parameters. Here, \\(E[{\\bf y}]\\) is a vector of the 184 expected values. X &lt;- model.matrix(y ~ limit + year + day, data = Traffic) X[c(1, 2, 184), ] ## (Intercept) limityes year1962 day ## 1 1 0 0 1 ## 2 1 0 0 2 ## 184 1 1 1 92 The binary predictors do the data come from 1961, yes or no? and there was no speed limit, yes or no? do not appear. These are the first factor levels of year and limit respectively, and are absorbed into the global intercept (\\(\\beta_{1}\\)) which is fitted by default in R. Hence the expected number of accidents for the four combinations (on day zero) are \\(\\beta_{1}\\) for 1961 with no speed limit, \\(\\beta_{1}+\\beta_{2}\\) for 1961 with a speed limit, \\(\\beta_{1}+\\beta_{3}\\) for 1962 with no speed limit and \\(\\beta_{1}+\\beta_{2}+\\beta_{3}\\) for 1962 with a speed limit. The simultaneous equations defined by Equation (3.2) cannot be solved directly because we do not know the left-hand side - expected values of \\(y\\). We only know the observed value, which we assume is distributed around the expected value with some error. In a normal linear model we assume that these errors (residuals) are normally distributed: \\[{\\bf y}-{\\bf X}{\\boldsymbol{\\mathbf{\\beta}}} = {\\bf e} \\sim N(0, \\sigma^{2}_{e}{\\bf I})\\] \\({\\bf I}\\) is a \\(184\\times 184\\) identity matrix. It has ones along the diagonal, and zeros in the off-diagonals. The zero off-diagonals imply that the residuals are uncorrelated, and the ones along the diagonal imply that they have the same variance (\\(\\sigma^{2}_{e}\\)). Thinking about the distribution of residuals is less helpful when we move on to GLM’s and so I prefer to think about the model in the form: \\[{\\bf y}\\sim N({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}, \\sigma^{2}_{e}{\\bf I})\\] and say the response is conditionally normal, with the conditioning on the model (\\({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}\\)). It is important to note that this is different from saying the response is normal. If having a speed limit had a very strong effect the (marginal) distribution of the response may be bimodal and far from normal, and yet by including speed-limit as a predictor, conditional normality may be achieved. We could use MCMCglmm to fit this model, but to connect better with what comes next, let’s use glm to estimate \\({\\bf \\beta}\\) and \\(\\sigma^{2}_{e}\\) assuming that the number of accidents follow a conditional normal distribution (the MCMCglmm syntax is identical): m2a.1 &lt;- glm(y ~ limit + year + day, data = Traffic) summary(m2a.1) ## ## Call: ## glm(formula = y ~ limit + year + day, data = Traffic) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.13111 1.45169 14.556 &lt; 2e-16 *** ## limityes -3.66427 1.35559 -2.703 0.00753 ** ## year1962 -1.34853 1.31121 -1.028 0.30511 ## day 0.05304 0.02355 2.252 0.02552 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 71.80587) ## ## Null deviance: 14128 on 183 degrees of freedom ## Residual deviance: 12925 on 180 degrees of freedom ## AIC: 1314.5 ## ## Number of Fisher Scoring iterations: 2 On day zero in 1961 in the absence of a speed limit we expect 21.1 accidents (the intercept). With a speed limit we expect 3.7 fewer accidents and we can quite confidently reject the null-hypothesis of no effect - particularly if we were willing to use a one-tailed test, which seems reasonable. There are 1.3 fewer accidents in 1962, although this could just be due to chance, and for every unit increase in \\(\\texttt{day}\\) the number of accidents is predicted to go up by 0.05. The \\(\\texttt{day}\\) variable is encoded as integers from 1 to 92 with the same \\(\\texttt{day}\\) in different years being comparable (for example, the same day of the week and roughly the same date). If \\(\\texttt{day}\\)’s are evenly spaced throughout the year the \\(\\texttt{day}\\) effect is roughly the effect of increasing calender date by four (365/92) days. The estimate of the residual variance, \\(\\sigma^2_e\\), is the dispersion parameter (71.8). Because the number of accidents are count data we might worry about the assumption of conditional normality, and indeed the residuals show the typical right skew: hist(resid(m2a.1)) Figure 3.1: Histogram of residuals from model m2a.1 which assumed they followed a Normal distribution. It’s not extreme, and the conclusions probably won’t change, but we could assume that the data follow some other distribution. 3.2 Generalised Linear Model (GLM) Generalised linear models extend the linear model to non-normal data. They are essentially the same as the linear model described above, except they differ in two aspects. First, it is not necessarily the mean response that is predicted, but some function of the mean response. This function is called the link function. For example, with a log link we are trying to predict the logged expectation: \\[\\textrm{log}(E[{\\bf y}]) = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}\\] or alternatively \\[E[{\\bf y}] = \\textrm{exp}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}})\\] where \\(\\textrm{exp}\\) is the inverse of the log link function- exponentiating. The second difference is that many distributions are single parameter distributions for which a variance does not need to be estimated because it can be inferred from the mean. For example, we could assume that the number of accidents are Poisson distributed, in which case we also make the assumption that the variance is equal to the expected value. Technically, GLM’s only apply to a restricted set of distributions (those in the exponential family) but \\(\\texttt{MCMCglmm}\\) can accommodate a range of GLM-like models for other distributions (see Table 11.1). 3.3 Poisson GLM For now we will concentrate on a Poisson GLM with log link (the default link function for the Poisson distribution): m2a.2 &lt;- glm(y ~ limit + year + day, family = poisson, data = Traffic) summary(m2a.2) ## ## Call: ## glm(formula = y ~ limit + year + day, family = poisson, data = Traffic) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.0467406 0.0372985 81.685 &lt; 2e-16 *** ## limityes -0.1749337 0.0355784 -4.917 8.79e-07 *** ## year1962 -0.0605503 0.0334364 -1.811 0.0702 . ## day 0.0024164 0.0005964 4.052 5.09e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 625.25 on 183 degrees of freedom ## Residual deviance: 569.25 on 180 degrees of freedom ## AIC: 1467.2 ## ## Number of Fisher Scoring iterations: 4 While the sign of the effects are comparable to that seen in the linear model, their numerical values are completely different and the significance of all effects has increased dramatically. Should we worry? The model is defined on the log scale and so to get back to the data scale we need to exponentiate. Exponentiating the intercept gives us the predicted number of accidents on day zero in 1961 without a speed limit: exp(m2a.2$coef[&quot;(Intercept)&quot;]) ## (Intercept) ## 21.04663 which is very close to the intercept in the linear model (21.131), which is reassuring. To get the prediction for the same day with a speed limit we need to add the \\(\\texttt{limityes}\\) coefficient exp(m2a.2$coef[&quot;(Intercept)&quot;] + m2a.2$coef[&quot;limityes&quot;]) ## (Intercept) ## 17.66892 With a speed limit there are expected to be 0.840 as many accidents than if there was no speed limit. This value can be more directly obtained: exp(m2a.2$coef[&quot;limityes&quot;]) ## limityes ## 0.8395127 and holds true for any given day in either year. The proportional change is identical because the model is linear on the log scale and \\(exp(\\beta+\\dots)=exp(\\beta)exp(\\dots)\\). There is not always a direct relationship with the corresponding coefficients from the linear model but we can reassure ourselves that the parameters have the same qualitative meaning. For example, for \\(\\texttt{day}\\) 0 in 1961 the linear model predicts a drop from 21.1 to 17.5 accidents when a speed limit is in place - around 0.83 as many accidents, comparable to that predicted in the log-linear model. So in terms of the reported coefficients, the linear model and the Poisson log-linear model are roughly consistent with each other. However, in terms of accurately quantifying the uncertainty in those coefficients the Poisson model has a serious problem - it is very over confident. 3.4 Overdispersion Most count data do not conform to a Poisson distribution because the variance in the response exceeds the expectation. In the summary to m2a.2 the ratio of the residual deviance to the residual degrees of freedom is 3.162 which means, roughly speaking, there is 3.2 times more variation in our response (after conditioning on the model) than what we expect. This is known as overdispersion and it is easy to see how it arises, and why it is so common. If the predictor data had not been available to us then the only model we could have fitted was one with just an intercept: m2a.3 &lt;- glm(y ~ 1, data = Traffic, family = &quot;poisson&quot;) summary(m2a.3) ## ## Call: ## glm(formula = y ~ 1, family = &quot;poisson&quot;, data = Traffic) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.07033 0.01588 193.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 625.25 on 183 degrees of freedom ## Residual deviance: 625.25 on 183 degrees of freedom ## AIC: 1517.2 ## ## Number of Fisher Scoring iterations: 4 for which the residual variance exceeds that expected by a factor of 3.5. Of course, the variability in the residuals must go up if there are factors that influence the number of accidents, but which we hadn’t measured. It’s likely that in most studies there are things that influence the response that haven’t been measured, and even if each thing has a small effect individually, in aggregate they can cause substantial overdispersion. 3.4.1 Multiplicative Overdispersion There are two ways of dealing with overdispersion. With glm the distribution name can be prefixed with quasi and a dispersion parameter estimated: m2a.4 &lt;- glm(y ~ limit + year + day, family = quasipoisson, data = Traffic) summary(m2a.4) ## ## Call: ## glm(formula = y ~ limit + year + day, family = quasipoisson, ## data = Traffic) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.046741 0.067843 44.909 &lt; 2e-16 *** ## limityes -0.174934 0.064714 -2.703 0.00753 ** ## year1962 -0.060550 0.060818 -0.996 0.32078 ## day 0.002416 0.001085 2.227 0.02716 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 3.308492) ## ## Null deviance: 625.25 on 183 degrees of freedom ## Residual deviance: 569.25 on 180 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 glm uses a multiplicative model of overdispersion and so the estimate of the dispersion parameter is roughly equivalent to how many times greater the variance is than expected, after taking into account the predictor variables. You will notice that although the parameter estimates have changed very little, the standard errors have gone up and the significance gone down. Overdispersion, if not dealt with, can result in extreme anti-conservatism. For example, the second lowest number of accidents (8) occurred on \\(\\texttt{day}\\) 91 of 1961 without a speed limit. Our model predicts this should have been the second worst day for accidents over the whole two years, and the probability of observing 8 or less accidents on this day is predicted to be approximately 3 in a 100,000: ppois(8, exp(m2a.2$coef[&quot;(Intercept)&quot;] + 91 * m2a.2$coef[&quot;day&quot;])) ## [1] 3.195037e-05 If we did not accommodate the overdispersion, anything additional we put in in the model that could potentially explain such an improbable occurrence would come out as significant even if in reality it wasn’t important. This is because there simply isn’t any flexibility in the null model to accommodate such occurrences. For example, if the extreme value happened to be associated with a particular level of a categorical predictor or happened to be associated with an extreme value of some continuous predictor, then the coefficients associated with these predictors may well come out as significant. However, under a more plausible null model the extreme observations may not be too surprising and there may be little support for the predictors having an effect on the response. A more plausible model, and one that we’ve alluded to, would be to allow the number of accidents to vary across sampling points due to unmeasured variables. This would allow the variation in the number of accidents to exceed the predicted mean based on the measured variables (the assumption of the standard Poisson). 3.4.2 Additive Overdispersion I believe that a model assuming all relevant variables have been measured or controlled for, should not be the default model, and so when you specify family=poisson in \\(\\texttt{MCMCglmm}\\), overdispersion is always dealt with4. However, \\(\\texttt{MCMCglmm}\\) does not use a multiplicative model, but an additive model. prior &lt;- list(R = list(V = 1, nu = 0.002)) m2a.5 &lt;- MCMCglmm(y ~ limit + year + day, family = &quot;poisson&quot;, data = Traffic, prior = prior, pl = TRUE) The element Sol contains the posterior distribution of the coefficients of the linear model, and we can plot their marginal distributions: Figure 3.2: MCMC summary plot for the coefficients from a Poisson glm (model m2a.5). Note that the posterior distribution for the year1962 spans zero, in agreement with the quasipoisson glm model, and that in general the estimates for the two models (and their uncertainty - see Section 2.4.2) are broadly similar: summary(m2a.4) ## ## Call: ## glm(formula = y ~ limit + year + day, family = quasipoisson, ## data = Traffic) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.046741 0.067843 44.909 &lt; 2e-16 *** ## limityes -0.174934 0.064714 -2.703 0.00753 ** ## year1962 -0.060550 0.060818 -0.996 0.32078 ## day 0.002416 0.001085 2.227 0.02716 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 3.308492) ## ## Null deviance: 625.25 on 183 degrees of freedom ## Residual deviance: 569.25 on 180 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 With additive overdispersion the linear predictor includes a ‘residual’, for which a residual variance is estimated (hence our prior specification). \\[E[{\\bf y}] = \\textrm{exp}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf e})\\] At this point it will be handy to represent the linear model in a new way: \\[{\\bf l} = {\\boldsymbol{\\mathbf{\\eta}}}+{\\bf e}\\] where \\({\\bf l}\\) is a vector of latent variables (\\(\\textrm{log}(E[{\\bf y}])\\) in this case) and \\({\\boldsymbol{\\mathbf{\\eta}}}\\) is the usual symbol for the linear predictor (\\({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}\\)). The data we observe are assumed to be Poisson variables with expectation equal to the exponentiated latent variables: \\[{\\bf y} \\sim Pois(\\textrm{exp}({\\bf l}))\\] Note that the latent variable does not exactly predict \\(y\\), as it would if the data were Normal, because there is additional variability in the Poisson process5. In the call to \\(\\texttt{MCMCglmm}\\) I specified pl=TRUE to indicate that I wanted to store the posterior distributions of the latent variables (also known as the liabilities). This is not usually necessary and can require a lot of memory (we have 1000 posterior samples for each of the 182 data points). However, as an example we can obtain the posterior mean residual for data point 92 which is the data from \\(\\texttt{day}\\) 92 in 1961 when there was no speed limit: lat92 &lt;- m2a.5$Liab[, 92] # posterior distribution of the 92nd latent variable (liability) eta92 &lt;- m2a.5$Sol[, &quot;(Intercept)&quot;] + m2a.5$Sol[, &quot;day&quot;] * Traffic$day[92] # posterior distribution of X\\beta for the 92nd observation resid92 &lt;- lat92 - eta92 # posterior distribution of e for the 92nd observation mean(resid92) ## [1] -0.1341791 # posterior mean of e for the 92nd observation This particular observation has a negative expected residual indicating that the probability of getting injured was less than expected for this particular realisation of that \\(\\texttt{day}\\) in that year without a speed limit. If that combination of predictors (\\(\\texttt{day}\\)=92, \\(\\texttt{year}\\)=1961 and \\(\\texttt{limit}\\)=\\(\\texttt{no}\\)) could be repeated it does not necessarily mean that the actual number of accidents would always be less than expected, because it would follow a Poisson distribution with a mean equal to exp(lat92) (21.974). Like residuals in a standard linear model, the residuals are assumed to be independently and normally distributed with an expectation of zero and an estimated variance. If the residual variance was zero then \\({\\bf e}\\) would be a vector of zeros and the model would conform to the standard Poisson GLM. However, the posterior distribution of the residual variance is located well away form zero: plot(m2a.5$VCV) Figure 3.3: MCMC summary plot for the residual (units) variance from a Poisson glm (model m2a.5). The residual variance models any overdispersion, and a residual variance of zero would imply that the response conforms to a standard Poisson. 3.5 Prediction in GLM To get the expected number of accidents for the 92nd observation we simply exponentiated the latent variable: exp(lat92). However, it is important to realise that this is the expected number had the residual been exactly equal to the observed residual for that observation (resid92): we are calculating the expected number conditional on the set of unmeasured variables that affected that particular realisation of \\(\\texttt{day}\\) 92 in 1961 without a speed limit. When calculating a prediction we usually aim to average over these residuals (or random effects - see ??) since we would like to know what the average response would be for observations made on a \\(\\texttt{day}\\) of type 92 in 1961 without a speed limit. On the log-scale the expectation is simply the linear predictor (\\(\\eta\\)): \\[ log(E_e[y]) = E_e[l] = E_e[\\eta+e] = \\eta+E_e[e]=\\eta \\] since the residuals have zero expectation (here I have subscripted the expectation with the variable we are averaging over). The predict function can be applied to MCMCglmm objects and if we specify type=\"terms\" we get the prediction of the link scale - the log scale in this case: predict(m2a.5, type = &quot;terms&quot;)[92] ## [1] 3.224037 which is equal to the posterior mean of eta92 obtained earlier. We can see this visually in Figure 3.4 where I have plotted the distribution of the latent variable on a \\(\\texttt{day}\\) of type 92 in 1961 without a speed limit. Figure 3.4: The predicted distribution for the average number of accidents on the log scale for a \\(\\texttt{day}\\) of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor (\\(\\eta=\\)) with a variance of \\(\\sigma^{2}_e\\). As a consequence the mean, median and mode of the distribution are equal to the linear predictor on the log scale. To get the prediction on the data scale (i.e. in terms of the actual expected number of accidents) it is tempting to think we could just calculate exp(eta92). However, this is the expected number of accidents had the residual been exactly zero. If we wish to average over the residuals we require: \\[ E_e[y] = E_e[\\textrm{exp}(l)] = E_e[\\textrm{exp}(\\eta+e)] \\] and because exponentiation is a non-linear function this average will deviate from \\(\\textrm{exp}(\\eta)\\) (Figure 3.5. Figure 3.5: The predicted distribution for the average number of accidents on the data scale for a \\(\\texttt{day}\\) of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor (\\(\\eta\\)) with a variance of \\(\\sigma^{2}_e\\) (see 3.4). However when transforming to the data scale (by exponentiating) the symmetry is lost and the different measures of central tendency do not coincide. Since the residuals are normal on the log scale, the distribution on the data scale is log-normal and so analytical solutions exist for the mean, mode and median. To obtain predictions on the data scale we can specify type=\"response\" (the default) when using predict: predict(m2a.5)[92] ## [1] 26.48956 which is slightly greater than exp(eta92) (25.183). For all link-functions, the median value on the data scale can be easily calculated by taking the inverse-link transform of the linear predictor. However, obtaining the mean and mode is often more challenging than it is for log-link, and numerical integration or approximations are required. The predict function is returning a single number for observation 92 yet the model object contains 1,000 samples from the posterior distribution of all model parameters. This is because the predict function returns the posterior mean of the predicted value. Since we have the complete posterior distribution we can also place a 95% credible interval on the prediction (see Section 2.4.2): predict(m2a.5, interval = &quot;confidence&quot;)[92, ] ## fit lwr upr ## 26.48956 23.27675 30.10716 3.5.1 Posterior Predictive Distribution In some cases we would like to visualise or summarise aspects of the predictive distribution other than the mean. The complete predictive distribution is hard to work with, but the simulate function allows you to draw samples from the predictive distribution. The default is to generate a sample using a random draw from the posterior distribution, resulting in a draw from what is known as the posterior predictive distribution: ypred &lt;- simulate(m2a.5) We can use these simulated values to characterise any aspect of the predictive distribution we want. For example, we can obtain quantiles and compare them to the quantiles of the actual data to see how well the model captures aspects of the observed marginal distribution: qqplot(ypred, Traffic$y) abline(0, 1) Figure 3.6: qq-plot of the posterior predictive distribution and the data distribution Not too bad, although the predictive distribution perhaps has greater support for extreme values than are observed. If we merely wish to know the interval in which some specified percentage of the data are predicted to lie, we can also use the predict function but with interval=\"prediction\". By default the 95% (highest posterior density) interval is calculated using as many simulated samples as there are saved posterior samples. For the 92nd observation the prediction interval is predict(m2a.5, interval = &quot;prediction&quot;)[92, ] ## fit lwr upr ## 26.696 10.000 47.000 Note that the reported mean (fit) differs from that returned by interval=\"confidence\" due to Monte Carol error only. 3.6 Binomial and Bernoulli GLM The general concepts introduced for the Poisson GLM extend naturally to Binomial data, albeit with a different link function. However, it is worth spending a little time exploring a Binomial GLM as I think overdispersion, and how we deal with it, is easier to understand with binomial data. However, we’ll also see how the ‘residuals’ defined earlier for capturing overdispersion complicate the analysis of Bernoulli data and result in MCMcglmm using a non-standard parameterisation. The Binomial distribution has two parameters - the number of trials \\(n\\) and the probability of success, \\(p\\). In a Binomial GLM the number of trials is assumed known leaving only \\(p\\) to be estimated from the number of trials that are ‘successes’ or ‘failures’. When family=\"binomial\" is specified (or equivalently family=\"multinomial2\") MCMCglmm uses the standard link function for the Binomial - the logit link - and the logit probability of success is modelled as \\[log\\left(\\frac{p}{1-p}\\right) = l = \\eta+e\\] The logit transform takes a probability and turns it into a log odds ratio. If we want to get back to the probability we use the inverse of the logit transform: \\[p = \\frac{exp(l)}{1+exp(l)}\\] The logit link is actually the quantile function for the logistic distribution and so is available as the function qlogis. The inverse of a quantile function is a cumulative distribution function and so the inverse-logit transform is plogis. To introduce the Binomial GLM we will analyse some data I collected on how grumpy my colleagues look. I took two photos (\\(\\texttt{photo}\\)) of 22 people (\\(\\texttt{person}\\)) working in the Institute of Evolution and Ecology, Edinburgh. In one photo the person was happy and in the other they were grumpy (\\(\\texttt{type}\\)). 122 respondents gave a score between 1 and 10 indicating how grumpy they thought each person looked in each photo (with 10 being the most grumpy). data(Grumpy) Grumpy[c(1:3, 44), ] ## y l5 g5 type photo person age ypub ## 1 4.032787 101 21 grumpy 4511 ally_p 38 13 ## 2 3.081967 113 9 happy 4512 ally_p 38 13 ## 3 7.885246 15 107 grumpy 4521 darren_o 38 16 ## 44 3.798319 105 14 happy 4516 laura_r 34 10 \\(\\texttt{y}\\) gives the average score given by the 122 respondents. The number of respondents giving a photo a score of five or less (\\(\\texttt{l5}\\)) or more than five (\\(\\texttt{g5}\\)) is also recorded in addition to the person’s age (\\(\\texttt{age}\\)) and a proxy for how long they had been academia - the number of years since they published their first academic paper (\\(\\texttt{ypub}\\)). Here, we will model the probability of getting a grumpy score greater than five as a function of whether the person was happy or grumpy and how long they had been in academia. As with glm, successes should be in the first column of the response, and failures in the second: mbinom.1 &lt;- MCMCglmm(cbind(g5, l5) ~ type + ypub, data = Grumpy, family = &quot;multinomial2&quot;, pl = TRUE) summary(mbinom.1) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 5377.999 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 1.348 0.7597 1.947 1000 ## ## Location effects: cbind(g5, l5) ~ type + ypub ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.72025 -1.67538 0.05147 1000 0.094 . ## typehappy -1.28167 -1.97027 -0.59820 1000 &lt;0.001 *** ## ypub 0.02064 -0.00613 0.05175 1105 0.156 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model coefficients are most easily interpreted after exponentiating as they then give the proportional change in the odds ratio. The odds of having a score greater than five is exp(-1.282)=0.278 times lower when happy, as expected. The odds increases by a factor exp(0.021)=1.021 for each year in academia. When the coefficient is small in magnitude like this, you can get a rough estimate by looking directly at the coefficient: 0.021 roughly translates into a 2.1% increase and -0.021 would translate into a 2.1% decrease6. The \\(\\texttt{units}\\) (residual) variance is also large with credible intervals that are far from zero. 3.6.1 Overdispersion As with overdispersion in the Poisson model, this excess variation can be attributed to predictors that are not included in the model but cause the probability of success to vary over observations (photos in this case). For example, the 3rd and 25th observation have the same values for every predictor Grumpy[c(3, 25), ] ## y l5 g5 type photo person age ypub ## 3 7.885246 15 107 grumpy 4521 darren_o 38 16 ## 25 5.319672 73 49 grumpy 4527 craig_w 38 16 and so in the absence of overdispersion these parameters would result in a predicted probability of success of \\[p = \\textrm{plogis}(\\beta_{\\texttt{(Intercept)}}+\\beta_{\\texttt{typehappy}}\\times0+\\beta_{\\texttt{ypub}}\\times16)=0.404 \\label{plogis-eq} \\tag{3.3}\\] Given there were 122 respondents (trials) we can calculate the two values between which the number of successes is expected to fall 95 % of the time. qbinom(c(0.025, 0.975), size = 122, prob = p) ## [1] 39 60 While photo \\(\\texttt{4527}\\) is well within the range with 49 successes, the number of respondents giving photo \\(\\texttt{4521}\\) is substantially higher (107) - it’s an outlier. In Figure 3.7 the two photos are shown and it is clear why their underlying probabilities may deviate from that predicted. Most obviously the two photos are of different people and people vary in how grumpy they look. Since we have two photos per person we could (and should) estimate \\(\\texttt{person}\\) effects, however this is best done by treating these effects as random, which we will cover later, in Chapter 4. Even if \\(\\texttt{person}\\) effects were fitted there’s also likely to be a whole host of observation-level (photo-specific) effects that are not captured in the model such as whether the person had their eyes closed or was wearing a dreary grey fleece. Figure 3.7: Photo 4521 (left) and photo 4527 (right). For the predictors fitted in model mbinom.1, these photos have the same values (\\(\\texttt{type}=\\texttt{grumpy}\\) and \\(\\texttt{ypub}=16\\) years) If we include the ‘residuals’ when calculating the predicted probability for these two photos we can see that indeed their probabilities are quite different: mean(plogis(mbinom.1$Liab[, 3])) ## [1] 0.8621967 # predicted probability for photo 4521 mean(plogis(mbinom.1$Liab[, 25])) ## [1] 0.4001446 # predicted probability for photo 4527 3.6.2 Prediction When calculating the predicted probability in the absence of the residuals in Equation (3.3) I was careful to say that the prediction assumed an absence of overdispersion. However, when overdispersion is present we need to average over the distribution of the residuals in order to get an average. As we saw in the log-linear Poisson model, because the inverse-link function (plogis) is non-linear the average of \\(E_e[\\texttt{plogis}(\\eta+e)]\\) is different from \\(\\texttt{plogis}(E_e[\\eta+e])=\\texttt{plogis}(\\eta)\\). Unlike the Poisson log-linear model this expectation cannot be calculated analytically and the predict function by default numerically evaluates the integral: \\[\\int_l \\texttt{plogis}(l)f_N(l | \\eta, \\sigma^2_e)dl\\] where \\(f_N\\) is the probability density function of the normal. For each of the 44 observations this is done 1,000 times (the number of saved posterior samples) to get the posterior mean prediction and can be very slow (it is actually faster to fit the model). The number reported by predict is \\(np\\) rather than \\(p\\) and so to get the predicted probability for a photo of someone who has grumpy and had been publishing for 16 years we can get the prediction for the 3rd observation and divide by the number of trials (122): predict(mbinom.1)[3]/122 ## [1] 0.4243498 A little different from \\(\\texttt{plogis}(\\eta)=\\) 0.404. For the binomial with logit link, analytical approximations have been developed in Diggle et al. (2004) and McCulloch and Searle (2001) which are considerably faster and reasonably accurate: predict(mbinom.1, approx = &quot;diggle&quot;)[3]/122 ## [1] 0.4210936 predict(mbinom.1, approx = &quot;mcculloch&quot;)[3]/122 ## [1] 0.4255396 3.6.3 Bernoulli GLM Bernoulli data are a special case of the Binomial in which the number of trials is equal to one and so either a success or a failure is observed. To explore a Brenoulli model we can take our average Grumpy scores and simply dichotomise them into whether the average score was five or less or more than five: Grumpy$majority &lt;- Grumpy$y &gt; 5 If we fit a binomial model to these data in MCMCglmm it has exactly the same form as before (although a single column of outcomes can be passed). But importantly, for Bernoulli data there is no information to estimate the residual variance. This does not necessarily mean that variation in the probability of success across observations is absent, only that we can’t estimate it. For example, imagine we took 100 people who had been publishing for 16 years and took a photograph of them when they were grumpy. Let’s say the probability that the mean score for such photos exceeded 5 was 0.5. If the probability for all photos was exactly 0.5 (i.e. the probability of success did not vary over observations) then we expect 50 success and 50 failures across our observations. However, imagine the case where the probability of success was 100% for 50 photos and 0% for 50 photos (i.e. the probability of success varies considerably over observations). We would also expect 50 success and 50 failures, and so the distribution of successes with and without variation in the underlying probability would be identical. In the absence of information most software sets the ‘residual’ variance to zero (i.e. the probability of success dose not vary over observations), but it is important to understand that this is a convenient but arbitrary choice. Given this, it is desirable that any conclusions drawn from the model do not depend on this arbitrary choice. Worryingly, both the location effects (fixed and random) and variance components are completely dependent on the magnitude of the residual variance. MCMCglmm allows the user to fix the residual variance at a value of their choice, but unfortunately a value of zero results in a chain that will not mix and so I usually fix the residual variance to one7: prior.mbinom.2 = list(R = list(V = 1, fix = 1)) mbinom.2 &lt;- MCMCglmm(majority ~ type + ypub, family = &quot;categorical&quot;, data = Grumpy, prior = prior.mbinom.2) However, it would have been equally valid to fixed the residual variance at three: prior.mbinom.3 = list(R = list(V = 3, fix = 1)) mbinom.3 &lt;- MCMCglmm(majority ~ type + ypub, family = &quot;categorical&quot;, data = Grumpy, prior = prior.mbinom.3) and if we compare the MCMC traces for the coefficients we can see that we are sampling different posterior distributions (Figure 3.8). plot(mcmc.list(mbinom.2$Sol, mbinom.3$Sol), density = FALSE) Figure 3.8: MCMC trace for coefficients of a Bernoulli GLM from two models (mbinom.2 in black and mbinom.3 in red). The data and model structure are identical but in mbinom.2 the residual variance was set to one and in mbinom.3 the residual variance was set to three. The data provide no information about the residual variance. Should we worry? Not really. The two models give almost identical predictions (Figure 3.9). plot(predict(mbinom.2), predict(mbinom.3)) abline(0, 1) Figure 3.9: Predicted probabilities from a Bernoulli GLM from two models. The data and model structure are identical but in mbinom.2 the residual variance was set to one and in mbinom.3 the residual variance was set to three. The data provide no information about the residual variance. We just have to be careful about how we express the results. Stating that the typehappy coefficient is -2.852 (the posterior mean estimate from mbinom.2) is meaningless without putting it in the context of the assumed residual variance (one). Although the Diggle et al. (2004) approximation is less accurate than that in McCulloch and Searle (2001) we can use it rescale the estimates by the assumed residual variance (we’ll call it \\(\\sigma^{2}_{\\texttt{units}}\\)) in order to obtain the posterior distributions of the parameters under the assumption that the actual residual variance (we’ll call it \\(\\sigma^{2}_{e}\\)) is equal to some other value. For location effects the posterior distribution needs to be multiplied by \\(\\sqrt{\\frac{1+c^{2}\\sigma^{2}_{e}}{1+c^{2}\\sigma^{2}_{\\texttt{units}}}}\\) where \\(c=16\\sqrt{3}/15\\pi\\). If obtain estimates under the assumption that \\(\\sigma^{2}_{e}=0\\) and we see the posterior distributions of the coefficients are very similar from the two models (Figure 3.10). c2 &lt;- ((16 * sqrt(3))/(15 * pi))^2 rescale.2 &lt;- mbinom.2$Sol * sqrt(1/(1 + c2 * 1)) rescale.3 &lt;- mbinom.3$Sol * sqrt(1/(1 + c2 * 3)) plot(mcmc.list(as.mcmc(rescale.2), as.mcmc(rescale.3)), density = FALSE) Figure 3.10: MCMC trace for rescaled coefficients of a Bernoulli GLM from two models (mbinom.2 in black and mbinom.3 in red). The data and model structure are identical but in mbinom.2 the residual variance was set to one and in mbinom.3 the residual variance was set to three. However, the coefficients have been rescaled using the Diggle et al. (2004) approximation such that they represent what the coefficients would be if the residual variance was zero. The data provide no information about the residual variance. In addition, the posterior distributions are centred on the ML estimates obtained by glm which implicitly assumes \\(\\sigma^2_e=0\\) (Figure 3.11). Figure 3.11: Posterior distributions for rescaled coefficients of a Bernoulli GLM (mbinom.3). The rescaling gives approximate (but accurate) posterior distributions had the residual variance been set to zero, rather than three. The red lines indicate estimates from glm that implicitly assumes the residual variance is zero and the blue lines indicate the unscaled posterior means. 3.6.4 Probit link The inverse-logit function is the cumulative distribution function for the logistic distribution. It makes sense that a cumulative distribution function for a continuous distribution that can take any value would serve as a good inverse link function for a probability: it takes any value between plus and minus infinity and squeezes it to be between zero and one. While the logit is the most common link function for binomial data, the probit link and complementary log-log (cloglog) link are also widely used. They are the cumulative distribution functions for the unit normal and standard Gumbel distributions respectively. In Figure 3.12 we can see how the probability changes as a function of a covariate (\\(x\\)) for the different links (with intercepts and slopes chosen so that the functions are matched at the origin). Figure 3.12: Predicted probabilities as a function of covariate \\(x\\) using the inverse logit (black), inverse probit (red) and inverse complementary log-log link functions. For each link the intercept and slope were chosen such that when \\(x= 0\\) the function equals 0.5 and has a derivative of one. We can see that the link functions generate rather similar predicted probabilities, particularly the logit and probit links. For Bernoulli data, another way to conceptualise these link functions is in terms of threshold models. Imagine the case where we have managed to set the non-identifiable residual variance to zero and all residuals \\(e\\) are zero and can be omitted. The probability of success is then given by the inverse-link of \\(\\eta\\). For probit link this would be the probability of getting a value less than \\(\\eta\\) from the unit normal (the grey area in the left panel of Figure 3.13). Equivalently, it is the probability of getting a value greater than 0 from a normal with a mean equal to \\(\\eta\\) and a standard deviation of one (the grey area in the right panel of Figure 3.13). Figure 3.13: Probability density function for \\(\\epsilon\\) (left) or \\(\\eta+\\epsilon\\) (right) where \\(\\epsilon\\) is normal with mean zero and a standard deviation of one. Applying pnorm to \\(\\eta\\) gives the shaded area on the left and is the probability of success using probit link. The shaded area on the right gives the same probability and is the chance of getting a value greater than zero from a normal with mean \\(\\eta\\) and a standard deviation of one. If we think explicitly about the normal deviates that underpin these probability calculations (we’ll call them \\(\\epsilon\\)), then in the left panel \\(\\epsilon\\) falls below the ‘threshold’ \\(\\eta\\) with the required probability or in the right panel, \\(\\eta+\\epsilon\\) falls above the ‘threshold’ zero with the required probability. Since \\(\\epsilon\\) is unit-normal we can equate \\(\\eta\\) with \\(e\\) and \\(\\eta+\\epsilon\\) with \\(l\\) and apply the inverse link function \\(\\mathbf{1}_{\\{l&gt;0\\}}\\). This function outputs 1 (a success) if \\(l&gt;0\\) and a failure otherwise (as in the right panel of Figure 3.13). This is implemented as family=threshold in MCMCglmm, and if the residual variance (i.e. the variance of \\(\\epsilon\\) or \\(e\\)) is fixed at one corresponds exactly to standard probit regression8. prior.mbinom.4 = list(R = list(V = 1, fix = 1)) mbinom.4 &lt;- MCMCglmm(majority ~ type + ypub, family = &quot;threshold&quot;, data = Grumpy, prior = prior.mbinom.4) Unfortunately the same trick can’t be used for other link functions because the \\(\\epsilon\\)’s’ cannot be equated with \\(e\\)’s because they come from different distributions. In some ways the probit link is a natural link function for models that contain random effects, which are also usually assumed to be normal. However, the downside is that the coefficients in a probit model do not have a direct interpretation like they do in a logit model. Nevertheless, both models give very similar predictions and are unlikely to be statistically distinguishable in the vast majority of cases (Figure 3.14). Figure 3.14: Predicted probabilities from a Bernoulli GLM from two models with the same model structure. However, mbinom.2 uses a logit link with a residual variance set to one and in mbinom.4 uses a standard probit link. 3.7 Ordinal Data Thinking about Bernoulli GLM’s in terms of thresholds provides a natural way of thinking about how we could model categorical data that falls into a natural ordering. For example, rather than dichotomising our outcome into those that had a mean score greater than, or less than, five, lets place the observations into three categories: (1, 4], (4, 6], (6, 10]. Grumpy$categories &lt;- as.numeric(cut(Grumpy$y, c(1, 4, 6, 10))) Rather than just having a single threshold at zero, we can imagine adding another threshold that chops the distribution of \\(\\eta+e=l\\) into three regions, and hence probabilities. To make things simple we will just fit an intercept only model (so all observations have the same value of \\(\\eta\\)) as this will be easier to visualise: prior.mordinal = list(R = list(V = 1, fix = 1)) mordinal &lt;- MCMCglmm(categories ~ 1, data = Grumpy, family = &quot;threshold&quot;, prior = prior.mordinal) The output of mordinal gives the intercept (\\(\\eta\\) in this case) as before but also the additional threshold (cutpoint - stored as CP in the model object): summary(mordinal) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 92.2748 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 1 1 1 0 ## ## Location effects: categories ~ 1 ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 0.4892 0.1356 0.9025 607 0.012 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Cutpoints: ## ## post.mean l-95% CI u-95% CI eff.samp ## cutpoint.traitcategories.1 1.322 0.8801 1.775 435.8 As we did in the right panel of Figure 3.13) we can draw this model with the estimated threshold (\\(\\gamma\\)) included (Figure 3.15). Figure 3.15: Probability density function for \\(l=\\eta+e\\) where \\(e\\) is normal with mean zero and a standard deviation of one. \\(\\eta=\\) 0.489 and was obtained from the model mordinal. The distribution is ‘cut’ into three regions by the fixed threshold at zero and the estimated threshold (\\(\\gamma\\)) at 1.322. The shaded areas correspond to the probabilities of observing the three (ordered) outcomes. The shaded areas give the probability for each category and the posterior mean probabilities can be easily calculated: mean(pnorm(0, mordinal$Sol)) ## [1] 0.3158146 mean(pnorm(mordinal$CP, mordinal$Sol) - pnorm(0, mordinal$Sol)) ## [1] 0.4768311 mean(1 - pnorm(mordinal$CP, mordinal$Sol)) ## [1] 0.2073543 These correspond closely to the observed frequencies in the data: table(Grumpy$categories)/44 ## ## 1 2 3 ## 0.3181818 0.4772727 0.2045455 3.8 Complete Separation To demonstrate we will use some data from a pilot study on the Indian meal moth (Plodia interpunctella) and its granulosis virus (PiGV) collected by Hannah Tidbury &amp; Mike Boots at the University of Sheffield. data(PlodiaRB) The data are taken from 874 moth pupae for which the Pupated variable is zero if they failed to pupate (because they were infected with the virus) or one if they successfully pupated. The 874 individuals are spread across 49 full-sib families, with family sizes ranging from 6 to 38. To start we will fix the residual variance at 1: prior.m2b.1 = list(R = list(V = 1, fix = 1), G = list(G1 = list(V = 1, nu = 0.002))) m2b.1 &lt;- MCMCglmm(Pupated ~ 1, random = ~FSfamily, family = &quot;categorical&quot;, data = PlodiaRB, prior = prior.m2b.1) and then fit a second model where the residual variance is fixed at 2: prior.m2b.2 = list(R = list(V = 2, fix = 1), G = list(G1 = list(V = 1, nu = 0.002))) m2b.2 &lt;- MCMCglmm(Pupated ~ 1, random = ~FSfamily, family = &quot;categorical&quot;, data = PlodiaRB, prior = prior.m2b.2) The posterior distribution for the intercept differs between the two models (see Figure 3.16): plot(mcmc.list(m2b.1$Sol, m2b.2$Sol)) Figure 3.16: MCMC summary plots for the intercept of a binary GLMM where the residual variance was fixed at one (black) and two (red). as do the variance components (see Figure 3.17): plot(mcmc.list(m2b.1$VCV, m2b.2$VCV)) Figure 3.17: MCMC summary plots for the between family variance component of a binary GLMM where the residual variance was fixed at one (black) and two (red). Should we worry? Not really. We just have to be careful about how we express the results. Stating that the family variance is 0.809 is meaningless without putting it in the context of the assumed residual variance. It is therefore more appropriate to report the intraclass correlation which in this context is the expected correlation between the state Pupated/Not Pupated, for members of the same family. It can be calculated as: \\[\\texttt{IC} = \\frac{\\sigma^{2}_{\\texttt{FSfamily}}}{\\sigma^{2}_{\\texttt{FSfamily}}+\\sigma^{2}_{\\texttt{units}}+\\pi^{2}/3}\\] for the logit link, which is used when family=categorical, or \\[\\texttt{IC} = \\frac{\\sigma^{2}_{\\texttt{FSfamily}}}{\\sigma^{2}_{\\texttt{FSfamily}}+\\sigma^{2}_{\\texttt{units}}+1}\\] for the probit link, which is used if family=ordinal was specified. Obtaining the posterior distribution of the intra-class correlation for each model shows that they are sampling very similar posterior distributions (see Figure 3.18) IC.1 &lt;- m2b.1$VCV[, 1]/(rowSums(m2b.1$VCV) + pi^2/3) IC.2 &lt;- m2b.2$VCV[, 1]/(rowSums(m2b.2$VCV) + pi^2/3) plot(mcmc.list(IC.1, IC.2)) Figure 3.18: MCMC summary plots for the intra-family correlation from a binary GLMM where the residual variance was fixed at one (black) and two (red). Using the approximation due to Diggle et al. (2004) described earlier we can also rescale the estimates by the estimated residual variance (\\(\\sigma^{2}_{\\texttt{units}}\\)) in order to obtain the posterior distributions of the parameters under the assumption that the actual residual variance (\\(\\sigma^{2}_{e}\\)) is equal to some other value. For location effects the posterior distribution needs to be multiplied by \\(\\sqrt{\\frac{1+c^{2}\\sigma^{2}_{e}}{1+c^{2}\\sigma^{2}_{\\texttt{units}}}}\\) and for the variance components the posterior distribution needs to be multiplied by \\(\\frac{1+c^{2}\\sigma^{2}_{e}}{1+c^{2}\\sigma^{2}_{\\texttt{units}}}\\) where \\(c\\) is some constant that depends on the link function. For the probit \\(c=1\\) and for the logit \\(c=16\\sqrt{3}/15\\pi\\). We can obtain estimates under the assumption that \\(\\sigma^{2}_{e}=0\\): c2 &lt;- ((16 * sqrt(3))/(15 * pi))^2 Int.1 &lt;- m2b.1$Sol/sqrt(1 + c2 * m2b.1$VCV[, 2]) Int.2 &lt;- m2b.2$Sol/sqrt(1 + c2 * m2b.2$VCV[, 2]) plot(mcmc.list(as.mcmc(Int.1), as.mcmc(Int.2))) Figure 3.19: MCMC summary plots for the expected proportion of caterpillars pupating from a binary GLMM where the residual variance was fixed at one (black) and two (red). The posteriors should be virtually identical under a flat prior (See Figure 3.19) although with different priors this is not always the case. Remarkably, Dyk and Meng (2001) show that leaving a diffuse prior on \\(\\sigma^{2}_{\\texttt{units}}\\) and rescaling the estimates each iteration, a Markov chain with superior mixing and convergence properties can be obtained (See section 9). It should also be noted that a diffuse prior on the logit scale is not necessarily weakly informative on the probability scale. For example, the default setting for the prior on the intercept is \\(N(0, 10^{8})\\) on the logit scale, which although relatively flat across most of the probability scale, has a lot of density close to zero and one: hist(plogis(rnorm(1000, 0, sqrt(1e+08)))) Figure 3.20: Histogram of 1000 random deviates from a normal distribution with a mean of zero and a large variance (\\(10^8\\)) after undergoing an inverse logit transformation. This diffuse prior can cause problems if there is complete (or near complete) separation. Generally this happens when the binary data associated with some level of a categorical predictor are all success or all failures. For example, imagine we had 50 binary observations from an experiment with two treatments, for the first treatment the probability of success is 0.5 but in the second it is only one in a thousand: treatment &lt;- gl(2, 25) y &lt;- rbinom(50, 1, c(0.5, 0.001)[treatment]) data.bin &lt;- data.frame(treatment = treatment, y = y) table(data.bin) ## y ## treatment 0 1 ## 1 12 13 ## 2 25 0 if we analyse using glm we see some odd behaviour: m2c.1 &lt;- glm(y ~ treatment, data = data.bin, family = &quot;binomial&quot;) summary(m2c.1) ## ## Call: ## glm(formula = y ~ treatment, family = &quot;binomial&quot;, data = data.bin) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.08004 0.40032 0.200 0.842 ## treatment2 -19.64611 2150.80263 -0.009 0.993 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 57.306 on 49 degrees of freedom ## Residual deviance: 34.617 on 48 degrees of freedom ## AIC: 38.617 ## ## Number of Fisher Scoring iterations: 18 the effect of treatment does not appear significant despite the large effect size. This is in direct contrast to an exact binomial test: m2c.2 &lt;- binom.test(table(data.bin)[2, 2], 25) m2c.2 ## ## Exact binomial test ## ## data: table(data.bin)[2, 2] and 25 ## number of successes = 0, number of trials = 25, p-value = 5.96e-08 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.0000000 0.1371852 ## sample estimates: ## probability of success ## 0 where the 95% confidence interval for the probability of success is 0.000 to 0.137. The default \\(\\texttt{MCMCglmm}\\) model also behaves oddly (see Figure 3.21): prior.m2c.3 = list(R = list(V = 1, fix = 1)) m2c.3 &lt;- MCMCglmm(y ~ treatment, data = data.bin, family = &quot;categorical&quot;, prior = prior.m2c.3) plot(m2c.3$Sol) Figure 3.21: MCMC summary plots for the intercept and treatment effect in a binary GLM. In treatment 2 all 25 observations were failures and so the ML estimator on the probability scale is zero and \\(-\\infty\\) on the logit scale. With a flat prior on the treatment effect the posterior distribution is improper, and with a diffuse prior (as used here) the posterior is dominated by the high prior densities at extreme values. For these types of problems, I usually remove the global intercept (-1) and use the prior \\(N(0, \\sigma^{2}_{\\texttt{units}}+\\pi^2/3)\\) because this is reasonably flat on the probability scale when a logit link is used. For example, prior.m2c.4 = list(B = list(mu = c(0, 0), V = diag(2) * (1 + pi^2/3)), R = list(V = 1, fix = 1)) m2c.4 &lt;- MCMCglmm(y ~ treatment - 1, data = data.bin, family = &quot;categorical&quot;, prior = prior.m2c.4) plot(m2c.4$Sol) looks a little better (see Figure 3.21), and the posterior distribution for the probability of success in treatment 2 is consistent with the exact binomial test for which the 95% CI were (0.000 -0.137). With such a simple model, the prediction for observation 26 is equal to the treatment 2 effect and so we can get the the credible interval (on the data scale) for treatment 2 using the predict function: Figure 3.22: MCMC summary plots for the intercept and treatment effect in a binary GLM. In treatment 2 all 25 observations were failures and so the ML estimator on the probability scale is zero and \\(-\\infty\\) on the logit scale. A flat prior on the probability scale was used and the posterior distribution is better behaved than if a flat prior on the logit scale had been used (see Figure \\(\\ref{separation1-fig}\\)). predict(m2c.4, interval = &quot;confidence&quot;)[26, ] ## fit lwr upr ## 0.0303016922 0.0001343459 0.0898198607 References "],["ranef.html", "4 Random effects 4.1 Prediction with Random effects 4.2 A note on fixed effect priors and covariances", " 4 Random effects In some cases we may have measured variables whose effects we would like to treat as random. Often the distinction between fixed and random is given by example; things like population, species, individual and vial are random, but sex, treatment and age are not. Or the distinction is made using rules of thumb; if there are few factor levels and they are interesting to other people they are fixed. However, this doesn’t really confer any understanding about what it means to treat something as fixed or random, and doesn’t really allow judgements to be made regarding ambiguous variables (for example year) or give any insight into the fact that in a Bayesian analysis all effects are technically random. When we treat an effect as fixed we believe that the only information regarding its value comes from data associated with that particular level. If we treat an effect as random we also use this information, but we weight it by what other data tell us about the likely values that the effects could take. In a Bayesian analysis this additional information could come from data not formally included in the analysis, in which case it would be called a prior. In hierarchical models this additional information comes from data associated with other factor levels of the same type. The degree to which this additional information is important depends on the variability of the effects, as measured by the estimated variance component, and the degree of replication within a particular level. If variability is high then most of the information must come from data associated with an individual effect, particularly if replication within that effect is high. However, if variability and replication are low then extreme mean values of the response for a given level are more likely to be due to sampling error alone, and so the estimates are shrunk towards zero. It is common to hear things like ‘year is a random effect’ as if you just have to estimate a single effect for all years. It is also common to hear things like ‘years is random’ as if years were sampled at random. Better to say year effects are random and understand that it is the effects that are random not the years, and that we’re trying to estimate as many effects as there are years. In this sense they’re the same as fixed effects, and we can easily treat the year effects as random to see what difference it makes. Random effect models are often expressed as: \\[E[{\\bf y}] = \\textrm{exp}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}{\\bf u}+{\\bf e})\\] where \\({\\bf Z}\\) is a design matrix like \\({\\bf X}\\), and \\({\\bf u}\\) is a vector of parameters like \\({\\boldsymbol{\\mathbf{\\beta}}}\\). In Chapter {#glm} we got introduced to the Traffic data set consisting of the the number of injuries on Sweedish roads in 1961 and 1962 when speed-limits were in place or not. We can specify simple random effect models in the same way that we specified the fixed effects: random = ~ year although we don’t need anything to the left of the \\(\\sim\\) because the response is known from the fixed effect specification. In addition, the global intercept is suppressed by default, so in fact this specification produces the design matrix: Z &lt;- model.matrix(~year - 1, data = Traffic) Z[c(1, 2, 184), ] ## year1961 year1962 ## 1 1 0 ## 2 1 0 ## 184 0 1 Earlier I said that there was no distinction between fixed and random effects in a Bayesian analysis - all effects are random - so lets not make the distinction and combine the design matrices (\\({\\bf W} = [{\\bf X}, {\\bf Z}]\\)) and combine the vectors of parameters (\\(\\boldsymbol{\\theta} = [{\\boldsymbol{\\mathbf{\\beta}}}^{&#39;}, {\\bf u}^{&#39;}]^{&#39;}\\)): \\[E[{\\bf y}] = \\textrm{exp}({\\bf W}\\boldsymbol{\\theta}+{\\bf e}) \\label{MM-eq} \\tag{4.1}\\] If we drop year from the fixed terms, the new fixed effect design matrix looks like: X2 &lt;- model.matrix(y ~ limit + day, data = Traffic) X2[c(1, 2, 184), ] ## (Intercept) limityes day ## 1 1 0 1 ## 2 1 0 2 ## 184 1 1 92 and W &lt;- cbind(X2, Z) W[c(1, 2, 184), ] ## (Intercept) limityes day year1961 year1962 ## 1 1 0 1 1 0 ## 2 1 0 2 1 0 ## 184 1 1 92 0 1 You will notice that this new design matrix is exactly equivalent to the original design matrix X except we have one additional variable year1961. In our first model this variable was absorbed in to the global intercept because it could no be uniquely estimated from the data. What has changed that could make this additional parameter estimable? As is usual in a Bayesian analysis, if there is no information in the data it has to come from the prior. In model m2a.5 we used the default normal prior for the fixed effects with means of zero, large variances of \\(10^{8}\\), and no covariances. Lets treat the year effects as random, but rather than estimate a variance component for them we’ll fix the variance at \\(10^{8}\\) in the prior: prior &lt;- list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1e+08, fix = 1))) m2a.6 &lt;- MCMCglmm(y ~ limit + day, random = ~year, family = &quot;poisson&quot;, data = Traffic, prior = prior, pr = TRUE) plot(m2a.6$Sol) Figure 4.1: MCMC summary plots for the intercept, speed limit and day coefficients from model m2a.6 where year effects were treated as random. Note the high posterior variance for the intercept. The estimates for the intercept, day and the effect of a speed limit now appear completely different (Figure 4.1. However, in the original model (m2a.5) the prediction for each year is obtained by: y1961.m2a.5 &lt;- m2a.5$Sol[, &quot;(Intercept)&quot;] y1962.m2a.5 &lt;- m2a.5$Sol[, &quot;(Intercept)&quot;] + m2a.5$Sol[, &quot;year1962&quot;] However, for this model we have to add the intercept to both random effects to get the year predictions. \\(\\texttt{MCMCglmm}\\) does not store the posterior distribution of the random effects by default, but because we specified pr=TRUE, the whole of \\(\\boldsymbol{\\theta}\\) is stored rather than just \\({\\boldsymbol{\\mathbf{\\beta}}}\\): y1961.m2a.6 &lt;- m2a.6$Sol[, &quot;(Intercept)&quot;] + m2a.6$Sol[, &quot;year.1961&quot;] y1962.m2a.6 &lt;- m2a.6$Sol[, &quot;(Intercept)&quot;] + m2a.6$Sol[, &quot;year.1962&quot;] We can merge the two posterior distributions to see how they compare: y.m2a.5 &lt;- mcmc(cbind(y1961 = y1961.m2a.5, y1962 = y1962.m2a.5)) y.m2a.6 &lt;- mcmc(cbind(y1961 = y1961.m2a.6, y1962 = y1962.m2a.6)) plot(mcmc.list(y.m2a.5, y.m2a.6)) Figure 4.2: MCMC summary plots for the year effects from a model where year effects were treated as fixed (black) and where they were treated as random (red) but with the variance component set at a large value rather than being estimated. The posterior distributions are virtually identical. The posterior distributions are very similar (Figure 4.2 but see Section 7 why they are not identical), highlighting the fact that effects that are fixed are those associated with a variance component which has been set a priori to something large (\\(10^8\\) in this case), where effects that are random are associated with a variance component which is not set a priori but is estimated from the data. As the variance component tends to zero then no matter how many random effects there are, we are effectively only estimating a single parameter (the variance). This makes sense, if there were no differences between years we only need to estimate a global intercept and not separate effects for each year. Alternatively if the variance is infinite then we need to estimate separate effects for each year. In this case the intercept is confounded with the average value of the random effect, resulting in a wide marginal distribution for the intercept, and strong posterior correlations between the intercept and the mean of the random effects: plot(c(m2a.6$Sol[, &quot;year.1961&quot;] + m2a.6$Sol[, &quot;year.1962&quot;])/2, c(m2a.6$Sol[, &quot;(Intercept)&quot;])) Figure 4.3: Joint posterior distribution of the intercept and the mean of the two random year effects. The variance component associated with year was fixed at a large value (\\(10^8\\)) and so the effects are almost completely confounded. With only two levels, there is very little information to estimate the variance, and so we would often make the a priori decision to treat year effects as fixed, and fix the variance components to something large (or infinity in a frequentist analysis). At the moment we have day as a continuous covariate, but we could also have random day effects and ask whether the number of injuries on the same day but in different years are correlated. Rather than fixing the variance component at something large, we’ll use the same weaker prior that we used for the residual variance: Traffic$day &lt;- as.factor(Traffic$day) prior &lt;- list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1, nu = 0.002))) m2a.7 &lt;- MCMCglmm(y ~ year + limit + as.numeric(day), random = ~day, family = &quot;poisson&quot;, data = Traffic, prior = prior) day has also gone in the fixed formula, but as a numeric variable, in order to capture any time trends in the number of injuries. Most of the overdispersion seems to be captured by fitting day as a random term (Figure 4.4): plot(m2a.7$VCV) Figure 4.4: MCMC summary plot of the variance component associated with day (top) and the residual variance component (below). The trace for the residual variance shows strong autocorrelation and needs to be ran for longer. In fact it explains so much that the residual variance is close to zero and mixing seems to be a problem. The chain would have to be run for longer, and the perhaps an alternative prior specification used. 4.1 Prediction with Random effects In section 3.2 we showed that for non-Gaussian data the expectation of the response variable \\(y\\) is different from the linear predictor if we wish to average over the residuals. Often it is important to get the expectation after marginalising residuals, and indeed after marginalising other random effects. For example we may not be so interested in knowing the expected number of injuries on the average day, but knowing the expected number of injuries on any random day. For the Poisson mixed model: \\[E[y] = \\texttt{exp}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}{\\bf u}+{\\bf e})\\] we can marginalise with respect to the random effects, including the overdispersion residual: \\[E_{{u,e}}[y] = \\textrm{exp}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+0.5\\sigma^{2})\\] where \\(\\sigma^{2}\\) is the sum of the variance components. For the Binomial mixed model with logit link \\[E[y] = \\textrm{logit}^{-1}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}{\\bf u}+{\\bf e})\\] it is not possible to marginilse with respect to the random effects analytically, but two approximations exist. The first \\[E_{{u,e}}[y] \\approx \\textrm{logit}^{-1}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}-0.5\\sigma^{2}\\textrm{tanh}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}(1+2\\textrm{exp}(-0.5\\sigma^{2}))/6)))\\] can be found on p452 in McCulloch and Searle (2001) and the second (and possibly less accurate) approximation in Diggle et al. (2004): \\[E_{{u,e}}[y] \\approx \\textrm{logit}^{-1}\\left(\\frac{{\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}}{\\sqrt{1+(\\frac{16\\sqrt{3}}{15\\pi})^{2}\\sigma^{2}}}\\right)\\] The predict function for \\(\\texttt{MCMCglmm}\\) object allows us to predict the laibality on the latent scale after marginalising the random effects in model m2a.7: predict(m2a.7, marginal = ~day, type = &quot;terms&quot;)[1:5] ## [1] 3.017828 3.020323 3.022818 3.025313 3.027808 or we can predict on the data scale: predict(m2a.7, marginal = ~day, type = &quot;response&quot;)[1:5] ## [1] 21.53217 21.58403 21.63606 21.68825 21.74062 In addition, credible intervals can be obtained predict(m2a.7, marginal = ~day, type = &quot;response&quot;, interval = &quot;confidence&quot;)[1:5, ] ## fit lwr upr ## 1 21.53217 18.21407 24.61805 ## 2 21.58403 18.36276 24.68876 ## 3 21.63606 18.60831 24.86552 ## 4 21.68825 18.69513 24.85381 ## 5 21.74062 18.78710 24.84210 as can prediction intervals through posterior predictive simulation: predict(m2a.7, marginal = ~day, type = &quot;response&quot;, interval = &quot;prediction&quot;)[1:5, ] ## fit lwr upr ## 1 21.567 7 37 ## 2 21.464 6 38 ## 3 21.822 7 39 ## 4 21.606 7 38 ## 5 21.266 5 37 4.2 A note on fixed effect priors and covariances Fixed and random effects are essentially the same thing. The only difference is that the variance component for the fixed effects is usually fixed at some large value, whereas the variance component for the random effects is estimated. In Section ?? I demonstrated this by claiming that a model where year effects were fixed (m2a.5) was identical to one where they were treated as random, but with the variance component set to a large value (m2a.6). This was a white lie as I did not want to distract attention from the main point. The reason why they were not identical is as follows: In the fixed effect model (m2a.5) we had the prior: \\[\\begin{array}{rcl} \\left[ \\begin{array}{c} \\beta_{\\texttt{(Intercept)}}\\\\ \\beta_{\\texttt{year1962}}\\\\ \\end{array} \\right] \\sim &amp; \\left[ \\begin{array}{cc} 10^8&amp;0\\\\ 0&amp;10^8\\\\ \\end{array} \\right]\\\\ \\end{array}\\] Where \\(\\beta_{\\texttt{(Intercept)}}\\) and \\(\\beta_{\\texttt{year1962}}\\) are the fixed effects to be estimated. Remembering the identity \\(\\sigma^{2}_{(a+b)} = \\sigma^{2}_{a}+ \\sigma^{2}_{b}+2\\sigma_{a,b}\\), this implies: \\[\\begin{array}{rccl} \\left[ \\begin{array}{c} \\beta_{1961}\\\\ \\beta_{1962}\\\\ \\end{array} \\right] = &amp; \\left[ \\begin{array}{c} \\beta_{\\texttt{(Intercept)}}\\\\ \\beta_{\\texttt{(Intercept)}}+\\beta_{\\texttt{year1962}}\\\\ \\end{array} \\right] \\sim &amp; \\left[ \\begin{array}{cc} 10^8&amp;10^8\\\\ 10^8&amp;10^8+10^8\\\\ \\end{array} \\right] &amp;= \\left[ \\begin{array}{cc} 10^8&amp;10^8\\\\ 10^8&amp;20^8\\\\ \\end{array} \\right]\\\\ \\end{array}\\] where \\(\\beta_{1961}\\) and \\(\\beta_{1962}\\) are the actual year effects, rather than the global intercept and the contrast. In hindsight this is a bit odd, for one thing we expect the 1962 effect to be twice as variable as the 1961 effect. With such weak priors it makes little difference, but lets reparameterise the model anyway. Rather than having a global intercept and a year contrast, we will have separate intercepts for each year: X3 &lt;- model.matrix(y ~ year - 1, data = Traffic) X3[c(1, 2, 184), ] ## year1961 year1962 ## 1 1 0 ## 2 1 0 ## 184 0 1 and a prior that has a covariance between the two year effects: PBV.yfixed &lt;- diag(2) * 1e+08 PBV.yfixed[1, 2] &lt;- PBV.yfixed[2, 1] &lt;- 1e+08/2 PBV.yfixed ## [,1] [,2] ## [1,] 1e+08 5e+07 ## [2,] 5e+07 1e+08 prior.m2a.5.1 &lt;- list(B = list(mu = rep(0, 2), V = PBV.yfixed), R = list(V = 1, nu = 0.002)) This new model: m2a.5.1 &lt;- MCMCglmm(y ~ year - 1, family = &quot;poisson&quot;, data = Traffic, prior = prior.m2a.5.1) has the same form as a mixed effect model with a prior variance of \\(\\frac{10^{8}}{2}\\) for the intercept, and the variance component associated with the random year effects also fixed at \\(\\frac{10^{8}}{2}\\): prior.m2a.6.1 &lt;- list(B = list(mu = 0, V = 1e+08/2), R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1e+08/2, fix = 1))) This arises because the two random effects have the joint prior distribution: \\[\\begin{array}{rl} \\left[ \\begin{array}{c} \\beta_{\\texttt{year.1961}}\\\\ \\beta_{\\texttt{year.1962}}\\\\ \\end{array} \\right] \\sim &amp; \\left[ \\begin{array}{cc} \\frac{10^{8}}{2}&amp;0\\\\ 0&amp;\\frac{10^{8}}{2}\\\\ \\end{array} \\right]\\\\ \\end{array}\\] which when combined with the prior for the intercept, \\(N(0, \\frac{10^{8}}{2})\\), gives: \\[\\begin{array}{rccl} \\left[ \\begin{array}{c} \\beta_{1961}\\\\ \\beta_{1962}\\\\ \\end{array} \\right] = &amp; \\left[ \\begin{array}{c} \\beta_{\\texttt{(Intercept)}}+\\beta_{\\texttt{year.1961}}\\\\ \\beta_{\\texttt{(Intercept)}}+\\beta_{\\texttt{year.1962}}\\\\ \\end{array} \\right] \\sim &amp; \\left[ \\begin{array}{cc} \\frac{10^{8}}{2}+\\frac{10^{8}}{2}&amp;\\frac{10^{8}}{2}\\\\ \\frac{10^{8}}{2}&amp;\\frac{10^{8}}{2}+\\frac{10^{8}}{2}\\\\ \\end{array} \\right] &amp;= \\left[ \\begin{array}{cc} 10^8&amp;\\frac{10^{8}}{2}\\\\ \\frac{10^{8}}{2}&amp;10^8\\\\ \\end{array} \\right] \\\\ \\end{array}\\] which is equivalent to the PBV.yfixed parameteristaion of for the two years. The model: m2a.6.1 &lt;- MCMCglmm(y ~ 1, random = ~year, family = &quot;poisson&quot;, data = Traffic, prior = prior.m2a.6.1, pr = TRUE) is therefore sampling from the same posterior distribution as model m2a.5.1. References "],["cat-int.html", "5 Categorical Random Interactions 5.1 idh Variance Structure 5.2 us Variance Structure 5.3 Compound Variance Structures 5.4 Heterogenous Residual Variance 5.5 Contrasts and Covariances 5.6 Priors for Covariance Matrices", " 5 Categorical Random Interactions Random effect specification is a common cause of confusion, especially when we want to form interactions in the random terms. To illustrate the possibilities we’ll use data collected on Blue tits. data(BTdata) The data are morphological measurements (tarsus length and back colour) made on 828 blue tit chicks from 106 mothers (dam). Half the offspring from each mother were swapped with half the offspring from another mother soon after hatching. The nest they were reared in is recorded as fosternest. prior = list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1, nu = 0.002), G2 = list(V = 1, nu = 0.002))) m3a.1 &lt;- MCMCglmm(tarsus ~ sex, random = ~dam + fosternest, data = BTdata, prior = prior) fits sex as a fixed effect, and dam and fosternest as random effects. diag(autocorr(m3a.1$VCV)[2, , ]) ## dam fosternest units ## 0.02288959 0.42344056 0.01105716 plot(m3a.1$VCV) Figure 5.1: MCMC summary plot for the variance components from model . Perhaps the autocorrelation for the fosternest variance is a little higher than we would like, and so we may like to run it for longer. effectiveSize(m3a.1$VCV) ## dam fosternest units ## 1000.0000 360.4326 1000.0000 Indeed, we’ve only sampled the fosternest variance about half as well as the other two variance components. The posterior correlation between the parameters is low cor(m3a.1$VCV) ## dam fosternest units ## dam 1.00000000 -0.2178245 -0.01944377 ## fosternest -0.21782455 1.0000000 -0.19831020 ## units -0.01944377 -0.1983102 1.00000000 which is not that surprising give the data come from an experiment which was designed in order to estimate these variance components. In general, variance components will show negative posterior correlations because the the total variance is being divided up. Imagine cutting a piece of string; making one bit longer has to reduce the size of the other bits, by necessity. If we hadn’t experimentally manipulated the birds then all chicks with the same mother, would be raised in the same nest, and there would be no information in the data to separate these terms. In this case the posterior correlation between these parameters would approach -1 as the prior information goes to zero. The lower 95% credible interval for the fosternest variance is low HPDinterval(m3a.1$VCV) ## lower upper ## dam 0.136992555 0.3200559 ## fosternest 0.009637628 0.1200952 ## units 0.514395233 0.6394973 ## attr(,&quot;Probability&quot;) ## [1] 0.95 and perhaps a model without it would be better supported, although the DIC suggest not: priorb &lt;- prior priorb[[2]] &lt;- priorb[[2]][-2] m3a.2 &lt;- MCMCglmm(tarsus ~ sex, random = ~dam, data = BTdata, prior = priorb) m3a.2$DIC ## [1] 2014.655 m3a.1$DIC ## [1] 1991.869 The tarsus lengths were standardised prior to analysis - this is not recommended, but was done in the original analyses of these data (Hadfield et al. 2007) so that comparisons would be scale invariant. The original analyses were done in REML where it is hard to get accurate confidence intervals for functions of variance components. With MCMC procedures this is simple. For example if we want to know what proportion of the total variance is explained by dams HPDinterval(m3a.1$VCV[, 1]/rowSums(m3a.1$VCV)) ## lower upper ## var1 0.1809462 0.3490566 ## attr(,&quot;Probability&quot;) ## [1] 0.95 One nice thing though about standardised data is that effect sizes are immediately apparent. For example, fixed effects are in standard deviation units and the sex effects are non-trivial: summary(m3a.1) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 1991.869 ## ## G-structure: ~dam ## ## post.mean l-95% CI u-95% CI eff.samp ## dam 0.2269 0.137 0.3201 1000 ## ## ~fosternest ## ## post.mean l-95% CI u-95% CI eff.samp ## fosternest 0.0666 0.009638 0.1201 360.4 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 0.5737 0.5144 0.6395 1000 ## ## Location effects: tarsus ~ sex ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.40639 -0.53785 -0.28337 1047 &lt;0.001 *** ## sexMale 0.76979 0.67221 0.88899 1000 &lt;0.001 *** ## sexUNK 0.20825 -0.03721 0.46103 1000 0.108 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Given that the sexes differ in their mean phenotype it may be worth exploring whether they vary in other ways. For example, perhaps there are sex-limited genes that mean that related brothers resemble each other more than they do their sisters. Perhaps females are less sensitive to environmental variation? To fit these models it will be necessary to understand how the variance functions, such as us() and idh(), work. We could refit the model m3a.1 using the random effect specifications: random = ~us(1):dam + us(1):fosternest or random = ~idh(1):dam + idh(1):fosternest and these would give exactly the same answer as the model specified as ~dam+fosternest. The term inside the brackets is a model formula and is interpreted exactly how you would interpret any R formula expect the intercept is not fitted by default. These formula are therefore fitting an intercept which is interacted with the random effects. For the dam terms we can get a representation of the interaction for the first few levels of dam (Fem2, Fem20, Fem3, Fem5, K983388): \\[ \\begin{array}{c|rrrrrc} &amp;{\\color{red}{\\texttt{Fem2}}}&amp;{\\color{red}{\\texttt{Fem20}}}&amp;{\\color{red}{\\texttt{Fem3}}}&amp;{\\color{red}{\\texttt{Fem5}}}&amp;{\\color{red}{\\texttt{K983388}}}&amp;\\dots\\\\ \\hline {\\color{blue}{\\texttt{(1)}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{Fem2}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{Fem20}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{Fem3}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{Fem5}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{K983388}}}&amp;\\dots\\\\ \\end{array} \\] Across the top, we have the original dam effects in red, and along the side we have the term defined by the variance structure formula (just the intercept in this case). The interaction forms a new set of factors. Although they have different names from the original dam levels, it is clear that there is a one to one mapping between the original and the new factor levels and the models are therefore equivalent. For more complex interactions this is not the case. We could also fit sex in the variance structure model, (i.e. us(sex):dam or idh(sex):dam)9: \\[\\begin{array}{c|rrrrrc} &amp;{\\color{red}{\\texttt{Fem2}}}&amp;{\\color{red}{\\texttt{Fem20}}}&amp;{\\color{red}{\\texttt{Fem3}}}&amp;{\\color{red}{\\texttt{Fem5}}}&amp;{\\color{red}{\\texttt{K983388}}}&amp;\\dots\\\\ \\hline {\\color{blue}{\\texttt{Fem}}}&amp;{\\color{blue}{\\texttt{Fem}}}.{\\color{red}{\\texttt{Fem2}}}&amp;{\\color{blue}{\\texttt{Fem}}}.{\\color{red}{\\texttt{Fem20}}}&amp;{\\color{blue}{\\texttt{Fem}}}.{\\color{red}{\\texttt{Fem3}}}&amp;{\\color{blue}{\\texttt{Fem}}}.{\\color{red}{\\texttt{Fem5}}}&amp;{\\color{blue}{\\texttt{Fem}}}.{\\color{red}{\\texttt{K983388}}}&amp;\\dots\\\\ {\\color{blue}{\\texttt{Male}}}&amp;{\\color{blue}{\\texttt{Male}}}.{\\color{red}{\\texttt{Fem2}}}&amp;{\\color{blue}{\\texttt{Male}}}.{\\color{red}{\\texttt{Fem20}}}&amp;{\\color{blue}{\\texttt{Male}}}.{\\color{red}{\\texttt{Fem3}}}&amp;{\\color{blue}{\\texttt{Male}}}.{\\color{red}{\\texttt{Fem5}}}&amp;{\\color{blue}{\\texttt{Male}}}.{\\color{red}{\\texttt{K983388}}}&amp;\\dots\\\\ {\\color{blue}{\\texttt{UNK}}}&amp;{\\color{blue}{\\texttt{UNK}}}.{\\color{red}{\\texttt{Fem2}}}&amp;{\\color{blue}{\\texttt{UNK}}}.{\\color{red}{\\texttt{Fem20}}}&amp;{\\color{blue}{\\texttt{UNK}}}.{\\color{red}{\\texttt{Fem3}}}&amp;{\\color{blue}{\\texttt{UNK}}}.{\\color{red}{\\texttt{Fem5}}}&amp;{\\color{blue}{\\texttt{UNK}}}.{\\color{red}{\\texttt{K983388}}}&amp;\\dots\\\\ \\end{array}\\] which creates three times as many random factors, one associated with offspring of each sex for each each dam. 5.1 idh Variance Structure The different variance functions make different assumptions about how the effects associated with these different factors are distributed. First, we may want to allow the variance in the effects to be different for each row of factors; i.e. does the identity of a chicks mother explain different amounts of variation depending on the sex of the chick. We can fit this model using the idh function and represent our belief in how the effects are distributed as a \\(3\\times3\\) covariance matrix \\({\\bf V}\\): \\[{\\bf V}_{{\\color{red}{\\texttt{dam}}}}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{\\color{blue}{\\texttt{Female}}}&amp;0&amp;0\\\\ 0&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Male}}}&amp;0\\\\ 0&amp;0&amp;\\sigma^{2}_{\\color{blue}{\\texttt{UNK}}}\\\\ \\end{array} \\right]\\] In the simpler models we had fitted in Chapters 2 and 3 \\({\\bf V}\\) was a scalar (\\({\\bf V} = \\sigma^{2}\\)) rather than a matrix, and the prior specification was relatively simple. We will come back to prior specifications for covariance matrices in Section 5.6, but for now note that the prior for the dam component has V as a \\(3\\times3\\) identity matrix: priorb = list(R = list(V = diag(1), nu = 0.002), G = list(G1 = list(V = diag(3), nu = 0.002), G2 = list(V = 1, nu = 0.002))) m3a.3 &lt;- MCMCglmm(tarsus ~ sex, random = ~idh(sex):dam + fosternest, data = BTdata, prior = priorb) Figure 5.2: MCMC summary plot for the sex-specific dam variance components from model m3a.3. The number of chicks with unknown (\\(\\texttt{UNK}\\)) sex is low, with very little replication within dams. The posterior distribution for the \\(\\texttt{UNK}\\) variance component is dominated by the prior which has a marginal distribution of \\(\\texttt{V}=1\\) and \\(\\texttt{nu}=0.002\\). The sex specific variances for males and females look pretty similar, but the sex-specific variance for birds with unknown sex is not behaving well. This is not that surprising given that there are only 47 birds with unknown sex and these tend to be thinly spread across dams. This variance component is likely to be dominated by the prior, but for now we’ll leave the model as it is and come back to some possible alternative solutions later. We can extract the marginal means for each variance and place them into a matrix: Vdam.3 &lt;- diag(colMeans(m3a.3$VCV)[1:3]) colnames(Vdam.3) &lt;- colnames(m3a.3$VCV)[1:3] Vdam.3 ## sexFem.dam sexMale.dam sexUNK.dam ## [1,] 0.1762725 0.000000 0.00000000 ## [2,] 0.0000000 0.172467 0.00000000 ## [3,] 0.0000000 0.000000 0.05600859 Note, that they are in general less than the marginal mean of the dam variance in model m3a.1 (0.227) where a sex interaction was not fitted. Because the dam effects are assumed to be multivariate normal we can plot an ellipsoid that completely represents their distribution: plotsubspace(Vdam.3, axes.lab = TRUE) Widget 4.1: Ellipsoid that circumscribes 95% of the expected dam effects as estimated in model m3a.3. This can be thought of as a scatter plot of the dam effects between each sex, if the dam effects could be directly measured. Because the covariances of the dam effects between the sexes were set to zero the axes of the ellipsoids are all parallel to the figure axes. If we had measured the offspring of a lot of dams, and for each dam we had measured a very large number of offspring of each sex, then we could calculate the average tarsus lengths within a nest for males, females and unknowns separately. If we produced a scatter plot of these means the data would have the same shape as this ellipsoid and 95% of the data would lie inside it. 5.2 us Variance Structure The oddity of the model, and the meaning of the off-diagonal zeros, should become apparent. We have assumed that the different sexes within a nest are independent. If we plotted the average tarsus lengths for males against the average tarsus lengths for females for each dam this model implies we should see no relationship. We can relax this assumption using the us function which estimates the matrix: \\[{\\bf V}_{{\\color{red}{\\texttt{dam}}}}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{\\color{blue}{\\texttt{Female}}}&amp;\\sigma_{\\color{blue}{\\texttt{Female}, \\texttt{Male}}}&amp;\\sigma_{\\color{blue}{\\texttt{Female}, \\texttt{UNK}}}\\\\ \\sigma_{\\color{blue}{\\texttt{Female}, \\texttt{Male}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Male}}}&amp;\\sigma_{\\color{blue}{\\texttt{Male}, \\texttt{UNK}}}\\\\ \\sigma_{\\color{blue}{\\texttt{Female}, \\texttt{UNK}}}&amp;\\sigma_{\\color{blue}{\\texttt{Male}, \\texttt{UNK}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{UNK}}}\\\\ \\end{array} \\right]\\] We will now use a prior for the covariance matrix where \\(\\texttt{nu}=4\\) (1 more than the dimension of \\(\\texttt{V}\\)) and the prior covariance matrix is an diagonal matrix with small variances. This may seem surprising but the motivation is laid out in Section 5.6: prior.m3a.4 = list(R = list(V = diag(1), nu = 0.002), G = list(G1 = list(V = diag(3) * 0.02, nu = 4), G2 = list(V = 1, nu = 0.002))) m3a.4 &lt;- MCMCglmm(tarsus ~ sex, random = ~us(sex):dam + fosternest, data = BTdata, prior = prior.m3a.4) The posterior mean (co)variances for this model show that the covariances are almost the same magnitude as the variances suggesting strong correlations: Vdam.4 &lt;- matrix(colMeans(m3a.4$VCV)[1:9], 3, 3) colnames(Vdam.4) &lt;- colnames(m3a.4$VCV)[1:3] Vdam.4 ## sexFem:sexFem.dam sexMale:sexFem.dam sexUNK:sexFem.dam ## [1,] 0.2325853 0.2021226 0.2200180 ## [2,] 0.2021226 0.2160452 0.2153040 ## [3,] 0.2200180 0.2153040 0.2817686 The distribution of dam effects in this model look substantially different from those when using an idh structure: plotsubspace(Vdam.4, axes.lab = TRUE, wire.frame = T) Widget 4.2: Ellipsoid that circumscribes 95% of the expected dam effects as estimated in model m3a.4. This can be thought of as a scatter plot of the dam effects between each sex, if the dam effects could be directly measured. The correlations of the dam effects between the sexes were estimated and found to be close to one, and the sex-specific variances were all roughly equal in magnitude. Consequently the major axis of the ellipsoid lies at \\(45^{o}\\) to the figure axes. Covariances can be hard to interpret, and I usually find correlations easier to think about. They can also be useful for detecting problems in the chain. In model m3a.1 the dam variance for chicks with unknown sex was behaving badly and was getting ‘trapped’ at zero. When fitting a \\(2\\times2\\) covariance matrix similar things can happen when correlations are close to -1 and 1, and this may not be obvious from the marginal distribution of the covariances: plot(posterior.cor(m3a.4$VCV[, 1:9])[, c(2, 3, 7)]) Figure 5.3: MCMC summary plot for the between sex correlations in dam effects from model . All the correlations are very close to one, and the variances all pretty equal so we’d probably consider the simpler model. We could try using DIC to compare models, although given the different prior specifications for the two models it is unclear whether this would be meaningful. However, the simpler model does seem to have better support as intuition suggests: m3a.4$DIC ## [1] 1998.454 m3a.1$DIC ## [1] 1991.869 5.3 Compound Variance Structures There are also ways of specifying models that lie somewhere between the simple model (mBT), where dam effects are assumed to be equivalent across the sexes, and the most complex model (mBT2), where dam effects are allowed to vary across the sexes and covary between the sexes to different degrees. Some alternatives are listed in Table 5.1. Table 5.1: Different random effect specifications in \\(\\texttt{MCMCglmm}\\) (with equivalent \\(\\texttt{lmer}\\) syntax when possible). \\(\\texttt{sex}\\) is a factor with three levels so the resulting covariance and correlation matrices are \\(3\\times3\\). For the cors(sex):dam structure, the prior specification had fix=2 which forces the last diagonal block (starting from position 2,2) to be a correlation matrix rather than a covariance matrix. fix can also be specified for other structures but then it fixes the last diagonal block to what ever is specified in the V element of the prior. ante structures are not covered here but in Section 10.2 MCMCglmm lmer nparameters Variance Correlation dam (1|dam) 1 \\(\\left[\\begin{array}{ccc}V&amp;V&amp;V\\\\V&amp;V&amp;V\\\\V&amp;V&amp;V\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;{\\color{red}{1}}&amp;{\\color{red}{1}}\\\\{\\color{red}{1}}&amp;{\\color{red}{1}}&amp;{\\color{red}{1}}\\\\{\\color{red}{1}}&amp;{\\color{red}{1}}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) us(sex):dam (sex|dam) 6 \\(\\left[\\begin{array}{ccc}V_{1,1}&amp;C_{1,2}&amp;C_{1,3}\\\\C_{2,1}&amp;V_{2,2}&amp;C_{2,3}\\\\C_{3,1}&amp;C_{3,2}&amp;V_{3,3}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;r_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) sex:dam (1|sex:dam) 1 \\(\\left[\\begin{array}{ccc}V&amp;{\\color{red}{0}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;V&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{0}}&amp;V\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;{\\color{red}{0}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{1}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{0}}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) dam+sex:dam (1|dam)+(1|sex:dam) 2 \\(\\left[\\begin{array}{ccc}V_1+V_2&amp;V_1&amp;V_1\\\\V_1&amp;V_1+V_2&amp;V_1\\\\V_1&amp;V_1&amp;V_1+V_2\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r&amp;r\\\\r&amp;{\\color{red}{1}}&amp;r\\\\r&amp;r&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) idh(sex):dam 3 \\(\\left[\\begin{array}{ccc}V_{1,1}&amp;{\\color{red}{0}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;V_{2,2}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{0}}&amp;V_{3,3}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;{\\color{red}{0}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{1}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{0}}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) corg(sex):dam 3 \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;r_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;r_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) corgh(sex):dam 3 \\(\\left[\\begin{array}{ccc}{\\color{red}{V_{1,1}}}&amp;r_{1,2}{\\color{red}{\\sqrt{V_{1,1}V_{2,2}}}}&amp;r_{1,3}{\\color{red}{\\sqrt{V_{1,1}V_{3,3}}}}\\\\r_{2,1}{\\color{red}{\\sqrt{V_{2,2}V_{1,1}}}}&amp;{\\color{red}{V_{2,2}}}&amp;r_{2,3}{\\color{red}{\\sqrt{V_{2,2}V_{3,3}}}}\\\\r_{3,1}{\\color{red}{\\sqrt{V_{3,3}V_{1,1}}}}&amp;r_{3,2}{\\color{red}{\\sqrt{V_{3,3}V_{2,2}}}}&amp;{\\color{red}{V_{3,3}}}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;r_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) cors(sex):dam 5 \\(\\left[\\begin{array}{ccc}V_{1,1}&amp;C_{1,2}&amp;r_{1,3}\\\\C_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;C_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\C_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) 5.4 Heterogenous Residual Variance To be started... In short - if you’ve fitted a sex by dam interaction I would always allow the sexes to have different residual variances. Use rcov=\\(\\sim\\)idh(sex):units. 5.5 Contrasts and Covariances A general method for seeing what a particular random specification means in terms of the original variables is to realise that \\[ {\\boldsymbol{\\mathbf{\\Sigma}}} = {\\bf Z}{\\bf V}{\\bf Z}^{&#39;} \\tag{5.1} \\] where \\({\\boldsymbol{\\mathbf{\\Sigma}}}\\) is the covariance matrix for the original set of variables and \\({\\bf V}\\) the variances associated with the variance structure model. \\({\\bf Z}\\) is the random effect design matrix. Equation (5.1) implies: \\[{\\bf V} = {\\bf Z}^{-1}{\\boldsymbol{\\mathbf{\\Sigma}}}({\\bf Z}^{&#39;})^{-1}\\] or alternatively: \\[{\\bf V} = ({\\bf Z}{\\bf Z}^{&#39;})^{-}{\\bf Z}^{&#39;}{\\boldsymbol{\\mathbf{\\Sigma}}}{\\bf Z}({\\bf Z}^{&#39;}{\\bf Z})^{-}\\] if \\({\\bf Z}\\) is non-square and/or singular, where \\(^{-}\\) is a generalised inverse. 5.6 Priors for Covariance Matrices Priors for covariance matrices are tricky. What maybe non-informative for a covariance may be informative for a correlation and vice versa. 5.6.1 Priors for us structures A useful result is that the marginal distribution of a variance is also inverse-Wishart distributed: \\[\\sigma^{2}_{1} \\sim IW\\left(\\texttt{nu}^{\\ast}\\texttt{=nu-dim(V)+1},\\ \\texttt{V}^{\\ast}=\\frac{\\texttt{nu}}{\\texttt{nu}^{\\ast}}\\texttt{V[1,1]}\\right)\\] using the first variance as an example, and indicating the new parameters with an asterisk. An uninformative prior for the correlations is an improper prior with V=diag(dim(V))\\(\\ast\\)0 and nu=dim(V)+1. For the \\(3\\times3\\) sex by dam covariance matrix in model m3a.4 we used a proper prior with V=diag(3)\\(\\ast\\)0.02 and nu=4 in the hope that this would be relatively uninformative for the correlations. We can plot the marginal density of the variances for this distribution as we did in Chapter 2: nu.ast &lt;- prior.m3a.4$G$G1$nu - dim(prior.m3a.4$G$G1$V)[1] + 1 V.ast &lt;- prior.m3a.4$G$G1$V[1, 1] * (prior.m3a.4$G$G1$nu/nu.ast) xv &lt;- seq(1e-16, 1, length = 100) dv &lt;- dgamma(1/xv, shape = nu.ast/2, rate = (nu.ast * V.ast)/2)/(xv^2) plot(dv ~ xv, type = &quot;l&quot;) Figure 5.4: Marginal prior distribution of a variance using an inverse Wishart prior for the covariance matrix with and . In Chapter 2 we saw that a non-informative prior for a variance component was V=0 and nu=-2. This result generalises to covariance matrices where the improper prior V=diag(dim(V))\\(\\ast\\)0 and nu=dim(V)-3 is non-informative for the variances and covariances. This can be verified for the variances using the results derived above for the marginal distribution: $$ \\[\\begin{array}{rl} \\sigma^{2}_{1} \\sim&amp; IW\\left(\\texttt{nu}^{\\ast}\\texttt{=dim(V)-3-dim(V)+1},\\ \\texttt{V}^{\\ast}=\\frac{\\texttt{nu}}{\\texttt{nu}^{\\ast}}\\texttt{0}\\right)\\\\ \\sim&amp; IW\\left(\\texttt{nu}^{\\ast}\\texttt{=-2},\\ \\texttt{V}^{\\ast}=\\texttt{0}\\right)\\\\ \\end{array}\\] $$ 5.6.2 Priors for idh structures For idh the diagonal elements of the matrix are independent and each variance is distributed as10: \\[\\sigma^{2}_{1} \\sim IW\\left(\\texttt{nu}^{\\ast}\\texttt{=nu},\\ \\texttt{V}^{\\ast}=\\texttt{V[1,1]}\\right)\\] 5.6.3 Priors for corg and corgh structures For corg and corgh structures11 the diagonals of V define the fixed variances (corgh) or are ignored and the variances set to one (corg). I use the prior specification in Barnard, McCulloch, and Meng (2000) where nu controls how much the correlation matrix approaches an identity matrix. The marginal distribution of individual correlations (\\(r\\)) is given by Barnard, McCulloch, and Meng (2000; and Box and Tiao 1973): \\[\\begin{array}{lr} Pr(r) \\propto (1-r^{2})^\\frac{\\texttt{nu-dim(V)-1}}{\\texttt{2}}, &amp; |r|&lt;1\\\\ \\end{array}\\] and as shown above setting nu =dim(V)+1 results in marginal correlations that are uniform on the interval \\[-1,1\\]. In most cases correlation matrices do not have known form and so cannot be directly Gibbs sampled. \\(\\texttt{MCMCglmm}\\) uses a method proposed by X. F. Liu and Daniels (2006) with the target prior as in Barnard, McCulloch, and Meng (2000). Generally this algorithm is very efficient as the Metropolis-Hastings acceptance probability only depends on the degree to which the candidate prior and the target prior (the prior you specify) conflict. The candidate prior is equivalent to the prior in Barnard, McCulloch, and Meng (2000) with nu=0 so as long as a diffuse prior is set, mixing is generally not a problem. If nu=0 is set (the default) then the Metropolis-Hastings steps are always accepted resulting in Gibbs sampling. However, a prior of this form puts high density on extreme correlations which can cause problems if the data give support to correlations in this region. References "],["cont-int.html", "6 Continuous Random Interactions 6.1 Random Regression 6.2 Expected Variances and Covariances 6.3 us versus idh and mean centering 6.4 Meta-analysis 6.5 Splines", " 6 Continuous Random Interactions In Chapter 5 we saw how we could define a linear model within a variance function and then interact these terms with a random effect. In the example, we did this in order to fit a sex by dam interaction: us(sex):dam. The term entering into the variance function model was categorical, and we saw that by fitting the interaction we were essentially estimating the parameters of the covariance matrix: \\[{\\bf V}_{{\\color{red}{\\texttt{dam}}}}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{\\color{blue}{\\texttt{Female}}}&amp;\\sigma_{\\color{blue}{\\texttt{Female}, \\texttt{Male}}}&amp;\\sigma_{\\color{blue}{\\texttt{Female}, \\texttt{UNK}}}\\\\ \\sigma_{\\color{blue}{\\texttt{Female}, \\texttt{Male}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Male}}}&amp;\\sigma_{\\color{blue}{\\texttt{Male}, \\texttt{UNK}}}\\\\ \\sigma_{\\color{blue}{\\texttt{Female}, \\texttt{UNK}}}&amp;\\sigma_{\\color{blue}{\\texttt{Male}, \\texttt{UNK}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{UNK}}}\\\\ \\end{array} \\right]\\] We are also free to define the variance function model with continuous covariates, or even a mixture of continuous and categorical factors, and the resulting covariance matrix is interpreted in the same way. 6.1 Random Regression As an example, we’ll use a longitudinal data set on chicken growth (See Figure 6.1): data(ChickWeight) The data consist of body weights (weight) for 50 chicks (Chick) measured up to 12 times over a 3 week period. The variable Time is the number of days since hatching, and Diet is a four level factor indicating the type of protein diet the chicks received. xyplot(weight ~ Time | Chick, data = ChickWeight) Figure 6.1: Weight data of 50 chicks from hatching until three weeks old. Growth curves tend to be sigmoidal and so one of the non-linear growth curves such as the Gompertz or logistic may be a good starting model. However, these can be tricky to use and an alternative is to try and capture the form of the curve using polynomials. We’ll start with a quadratic function at the population level and and fit chick as a random term: prior.m4a.1 &lt;- list(R = list(V = 1e-07, nu = -2), G = list(G1 = list(V = 1, nu = 1))) m4a.1 &lt;- MCMCglmm(weight ~ Diet + poly(Time, 2, raw = TRUE), random = ~Chick, data = ChickWeight, pr = TRUE, prior = prior.m4a.1) We’ve saved the random chick effects so we can plot the predicted growth functions for each bird. For now we will just predict the growth function assuming that all birds were on Diet 1 (the intercept): pop.int &lt;- posterior.mode(m4a.1$Sol[, 1]) pop.slope &lt;- posterior.mode(m4a.1$Sol[, 5]) pop.quad &lt;- posterior.mode(m4a.1$Sol[, 6]) chick.int &lt;- posterior.mode(m4a.1$Sol[, c(7:56)]) We need to combine these parameter estimates with the polynomials for Time which are just the sequence \\(\\texttt{Time}^{0}, \\texttt{Time}^1, \\texttt{Time}^2 \\dots\\) and so on. We can then plot the population expected population growth curve, and around that the predicted growth curves for each chick (we don’t need to bother with \\(\\texttt{Time}^{0}\\) since this is always one): pos.time &lt;- seq(0, 21, length = 100) plot(pop.int + pop.slope * I(pos.time^1) + pop.quad * I(pos.time^2) ~ pos.time, type = &quot;l&quot;, lwd = 2, ylim = c(-25, 400)) for (i in 1:50) { lines(pop.int + chick.int[i] + pop.slope * I(pos.time^1) + pop.quad * I(pos.time^2) ~ pos.time, col = &quot;red&quot;, lty = 2) } Figure 6.2: Predicted weights of each chick as a function of age. A quadratic population growth curve (black) is fitted with random chick intercepts. The population growth curve is slightly convex because of the quadratic term, and the predictions for each chick are parallel to this curve. By fitting chick as a random effect we have allowed variation in the intercept only, and often this is not enough. We can get a feel for how well the model fits the data by overlaying the predictions with actual values. We can form design matrix \\({\\bf W} = \\left[{\\bf X},\\ {\\bf Z}\\right]\\) and multiply by the parameter vector \\(\\boldsymbol{\\theta}\\) to get the predictions (See Chapter 11): W.1 &lt;- cbind(m4a.1$X, m4a.1$Z) prediction.1 &lt;- W.1 %*% posterior.mode(m4a.1$Sol) xyplot(weight + prediction.1@x ~ Time | Chick, data = ChickWeight) Figure 6.3: Weights of each chick as a function of age in blue, with the predicted weights in purple. A quadratic population growth curve was fitted with random chick intercepts. The predictions don’t look that bad, but you will notice that for some chicks (e.g. 13,19,34) the slope of the predicted growth seems either to shallow, or too steep. To account for this we can start by fitting us(1+time):Chick. The linear model inside the variance function has two parameters, an intercept (1) and a regression slope associated with Time which define the set of interactions: \\[\\begin{array}{c|rrrrrc} &amp;{\\color{red}{\\texttt{Chick1}}}&amp;{\\color{red}{\\texttt{Chick2}}}&amp;{\\color{red}{\\texttt{Chick3}}}&amp;\\dots\\\\ \\hline\\\\ {\\color{blue}{\\texttt{(Intercept)}}}&amp;{\\color{blue}{\\texttt{(Intercept)}}}.{\\color{red}{\\texttt{Chick1}}}&amp;{\\color{blue}{\\texttt{(Intercept)}}}.{\\color{red}{\\texttt{Chick2}}}&amp;{\\color{blue}{\\texttt{(Intercept)}}}.{\\color{red}{\\texttt{Chick3}}}&amp;\\dots\\\\ {\\color{blue}{\\texttt{Time}}}&amp;{\\color{blue}{\\texttt{Time}}}.{\\color{red}{\\texttt{Chick1}}}&amp;{\\color{blue}{\\texttt{Time}}}.{\\color{red}{\\texttt{Chick2}}}&amp;{\\color{blue}{\\texttt{Time}}}.{\\color{red}{\\texttt{Chick3}}}&amp;\\dots\\\\ \\end{array}\\] Each chick now has an intercept and a slope, and because we have used the us function we are estimating the \\(2\\times2\\) matrix: \\[{\\bf V}_{{\\color{red}{\\texttt{Chick}}}}= \\left[ \\begin{array}{cc} \\sigma^{2}_{\\color{blue}{\\texttt{(Intercept)}}}&amp;\\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}}}\\\\ \\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Time}}}\\\\ \\end{array} \\right]\\] \\(\\sigma^{2}_{\\color{blue}{\\texttt{(Intercept)}}}\\) is the amount of variation in intercepts between chicks, and \\(\\sigma^{2}_{\\color{blue}{\\texttt{Time}}}\\) is the amount of variation in the regression slopes between chicks. If the idh function had been used the covariance would have been set to zero and we could have interpreted variation in intercepts as variation in overall size, and variation in slopes as variation in growth rate. However, there is often covariance between intercepts and slopes and it is usually a good idea to use the us function and estimate them (see Section 6.3). We shall do so: prior.m4a.2 &lt;- list(R = list(V = 1e-07, nu = -2), G = list(G1 = list(V = diag(2), nu = 2))) # non-informative prior for the (co)variances m4a.2 &lt;- MCMCglmm(weight ~ Diet + poly(Time, 2, raw = TRUE), random = ~us(1 + Time):Chick, data = ChickWeight, pr = TRUE, prior = prior.m4a.2, saveX = TRUE, saveZ = TRUE) Figure 6.4: MCMC summary plots for the chick covariance components from model . The lower and upper plots are the intercept and slope variance components respectively, and the middle two plots are the intercept-slope covariance. The traces look OKish for the chick (co)variance matrices (Figure 6.4 but notice that the the estimate of intercept-slope correlation is close to the boundary of parameter space (-1): int.slope.cor &lt;- m4a.2$VCV[, 2]/sqrt(m4a.2$VCV[, 1] * m4a.2$VCV[, 4]) posterior.mode(int.slope.cor) ## var1 ## -0.979356 and shows strong autocorrelation autocorr(int.slope.cor) ## , , 1 ## ## [,1] ## Lag 0 1.00000000 ## Lag 10 0.17413843 ## Lag 50 0.02122664 ## Lag 100 0.03364253 ## Lag 500 -0.01977366 and we should run it for longer in order to sample the posterior adequately. For now we will carry on and obtain the predictions from the model we ran, but using the perdict function rather than dong it ‘by hand’: xyplot(weight + predict(m4a.2, marginal = NULL) ~ Time | Chick, data = ChickWeight) Figure 6.5: Weights of each chick as a function of age in blue, with the predicted weights in purple. A quadratic population growth curve was fitted with a first order random regression for chicks (i.e. a random intercept-slope model). and we can see that the fit is much better (See Figure 6.5). In theory we could fit higher order random regressions (data and prior permitting) and use something like DIC to choose which is the best compromise between the fit of the model to the data and how many effective parameters were fitted. For example we could go from the \\(1^{st}\\) order random regression to a \\(2^{nd}\\) order model: prior.m4a.3 &lt;- list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = diag(3), nu = 3))) # non-informative prior for the (co)variances m4a.3 &lt;- MCMCglmm(weight ~ Diet + poly(Time, 2, raw = TRUE), random = ~us(1 + poly(Time, 2, raw = TRUE)):Chick, data = ChickWeight, pr = TRUE, prior = prior.m4a.3, saveX = TRUE, saveZ = TRUE) and obtain the \\(3\\times3\\) covariance matrix: \\[{\\bf V}_{{\\color{red}{\\texttt{Chick}}}}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{\\color{blue}{\\texttt{(Intercept)}}}&amp;\\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}}}&amp;\\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time^{2}}}}\\\\ \\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Time}}}&amp;\\sigma_{\\color{blue}{\\texttt{Time}, \\texttt{Time^{2}}}}\\\\ \\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time^2}}}&amp;\\sigma_{\\color{blue}{\\texttt{Time}, \\texttt{Time^{2}}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Time^2}}}\\\\ \\end{array} \\right]\\] The model predicts the chick weights to an even better degree (See Figure 6.6) xyplot(weight + predict(m4a.3, marginal = NULL) ~ Time | Chick, data = ChickWeight) Figure 6.6: Weights of each chick as a function of age in blue, with the predicted weights in purple. A quadratic population growth curve was fitted with a second order random regression for chicks (i.e. a random intercept-slope-quadratic model). and the DIC has gone down, suggesting that the model is better: m4a.1$DIC ## [1] 5525.505 m4a.2$DIC ## [1] 4544.608 m4a.3$DIC ## [1] 3932.343 It is worth seeing whether using an AIC measure using REML also suggests the highest order model is the better model. library(lme4, warn.conflicts = FALSE) m5a.1.REML &lt;- lmer(weight ~ Diet + poly(Time, 2, raw = TRUE) + (1 | Chick), data = ChickWeight) AIC(m5a.1.REML) ## [1] 5578.963 m5a.2.REML &lt;- lmer(weight ~ Diet + poly(Time, 2, raw = TRUE) + (poly(Time, 1, raw = TRUE) | Chick), data = ChickWeight) AIC(m5a.2.REML) ## [1] 4732.387 m5a.3.REML &lt;- lmer(weight ~ Diet + poly(Time, 2, raw = TRUE) + (poly(Time, 2, raw = TRUE) | Chick), data = ChickWeight) AIC(m5a.3.REML) ## [1] 4267.013 detach(package:lme4) # causes conflicts with coda 6.2 Expected Variances and Covariances Random regression models make strong assumptions about how the variance should change as a function of the predictor variable. Imagine that the intercept variance was zero, such that all regressions give the same prediction when Time=0. Imagine also that there was variance for slope, the predictions would look something like this (Figure 6.7): slope &lt;- rnorm(30) plot(0, type = &quot;n&quot;, xlim = c(-1, 1), ylim = c(-3, 3), ylab = &quot;y&quot;, xlab = &quot;Time&quot;) for (i in 1:30) { lines(c(-1, 1), c(-slope[i], slope[i])) } Figure 6.7: Hypothetical regression lines where the variance in slopes is one but the variance in intercepts is zero. The expected variance of y is a quadratic function of Time, being zero when Time=0, and increasing with positive or negative values. with the variance increasing at extreme values of Time and being zero at Time=0. For an intercept-slope model such as this the expected variance is quadratic in the predictor, and for a intercept-slope-quadratic model the variance is cubic in the predictor. Generally the expected variance can be obtained using: \\[VAR[y] = \\textrm{diag}({\\bf Z}{\\bf V}{\\bf Z}^{&#39;}) \\label{RRvar-eq} \\tag{6.1}\\] and we can use this to predict the change in variance as a function of Time for the three models: pos.time &lt;- seq(0, 21, length = 100) polynomial &lt;- cbind(rep(1, 100), poly(pos.time, 2, raw = TRUE)) beta.1 &lt;- c(posterior.mode(m4a.1$Sol[, 1]), posterior.mode(m4a.1$Sol[, 5]), posterior.mode(m4a.1$Sol[, 6])) beta.2 &lt;- c(posterior.mode(m4a.2$Sol[, 1]), posterior.mode(m4a.2$Sol[, 5]), posterior.mode(m4a.2$Sol[, 6])) beta.3 &lt;- c(posterior.mode(m4a.3$Sol[, 1]), posterior.mode(m4a.3$Sol[, 5]), posterior.mode(m4a.3$Sol[, 6])) VCV.1 &lt;- matrix(posterior.mode(m4a.1$VCV)[1], 1, 1) VCV.2 &lt;- matrix(posterior.mode(m4a.2$VCV)[1:(2^2)], 2, 2) VCV.3 &lt;- matrix(posterior.mode(m4a.3$VCV)[1:(3^2)], 3, 3) units.1 &lt;- posterior.mode(m4a.1$VCV)[2] units.2 &lt;- posterior.mode(m4a.2$VCV)[5] units.3 &lt;- posterior.mode(m4a.3$VCV)[10] plot(weight ~ Time, data = ChickWeight) mu.1 &lt;- polynomial %*% beta.1 sd.1 &lt;- sqrt(units.1 + diag(polynomial[, 1, drop = FALSE] %*% VCV.1 %*% t(polynomial[, 1, drop = FALSE]))) lines(mu.1 ~ pos.time) lines(I(mu.1 + 1.96 * sd.1) ~ pos.time, lty = 2) lines(I(mu.1 - 1.96 * sd.1) ~ pos.time, lty = 2) (#fig:VCVpred.1)Chick weights plotted as a function of time. 95% of the data are expected to fall within the dashed lines assuming the model with random intercepts is the correct model, and the diet treatments have small effects. The simple model, without a slope term has constant variance across the range, and is clearly inconsistent with the data (Figure @ref(fig:VCVpred.1). The second model on the other hand plot(weight ~ Time, data = ChickWeight) mu.2 &lt;- polynomial %*% beta.2 sd.2 &lt;- sqrt(units.2 + diag(polynomial[, 1:2, drop = FALSE] %*% VCV.2 %*% t(polynomial[, 1:2, drop = FALSE]))) lines(mu.2 ~ pos.time) lines(I(mu.2 + 1.96 * sd.2) ~ pos.time, lty = 2) lines(I(mu.2 - 1.96 * sd.2) ~ pos.time, lty = 2) (#fig:VCVpred.2)Chick weights plotted as a function of time. 95% of the data are expected to fall within the dashed lines assuming the model with random intercepts and slopes is the correct model, and the diet treatments have small effects. has an expected variance structure reasonably close to that observed (Figure @ref(fig:VCVpred.2)). The highest order model, which was the best using information criteria such as AIC an DIC, also does badly (Figure @ref(fig:VCVpred.3)): plot(weight ~ Time, data = ChickWeight, ylim = c(-150, 600)) mu.3 &lt;- polynomial %*% beta.3 sd.3 &lt;- sqrt(units.3 + diag(polynomial[, 1:3, drop = FALSE] %*% VCV.3 %*% t(polynomial[, 1:3, drop = FALSE]))) lines(mu.3 ~ pos.time) lines(I(mu.3 + 1.96 * sd.3) ~ pos.time, lty = 2) lines(I(mu.3 - 1.96 * sd.3) ~ pos.time, lty = 2) (#fig:VCVpred.3)Chick weights plotted as a function of time. 95% of the data are expected to fall within the dashed lines assuming the model with random intercepts, slopes and quadratic effects is the correct model, and the diet treatments have small effects. In general, I would not draw conclusions about changes in variance from random regression models (Pletcher and Geyer 1999). 6.3 us versus idh and mean centering bad reparametersitaion prior.m4b.1 &lt;- list(R = list(V = 1e-07, nu = -2), G = list(G1 = list(V = diag(2), nu = 2))) m4b.1 &lt;- MCMCglmm(weight ~ Diet + poly(Time, 2, raw = TRUE), random = ~us(1 + I(Time - 3)):Chick, data = ChickWeight, pr = TRUE, prior = prior.m4b.1, saveX = TRUE, saveZ = TRUE) 6.4 Meta-analysis Random intercept-slope models implicitly assume that the variance changes as a quadratic function of the predictor. This can be used to our advantage because it allows us to fit meta-analytic models. In meta-analysis the data are usually some standardised statistic which has been estimated with different levels of measurement error. If we wanted to know the expected value of these statistics we would want to weight our answer to those measurements made with the smallest amount of error. If we assume that measurement error around the true value is normally distributed then we could assume the model: \\[y_{i} = \\beta_{1} + m_{i} +e _{i}\\] where \\(\\beta_{1}\\) is the expected value, \\(m_{i}\\) is some deviation due to measurement error, and \\(e_{i}\\) is the deviation of the statistic from the global intercept not due to measurement error. Some types of meta-analysis presume \\(e_{i}\\) does not exist and that the only variation between studies is due to measurement error. This is not realistic, I think. Often, standard errors are reported in the literature, and these can be viewed as an approximation to the expected standard deviation of the measurement error. If we put the standard errors for each statistic as a column in the data frame (and call it SE) then the random term idh(SE):units defines a diagonal matrix with the standard errors on the diagonal. Using results from Equation (6.1) \\[\\begin{array}{rl} \\textrm{VAR}[{\\bf m}] =&amp; {\\bf Z}{\\bf V}{\\bf Z}^{&#39;}\\\\ =&amp; {\\bf Z}\\sigma^{2}_{m}{\\bf I}{\\bf Z}^{&#39;}\\\\ =&amp; \\sigma^{2}_{m}{\\bf Z}{\\bf Z}^{&#39;}\\\\ \\end{array}\\] fixing \\(\\sigma^{2}_{m}=1\\) in the prior, the expected variance in the measurement errors are therefore the standard errors squared (the sampling variance) and all measurement errors are assumed to be independent of each other. The random regression therefore fits a random effect meta-analysis. 6.5 Splines blah blah random = ~idv(spl(covariate)) fits a penalised thin-plate spline, random = ~idv(smspline(covariate)) fits a penalised cubic spline. The coefficients are random effects, stored in the Sol element of the model output and the single variance component (the penalising bit) is in the VCV element. Its usually a good idea to scale the covariate to lie in the interval [0,1] or some such thing. References "],["multi.html", "7 Multi-response models 7.1 Relaxing the univariate assumptions of causality 7.2 Multinomial Models 7.3 Zero-inflated Models 7.4 Hurdle Models 7.5 Zero-altered Models", " 7 Multi-response models So far we have only fitted models to a single response variable. Multi-response models are not that widely used, except perhaps in quantitative genetics, and deserve wider use. They allow some of the assumptions of single response models to be relaxed and can be an effective way of dealing with missing data problems. 7.1 Relaxing the univariate assumptions of causality Imagine we knew how much money 200 people had spent on their holiday and on their car in each of four years, and we want to know whether a relationship exists between the two. A simple correlation would be one possibility, but then how do we control for the repeated measures? An often used solution to this problem is to choose one variable as the response (lets say the amount spent on a car) and have the other variable as a fixed covariate (the amount spent on a holiday). The choice is essentially arbitrary, highlighting the belief that any relationship between the two types of spending maybe in part due to unmeasured variables, rather than being completely causal. In practice does this matter? Lets imagine there was only one unmeasured variable: disposable income. There are repeatable differences between individuals in their disposable income, but also some variation within individuals across the four years. Likewise, people vary in what proportion of their disposable income they are willing to spend on a holiday versus a car, but this also changes from year to year. We can simulate some toy data to get a feel for the issues: id&lt;-gl(200,4) # 200 people recorded four times av_wealth&lt;-rlnorm(200, 0, 1) ac_wealth&lt;-av_wealth[id]+rlnorm(800, 0, 1) # expected disposable incomes + some year to year variation av_ratio&lt;-rbeta(200,10,10) ac_ratio&lt;-rbeta(800, 2*(av_ratio[id]), 2*(1-av_ratio[id])) # expected proportion spent on car + some year to year variation y.car&lt;-(ac_wealth*ac_ratio)^0.25 # disposable income * proportion spent on car y.hol&lt;-(ac_wealth*(1-ac_ratio))^0.25 # disposable income * proportion spent on holiday Spending&lt;-data.frame(y.hol=y.hol, y.car=y.car, id=id) A simple regression suggests the two types of spending are negatively related but the association is weak with the \\(R^2\\) close to zero. summary(lm(y.car ~ y.hol, data = Spending)) ## ## Call: ## lm(formula = y.car ~ y.hol, data = Spending) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.89695 -0.19739 0.00361 0.18039 1.27142 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.04197 0.03436 30.321 &lt;2e-16 *** ## y.hol -0.02845 0.03285 -0.866 0.387 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2863 on 798 degrees of freedom ## Multiple R-squared: 0.0009388, Adjusted R-squared: -0.0003132 ## F-statistic: 0.7498 on 1 and 798 DF, p-value: 0.3868 With id added as a random term to deal with the the repeated measures, a similar conclusion is reached although the estimate is more negative: m5a.1 &lt;- MCMCglmm(y.car ~ y.hol, random = ~id, data = Spending) summary(m5a.1$Sol[, &quot;y.hol&quot;]) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Number of chains = 1 ## Sample size per chain = 1000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## -0.095093 0.033244 0.001051 0.001051 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## -0.16122 -0.11638 -0.09574 -0.07248 -0.03412 We may be inclined to stop there, but lets proceed with a multi-response model of the problem. The two responses are passed as a matrix using cbind(), and the rows of this matrix are indexed by the reserved variable units, and the columns by the reserved variable trait. It is useful to think of a new data frame where the response variables have been stacked column-wise and the other predictors duplicated accordingly. Below is the original data frame on the left (Spending) and the stacked data frame on the right: \\[\\begin{array}{cc} \\begin{array}{cccc} &amp;{\\color{blue}{\\texttt{y.hol}}}&amp;{\\color{blue}{\\texttt{y.car}}}&amp;\\texttt{id}\\\\ {\\color{red}{\\texttt{1}}}&amp;\\texttt{1.408191}&amp;\\texttt{0.682300}&amp;\\texttt{1}\\\\ {\\color{red}{\\texttt{2}}}&amp;\\texttt{0.749746}&amp;\\texttt{1.348520}&amp;\\texttt{1}\\\\ \\vdots&amp;\\vdots&amp;\\vdots\\\\ {\\color{red}{\\texttt{800}}}&amp;\\texttt{0.947474}&amp;\\texttt{0.656702}&amp;\\texttt{200}\\\\ \\end{array}&amp; \\Longrightarrow \\begin{array}{ccccc} &amp;\\texttt{y}&amp;{\\color{blue}{\\texttt{trait}}}&amp;\\texttt{id}&amp;{\\color{red}{\\texttt{units}}}\\\\ 1&amp;\\texttt{1.408191}&amp;{\\color{blue}{\\texttt{y.hol}}}&amp;\\texttt{1}&amp;{\\color{red}{\\texttt{1}}}\\\\ 2&amp;\\texttt{0.749746}&amp;{\\color{blue}{\\texttt{y.hol}}}&amp;\\texttt{1}&amp;{\\color{red}{\\texttt{2}}}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 800&amp;\\texttt{0.947474}&amp;{\\color{blue}{\\texttt{y.hol}}}&amp;\\texttt{200}&amp;{\\color{red}{\\texttt{800}}}\\\\ 801&amp;\\texttt{0.682300}&amp;{\\color{blue}{\\texttt{y.car}}}&amp;\\texttt{1}&amp;{\\color{red}{\\texttt{1}}}\\\\ 802&amp;\\texttt{1.348520}&amp;{\\color{blue}{\\texttt{y.car}}}&amp;\\texttt{1}&amp;{\\color{red}{\\texttt{2}}}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 1600&amp;\\texttt{0.656702}&amp;{\\color{blue}{\\texttt{y.car}}}&amp;\\texttt{200}&amp;{\\color{red}{\\texttt{800}}}\\\\ \\end{array} \\end{array}\\] From this we can see that fitting a multi-response model is a direct extension to how we fitted models with categorical random interactions (Chapter 5): prior.m5a.2 &lt;- list(R = list(V = diag(2), nu = 1.002), G = list(G1 = list(V = diag(2), nu = 2, alpha.mu = rep(0, 2), alpha.V = diag(2) * 1000))) m5a.2 &lt;- MCMCglmm(cbind(y.hol, y.car) ~ trait - 1, random = ~us(trait):id, rcov = ~us(trait):units, data = Spending, prior = prior.m5a.2, family = c(&quot;gaussian&quot;, &quot;gaussian&quot;)) We have fitted the fixed effect trait so that the two types of spending can have different intercepts. I usually suppress the intercept (-1) for these types of models so the second coefficient is not the difference between the intercept for the first level of trait (y.hol) and the second level (y.car) but the actual trait specific intercepts. In other words the design matrix for the fixed effects has the form: \\[\\begin{array}{rl} \\left[ \\begin{array}{cc} \\texttt{trait[1]==&quot;y.hol&quot;}&amp;\\texttt{trait[1]==&quot;y.car&quot;}\\\\ \\texttt{trait[2]==&quot;y.hol&quot;}&amp;\\texttt{trait[2]==&quot;y.car&quot;}\\\\ \\vdots&amp;\\vdots\\\\ \\texttt{trait[800]==&quot;y.hol&quot;}&amp;\\texttt{trait[800]==&quot;y.car&quot;}\\\\ \\texttt{trait[801]==&quot;y.hol&quot;}&amp;\\texttt{trait[801]==&quot;y.car&quot;}\\\\ \\texttt{trait[802]==&quot;y.hol&quot;}&amp;\\texttt{trait[802]==&quot;y.car&quot;}\\\\ \\vdots&amp;\\vdots\\\\ \\texttt{trait[1600]==&quot;y.hol&quot;}&amp;\\texttt{trait[1600]==&quot;y.car&quot;}\\\\ \\end{array} \\right] =&amp; \\left[ \\begin{array}{cc} 1&amp;0\\\\ 1&amp;0\\\\ \\vdots&amp;\\vdots\\\\ 1&amp;0\\\\ 0&amp;1\\\\ 0&amp;1\\\\ \\vdots&amp;\\vdots\\\\ 0&amp;1\\\\ \\end{array} \\right]\\\\ \\end{array}\\] A \\(2\\times2\\) covariance matrix is estimated for the random term where the diagonal elements are the variance in consistent individual effects for each type of spending. The off-diagonal is the covariance between these effects which if positive suggests that people that consistently spend more on their holidays consistently spend more on their cars. A \\(2\\times2\\) residual covariance matrix is also fitted. In Section5.4 we fitted heterogeneous error models using idh():units which made sense in this case because each level of unit was specific to a particular datum and so any covariances could not be estimated. In multi-response models this is not the case because both traits have often been measured on the same observational unit and so the covariance can be measured. In the context of this example a positive covariance would indicate that in those years an individual spent a lot on their car they also spent a lot on their holiday. A univariate regression is defined as the covariance between the response and the predictor divided by the variance in the predictor. We can therefore estimate a regression coefficient for these two levels of random variation, and compare them with the regression coefficient we obtained in the simpler model: id.regression &lt;- m5a.2$VCV[, 2]/m5a.2$VCV[, 1] units.regression &lt;- m5a.2$VCV[, 6]/m5a.2$VCV[, 5] plot(mcmc.list(m5a.1$Sol[, &quot;y.hol&quot;], id.regression, units.regression), density = FALSE) The regression coefficients (see Figure 7.1) differ substantially at the within individual (green) and between individual (red) levels, and neither is entirely consistent with the regression coefficient from the univariate model (black). The process by which we generated the data gives rise to this phenomenon - large variation between individuals in their disposable income means that people who are able to spend a lot on their holiday can also afford to spend a lot on their holidays (hence positive covariation between id effects). However, a person that spent a large proportion of their disposable income in a particular year on a holiday, must have less to spend that year on a car (hence negative residual (within year) covariation). Figure 7.1: MCMC summary plot of the coefficient from a regression of car spending on holiday spending in black. The red and green traces are from a model where the regression coefficient is estimated at two levels: within an individual (green) and across individuals (red). The relationship between the two types of spending is in part mediating by a third unmeasured variable, disposable income. When fitting the simpler univariate model we make the assumption that the effect of spending money on a car directly effects how much you spend on a holiday. If this relationship was purely causal then all regression coefficients would have the same expectation, and the simpler model would be justified. For example, we could set up a simpler model where two thirds of the variation in holiday expenditure is due to between individual differences, and holiday expenditure directly affects how much an individual will spend on their car (using a regression coefficient of -0.3). The variation in car expenditure not caused by holiday expenditure is also due to individual differences, but in this case they only explain a third of the variance. Spending$y.hol2 &lt;- rnorm(200, 0, sqrt(2))[Spending$id] + rnorm(800, 0, 1) Spending$y.car2 &lt;- Spending$y.hol2 * -0.3 + rnorm(200, 0, 1)[Spending$id] + rnorm(800, 0, sqrt(2)) We can fit the univariate and multivariate models to these data, and compare the regression coefficients as we did before. Figure 7.2 shows that the regression coefficients are all very similar and a value of -0.3 has a reasonably high posterior probability. However, it should be noted that the posterior standard deviation is smaller in the simpler model because the more strict assumptions have allowed us to pool information across the two levels to get a more precise answer. Figure 7.2: MCMC summary plot of the coefficient from a regression of car spending on holiday spending in black. The red and green traces are from a model where the regression coefficient is estimated at two levels: within an individual (green) and across individuals (red). In this model the relationship between the two types of spending is causal and the regression coefficients have the same expectation. However, the posterior standard deviation from the simple regression is smaller because information from the two different levels is pooled. 7.2 Multinomial Models Multinomial models are difficult - both to fit and interpret. This is particularly true when each unit of observation only has a single realisation from the multinomial. In these instances the data can be expressed as a single vector of factors, and the family argument can be specified as categorical. To illustrate, using a very simple example, we’ll use data collected on 666 Soay sheep from the island of Hirta in the St. Kilda archipelago (Clutton-Brock and Pemberton 2004 Table A2.5). data(SShorns) head(SShorns) ## id horn sex ## 1 1 scurred female ## 2 2 scurred female ## 3 3 scurred female ## 4 4 scurred female ## 5 5 polled female ## 6 6 polled female The sex and horn morph were recorded for each individual, giving the contingency table: Ctable &lt;- table(SShorns$horn, SShorns$sex) Ctable ## ## female male ## normal 83 352 ## polled 65 0 ## scurred 96 70 and we’ll see if the frequencies of the three horn types differ, and if the trait is sex dependent. The usual way to do this would be to use a Chi square test, and to address the first question we could add the counts of the two sexes: chisq.test(rowSums(Ctable)) ## ## Chi-squared test for given probabilities ## ## data: rowSums(Ctable) ## X-squared = 329.52, df = 2, p-value &lt; 2.2e-16 which strongly suggests the three morphs differ in frequency. We could then ask whether the frequencies differ by sex: chisq.test(Ctable) ## ## Pearson&#39;s Chi-squared test ## ## data: Ctable ## X-squared = 202.3, df = 2, p-value &lt; 2.2e-16 which again they do, which is not that surprising since the trait is partly sex limited, with males not expressing the polled phenotype. If there were only two horn types, polled and normal for example, then we could have considered transforming the data into the binary variable polled or not? and analysing using a glm with sex as a predictor. In doing this we have reduced the dimension of the data from \\(J=2\\) categories to a single (\\(J-1=1\\)) contrast. The motivation for the dimension reduction is obvious; if being a male increased the probability of expressing normal horns by 10%, it must by necessity reduce the probability of expressing polled horn type by 10%, because an individual cannot express both horn types simultaneously. The dimension reduction essentially constrains the probability of expressing either horn type to unity: \\[Pr(\\texttt{horn[i]}=\\textrm{normal})+Pr(\\texttt{horn[i]}=\\textrm{polled}) = 1\\] These concepts can be directly translated into situations with more than two categories where the unit sum constraint has the general form: \\[\\sum_{k=1}^{J}Pr(y_{i}=k)=1\\] For binary data we designated one category to be the success (polled) and one category to be the failure (normal) which we will call the baseline category. The latent variable in this case was the log-odds ratio of succeeding versus failing: \\[l_{i} = \\textrm{log}\\left(\\frac{Pr(\\texttt{horn[i]}=\\textrm{polled})}{Pr(\\texttt{horn[i]}=\\textrm{normal})}\\right) = \\textrm{logit}\\left(Pr(\\texttt{horn[i]}=\\textrm{polled})\\right)\\] With more than two categories we need to have \\(J-1\\) latent variables, which in the original horn type example are: \\[l_{i,\\textrm{polled}} = \\textrm{log}\\left(\\frac{Pr(\\texttt{horn[i]}=\\textrm{polled})}{Pr(\\texttt{horn[i]}=\\textrm{normal})}\\right)\\] and \\[l_{i,\\textrm{scurred}} = \\textrm{log}\\left(\\frac{Pr(\\texttt{horn[i]}=\\textrm{scurred})}{Pr(\\texttt{horn[i]}=\\textrm{normal})}\\right)\\] The two latent variables are indexed as trait, and the unit of observation (\\(i\\)) as units, as in multi-response models. As with binary models the residual variance is not identified, and can be set to any arbitrary value. For reasons that will become clearer later I like to work with the residual covariance matrix \\(\\frac{1}{J}({\\bf I}+{\\bf J})\\) where \\({\\bf I}\\) and \\({\\bf J}\\) are \\(J-1\\) dimensional identity and unit matrices, respectively. To start we will try a simple model with an intercept: IJ &lt;- (1/3) * (diag(2) + matrix(1, 2, 2)) prior = list(R = list(V = IJ, fix = 1)) m5c.1 &lt;- MCMCglmm(horn ~ trait - 1, rcov = ~us(trait):units, prior = prior, data = SShorns, family = &quot;categorical&quot;) The posterior distribution for the intercepts is shown in Figure 7.3, and the model clearly needs to be run for longer (Figure 7.3. However… Figure 7.3: Posterior distribution of fixed effects from model m5c.1: a simple multinomial logit model with intercepts only} The problem can also be represented using the contrast matrix \\({\\bf \\Delta}\\) (Bunch 1991): \\[{\\boldsymbol{\\mathbf{\\Delta}}}= \\left[ \\begin{array}{c c} -1&amp;-1\\\\ 1&amp;0\\\\ 0&amp;1\\\\ \\end{array} \\right]\\] where the rows correspond to the factor levels (normal, polled and scurred) and the columns to the two latent variables. For example column one corresponds to \\(l_{i,\\textrm{polled}}\\) which on the log scale is \\(Pr(\\texttt{horn[i]}=\\textrm{polled}) - Pr(\\texttt{horn[i]}=\\textrm{normal})\\). \\[\\textrm{exp}\\left(({\\boldsymbol{\\mathbf{\\Delta}}}{\\boldsymbol{\\mathbf{\\Delta}}}^{&#39;})^{-1}{\\boldsymbol{\\mathbf{\\Delta}}}{\\bf l}_{i}\\right) \\propto E\\left[\\begin{array}{c} Pr(\\texttt{horn[i]}=\\textrm{normal})\\\\ Pr(\\texttt{horn[i]}=\\textrm{polled})\\\\ Pr(\\texttt{horn[i]}=\\textrm{scurred}) \\end{array} \\right]\\] The residual and any random effect covariance matrices are for identifiability purposes estimated on the \\(J-1\\) space with \\({\\boldsymbol{\\mathbf{V}}}={\\boldsymbol{\\mathbf{\\Delta}}}^{&#39;}\\tilde{\\bf V}{\\boldsymbol{\\mathbf{\\Delta}}}\\) where \\(\\tilde{\\bf V}\\) is the covariance matrix estimated on the \\(J-1\\) space. To illustrate, we will rescale the intercepts as if the residual covariance matrix was zero (see Sections 4.1 and 5) and predict the expected probability for each horn type: Delta &lt;- cbind(c(-1, 1, 0), c(-1, 0, 1)) c2 &lt;- (16 * sqrt(3)/(15 * pi))^2 D &lt;- ginv(Delta %*% t(Delta)) %*% Delta Int &lt;- t(apply(m5c.1$Sol, 1, function(x) { D %*% (x/sqrt(1 + c2 * diag(IJ))) })) summary(mcmc(exp(Int)/rowSums(exp(Int)))) ## ## Iterations = 1:1000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 1000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.65604 0.01793 0.0005670 0.001479 ## [2,] 0.09957 0.01064 0.0003365 0.001348 ## [3,] 0.24439 0.01638 0.0005180 0.001503 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 0.62315 0.64412 0.65531 0.6674 0.6915 ## var2 0.07917 0.09223 0.09916 0.1066 0.1223 ## var3 0.21061 0.23397 0.24486 0.2556 0.2744 which agrees well with those observed: prop.table(rowSums(Ctable)) ## normal polled scurred ## 0.6531532 0.0975976 0.2492492 To test for the effects of sex specific expression we can also fit a model with a sex effect: m5c.2 &lt;- MCMCglmm(horn ~ trait + sex - 1, rcov = ~us(trait):units, data = SShorns, family = &quot;categorical&quot;, prior = prior) In this case we have not interacted sex with trait, and so we are estimating the difference between the sexes in their expression of normal and polled+scurred jointly. The posterior distribution is plotted in Figure 7.4 and clearly shows that males are more likely to express the normal horn phenotype than females. Figure 7.4: Posterior distribution of fixed effects from model m5c.2 in which a main effect of sex was included A more general model would be to estimate separate probabilities for each cell, but the contingency table indicates that one cell (polled males) has zero counts which will cause extreme separation problems. We could choose to have a better prior for the fixed effects, that is close to being flat for the two-way (i.e. polled vs scurred, normal vs.scurred &amp; polled vs. normal) marginal probabilities within each sex: prior$B = list(mu = rep(0, 4), V = kronecker(IJ, diag(2)) * (1.7 + pi^2/3)) m5c.3 &lt;- MCMCglmm(horn ~ at.level(sex, 1):trait + at.level(sex, 2):trait - 1, rcov = ~us(trait):units, data = SShorns, family = &quot;categorical&quot;, prior = prior) The female specific probabilities appear reasonable: Int &lt;- t(apply(m5c.3$Sol[, 1:2], 1, function(x) { D %*% (x/sqrt(1 + c2 * diag(IJ))) })) summary(mcmc(exp(Int)/rowSums(exp(Int)))) ## ## Iterations = 1:1000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 1000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.3462 0.02905 0.0009186 0.002506 ## [2,] 0.2647 0.02727 0.0008623 0.002739 ## [3,] 0.3891 0.02880 0.0009109 0.003488 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 0.2887 0.3257 0.3456 0.3671 0.4051 ## var2 0.2149 0.2459 0.2637 0.2848 0.3194 ## var3 0.3352 0.3698 0.3871 0.4093 0.4460 compared to the observed frequencies: prop.table(Ctable[, 1]) ## normal polled scurred ## 0.3401639 0.2663934 0.3934426 as do the male probabilities: Int &lt;- t(apply(cbind(m5c.3$Sol[, 3:4]), 1, function(x) { D %*% (x/sqrt(1 + c2 * diag(IJ))) })) summary(mcmc(exp(Int)/rowSums(exp(Int)))) ## ## Iterations = 1:1000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 1000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.829214 0.019460 0.0006154 0.0025721 ## [2,] 0.007697 0.003293 0.0001041 0.0009897 ## [3,] 0.163089 0.018762 0.0005933 0.0028888 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 0.786134 0.817934 0.830431 0.842241 0.86501 ## var2 0.002106 0.005246 0.007568 0.009989 0.01421 ## var3 0.128436 0.150627 0.161718 0.173435 0.20569 compared to the observed frequencies: prop.table(Ctable[, 2]) ## normal polled scurred ## 0.8341232 0.0000000 0.1658768 7.3 Zero-inflated Models Each datum in a zero-inflated model is associated with two latent variables. The first latent variable is associated with the named distribution and the second latent variable is associated with zero inflation. I’ll work through a zero-inflated Poisson (ZIP) model to make things clearer. As the name suggests, a ZIP distribution is a Poisson distribution with extra zero’s. The observed zeros are modelled as a mixture distribution of zero’s originating form the Poisson process and zero’s arising through zero-inflation. It is the probability (on the logit scale) that a zero is from the zero-inflation process that we aim to model with the second latent variable. The likelihood has the form: \\[\\begin{array}{rl} Pr(y=0) =&amp; \\texttt{plogis}(l_{2})+\\texttt{plogis}(-l_{2})\\ast \\texttt{dpois}(0, \\texttt{exp}(l_{1}))\\\\ Pr(y | y&gt;0) =&amp; \\texttt{plogis}(-l_{2})\\ast \\texttt{dpois}(y, \\texttt{exp}(l_{1}))\\\\ \\end{array}\\] \\(\\texttt{pscl}\\) fits zero-inflated models very well through the zeroinfl function, and I strongly recommend using it if you do not want to fit random effects. To illustrate the syntax for fitting ZIP models in MCMCglmm I will take one of their examples: data(&quot;bioChemists&quot;, package = &quot;pscl&quot;) head(bioChemists) ## art fem mar kid5 phd ment ## 1 0 Men Married 0 2.52 7 ## 2 0 Women Single 0 2.05 6 ## 3 0 Women Single 0 3.75 6 ## 4 0 Men Married 1 1.18 3 ## 5 0 Women Single 0 3.75 26 ## 6 0 Women Married 2 3.59 2 art is the response variable - the number of papers published by a Ph.D student - and the remaining variables are to be fitted as fixed effects. Naively, we may expect zero-inflation to be a problem given 30% of the data are zeros, and based on the global mean we only expect around 18%. table(bioChemists$art == 0) ## ## FALSE TRUE ## 640 275 ppois(0, mean(bioChemists$art)) ## [1] 0.1839859 As with binary models we do not observe any residual variance for the zero-inflated process, and in addition the residual covariance between the zero-inflation and the Poisson process cannot be estimated because both processes cannot be observed in a single data point. To deal with this I’ve fixed the residual variance for the zero-inflation at 1, and the covariance is set to zero using the idh structure. Setting V=diag(2) and nu=0.00212 we have the inverse-gamma prior with shape=scale=0.001 for the residual component of the Poisson process which captures overdispersion: prior.m5d.1 = list(R = list(V = diag(2), nu = 0.002, fix = 2)) m5d.1 &lt;- MCMCglmm(art ~ trait - 1 + at.level(trait, 1):fem + at.level(trait, 1):mar + at.level(trait, 1):kid5 + at.level(trait, 1):phd + at.level(trait, 1):ment, rcov = ~idh(trait):units, data = bioChemists, prior = prior.m5d.1, family = &quot;zipoisson&quot;) As is often the case the parameters of the zero-inflation model mixes poorly (See Figure 7.5 especially when compared to equivalent hurdle models (See Section 7.4). Poor mixing is often associated with distributions that may not be zero-inflated but instead overdispersed. Figure 7.5: Posterior distribution of fixed effects from model m5d.1 in which trait 1 (\\(\\texttt{art}\\)) is the Poisson process and trait 2 (\\(\\texttt{zi.art}\\)) is the zero-inflation. The model would have to be run for (much) longer to say something concrete about the level of zero-inflation but my guess would be it’s not a big issue, given the probability is probably quite small: quantile(plogis(m5d.1$Sol[, 2]/sqrt(1 + c2))) ## 0% 25% 50% 75% 100% ## 0.008582525 0.023880663 0.031663207 0.038768821 0.054670641 7.3.1 Posterior predictive checks Another useful check is to fit the standard Poisson model and use posterior predictive checks to see how many zero’s you would expect under the simple model: prior.m5d.2 = list(R = list(V = diag(1), nu = 0.002)) m5d.2 &lt;- MCMCglmm(art ~ fem + mar + kid5 + phd + ment, data = bioChemists, prior = prior.m5d.2, family = &quot;poisson&quot;, saveX = TRUE) nz &lt;- 1:1000 oz &lt;- sum(bioChemists$art == 0) for (i in 1:1000) { pred.l &lt;- rnorm(915, (m5d.2$X %*% m5d.2$Sol[i, ])@x, sqrt(m5d.2$VCV[i])) nz[i] &lt;- sum(rpois(915, exp(pred.l)) == 0) } Figure 7.6 shows a histogram of the posterior predictive distribution of zero’s (nz) from the model compared to the observed number of zeros (oz). The simpler model seems to be consistent with the data, suggesting that a ZIP model may not be required. Figure 7.6: Posterior predictive distribution of zeros from model m5d.2 with the observed number in red. 7.4 Hurdle Models Hurdle models are very similar to zero-inflated models but they can be used to model zero-deflation as well as zero-inflation and seem to have much better mixing properties in \\(\\texttt{MCMCglmm}\\). As in ZIP models each datum in the hurdle model is associated with two latent variables. However, whereas in a ZIP model the first latent variable is the mean parameter of a Poisson distribution the equivalent latent variable in the hurdle model is the mean parameter of a zero-truncated Possion distribution (i.e. a Poisson distribution without the zeros observed). In addition the second latent variable in a ZIP model is the probability that an observed zero is due to zero-inflation rather than the Poisson process. In hurdle models the second latent variable is simply the probability (on the logit scale) that the response variable is zero or not. The likelihood is: \\[\\begin{array}{rl} Pr(y=0) =&amp; \\texttt{plogis}(l_{2})\\\\ Pr(y | y&gt;0) =&amp; \\texttt{plogis}(-l_{2})\\ast \\texttt{dpois}(y, \\texttt{exp}(l_{1}))/(1-\\texttt{ppois}(0, \\texttt{exp}(l_{1})))\\\\ \\end{array}\\] To illustrate, we will refit the ZIP model (m5d.1) as a hurdle-Poisson model. m5d.3 &lt;- MCMCglmm(art ~ trait - 1 + at.level(trait, 1):fem + at.level(trait, 1):mar + at.level(trait, 1):kid5 + at.level(trait, 1):phd + at.level(trait, 1):ment, rcov = ~idh(trait):units, data = bioChemists, prior = prior.m5d.1, family = &quot;hupoisson&quot;) Plotting the Markov chain for the equivalent parameters that were plotted for the ZIP model shows that the mixing properties are much better (compare Figure 7.5 with Figure 7.7). Figure 7.7: Posterior distribution of fixed effects from model m5d.3 in which trait 1 (\\(\\texttt{art}\\)) is the zero-truncated Poisson process and trait 2 (\\(\\texttt{hu.art}\\)) is the binary trait zero or non-zero. The interpretation of the model is slightly different. Fitting just an intercept in the hurdle model implies that the proportion of zeros observed across different combinations of those fixed effects fitted for the Poisson process is constant. Our 95% credible intervals for this proportion is (See section 4.1): c2 &lt;- (16 * sqrt(3)/(15 * pi))^2 HPDinterval(plogis(m5d.3$Sol[, 2]/sqrt(1 + c2))) ## lower upper ## var1 0.2620345 0.3253519 ## attr(,&quot;Probability&quot;) ## [1] 0.95 and we can compare this to the predicted number of zero’s from the Poisson process if it had not been zero-truncated: HPDinterval(ppois(0, exp(m5d.3$Sol[, 1] + 0.5 * m5d.3$VCV[, 1]))) ## lower upper ## var1 0.1462289 0.3555906 ## attr(,&quot;Probability&quot;) ## [1] 0.95 The credible intervals largely overlap, strongly suggesting a standard Poisson model would be adequate. However, our prediction for the number of zero’s that would arise form a non-truncated Poisson process only involved the intercept term. This prediction therefore pertains to the number of articles published by single women with no young children who obtained their Ph.D’s from departments scoring zero for prestige (phd) and whose mentors had published nothing in the previous 3 years. Our equivalent prediction for men is a little lower HPDinterval(ppois(0, exp(m5d.3$Sol[, 1] + m5d.3$Sol[, 3] + 0.5 * m5d.3$VCV[, 1]))) ## lower upper ## var1 0.09298348 0.2890514 ## attr(,&quot;Probability&quot;) ## [1] 0.95 suggesting that perhaps the number of zero’s is greater than we expected for this group. However, this may just be a consequence of us fixing the proportion of zero’s to be constant across these groups. We can relax this assumption by fitting a separate term for the proportion of zeros for men: m5d.4 &lt;- MCMCglmm(art ~ trait - 1 + at.level(trait, 1:2):fem + at.level(trait, 1):mar + at.level(trait, 1):kid5 + at.level(trait, 1):phd + at.level(trait, 1):ment, rcov = ~idh(trait):units, data = bioChemists, prior = prior.m5d.1, family = &quot;hupoisson&quot;) which reveals that although this proportion is expected to be (slightly) smaller: HPDinterval(plogis((m5d.4$Sol[, 2] + m5d.4$Sol[, 4])/sqrt(1 + c2))) ## lower upper ## var1 0.2247536 0.3008584 ## attr(,&quot;Probability&quot;) ## [1] 0.95 the proportion of zeros expected for men is probably still less than what we expect from a non-truncated Poisson process for which the estimates have changed very little: HPDinterval(ppois(0, exp(m5d.4$Sol[, 1] + m5d.4$Sol[, 3] + 0.5 * m5d.4$VCV[, 1]))) ## lower upper ## var1 0.07168532 0.2473888 ## attr(,&quot;Probability&quot;) ## [1] 0.95 This highlights one of the disadvantages of hurdle models. If explanatory variables have been fitted that affect the expectation of the Poisson process then this implies that the proportion of zero’s observed will also vary across these same explanatory variables, even in the absence of zero-inflation. It may then be necessary to fit an equally complicated model for both processes even though a single parameter would suffice in a ZIP model. However, in the absence of zero-inflation the intercept of the zero-inflation process in a ZIP model is \\(-\\infty\\) on the logit scale causing numerical and inferential problems. An alternative type of model are zero-altered models. 7.5 Zero-altered Models Zero-altered Poisson (ZAP) models are identical to Poisson-hurdle models except a complementary log-log link is used instead of the logit link when modelling the proportion of zeros. However for reasons that will become clearer below, the zero-altered process (za) is predicting non-zeros as opposed to the ZIP and hurdle-Poisson models where it is the number of zeros. The likelihood is: \\[\\begin{array}{rl} Pr(y=0) =&amp; 1-\\texttt{pexp}(\\texttt{exp}(l_{2}))\\\\ Pr(y | y&gt;0) =&amp; \\texttt{pexp}(\\texttt{exp}(l_{2}))\\ast \\texttt{dpois}(y, \\texttt{exp}(l_{1}))/(1-\\texttt{ppois}(0, \\texttt{exp}(l_{1})))\\\\ \\end{array}\\] since the inverse of the complementary log-log transformation is the distribution function of the extreme value (log-exponential) distribution. It happens that \\(\\texttt{ppois}(0,\\texttt{exp}(l)) = \\texttt{dpois}(0,\\texttt{exp}(l)) = 1-\\texttt{pexp}(\\texttt{exp}(l))\\) so that if \\(l = l_{1} = l_{2}\\) then the likelihood reduces to: \\[\\begin{array}{rl} Pr(y=0) =&amp; \\texttt{dpois}(0,\\texttt{exp}(l))\\\\ Pr(y | y&gt;0) =&amp; \\texttt{dpois}(y, \\texttt{exp}(l))\\\\ \\end{array}\\] which is equivalent to a standard Poisson model. We can then test for zero-flation by constraining the overdispersion to be the same for both process using a trait by units interaction in the R-structure, and by setting up the contrasts so that the zero-altered regression coefficients are expressed as differences from the Poisson regression coefficients. When this difference is zero the variable causes no zero-flation, when it is negative it causes zero-inflation and when it is positive it causes zero-deflation: m5d.5 &lt;- MCMCglmm(art ~ trait * (fem + mar + kid5 + phd + ment), rcov = ~trait:units, data = bioChemists, family = &quot;zapoisson&quot;) summary(m5d.5) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 3038.092 ## ## R-structure: ~trait:units ## ## post.mean l-95% CI u-95% CI eff.samp ## trait:units 0.3691 0.2581 0.4886 48.11 ## ## Location effects: art ~ trait * (fem + mar + kid5 + phd + ment) ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 0.33666 0.02381 0.68347 155.4 0.054 . ## traitza_art -0.53585 -1.03963 -0.06203 211.8 0.038 * ## femWomen -0.19986 -0.36348 -0.00470 256.5 0.028 * ## marMarried 0.08948 -0.08710 0.27455 309.5 0.374 ## kid5 -0.13506 -0.25609 -0.01902 138.4 0.038 * ## phd 0.01468 -0.06209 0.09118 309.4 0.728 ## ment 0.01959 0.01249 0.02624 427.8 &lt;0.001 *** ## traitza_art:femWomen 0.02809 -0.25661 0.32862 172.5 0.792 ## traitza_art:marMarried 0.13720 -0.16182 0.47253 175.9 0.382 ## traitza_art:kid5 -0.06897 -0.27442 0.12546 194.6 0.516 ## traitza_art:phd 0.01401 -0.13229 0.14663 202.7 0.854 ## traitza_art:ment 0.02817 0.01086 0.04358 114.8 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 we can see from this that the more papers a mentor produces, the more zero-deflation (or conversely the less papers a mentor produces, the more zero-inflation). References "],["pedigree.html", "8 Pedigrees and Phylogenies 8.1 Pedigree and phylogeny formats 8.2 The animal model and the phylogenetic mixed model", " 8 Pedigrees and Phylogenies Pedigrees and phylogenies are similar things: they are both ways of representing shared ancestry. Under a quantitative genetic model of inheritance, or a Brownian motion model of evolution, GLMM’s can be readily extended to model the similarities that exist between the phenotypes of related individuals or taxa. In the context of quantitative genetics these models are known as ‘animal’ models (Henderson 1976), and in the context of the comparative method these models are known as phylogenetic mixed models (Lynch 1991). The two models are almost identical, and are relatively minor modifications to the basic mixed model (Hadfield and Nakagawa 2010). 8.1 Pedigree and phylogeny formats 8.1.1 Pedigrees \\(\\texttt{MCMCglmm}\\) handles pedigrees stored in 3-column tabular form, with each row representing a single individual. The first column should contain the unique identifier of the individual, and columns 2 and 3 should be the unique identifiers of the individual’s parents. Parents must appear before their offspring in the table. I usually have the dam (mother) in the first column and the sire (father) in the third. I prefer the words dam and sire because if I subscript things with \\(m\\) and \\(f\\) I can never remember whether I mean male and female, or mother and father. In hermaphrodite systems the same individual may appear in both columns, even within the same row if an individual was produced through selfing. This is not a problem, but \\(\\texttt{MCMCglmm}\\) will issue a warning in case hermaphrodites are not present and a data entry mistake has been made. Impossible pedigrees (for example individual’s that give birth to their own mother) are a problem and \\(\\texttt{MCMCglmm}\\) will issue an error, hopefully with an appropriate message, when impossibilities are detected. If the parent(s) of an individual are unknown then a missing value (NA) should be assigned in the relevant column. All individuals appearing as dams or sires need to have their own record, even if both of their parents are unknown. Often the number of individuals in a pedigree will be greater than the number of individuals for which phenotypic data exist. \\(\\texttt{MCMCglmm}\\) can handle this, as long as all the individuals appearing in the data frame passed to data also appear in the pedigree. To illustrate, we can load a pedigree for a population of blue tits and display the pedigree for the nuclear family that has individuals \\(\\texttt{R187920}\\) and \\(\\texttt{R187921}\\) as parents: data(BTped) Nped &lt;- BTped[which(apply(BTped, 1, function(x) { any(x == &quot;R187920&quot; | x == &quot;R187921&quot;) })), ] Nped ## animal dam sire ## 66 R187920 &lt;NA&gt; &lt;NA&gt; ## 172 R187921 &lt;NA&gt; &lt;NA&gt; ## 325 R187726 R187920 R187921 ## 411 R187724 R187920 R187921 ## 503 R187723 R187920 R187921 ## 838 R187613 R187920 R187921 ## 932 R187612 R187920 R187921 ## 1030 R187609 R187920 R187921 Both parents form part of what is known as the base population - they are outbred and unrelated to anybody else in the pedigree. \\(\\texttt{MCMCglmm}\\) and MasterBayes have several pedigree manipulation functions. (MasterBayes::orderPed) orders a pedigree so parents appear before their offspring, (MasterBayes::insertPed) inserts records for individuals that only appear as parents (or a vector of specified individuals). When the number of individuals with phenotypic data is less than the number of individuals in the pedigree it is sometimes possible to remove uninformative individuals from the pedigree and thus reduce the computation time. This is known as pruning the pedigree and is implemented in the \\(\\texttt{MCMCglmm}\\) function prunePed. A vector of measured individuals is specified in the argument keep and specifying make.base=TRUE implements the most complete pruning. Note, make.base=FALSE is the default argument so you’ll need to explicitly specify TRUE in the call to prunePed. Michael Morrissey’s pedantics package, and the kinship2 package also have many other useful pedigree orientated functions. In fact, the orderPed function in MasterBayes is built around functions provided by kinship2. 8.1.2 Phylogenies Phylogenies can be expressed in tabular form, although only two columns are required because each species only has a single parent. In general however, phylogenies are not expressed in this form presumably because it is hard to traverse phylogenies (and pedigrees) backwards in time when they are stored this way. For phylogenetic mixed models we generally only need to traverse phylogenies forward in time (if at all) but I have stuck with convention and used the phylo class from the ape package to store phylogenies. As with pedigrees, all species appearing in the data frame passed to data need to appear in the phylogeny. Typically, this will only include species at the tips of the phylogeny and so the measured species should appear in the tip.label element of the phylo object. An error message will be issued if this is not the case. Data may also exist for ancestral species, or even for species present at the tips but measured many generations before. It is possible to include these data as long as the phylogeny has labelled internal nodes. If nodes are unlabelled then \\(\\texttt{MCMCglmm}\\) names them internally using the default arguments of makeNodeLabel from ape. To illustrate, lets take the phylogeny of bird families included in the ape package, and extract the phylogeny in tabular form for the Paridae (Tits), Certhiidae (Treecreepers), Gruidae (Cranes) and the Struthionidae (Ostriches): data(&quot;bird.families&quot;) bird.families &lt;- makeNodeLabel(bird.families) some.families &lt;- c(&quot;Certhiidae&quot;, &quot;Paridae&quot;, &quot;Gruidae&quot;, &quot;Struthionidae&quot;) Nphylo &lt;- drop.tip(bird.families, setdiff(bird.families$tip.label, some.families)) INphylo &lt;- inverseA(Nphylo) INphylo$pedigree ## node.names ## [1,] &quot;Node58&quot; NA NA ## [2,] &quot;Node122&quot; &quot;Node58&quot; NA ## [3,] &quot;Struthionidae&quot; NA NA ## [4,] &quot;Gruidae&quot; &quot;Node58&quot; NA ## [5,] &quot;Certhiidae&quot; &quot;Node122&quot; NA ## [6,] &quot;Paridae&quot; &quot;Node122&quot; NA (#fig:bird.families)A phylogeny of bird families from The families in red are the Tits (Paridae), Treecreepers (Certhiidae), Cranes (Gruidae) and the Ostriches (Struthionidae) from top to bottom. Blue tits are in the Paridae, and the word pedigree comes from the french for crane’s foot. The full phylogeny, with these families and their connecting notes displayed, is shown in Figure @ref(fig:bird.families). You will notice that \\(\\texttt{Node1}\\) - the root - does not appear in the phylogeny in tabular form. This is because the root is equivalent to the base population in a pedigree analysis, an issue which we will come back to later. Another piece of information that seems to be lacking in the tabular form is the branch length information. Branch lengths are equivalent to inbreeding coefficients in a pedigree. As with pedigrees the inbreeding coefficients are calculated by inverseA: INphylo$inbreeding ## [1] 0.2285714 0.3857143 1.0000000 0.7714286 0.3857143 0.3857143 You will notice that the Struthionidae have an inbreeding coefficient of 1 because we used the default scale=TRUE in the call to inverseA. Only ultrametric trees can be scaled in \\(\\texttt{MCMCglmm}\\) and in this case the sum of the inbreeding coefficients connecting the root to a terminal node is one. To take the Paridae as an example: sum(INphylo$inbreeding[which(INphylo$pedigree[, 1] %in% c(&quot;Paridae&quot;, &quot;Node122&quot;, &quot;Node58&quot;))]) ## [1] 1 The inbreeding coefficients for the members of the blue tit nuclear family are of course all zero: inverseA(Nped)$inbreeding ## [1] 0 0 0 0 0 0 0 0 8.2 The animal model and the phylogenetic mixed model The structure of pedigrees and phylogenies can be expressed in terms of the relatedness matrix \\({\\bf A}\\). This matrix is symmetric, square, and has dimensions equal to the number of individuals in the pedigree (or the number of taxa in the phylogeny). For pedigrees, element \\(A_{i,j}\\) is twice the probability that an allele drawn from individual \\(i\\) is identical by descent to an allele in individual \\(j\\). For phylogenies, element \\(A_{i,j}\\) is the amount of time that elapsed (since the common ancestor of all sampled taxa) before the speciation event that resulted in taxa \\(i\\) and \\(j\\). Simple, but perhaps slow, recursive methods exist for calculating \\({\\bf A}\\) in both cases: Aped &lt;- 2 * kinship2::kinship(Nped[, 1], Nped[, 2], Nped[, 3]) Aped ## R187920 R187921 R187726 R187724 R187723 R187613 R187612 R187609 ## R187920 1.0 0.0 0.5 0.5 0.5 0.5 0.5 0.5 ## R187921 0.0 1.0 0.5 0.5 0.5 0.5 0.5 0.5 ## R187726 0.5 0.5 1.0 0.5 0.5 0.5 0.5 0.5 ## R187724 0.5 0.5 0.5 1.0 0.5 0.5 0.5 0.5 ## R187723 0.5 0.5 0.5 0.5 1.0 0.5 0.5 0.5 ## R187613 0.5 0.5 0.5 0.5 0.5 1.0 0.5 0.5 ## R187612 0.5 0.5 0.5 0.5 0.5 0.5 1.0 0.5 ## R187609 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.0 Aphylo &lt;- vcv.phylo(Nphylo, cor = T) Aphylo ## Struthionidae Gruidae Certhiidae Paridae ## Struthionidae 1 0.0000000 0.0000000 0.0000000 ## Gruidae 0 1.0000000 0.2285714 0.2285714 ## Certhiidae 0 0.2285714 1.0000000 0.6142857 ## Paridae 0 0.2285714 0.6142857 1.0000000 Note that specifying cor=T is equivalent to scaling the tree as we did in the argument to inverseA. In fact, all of the mixed models we fitted in earlier sections also used an \\({\\bf A}\\) matrix, but in those cases the matrix was an identity matrix (i.e. \\({\\bf A}={\\bf I}\\)) and we didn’t have to worry about it. Let’s reconsider the Blue tit model m3a.1 from Section 5 where we were interested in estimating sex effects for tarsus length together with the amount of variance explained by genetic mother (dam) and foster mother (fosternest): data(BTdata) m3a.1 &lt;- MCMCglmm(tarsus ~ sex, random = ~dam + fosternest, data = BTdata, verbose = FALSE) All individuals that contributed to that analysis are from a single generation and appear in BTped together with their parents. However, individuals in the parental generation do not have tarsus length measurements so they do not have their own records in BTdata. The model can be expressed as: \\[{\\bf y} = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}_{1}{\\bf u}_{1}+{\\bf Z}_{2}{\\bf u}_{2}+{\\bf e}\\] where the design matrices contain information relating each individual to a sex (\\({\\bf X}\\)) a dam (\\({\\bf Z}_{1}\\)) and a fosternest(\\({\\bf Z}_{2}\\)). The associated parameter vectors (\\({\\boldsymbol{\\mathbf{\\beta}}}\\), \\({\\bf u}_{1}\\) and \\({\\bf u}_{2}\\)) are the effects of each sex, mother and fosternest on tarsus length, and \\({\\bf e}\\) is the vector of residuals. In the model, the \\(u\\)’s are treated as random so we estimate their variance instead of fixing it in the prior at some (large) value, as we did with the \\(\\beta\\)’s. We can be a little more explicit about what this means: \\[{\\bf u}_{1} \\sim N({\\bf 0},\\ {\\bf I}\\sigma^{2}_{1})\\] where \\(\\sim\\) stands for ‘is distributed as’ and \\(N\\) a (multivariate) normal distribution. The distribution has two sets of parameters; a vector of means and a covariance matrix. We assume the random effects are deviations around the fixed effect part of the model and so they have a prior expectation of zero. The (co)variance matrix of the random effects is \\({\\bf I}\\sigma^{2}_{1}\\) where \\(\\sigma^{2}_{1}\\) is the variance component to be estimated. The use of the identity matrix makes two things explicit. First, because all off-diagonal elements of an identity matrix are zero we are assuming that all dam effects are independent (no covariance exists between any two dam effects). Second, all diagonal elements of an identity matrix are 1 implying that the range of possible values the dam effect could take is equivalent for every dam, this range being governed by the magnitude of the variance component. Since dam’s have very little interaction with the subset of offspring that were moved to a fosternest, we may be willing to assume that any similarity that exists between the tarsus lengths of this susbset and the subset that remained at home must be due to genetic effects. Although not strictly true we can assume that individuals that shared the same dam also shared the same sire, and so share around 50% of their genes. References "],["parameter-expansion.html", "9 Parameter Expansion", " 9 Parameter Expansion As the covariance matrix approaches a singularity the mixing of the chain becomes notoriously slow. This problem is often encountered in single-response models when a variance component is small and the chain becomes stuck at values close to zero. Similar problems occur for the EM algorithm and C. H. Liu, Rubin, and Wu (1998) introduced parameter expansion to speed up the rate of convergence. The idea was quickly applied to Gibbs sampling problems (J. S. Liu and Wu 1999) and has now been extensively used to develop more efficient mixed-model samplers (e.g. Dyk and Meng 2001; Gelman et al. 2008; Browne et al. 2009). The columns of the design matrix (\\({\\bf W}\\)) can be multiplied by the non-identified working parameters \\({\\boldsymbol{\\mathbf{\\alpha}}} = \\left[1,\\ \\alpha_{1},\\ \\alpha_{2},\\ \\dots \\alpha_{k}\\right]^{&#39;}\\): \\[{\\bf W}_{\\alpha} = \\left[{\\bf X}\\ {\\bf Z}_{1}\\alpha_{1}\\ {\\bf Z}_{2}\\alpha_{2}\\ \\dots\\ {\\bf Z}_{k}\\alpha_{k}\\right] \\label{wstar} \\tag{9.1}\\] where the indices denote submatrices of \\({\\bf Z}\\) which pertain to effects associated with the same variance component. Replacing \\({\\bf W}\\) with \\({\\bf W}_{\\alpha}\\) we can sample the new location effects \\(\\boldsymbol{\\theta}_{\\alpha}\\) as described above, and rescale them to obtain \\(\\boldsymbol{\\theta}\\): \\[\\boldsymbol{\\theta} = ({\\bf I}_{\\beta}\\oplus_{i=1}^{k}{\\bf I}_{u_{i}}\\ \\alpha_{i})\\boldsymbol{\\theta}_{\\alpha}\\] where the identity matrices are of dimension equal to the length of the subscripted parameter vectors. Likewise, the (co)variance matrices can be rescaled by the set of \\(\\alpha\\)’s associated with the variances of a particular variance structure component (\\({\\boldsymbol{\\mathbf{\\alpha}}}_{\\mathcal{V}}\\)): \\[{\\bf V} = Diag({\\boldsymbol{\\mathbf{\\alpha}}}_{\\mathcal{V}}){\\bf V}_{\\alpha}Diag({\\boldsymbol{\\mathbf{\\alpha}}}_{\\mathcal{V}})\\] The working parameters are not identifiable in the likelihood, but do have a proper conditional distribution. Defining the \\(n\\times(k+1)\\) design matrix \\({\\bf X}_{\\alpha}\\) with each column equal to the submatrices in Equation (9.1) postmultiplied by the relevant subvectors of \\(\\boldsymbol{\\theta}_{\\alpha}\\), we can see that \\({\\boldsymbol{\\mathbf{\\alpha}}}\\) is a vector of regression coefficients: \\[\\begin{array}{rl} \\bf{l} =&amp; {\\bf X}_{\\alpha}{\\boldsymbol{\\mathbf{\\alpha}}}+\\bf{e}\\\\ \\end{array}\\] and so the methods described above can be used to update them. 9.0.1 Variances close to zero To use parameter exapnsion in \\(\\texttt{MCMCglmm}\\) it is necessary to specify a prior covariance matrix for \\({\\boldsymbol{\\mathbf{\\alpha}}}\\) which is non-null. In section I discuss what this prior means in the context of posterior inference but for now we will specify two models, one parameter expanded and the other not. To illustrate I will fit a model that estimates the between mother variation in offsping sex ratio using parameter expansions: data(BTdata) BTdata$sex[which(BTdata$sex == &quot;UNK&quot;)] &lt;- NA BTdata$sex &lt;- gdata::drop.levels(BTdata$sex) # treat unknowns as missing data prior1b = list(R = list(V = 1, fix = 1), G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000))) m7b.1 &lt;- MCMCglmm(sex ~ 1, random = ~dam, data = BTdata, family = &quot;categorical&quot;, prior = prior1b) and fit a model that does not use parameter expansion: prior2b = list(R = list(V = 1, fix = 1), G = list(G1 = list(V = 1e-10, nu = -1))) m7b.2 &lt;- MCMCglmm(sex ~ 1, random = ~dam, data = BTdata, family = &quot;categorical&quot;, prior = prior2b) The prior densities in the two models are very similar across the range of variances with reasonable posterior support, and running the models for long enough will verify that they are sampling from very similar posterior densities. However, the mixing properties of the two chains are very different, with the non-parameter expanded chain (in red) getting stuck at values close to zero more often (Figure 9.1). plot(mcmc.list(m7b.1$VCV[, 1], m7b.2$VCV[, 1])) Figure 9.1: Traces of the sampled posterior distribution for between female variance in sex ratio. The black trace is from a parameter expanded model, and the red trace from a non-parameter expanded model. The parameter expanded model is 25% slower per iteration but the effective sample size is 2.795 times greater: effectiveSize(m7b.1$VCV[, 1]) ## var1 ## 186.9935 effectiveSize(m7b.2$VCV[, 1]) ## var1 ## 66.90871 9.0.2 Parameter expanded priors The original aim of applying parameter expanded methods to Gibbs sampling was to speed up the convergence and mixing properties of the chain. They achieve this by introducing parameters that are not identified in the likelihood, and for which all information comes from the prior distribution. By placing priors on these parameters we can induce different prior distributions for the variance components. These priors are all from the non-central scaled F-distribution, which implies the prior for the standard deviation is a non-central folded scaled t-distribution (Gelman 2006). To use parameter expansion it is necessary to specify the prior means (alpha.mu) and prior covariance matrix (alpha.V) in the prior. Without loss of generality V can be set to one, so that the prior for the variance (v) has density function: df(v/alpha.V, df1 = 1, df2 = nu, ncp = (alpha.mu^2)/alpha.V) and the prior for the standard deviation: 2 * dt(sqrt(v)/sqrt(alpha.V), df = nu, ncp = alpha.mu/sqrt(alpha.V)) where v\\(&gt;0\\). To illustrate I’ll use the original Schools example from (Gelman 2006) schools &lt;- data.frame(school = letters[1:8], estimate = c(28.39, 7.94, -2.75, 6.82, -0.64, 0.63, 18.01, 12.16), sd = c(14.9, 10.2, 16.3, 11, 9.4, 11.4, 10.4, 17.6)) head(schools) ## school estimate sd ## 1 a 28.39 14.9 ## 2 b 7.94 10.2 ## 3 c -2.75 16.3 ## 4 d 6.82 11.0 ## 5 e -0.64 9.4 ## 6 f 0.63 11.4 The response variable estimate is the relative effect of Scholastic Aptitude Test coaching programs in 8 schools, and sd are the standard errors of the estimate. In the original example Gelman focused on the standard deviation of the between school effects and so we will place an improper flat prior on the standard deviation: prior1 &lt;- list(R = list(V = diag(schools$sd^2), fix = 1), G = list(G1 = list(V = 1e-10, nu = -1))) m7a.1 &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior1) In this example there is information on the between school variance although we only have a single estimate for each school. This is possible because the within school variance was available for each school and we were able to fix the residual variance for each school at this value (See Section 6.4). The posterior distribution of the between school standard deviation is shown in Figure 9.2 with the flat prior shown as a solid line. Figure 9.2: Between school standard deviation in educational test scores, with an improper uniform prior We can also use the inverse-gamma prior with scale and shape equal to 0.001: prior2 &lt;- list(R = list(V = diag(schools$sd^2), fix = 1), G = list(G1 = list(V = 1, nu = 0.002))) m7a.2 &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior2) but Figure 9.3 indicates that such a prior in this context may put too much density and values close to zero. Figure 9.3: Between school standard deviation in educational test scores, with an inverse-gamma prior with shape and scale set to 0.001 For the final prior we have V=1, nu=1, alpha.mu=0 which is equivalent to a proper Cauchy prior for the standard deviation with scale equal to sqrt(alpha.V). Following Gelman.2006 we use a scale of 25: prior3 &lt;- list(R = list(V = diag(schools$sd^2), fix = 1), G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 25^2))) m7a.3 &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior3) and Figure 9.4 shows that the prior may have better properties than the inverse-gamma, and that the posterior is less distorted. Figure 9.4: Between school standard deviation in educational test scores, with a Cauchy prior with a scale of 25. 9.0.3 Binary response models When analysing binary responses the residual variance is not identified in the likelihood and without a prior the posterior is improper. If a weak prior is placed on the residual variance then the chain appears to mix poorly and the MCMC output often looks terrible. However, this poor mixing is in some ways superficial. As discussed in Section 5 we can rescale the location effects and variances by the estimated residual variance to obtain the posterior distribution for some fixed value of the actual residual variance. For example, we can refit the sex ratio model using a residual variance fixed at ten rather than one: prior3b = list(R = list(V = 10, fix = 1), G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000))) m7b.3 &lt;- MCMCglmm(sex ~ 1, random = ~dam, data = BTdata, family = &quot;categorical&quot;, prior = prior3b) The two models appear to give completely different posteriors (Figure 9.5) plot(mcmc.list(m7b.1$VCV[, 1], m7b.3$VCV[, 1])) Figure 9.5: Between mother variation in sex ratio with the residual variance fixed at 1 (black trace) and 10 (red trace). but rescaling indicates that they are very similar: c2 &lt;- (16 * sqrt(3)/(15 * pi))^2 plot(mcmc.list(m7b.1$VCV[, 1]/(1 + c2 * m7b.1$VCV[, &quot;units&quot;]), m7b.3$VCV[, 1]/(1 + c2 * m7b.3$VCV[, &quot;units&quot;]))) Figure 9.6: Between mother variation in sex ratio with the residual variance fixed at 1 (black trace) and 10 (red trace) but with both estimates rescaled to what would be observed under no residual variance. The prior specification for the between mother variance is different in the two models but Figure 9.6 suggests that the difference has little influence. However, the mixing properties of the second chain are much better (Dyk and Meng 2001): effectiveSize(m7b.1$VCV[, 1]/(1 + c2 * m7b.1$VCV[, &quot;units&quot;])) ## var1 ## 186.9935 effectiveSize(m7b.3$VCV[, 1]/(1 + c2 * m7b.3$VCV[, &quot;units&quot;])) ## var1 ## 721.58 Although the chain mixes faster as the residual variance is set to be larger, numerical problem are often encountered because the latent variables can take on extreme values. For most models a variance of 1 is safe, but care needs to be taken so that the absolute value of the latent variable is less than 20 in the case of the logit link and less than 7 for the probit link. If the residual variance is not fixed but has an alternative proper prior placed on it then the Metropolis-Hastings proposal distribution for the latent variables may not be well suited to the local properties of the conditional distribution and the acceptance ratio may fluctuate widely around the optimal 0.44. This can be fixed by using the slice sampling methods outlined in Damien, Wakefield, and Walker (1999) by passing slice=TRUE to \\(\\texttt{MCMCglmm}\\). Slice sampling can also be more efficient even if the prior is fixed at some value: m7b.4 &lt;- MCMCglmm(sex ~ 1, random = ~dam, data = BTdata, family = &quot;categorical&quot;, prior = prior3b, slice = TRUE) effectiveSize(m7b.4$VCV[, 1]/(1 + c2 * m7b.3$VCV[, &quot;units&quot;])) ## var1 ## 851.0796 References "],["path.html", "10 Path Analysis &amp; Antedependence Structures 10.1 Path Anlaysis 10.2 Antedependence 10.3 Scaling", " 10 Path Analysis &amp; Antedependence Structures There are many situations where it would seem reasonable to put some aspect of a response variable in as a predictor, and the only thing that stops us is some (often vague) notion that this is a bad thing to do from a statistical point of view. The approach appears to have a long history in economics but I came across the idea in a paper written by Gianola and Sorensen (2004). The notation of this section, and indeed the sampling strategy employed in MCMCglmm is derived from this paper. 10.1 Path Anlaysis \\(\\boldsymbol{\\Psi}^{(l)}\\) is a square matrix of dimension \\(N \\times N\\) where \\(\\Psi_{i,j}^{(l)}=k\\) sets up the equation \\(y_{i} = \\lambda_{l}ky_{j} \\dots\\). In matrix terms: \\[\\boldsymbol{\\Lambda}{\\bf y} = {\\bf y} - \\sum_{l}\\boldsymbol{\\Psi}^{(l)}{\\bf y}{\\lambda_{l}}\\] Having \\(\\boldsymbol{\\Psi} = \\left[\\boldsymbol{\\Psi}^{(1)}\\ \\boldsymbol{\\Psi}^{(2)}\\ \\dots \\boldsymbol{\\Psi}^{(L-1)}\\ \\boldsymbol{\\Psi}^{(L)}\\right]\\) we have \\[\\begin{array}{rl} \\boldsymbol{\\Lambda}{\\bf y} =&amp; {\\bf y}-\\boldsymbol{\\Psi}\\left({\\bf I}_{L} \\otimes {\\bf y} \\right)\\boldsymbol{\\Lambda}\\\\ =&amp; {\\bf y}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{N}\\right) {\\bf y}\\\\ \\end{array} \\label{rs-Eq} \\tag{10.1}\\] where \\(\\boldsymbol{\\Lambda} = \\left[\\lambda_{1}\\ \\lambda_{2} \\dots \\lambda_{L-1}\\ \\lambda_{L} \\right]^{\\top}\\), and \\(\\boldsymbol{\\Lambda}={\\bf I}_{N}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{N}\\right)\\) Each \\(\\boldsymbol{\\Psi}^{(l)}\\) can be formed using the function sir which takes two formulae. \\(\\boldsymbol{\\Psi} = {\\bf X}_{1}{\\bf X}_{2}^{\\top}\\) where \\({\\bf X}_{1}\\) and \\({\\bf X}_{2}\\) are the model matrices defined by the formulae (with intercept removed). \\({\\bf X}_{1}\\) and \\({\\bf X}_{2}^{\\top}\\) have to be conformable, and although this could be achieved in many ways, one way to ensure this is to have categorical predictors in each which have common factor levels. To give a concrete example, lets take a sample of individuals measured a variable number of times for 2 traits: id &lt;- sample(1:100, 100, replace = T) y1 &lt;- rnorm(100) y2 &lt;- rnorm(100) y &lt;- c(y1, y2) trait &lt;- gl(2, 100) Lets then imagine that each of these individuals interacts with another randomly chosen individual - indexed in the vector id1 id1 &lt;- sample(id, 100, replace = T) id &lt;- as.factor(c(id, id)) id1 &lt;- factor(c(id1, id1), levels = levels(id)) we will adopt a recursive model where by the phenotypes of individuals in the id1 vector affect those in the id vector: Psi &lt;- sir(~id1, ~id) we can see that the first record for individual id[1]=92 is directly affected by individual id1[1]=22’s traits: Psi[1, which(id == id1[1])] ## 35 135 ## 1 1 i.e individual id1[1]=22 has 2 records. We can build on this simple model by stating that only trait 2 affects trait 1: Psi &lt;- sir(~id1:at.level(trait, 1), ~id:at.level(trait, 2)) Psi[c(1, 101), which(id == id1[1])] ## 35 135 ## 1 0 1 ## 101 0 0 or that trait 2 affect both trait 2 and trait 1: Psi &lt;- sir(~id1, ~id:at.level(trait, 2)) Psi[c(1, 101), which(id == id1[1])] ## 35 135 ## 1 0 1 ## 101 0 1 my.data &lt;- data.frame(y1 = y1, y2 = y2, id = id[1:100], id1 = id1[1:100], x = rnorm(100)) m1 &lt;- MCMCglmm(y1 ~ x + sir(~id1, ~id) + y2, data = my.data) One problem is that \\({\\bf e}^{\\star}\\) the residual vector that appears in the likelihood for the latent variable does not have a simple (block) diagonal structure when (as in the case above) the elements of the response vector that are regressed on each other are not grouped in the R-structure: \\[{\\bf e}^{\\star} \\sim N\\left({\\boldsymbol{\\mathbf{0}}}, \\boldsymbol{\\Lambda}^{-1}{\\bf R}\\boldsymbol{\\Lambda}^{-\\top}\\right)\\] Consequently, analyses that involve latent variables (i.e. non-Gaussian data, or analyses that have incomplete records for determining the R-structure) are currently not implemented in MCMCglmm. The path function is a way of specifying path models that are less general than those formed by sir but are simple enough to allow updating of the latent variables associated with non-Gaussian data. Imagine a residual structure is fitted where the \\(N\\) observations are grouped into \\(n\\) blocks of \\(k\\). For instance this might be \\(k\\) different characteristics measured in \\(n\\) individuals. A path model may be entertained whereby an individual’s characteristics only affect their own characteristics rather than anyone else’s. In this case, \\(\\boldsymbol{\\Psi}^{(l)}=\\boldsymbol{\\Psi}^{(l)}\\otimes{\\bf I}_{n}\\) is block diagonal, and \\(\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}\\otimes{\\bf I}_{n}\\) where \\(\\boldsymbol{\\Psi} = \\left[\\boldsymbol{\\Psi}^{(1)}, \\boldsymbol{\\Psi}^{(2)} \\dots \\boldsymbol{\\Psi}^{(L)}\\right]\\). Consequently, \\[\\begin{array}{rl} \\boldsymbol{\\Lambda}=&amp;{\\bf I}_{N}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{N}\\right)\\\\ =&amp;{\\bf I}_{N}-\\left(\\boldsymbol{\\Psi}\\otimes{\\bf I}_{n}\\right)\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{N}\\right)\\\\ =&amp;\\left({\\bf I}_{k}\\otimes{\\bf I}_{n}\\right)-\\left(\\boldsymbol{\\Psi}\\otimes{\\bf I}_{n}\\right)\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{k}\\otimes{\\bf I}_{n}\\right)\\\\ =&amp;\\left({\\bf I}_{k}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{k}\\right)\\right)\\otimes{\\bf I}_{n}\\\\ \\end{array}\\] and so \\(\\boldsymbol{\\Lambda}^{-1}=\\left({\\bf I}_{k}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{k}\\right)\\right)^{-1}\\otimes{\\bf I}_{n}\\) and \\(|\\boldsymbol{\\Lambda}| = |{\\bf I}_{k}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{k}\\right)|^{n}\\) Mean-centring responses can help mixing, because \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\lambda}\\) are not sampled in a block. (Jarrod - I guess when \\(|\\boldsymbol{\\Lambda}|=1\\) this could be detected and updating occur in a block?) 10.2 Antedependence An \\(n\\times n\\) unstructured covariance matrix can be reparameterised in terms of regression coefficients and residual variances from a set of \\(n\\) nested multiple regressions. For example, for \\(n=3\\) the following 3 multiple regressions can be defined: \\[\\begin{array}{rl} u_{3} =&amp; u_{2}\\beta_{3|2}+u_{1}\\beta_{3|1}+e_{u_{3}}\\\\ u_{2} =&amp; u_{1}\\beta_{2|1}+e_{u_{2}}\\\\ u_{1} =&amp; e_{u_{1}}\\\\ \\end{array}\\] Arranging the regression coefficients and residual (‘innovation’) variances into a lower triangular matrix and diagonal matrix respectively: \\[{\\bf L}_{u}= \\left[ \\begin{array}{ccc} 1&amp;0&amp;0\\\\ -\\beta_{2|1}&amp;1&amp;0\\\\ -\\beta_{3|1}&amp;-\\beta_{3|2}&amp;1\\\\ \\end{array} \\right]\\] and \\[{\\bf D}_{u}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{e_{u_{1}}}&amp;0&amp;0\\\\ 0&amp;\\sigma^{2}_{e_{u_{2}}}&amp;0\\\\ 0&amp;0&amp;\\sigma^{2}_{e_{u_{3}}}\\\\ \\end{array} \\right]\\] gives \\[{\\bf V}_{u} = {\\bf L}_{u}{\\bf D}_{u}{\\bf L}_{u}^{\\top}\\] Rather than fit the saturated model (in this case all 3 regression coefficients) \\(k^{th}\\) order antedependence models seek to model \\({\\bf V}_{u}\\) whilst constraining the regression coefficients in \\({\\bf L}_{u}\\) to be zero if they are on sub-diagonals. For example, a first order antedependence model would set the regression coefficients in the second off-diagonal (i.e. \\(\\beta_{3|1}\\)) to zero, but estimate those in the first sub-diagonal (i.e. \\(\\beta_{2|1}\\) and \\(\\beta_{3|2}\\)). For a \\(3\\times3\\) matrix, a second order antedependence model would fit a fully unstructured covariance matrix. In terms of Gibbs sampling this parameterisation is less efficient because \\({\\bf V}_{u}\\) is sampled in two blocks (the regression coefficients followed by the innovation variances) rather than in a single block from the inverse Wishart. However, more flexible conjugate prior specifications are possible by placing multivariate normal priors on the regression coefficients and independent inverse Wishart priors on the innovation variances. By constraining arbitrary regression coefficients to be zero in a fully unstructured model allows any fully recursive path model to be constructed for a set of random effects. 10.3 Scaling The path analyses described above essentially allow elements of the response vector to be regressed on each other. Regressing an observation on itself would seem like a peculiar thing to do, although with a little work we can show that by doing this we can allow two sets of observations to conform to the same model except for a difference in scale. References "],["technical-details.html", "11 Technical Details 11.1 Model Form 11.2 MCMC Sampling Schemes", " 11 Technical Details 11.1 Model Form The probability of the \\(i^{th}\\) data point is represented by: \\[f_{i}(y_{i} | l_{i}) \\label{pyl-Eq} \\tag{11.1}\\] where \\(f_{i}\\) is the probability density function associated with \\(y_{i}\\). For example, if \\(y_{i}\\) was assumed to be Poisson distributed and we used the canonical log link function, then Equation (11.1) would have the form: \\[f_{P}\\left(y_{i} | \\lambda = \\textrm{exp}(l_{i})\\right) \\label{pyl2-Eq} \\tag{11.2}\\] where \\(\\lambda\\) is the canonical parameter of the Poisson density function \\(f_{P}\\). Table 11.1 has a full list of supported distributions and link functions. The vector of latent variables follow the linear model \\[{\\bf l} = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}{\\bf u}+{\\bf e} \\label{l-Eq} \\tag{11.3}\\] where \\({\\bf X}\\) is a design matrix relating fixed predictors to the data, and \\({\\bf Z}\\) is a design matrix relating random predictors to the data. These predictors have associated parameter vectors \\({\\boldsymbol{\\mathbf{\\beta}}}\\) and \\({\\bf u}\\), and \\({\\bf e}\\) is a vector of residuals. In the Poisson case these residuals deal with any overdispersion in the data after accounting for fixed and random sources of variation. The location effects (\\({\\boldsymbol{\\mathbf{\\beta}}}\\) and \\({\\bf u}\\)), and the residuals (\\({\\bf e}\\)) are assumed to come from a multivariate normal distribution: \\[\\left[ \\begin{array}{c} {\\boldsymbol{\\mathbf{\\beta}}}\\\\ {\\bf u}\\\\ {\\bf e} \\end{array} \\right] \\sim N\\left( \\left[ \\begin{array}{c} {\\boldsymbol{\\mathbf{\\beta}}}_{0}\\\\ {\\bf 0}\\\\ {\\bf 0}\\\\ \\end{array} \\right] , \\left[ \\begin{array}{ccc} {\\bf B}&amp;{\\bf 0}&amp;{\\bf 0}\\\\ {\\bf 0}&amp;{\\bf G}&amp;{\\bf 0}\\\\ {\\bf 0}&amp;{\\bf 0}&amp;{\\bf R}\\\\ \\end{array} \\right] \\right) \\label{V-Eq} \\tag{11.4}\\] where \\({\\boldsymbol{\\mathbf{\\beta}}}_{0}\\) is a vector of prior means for the fixed effects with prior (co)variance \\({\\bf B}\\), and \\({\\bf G}\\) and \\({\\bf R}\\) are the expected (co)variances of the random effects and residuals respectively. The zero off-diagonal matrices imply a priori independence between fixed effects, random effects, and residuals. Generally, \\({\\bf G}\\) and \\({\\bf R}\\) are large square matrices with dimensions equal to the number of random effects or residuals. Typically they are unknown, and must be estimated from the data, usually by assuming they are structured in a way that they can be parameterised by few parameters. Below we will focus on the structure of \\({\\bf G}\\), but the same logic can be applied to \\({\\bf R}\\). At its most general, \\(\\texttt{MCMCglmm}\\) allows variance structures of the form: \\[{\\bf G}= \\left({\\bf V}_{1}\\otimes{\\bf A}_{1}\\right) \\oplus \\left({\\bf V}_{2}\\otimes{\\bf A}_{2}\\right) \\oplus \\ldots \\label{G3-Eq} \\tag{11.5}\\] where the parameter (co)variance matrices (\\({\\bf V}\\)) are usually low-dimensional and are to be estimated, and the structured matrices (\\({\\bf A}\\)) are usually high dimensional and treated as known. In the case of ordinal probit models with \\(&gt;2\\) categories (i.e. \"threshold\" or \"ordinal\" models), \\(f_{T}/f_{O}\\) depends on an extra set of parameters in addition to the latent variable: the \\(\\textrm{max}(y)+1\\) cutpoints \\({\\boldsymbol{\\mathbf{\\gamma}}}\\). The probability of \\(y_{i}\\) is then: \\[f_{T}(y_{i} | l_{i}, {\\boldsymbol{\\mathbf{\\gamma}}}) = 1\\ \\textrm{if}\\ \\gamma_{y_{i}+1} &lt; l_{i} &lt; \\gamma_{y_{i}}\\] and \\[f_{O}(y_{i} | l_{i}, {\\boldsymbol{\\mathbf{\\gamma}}}) = F_{N}(\\gamma_{y_{i}} | l_{i}, 1)-F_{N}(\\gamma_{y_{i}+1} | l_{i},1)\\] where \\(F_{N}\\) is the cumulative density function for the normal. Note that the two models can be made equivalent. 11.2 MCMC Sampling Schemes 11.2.1 Updating the latent variables \\({\\bf l}\\) The conditional density of \\(l\\) is given by: \\[Pr(l_{i}| {\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}) \\propto f_{i}(y_{i} | l_{i})f_{N}(e_{i}|{\\bf r}_{i}{\\bf R}_{/i}^{-1}{\\bf e}_{/i}, r_{i}-{\\bf r}_{i}{\\bf R}_{/i}^{-1}{\\bf r}^{&#39;}_{i}) \\label{pcl-Eq} \\tag{11.6}\\] where \\(f_{N}\\) indicates a Multivariate normal density with specified mean vector and covariance matrix. Equation (11.6) is the probability of the data point \\(y_{i}\\) from distribution \\(f_{i}\\) with latent varaible \\(l_{i}\\), multiplied by the probability of the linear predictor residual. The linear predictor residual follows a conditional normal distribution where the conditioning is on the residuals associated with data points other than \\(i\\). Vectors and matrices with the row and/or column associated with \\(i\\) removed are denoted \\(/i\\). Three special cases exist for which we sample directly from Equation (11.6): i) When \\(y_{i}\\) is normal \\(f_{i}(y_{i} | l_{i})=1\\) if \\(y_{i}=l_{i}\\) and zero otherwise so \\(l_{i}=y_{i}\\) with out the need for updating, ii) when \\(y_{i}\\) is discrete and modelled using family=\"threshold\" then Equation defines a truncated normal distribution and can be slice sampled (Robert 1995) and iii) when \\(y_{i}\\) is missing \\(f_{i}(y_{i} | l_{i})\\) is not defined and samples can drawn directly from the normal. In practice, the conditional distribution in Equation (11.6) only involves other residuals which are expected to show some form of residual covariation, as defined by the \\({\\bf R}\\) structure. Because of this we actually update latent variables in blocks, where the block is defined as groups of residuals which are expected to be correlated: \\[Pr({\\bf l}_{j}|{\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}) \\propto \\prod_{i \\in j}{p}_{i}({y}_{i} | l_{i})f_{N}({\\bf e}_{j}|{\\bf 0}, {\\bf R}_{j}) \\label{pcl2-Eq} \\tag{11.7}\\] where \\(j\\) indexes blocks of latent variables that have non-zero residual covariances. For response variables that are neither Gaussian nor threshold, the density in equation (11.7) is in non-standard form and so Metropolis-Hastings updates are employed. We use adaptive methods during the burn-in phase to determine an efficient multivariate normal proposal distribution centered at the previous value of \\({\\bf l}_{j}\\) with covariance matrix \\(m{\\bf M}\\). For computational efficiency we use the same \\({\\bf M}\\) for each block \\(j\\), where \\({\\bf M}\\) is the average posterior (co)variance of \\({\\bf l}_{j}\\) within blocks and is updated each iteration of the burn-in period Haario, Saksman, and Tamminen (2001). The scalar \\(m\\) is chosen using the method of Ovaskainen et al. (2008) so that the proportion of successful jumps is optimal, with a rate of 0.44 when \\({\\bf l}_{j}\\) is a scalar declining to 0.23 when \\({\\bf l}_{j}\\) is high dimensional (Gelman et al. 2004). A special case arises for multi-parameter distributions in which each parameter is associated with a linear predictor. For example, in the zero-inflated Poisson two linear predictors are used to model the same data point, one to predict zero-inflation, and one to predict the Poisson variable. In this case the two linear predictors are updated in a single block even when the residual covariance between them is set to zero, because the first probability in Equation (11.7) cannot be factored: \\[Pr({\\bf l}_{j}|{\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}) \\propto {p}_{i}({y}_{i} | {\\bf l}_{j})({\\bf e}_{j}|{\\bf 0}, {\\bf R}_{j}) \\label{pcl3-Eq} \\tag{11.8}\\] When the block size is one (i.e. a univariate analysis) then the latent variables can be slice sampled for two-category ordinal and categorical models if slice=TRUE is passed to \\(\\texttt{MCMCglmm}\\). 11.2.2 Updating the location vector \\(\\boldsymbol{\\theta} = \\left[{\\boldsymbol{\\mathbf{\\beta}}}^{&#39;}\\; {\\bf u}^{&#39;}\\right]^{&#39;}\\) Garcia-Cortes and Sorensen (2001) provide a method for sampling \\(\\boldsymbol{\\theta}\\) as a complete block that involves solving the sparse linear system: \\[\\tilde{\\boldsymbol{\\theta}} = {\\bf C}^{-1}{\\bf W}^{&#39;}{\\bf R}^{-1}({\\bf l} - {\\bf W}\\boldsymbol{\\theta}_{\\star}-{\\bf e}_{\\star}) \\label{sMME-Eq} \\tag{11.9}\\] where \\({\\bf C}\\) is the mixed model coefficient matrix: \\[{\\bf C} = {\\bf W}^{&#39;}{\\bf R}^{-1}{\\bf W}+ \\left[ \\begin{array}{c c} {\\bf B}^{-1}&amp;{\\bf 0}\\\\ {\\bf 0}&amp;{\\bf G}^{-1}\\\\ \\end{array} \\right]\\] and \\({\\bf W} = \\left[{\\bf X}\\; {\\bf Z}\\right]\\), and \\({\\bf B}\\) is the prior (co)variance matrix for the fixed effects. \\(\\boldsymbol{\\theta}_{\\star}\\) and \\({\\bf e}_{\\star}\\) are random draws from the multivariate normal distributions: \\[\\boldsymbol{\\theta}_{\\star} \\sim N\\left( \\left[ \\begin{array}{c} {\\boldsymbol{\\mathbf{\\beta}}_{0}}\\\\ {\\bf 0}\\\\ \\end{array} \\right] , \\left[ \\begin{array}{c c} {\\bf B}&amp;{\\bf 0}\\\\ {\\bf 0}&amp;{\\bf G}\\\\ \\end{array} \\right] \\right)\\] and \\[{\\bf e}_{\\star} \\sim N\\left({\\bf 0},{\\bf R}\\right)\\] \\(\\tilde{\\boldsymbol{\\theta}} + \\boldsymbol{\\theta}_{\\star}\\) gives a realisation from the required probability distribution: \\[Pr(\\boldsymbol{\\theta} | {\\bf l}, {\\bf W}, {\\bf R}, {\\bf G})\\] Equation (11.9) is solved using Cholesky factorisation. Because \\({\\bf C}\\) is sparse and the pattern of non-zero elements fixed, an initial symbolic Cholesky factorisation of \\({\\bf P}{\\bf C}{\\bf P}^{&#39;}\\) is preformed where \\({\\bf P}\\) is a fill-reducing permutation matrix (Davis 2006). Numerical factorisation must be performed each iteration but the fill-reducing permutation (found via a minimum degree ordering of \\({\\bf C}+{\\bf C}^{&#39;}\\)) reduces the computational burden dramatically compared to a direct factorisation of \\({\\bf C}\\) (Davis 2006). Forming the inverse of the variance structures is usually simpler because they can be expressed as a series of direct sums and Kronecker products: \\[{\\bf G}= \\left({\\bf V}_{1}\\otimes{\\bf A}_{1}\\right) \\oplus \\left({\\bf V}_{2}\\otimes{\\bf A}_{2}\\right) \\oplus \\ldots\\] and the inverse of such a structure has the form \\[{\\bf G}^{-1} = \\left({\\bf V}^{-1}_{1}\\otimes{\\bf A}^{-1}_{1}\\right) \\oplus \\left({\\bf V}^{-1}_{2}\\otimes{\\bf A}^{-1}_{2}\\right) \\oplus \\ldots\\\\\\] which involves inverting the parameter (co)variance matrices (\\({\\bf V}\\)), which are usually of low dimension, and inverting \\({\\bf A}\\). For many problems \\({\\bf A}\\) is actually an identity matrix and so inversion is not required. When \\({\\bf A}\\) is a relationship matrix associated with a pedigree, Henderson (1976; Meuwissen and Luo 1992) give efficient recursive algorithms for obtaining the inverse, and Hadfield and Nakagawa (2010) derive a similar procedure for phylogenies. 11.2.3 Updating the variance structures \\({\\bf G}\\) and \\({\\bf R}\\) Components of the direct sum used to construct the desired variance structures are conditionally independent. The sum of squares matrix associated with each component term has the form: \\[{\\bf S} = {\\bf U}^{&#39;}{\\bf A}^{-1}{\\bf U}\\] where \\({\\bf U}\\) is a matrix of random effects where each column is associated with the relevant row/column of \\({\\bf V}\\) and each row associated with the relevant row/column of \\({\\bf A}\\). The parameter (co)variance matrix can then be sampled from the inverse Wishart distribution: \\[{\\bf V} \\sim IW(({\\bf S}_{p}+{\\bf S})^{-1},\\ n_{p}+n) \\label{pIW-Eq} \\tag{11.10}\\] where \\(n\\) is the number of rows in \\({\\bf U}\\), and \\({\\bf S}_{p}\\) and \\(n_{p}\\) are the prior sum of squares and prior degree’s of freedom, respectively. In some models, some elements of a parameter (co)variance matrix cannot be estimated from the data and all the information comes from the prior. In these cases it can be advantageous to fix these elements at some value and Korsgaard, Andersen, and Sorensen (1999) provide a strategy for sampling from a conditional inverse-Wishart distribution which is appropriate when the rows/columns of the parameter matrix can be permuted so that the conditioning occurs on some diagonal sub-matrix. When this is not possible Metropolis-Hastings updates can be made. 11.2.4 Ordinal Models For ordinal models it is necessary to update the cutpoints which define the bin boundaries for latent variables associated with each category of the outcome. To achieve good mixing we used the method developed by (Cowles 1996) that allows the latent variables and cutpoints to be updated simultaneously using a Hastings-with-Gibbs update. 11.2.5 Path Analyses Elements of the response vector can be regressed on each other using the sir and path functions. Using the matrix notation of Gianola and Sorensen (2004), Equation (11.3) can be rewritten as: \\[\\boldsymbol{\\Lambda}{\\bf l} = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}{\\bf u}+{\\bf e} \\label{rs-Eq1} \\tag{11.11}\\] where \\(\\boldsymbol{\\Lambda}\\) is a square matrix of the form: \\[\\begin{array}{rl} \\boldsymbol{\\Lambda} =&amp; {\\bf I}-\\sum_{l}\\boldsymbol{\\Psi}^{(l)}\\lambda_{l}\\\\ \\end{array} \\label{rs-Eq2} \\tag{11.12}\\] This sets up a regression where the \\(i^{th}\\) element of the response vector acts as a weighted (by \\(\\Psi^{(l)}_{i,j}\\)) predictor for the \\(j^{th}\\) element of the response vector with associated regression parameter \\(\\lambda_{l}\\). Often \\(\\boldsymbol{\\Psi}^{(l)}\\) is an incidence matrix with the patterns of ones determining which elements of the response are regressed on each other. Conditional on the vector of regression coefficients \\(\\boldsymbol{\\Lambda}\\), the location effects and variance structures can be updated as before by simply substituting \\({\\bf l}\\) for \\(\\boldsymbol{\\Lambda}{\\bf l}\\) in the necessary equations. Gianola and Sorensen (2004) provide a simple scheme for updating \\(\\boldsymbol{\\Lambda}\\). Note that Equation (11.11) can be rewritten as: \\[\\begin{array}{rl} {\\bf l} - {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}} - {\\bf Z}{\\bf u} =&amp; {\\bf e}+\\sum_{l}\\boldsymbol{\\Psi}^{(l)}{\\bf l}\\lambda_{l}\\\\ =&amp; {\\bf e}+{\\bf L}\\boldsymbol{\\Lambda}\\\\ \\end{array}\\] where \\({\\bf L}\\) is the design matrix \\(\\left[\\boldsymbol{\\Psi}^{(1)}{\\bf l}, \\boldsymbol{\\Psi}^{(2)}{\\bf l} \\dots \\boldsymbol{\\Psi}^{(L)}{\\bf l}\\right]\\) for the \\(L\\) path coefficients. Conditional on \\({\\boldsymbol{\\mathbf{\\beta}}}\\) and \\({\\boldsymbol{\\mathbf{u}}}\\), \\(\\boldsymbol{\\Lambda}\\) can then be sampled using the method of Garcia-Cortes and Sorensen (2001) with \\({\\bf l} - {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}} - {\\bf Z}{\\bf u}\\) as response and \\({\\bf L}\\) as predictor. However, only in a fully recursive system (there exists a row/column permutation by which all \\(\\boldsymbol{\\Psi}\\)’s are triangular) are the resulting draws from the appropriate conditional distribution, which requires multiplication by the Jacobian of the transform: \\(|\\boldsymbol{\\Lambda}|\\). An extra Metropolis Hastings step is used to accept/reject the proposed draw when \\(|\\boldsymbol{\\Lambda}|\\neq 1\\). When the response vector is Gaussian and fully observed, the latent variable does not need updating. For non-Gaussian data, or with missing responses, updating the latent variable is difficult because Equation (11.6) becomes: \\[Pr(l_{i}| {\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}, \\boldsymbol{\\Lambda}) \\propto f_{i}(y_{i} | l_{i})f_{N}((\\boldsymbol{\\Lambda}^{-1}{\\bf e})_{i}|{\\bf q}_{i}{\\bf Q}_{/i}^{-1}{\\bf e}_{/i}, q_{i}-{\\bf q}_{i}{\\bf Q}_{/i}^{-1}{\\bf q}^{&#39;}_{i})\\] where \\({\\bf Q} = \\boldsymbol{\\Lambda}^{-1}{\\bf R}\\boldsymbol{\\Lambda}^{-\\top}\\). In the general case \\({\\bf Q}\\) will not have block diagonal structure like \\({\\bf R}\\) and so the scheme for updating latent variables within residual blocks (i.e. Equation (11.7)) is not possible. However, in some cases \\(\\boldsymbol{\\Lambda}\\) may have the form where all non-zero elements correspond to elements of the response vector that are in the same residual block. In such cases updating the latent variables remains relatively simple: \\[Pr({\\bf l}_{j}|{\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}) \\propto {p}_{i}({y}_{i} | {\\bf l}_{j})(\\boldsymbol{\\Lambda}^{-1}_{j}{\\bf e}_{j}|{\\bf 0}, \\boldsymbol{\\Lambda}^{-1}_{j}{\\bf R}_{j}\\boldsymbol{\\Lambda}^{-\\top}_{j})\\] 11.2.6 Deviance and DIC The deviance \\(D\\) is defined as: \\[D = -2\\textrm{log}(\\Pr({\\bf y} | {\\boldsymbol{\\mathbf{\\Omega}}}))\\] where \\({\\boldsymbol{\\mathbf{\\Omega}}}\\) is some parameter set of the model. The deviance can be calculated in different ways depending on what is in ‘focus’, and \\(\\texttt{MCMCglmm}\\) calculates this probability for the lowest level of the hierarchy (Spiegelhalter et al. 2002). For fully-observed Gaussian response variables in the likelihood is the density: \\[f_{N}({\\bf y} | {\\bf W}\\boldsymbol{\\theta},\\ {\\bf R})\\] where \\({\\boldsymbol{\\mathbf{\\Omega}}} = \\left\\{\\boldsymbol{\\theta},\\ {\\bf R}\\right\\}\\). For discrete response variables in univariate analyses modeled using family=\"threshold\" the density is \\[\\prod_{i} F_{N}(\\gamma_{y_{i}} | {\\bf w}_{i}\\boldsymbol{\\theta}, \\ r_{ii})-F_{N}(\\gamma_{y_{i}+1} | {\\bf w}_{i}\\boldsymbol{\\theta}, \\ r_{ii})\\] where \\({\\boldsymbol{\\mathbf{\\Omega}}} = \\left\\{{\\boldsymbol{\\mathbf{\\gamma}}},\\ \\boldsymbol{\\theta},\\ {\\bf R}\\right\\}\\). For other response variables variables (including discrete response variables modeled using family=\"ordinal\") it is the product: \\[\\prod_{i}f_{i}(y_{i} | l_{i}) \\label{LLikL} \\tag{11.13}\\] with \\({\\boldsymbol{\\mathbf{\\Omega}}} = {\\bf l}\\). For multivariate models with mixtures of Gaussian (g), threshold (t) and other non-Gaussian (n) data (including missing data) we can define the deviance in terms of three conditional densities: \\[\\begin{array}{rl} Pr({\\bf y} | {\\boldsymbol{\\mathbf{\\Omega}}}) =&amp; \\Pr({\\bf y}_{g}, {\\bf y}_{t}, {\\bf y}_{n} | {\\boldsymbol{\\mathbf{\\gamma}}}, \\boldsymbol{\\theta}_{g}, \\boldsymbol{\\theta}_{t}, {\\boldsymbol{\\mathbf{l}}}_{n}, {\\boldsymbol{\\mathbf{R}}})\\\\ =&amp; \\Pr({\\bf y}_{t} | {\\boldsymbol{\\mathbf{\\gamma}}}, \\boldsymbol{\\theta}_{t}, {\\bf y}_{g}, {\\boldsymbol{\\mathbf{l}}}_{n}, {\\boldsymbol{\\mathbf{R}}})\\Pr({\\bf y}_{g} | \\boldsymbol{\\theta}_{g}, {\\boldsymbol{\\mathbf{l}}}_{n}, {\\boldsymbol{\\mathbf{R}}})\\Pr({\\bf y}_{n} | {\\boldsymbol{\\mathbf{l}}}_{n})\\\\ \\label{Eq-MVdeviance} \\end{array} \\tag{11.14}\\] with \\({\\boldsymbol{\\mathbf{\\Omega}}} = \\left\\{{\\boldsymbol{\\mathbf{\\gamma}}},\\ \\boldsymbol{\\theta}_{/n},\\ {\\boldsymbol{\\mathbf{l}}}_{n}\\ {\\bf R}\\right\\}\\). Have \\(({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{a|b}={\\boldsymbol{\\mathbf{W}}}_{a}\\boldsymbol{\\theta}_{a}+{\\bf R}_{a,b}{\\bf R}^{-1}_{b,b}({\\bf l}_{b}-{\\boldsymbol{\\mathbf{W}}}_{b}\\boldsymbol{\\theta}_{b})\\) and \\({\\bf R}_{a |b} = {\\bf R}_{a,a}-{\\bf R}_{a,b}{\\bf R}^{-1}_{b,b}{\\bf R}_{a,b}\\) where the subscripts denote rows of the data vector/design matrices or rows/columns of the \\({\\bf R}\\)-structure. Then, the conditional density of \\({\\bf y}_{g}\\) in Equation (11.14) is: \\[f_{N}\\left({\\bf y}_{g} | ({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{g|n},\\ {\\bf R}_{g|n}\\right)\\] The conditional density of \\({\\bf y}_{n}\\) in Equation (11.14) is identical to that given in Equation (11.13), and for a single \"threshold\" trait \\[\\prod_{i} F_{N}(\\gamma_{y_{i}} | ({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{ti|g,n}, \\ r_{ti|g, n})-F_{N}(\\gamma_{y_{i}+1} | ({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{ti|g,n}, \\ r_{ti|g, n}) \\label{Eq-cpmvnorm} \\tag{11.15}\\] is the conditional density for \\({\\bf y}_{t}\\) in Equation (11.14), where \\(({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{ti|g,n}\\) is the \\(i^{\\textrm{th}}\\) element of \\(({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{t|g,n}\\). Currently the deviance (and hence the DIC) will not be returned if there is more than one threshold trait. The deviance is calculated at each iteration if DIC=TRUE and stored each thin\\(^{th}\\) iteration after burn-in. However, for computational reasons the deviance is calculated mid-iteration such that the deviance returned at iteration \\(i\\) uses \\({\\boldsymbol{\\mathbf{\\Omega}}}_{i} = \\left\\{{\\boldsymbol{\\mathbf{\\gamma}}}_{i},\\ \\boldsymbol{\\theta}_{/n, i},\\ {\\boldsymbol{\\mathbf{l}}}_{n, i-1}\\ {\\bf R}_{i}\\right\\}\\). The mean deviance (\\(\\bar{D}\\)) is calculated over all iterations, as is the mean of the latent variables (\\({\\bf l}\\)) the \\({\\bf R}\\)-structure and the vector of predictors (\\({\\bf W}\\boldsymbol{\\theta}\\)). The deviance is calculated at the mean estimate of the parameters (\\(D(\\bar{\\boldsymbol{\\mathbf{\\Omega}}})\\)) and the deviance information criterion calculated as: \\[\\textrm{DIC} = 2\\bar{D}-D(\\bar{\\boldsymbol{\\mathbf{\\Omega}}})\\] Table 11.1: Distribution types that can fitted using \\(\\texttt{MCMCglmm}\\). The prefixes \"zi\", \"zt\", \"hu\" and \"za\" stand for zero-inflated, zero-truncated, hurdle and zero-altered respectively. The prefix \"cen\" standards for censored where \\(y_{1}\\) and \\(y_{2}\\) are the upper and lower bounds for the unobserved datum \\(y\\). \\(J\\) stands for the number of categories in the multinomial, categorical and zero-truncated multivariate Bernoulli distributions and this must be specified in the family argument for the multinomial and zero-truncated multivariate Bernoulli distributions. The density function is for a single datum in a univariate model with \\({\\bf w}^{\\top}\\) being a row vector of \\({\\bf W}\\). \\(f\\) and \\(F\\) are the density and distribution functions for the subscripted distribution (\\(N\\)=Normal, \\(P\\)=Poisson, \\(E\\)=Exponential, \\(G\\)=Geometric, \\(B\\)=Binomial, \\(NCt\\)=non-central \\(t\\) and \\(t\\)=Student’s \\(t\\)). The \\(J-1\\) \\(\\gamma\\)’s in the ordinal/threshold models are the cut-points, with \\(\\gamma_{1}\\) set to zero. The normalising probability \\(P_k(k)\\) in the denominator of the zero-truncated multinomial is not given explicitly but is the probability of no zero’s in the non-zero categories under a standard multinomial. Distribution NoDataColumns NoTraits Density Function \"gaussian\" 1 1 \\(Pr(y)\\) \\(=f_{N}({\\bf w}^{\\top}\\boldsymbol{\\theta},\\sigma^{2}_{e})\\) \"poisson\" 1 1 \\(Pr(y)\\) \\(=f_{P}(\\textrm{exp}(l))\\) \"categorical\" 1 \\(J-1\\) \\(Pr(y = k| k\\neq k_1)\\) \\(Pr(y=k | k=k_1)\\) \\(=\\frac{\\textrm{exp}(l_{k})}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\) \\(=\\frac{1}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\) \"multinomialJ\" \\(J\\) \\(J-1\\) \\(Pr(y_k | k\\neq J)\\) \\(Pr(y_k | k=J)\\) \\(={\\sum^{J}_{j=1}y_j \\choose y_k}\\left(\\frac{\\textrm{exp}(l_{k})}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\right)^{y_k}\\left(1-\\frac{\\textrm{exp}(l_{k})}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\right)^{\\sum^{J}_{j\\neq k} y_j}\\) \\(={\\sum^{J}_{j=1}y_j \\choose y_k}\\left(\\frac{1}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\right)^{y_k}\\left(1-\\frac{1}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\right)^{\\sum^{J}_{j\\neq k} y_j}\\) \"ordinal\" 1 1 \\(Pr(y=k)\\) \\(=F_{N}(\\gamma_{k} | l,1)-F_{N}(\\gamma_{k+1} | l,1)\\) \"threshold\" 1 1 \\(Pr(y=k)\\) \\(=F_{N}(\\gamma_{k} | {\\bf w}^{\\top}\\boldsymbol{\\theta}, \\sigma^{2}_{e})-F_{N}(\\gamma_{k+1} | {\\bf w}^{\\top}\\boldsymbol{\\theta}, \\sigma^{2}_{e})\\) \"exponential\" 1 1 \\(Pr(y)\\) \\(=f_{E}(\\textrm{exp}(-l))\\) \"geometric\" 1 1 \\(Pr(y)\\) \\(=f_{G}(\\frac{\\textrm{exp}(l)}{1+\\textrm{exp}(l)})\\) \"cengaussian\" 2 1 \\(Pr(y_{1}&gt;y&gt;y_{2})\\) \\(=F_{N}(y_{2} | {\\bf w}^{\\top}\\boldsymbol{\\theta},\\sigma^{2}_{e})-F_{N}(y_{1} | {\\bf w}^{\\top}\\boldsymbol{\\theta},\\sigma^{2}_{e})\\) \"cenpoisson\" 2 1 \\(Pr(y_{1}&gt;y&gt;y_{2})\\) \\(=F_{P}(y_{2} | l)-F_{P}(y_{1} | l)\\) \"cenexponential\" 2 1 \\(Pr(y_{1}&gt;y&gt;y_{2})\\) \\(=F_{E}(y_{2} | l)-F_{E}(y_{1} | l)\\) \"zipoisson\" 1 2 \\(Pr(y=0)\\) \\(Pr(y | y&gt;0)\\) \\(=\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}+\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)f_{P}(y|\\textrm{exp}(l_{1}))\\) \\(=\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)f_{P}(y |\\textrm{exp}(l_{1}))\\) \"ztpoisson\" 1 1 \\(Pr(y)\\) \\(=\\frac{f_{P}(y |\\textrm{exp}(l))}{1-f_{P}(0 |\\textrm{exp}(l))}\\) \"hupoisson\" 1 2 \\(Pr(y=0)\\) \\(Pr(y | y&gt;0)\\) \\(=\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\) \\(=\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)\\frac{f_{P}(y |\\textrm{exp}(l_{1}))}{1-f_{P}(0 |\\textrm{exp}(l_{1}))}\\) \"zapoisson\" 1 2 \\(Pr(y=0)\\) \\(Pr(y | y&gt;0)\\) \\(=1-\\textrm{exp}(\\textrm{exp}(l_{2}))\\) \\(=\\textrm{exp}(\\textrm{exp}(l_{2}))\\frac{f_{P}(y |\\textrm{exp}(l_{1}))}{1-f_{P}(0 |\\textrm{exp}(l_{1}))}\\) \"hubinomial\" 2 2 \\(Pr(y_{1}=0)\\) \\(Pr(y_{1} | y_{1}&gt;0)\\) \\(=\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}+\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)f_{B}(0 | n=y_{1}+y_{2}, \\frac{\\textrm{exp}(l_{1})}{1+\\textrm{exp}(l_{1})})\\) \\(=\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)f_{B}(y_{1} | n=y_{1}+y_{2} \\frac{\\textrm{exp}(l_{1})}{1+\\textrm{exp}(l_{1})})\\) \"nzbinom\" 2 1 \\(Pr(y=1)\\) \\(Pr(y=0)\\) \\(=1-\\left(1-\\frac{\\textrm{exp}(l)}{1+\\textrm{exp}(l)}\\right)^{y_2}\\) \\(=\\left(1-\\frac{\\textrm{exp}(l)}{1+\\textrm{exp}(l)}\\right)^{y_2}\\) \"ncst\" 3 1 \\(Pr(y)\\) \\(=f_{NCt}(y_1 | \\nu=y_3, \\mu=l/y_2)/y_2\\) \"msst\" 3 1 \\(Pr(y)\\) \\(=f_{t}(y_1-l/y_2 | \\nu=y_3)/y_2\\) \"ztmbJ\" \\(J\\) \\(J\\) \\(Pr(y_k=1)\\) \\(Pr(y_k=0)\\) \\(=\\left(1-\\frac{\\textrm{exp}(l_k)}{1+\\textrm{exp}(l_k)}\\right)\\left(1-\\prod^{J}_{j=1}\\frac{\\textrm{exp}(l_k)}{1+\\textrm{exp}(l_k)}\\right)^{-1}\\) \\(=\\left(\\frac{\\textrm{exp}(l_k)}{1+\\textrm{exp}(l_k)}\\right)\\left(1-\\prod^{J}_{j=1}\\frac{\\textrm{exp}(l_k)}{1+\\textrm{exp}(l_k)}\\right)^{-1}\\) \"ztmultinomialJ\" \\(J\\) \\(J-1\\) \\(Pr(y_k | k\\neq J, y_k&gt;0)\\) \\(Pr(y_k | k=J, y_k&gt;0)\\) \\(Pr(y_k=0 | k=J)\\) \\(={\\sum^{J}_{j=1}y_j \\choose y_k}\\left(\\frac{\\textrm{exp}(l_{k})}{P_k(k)}\\right)^{y_k}\\left(1-\\frac{\\textrm{exp}(l_{k})}{P_k(k)}\\right)^{\\sum^{J}_{j\\neq k} y_j}\\) \\(={\\sum^{J}_{j=1}y_j \\choose y_k}\\left(\\frac{1}{P_k(k)}\\right)^{y_k}\\left(1-\\frac{1}{P_k(k)}\\right)^{\\sum^{J}_{j\\neq k} y_j}\\)      ignored References "],["references.html", "12 References", " 12 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
