[["overview.html", "MCMCglmm Course Notes 1 Overview 1.1 Outline", " MCMCglmm Course Notes Jarrod Hadfield 2026-02-25 1 Overview These are (incomplete) course notes about generalised linear mixed models (GLMM). Special emphasis is placed on understanding the underlying structure of a GLMM in order to show that slight modifications of this structure can produce a wide range of models. These include familiar models like regression and ANOVA, but also models with intimidating names: animal models, threshold models, meta-analysis, random regression The primary aim of the course is to show that these models are only daunting by name. The secondary aim is to show how these models can be fitted in a Bayesian framework using Markov chain Monte Carlo (MCMC) methods in the R package \\(\\texttt{MCMCglmm}\\) (Hadfield 2010). For those not comfortable using Bayesian methods, many of the models outlined in the course notes can be fitted using the packages \\(\\texttt{asreml-r}\\) (Butler et al. 2017), \\(\\texttt{lme4}\\) (Bates et al. 2015) or \\(\\texttt{glmmTMB}\\) (Brooks et al. 2017) with little extra work. 1.1 Outline Chapter 2 covers the very basics of Bayesian analysis and Markov chain Monte Carlo (MCMC) techniques. Chapter 3 covers simple generalised linear mixed models (GLMM). The response is assumed to be (conditionally) normal or from a single-parameter family such as the Poisson, and the distribution of the random effects can be parameterised through scalar variances. Chapters 5 and 6 cover situations where the distribution of the random effects are parameterised through a (co)variance matrix either because the random terms are interacted with categorical predictors (Chapter 5) or continuos predictors (random-regression - Chapter 6). Chapter 7 covers multi-response models in which multiple response variables, possibly from different distributions, are simultaneously analysed. This chapter also covers distributions that are multi-parameter such as multinomial and zero-inflated distributions. Chapter 8 discusses models where a set of random-effects are correlated through a known structure such as a pedigree or phylogeny. Much of the technical details regarding the implementation are in Chapter 11. Chapter 11.3 covers parameter-expansion, an algorithm designed to improve the mixing properties of MCMC but which also induces new, and useful, prior distributions. Chapter 10 covers path analysis and related antedependence models. Much of the material is currently taken from the old CourseNotes (c2009) but the hope is that over time these new notes will be better structured and include better/some documentation for more recent functionality. References "],["bayesian.html", "2 Bayesian Analysis and MCMC 2.1 Introduction 2.2 Likelihood 2.3 Prior Distribution 2.4 Posterior Distribution 2.5 MCMC 2.6 Priors for Residual Variances 2.7 Transformations", " 2 Bayesian Analysis and MCMC In this Chapter I cover the basics of Bayesian analysis and Markov chain Monte Carlo (MCMC) techniques. Exposure to these ideas, and statistics in general, has increased dramatically since these notes were first written. Many readers may therefore wish to skip straight to later chapters that cover \\(\\texttt{MCMCglmm}\\) more specifically. 2.1 Introduction There are fundamental differences between frequentist and Bayesian approaches, but for those of us interested in applied statistics the hope is that these differences do not translate into practical differences, and this is often the case. My advice would be if you can fit the same model using different packages and/or methods do so, and if they give very different answers worry. In some cases differences will exist, and it is important to know why, and which method is more appropriate for the data in hand. In the context of a generalised linear mixed model (GLMM), here are what I see as the pro’s and cons of using (restricted) maximum likelihood (REML) versus Bayesian MCMC methods. REML is fast and easy to use, whereas MCMC can be slow and technically more challenging. Particularly challenging is the specification of a sensible prior, something which is a non-issue in a REML analysis. However, analytical results for non-Gaussian GLMM are generally not available, and REML based procedures use approximate likelihood methods that may not work well. MCMC is also an approximation but the accuracy of the approximation increases the longer the analysis is run for, being exact at the limit. In addition, REML uses large-sample theory to derive approximate confidence intervals that may have very poor coverage, especially for variance components. Again, MCMC measures of confidence are exact, up to Monte Carlo error, and provide an easy and intuitive way of obtaining measures of confidence on derived statistics such as ratios of variances, correlations and predictions. To illustrate the differences between the approaches let’s imagine we’ve observed several draws (stored in the vector \\({\\bf y}\\)) from a standard normal (i.e. \\(\\mu=0\\) and \\(\\sigma^{2}=1\\)). The likelihood is the probability of the data given the parameters: \\[Pr({\\bf y} | \\mu, \\sigma^{2})\\] This is a conditional distribution, where the conditioning is on the model parameters which are taken as fixed and known. In a way this is quite odd because we’ve already observed the data, and we don’t know what the parameter values are. In a Bayesian analysis we evaluate the conditional probability of the model parameters given the observed data: \\[Pr(\\mu, \\sigma^{2} | {\\bf y}) \\label{post1-eq} \\tag{2.1}\\] which seems more reasonable, until we realise that this probability is proportional to \\[Pr({\\bf y} | \\mu, \\sigma^{2})Pr(\\mu, \\sigma^{2})\\] where the first term is the likelihood, and the second term represents our prior belief in the values that the model parameters could take. Because the choice of prior is rarely justified by an objective quantification of the state of knowledge it has come under criticism, and indeed we will see later that the choice of prior can make a difference. 2.2 Likelihood We can generate 5 observations from this distribution using rnorm: Ndata &lt;- data.frame(y = rnorm(5, mean = 0, sd = 1)) Ndata$y ## [1] -1.01500872 -0.07963674 -0.23298702 -0.81726793 0.77209084 We can plot the probability density function for the standard normal using dnorm and we can then place the 5 data on it: possible.y&lt;-seq(-3,3,0.1) # possible values of y Probability&lt;-dnorm(possible.y, mean=0, sd=1) # density of possible values plot(Probability~possible.y, type=&quot;l&quot;, ylab=&quot;Density&quot;, xlab=&quot;y&quot;) Probability.y&lt;-dnorm(Ndata$y, mean=0, sd=1) # density of actual values points(Probability.y~Ndata$y) Figure 2.1: Probability density function for the unit normal with the data points overlaid The likelihood of these data, conditioning on \\(\\mu=0\\) and \\(\\sigma^2=1\\), is proportional to the product of the densities (read off the y-axis on Figure 2.1): prod(dnorm(Ndata$y, mean = 0, sd = 1)) ## [1] 0.003113051 Of course we don’t know the true mean and variance and so we may want to ask how probable the data would be if, say, \\(\\mu=0\\), and \\(\\sigma^2=0.5\\): prod(dnorm(Ndata$y, mean = 0, sd = sqrt(0.5))) ## [1] 0.005424967 It would seem that the data are more likely under this set of parameters than the true parameters, which we must expect some of the time just from random sampling. To get some idea as to why this might be the case we can overlay the two densities (Figure 2.2), and we can see that although some data points (e.g. -1.015) are more likely with the true parameters, in aggregate the new parameters produce a higher likelihood. Figure 2.2: Two probability density functions for normal distributions with means of zero, and a variance of one (black line) and a variance of 0.5 (red line). The data points are overlaid. The likelihood of the data can be calculated on a grid of possible parameter values to produce a likelihood surface, as in Figure 2.3. The densities on the contours have been scaled so they are relative to the density of the parameter values that have the highest density (the maximum likelihood estimate of the two parameters). Two things are apparent. First, although the surface is symmetric about the line \\(\\mu = \\hat{\\mu}\\) (where \\(\\hat{}\\) stands for maximum likelihood estimate) the surface is far from symmetric about the line \\(\\sigma^{2} = \\hat{\\sigma}^{2}\\). Second, there are a large range of parameter values for which the data are only 10 times less likely than if the data were generated under the maximum likelihood estimates. Figure 2.3: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\sigma^{2})\\). The likelihood has been normalised so that the maximum likelihood has a value of one. 2.2.1 Maximum Likelihood (ML) The ML estimator is the combination of \\(\\mu\\) and \\(\\sigma^{2}\\) that make the data most likely. Although we could evaluate the density on a grid of parameter values (as we did to produce Figure 2.3) in order to locate the maximum, for such a simple problem the ML estimator can be derived analytically. However, so we don’t have to meet some nasty maths later, I’ll introduce and use one of R’s generic optimising routines that can be used to maximise the likelihood function (in practice, the log-likelihood is maximised to avoid numerical problems): lik &lt;- function(par, y, log = FALSE) { if (log) { l &lt;- sum(dnorm(y, mean = par[1], sd = sqrt(par[2]), log = TRUE)) } else { l &lt;- prod(dnorm(y, mean = par[1], sd = sqrt(par[2]))) } return(l) } # function which takes the parameter vector (mean and *variance*) and the data # and returns the (log) likelihood MLest &lt;- optim(c(mean = 0, var = 1), fn = lik, y = Ndata$y, log = TRUE, control = list(fnscale = -1, reltol = 1e-16))$par The first call to optim are starting values for the optimisation algorithm, and the second argument (fn) is the function to be maximised. By default optim will try to minimise the function hence multiplying by -1 (fnscale = -1). The algorithm has successfully found the mode: MLest ## mean var ## -0.2745619 0.3955995 Alternatively we could also fit the model using glm, which by default assumes the response is normal: m1a.1 &lt;- glm(y ~ 1, data = Ndata) summary(m1a.1) ## ## Call: ## glm(formula = y ~ 1, data = Ndata) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2746 0.3145 -0.873 0.432 ## ## (Dispersion parameter for gaussian family taken to be 0.4944994) ## ## Null deviance: 1.978 on 4 degrees of freedom ## Residual deviance: 1.978 on 4 degrees of freedom ## AIC: 13.553 ## ## Number of Fisher Scoring iterations: 2 Here we see that although the estimate of the mean (intercept) is the same, the estimate of the variance (the dispersion parameter: 0.494) is higher when fitting the model using glm. In fact the ML estimate is a factor \\(\\frac{n}{n-1}\\) smaller because the glm estimate has used Bessel’s correction: MLest[&quot;var&quot;] * (5/4) ## var ## 0.4944994 2.2.2 Restricted Maximum Likelihood (REML) Why do we use Bessel’s correction? Imagine we had only observed the first two values of \\({\\bf y}\\) (Figure 2.4). The variance is defined as the average squared distance between a random variable and the true mean. However, the ML estimator of the variance is the average squared distance between the random variable and the ML estimate of the mean. Since the ML estimator of the mean is the average of the two numbers (the dashed line) then the average squared distance will always be smaller than if the true mean was used, unless the ML estimate of the mean and the true mean coincide. This is why we use Bessel’s \\(n-1\\) correction when estimating the variance from the sum of squares, or why we divide by \\(n-n_p\\) when estimating the residual variance in a liner model with \\(n_p\\) parameters, or more generally why we use REML in linear mixed models. Why these corrections for uncertainty in the mean, or model parameters, have this form can be understood from a Bayesian perspective (see Section 2.6). Figure 2.4: Probability density function for the unit normal with two realisations overlaid. The solid vertical line is the true mean, whereas the vertical dashed line is the mean of the two realisations (the ML estimator of the mean). The variance is the expected squared distance between the true mean and the realisations. The ML estimator of the variance is the average squared distance between the ML mean and the realisations (horizontal dashed lines), which is always smaller than the average squared distance between the true mean and the realisations (horizontal solid lines) 2.3 Prior Distribution \\(\\texttt{MCMCglmm}\\) uses a normal prior for the fixed effects and an inverse-Wishart prior for the residual variance. In the current model their is a single fixed effect (\\(\\mu\\)) and a scalar (residual) variance (\\(\\sigma^2\\)). For the mean we will use the default prior - a diffuse normal centred around zero but with very large variance (\\(10^{8}\\)). For the residual variance, the inverse-Wishart prior takes two scalar parameters. In \\(\\texttt{MCMCglmm}\\) this is parameterised through the parameters \\(\\texttt{V}\\) and \\(\\texttt{nu}\\). The distribution tends to a point mass on \\(\\texttt{V}\\) as the degree of belief parameter, \\(\\texttt{nu}\\) goes to infinity. We will defer a full discussion of the inverse-Wishart prior to Section 2.6 and for now we will use the prior specification \\(\\texttt{V}=1\\) and \\(\\texttt{nu}=0.002\\) which used to be frequently used for variances. The function dprior can be used to obtain the prior density for a variance (or standard deviation) given a \\(\\texttt{MCMCglmm}\\) specification and we can plot this in order to visualise what the distribution looks like (Figure 2.5). Figure 2.5: Probability density function for a univariate inverse-Wishart with the variance at the limit set to 1 (\\(\\texttt{V}=1\\)) and a degree of belief parameter set to 0.002 (\\(\\texttt{nu}=0.002\\)). As before we can write a function for calculating the (log) prior probability: prior.p &lt;- function(par, priorB, priorR, log = FALSE) { if (log) { d &lt;- dnorm(par[1], mean = priorB$mu, sd = sqrt(priorB$V), log = TRUE) + dprior(par[2], priorR, log = TRUE) } else { d &lt;- dnorm(par[1], mean = priorB$mu, sd = sqrt(priorB$V)) * dprior(par[2], priorR) } return(d) } where priorR is a list with elements V and nu specifying the prior for the variance, and priorB is a list with elements mu and V specifying the prior for the mean. \\(\\texttt{MCMCglmm}\\) takes these prior specifications as a list: prior &lt;- list(R = list(V = 1, nu = 0.002), B = list(mu = 0, V = 1e+08)) 2.4 Posterior Distribution By multiplying the likelihood by the prior probability for that set of parameters we can get the posterior probability up to a proportional constant. We can write a function for doing this: likprior &lt;- function(par, y, priorB, priorR, log = FALSE) { if (log) { pd &lt;- lik(par, y, log = TRUE) + prior.p(par, priorB = priorB, priorR = priorR, log = TRUE) } else { pd &lt;- lik(par, y) * prior.p(par, priorB = priorB, priorR = priorR) } return(pd) } and we can overlay the posterior density (scaled by the posterior density at the posterior mode) on the likelihood surface we calculated before (Figure 2.3). Figure 2.6: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\sigma^{2})\\) in black, and the posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\) in red. The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. The prior distributions \\(Pr(\\mu)\\sim N(0, 10^8)\\) and \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=1, \\texttt{nu}=0.002)\\) were used. The prior has some influence on the posterior mode of the variance, and we can use an optimisation algorithm again to locate the mode: Best &lt;- optim(c(mean = 0, var = 1), fn = likprior, y = Ndata$y, priorB = prior$B, priorR = prior$R, log = TRUE, method = &quot;L-BFGS-B&quot;, lower = c(-1e+05, 1e-05), upper = c(1e+05, 1e+05), control = list(fnscale = -1, factr = 1e-16))$par Best ## mean var ## -0.2745613 0.2827783 The posterior mode for the mean is essentially identical to the ML estimate, but the posterior mode for the variance is even less than the ML estimate, which is known to be downwardly biased. The reason that the ML estimate is downwardly biased is because it did not take into account the uncertainty in the mean, as we saw when discussing the motivation behind REML. In a Bayesian analysis we can do this by evaluating the marginal distribution of \\(\\sigma^{2}\\) by averaging over the uncertainty in the mean. Before we do this, however, it will be instructive to see why this would be hard using our function that simply multiplies the likelihood by the prior (likprior). This function (the probabilities on the right-hand side) is only proprtional to the posterior density (the left-hand side), not equal to it, implying \\[Pr(\\mu, \\sigma^{2} | {\\bf y}) = \\frac{1}{C} Pr({\\bf y} | \\mu, \\sigma^{2})Pr(\\mu, \\sigma^{2})\\] where \\(C\\) is some constant. In some cases this is not an issue - when finding the posterior mode it was not an issue since the parameters that maximise the posterior density would also maximise the posterior density scaled by \\(C\\). Similarly, if we wanted to make relative statements about posterior probabilities, then \\(C\\) would cancel. For example, we can see how much more likely a variance of a half is versus a variance of one (assuming the mean is zero): p.0.5 &lt;- likprior(c(0, 0.5), y = Ndata$y, priorB = prior$B, priorR = prior$R) p.1.0 &lt;- likprior(c(0, 1), y = Ndata$y, priorB = prior$B, priorR = prior$R) p.0.5/p.1.0 ## [1] 3.484236 However, in many instances we would like to work with the normalised posterior density, so how do we get \\(C\\)? Well we know that if we took the posterior probability of being any combination of \\(\\mu\\) and \\(\\sigma^{2}\\) it must be equal to one: \\[\\int_{\\sigma^2}\\int_\\mu Pr(\\mu, \\sigma^{2} | {\\bf y})d\\mu d\\sigma^2=1\\] and so because \\[1 =\\frac{1}{C}\\int_{\\sigma^2}\\int_\\mu Pr({\\bf y} | \\mu, \\sigma^{2})Pr(\\mu, \\sigma^{2})d\\mu d\\sigma^2\\] we can integrate our parameters over likprior to get \\(C\\). Not easy, and requires numerical integration: C &lt;- adaptIntegrate(likprior, y = Ndata$y, priorR = prior$R, priorB = prior$B, lower = c(-Inf, 0), upper = c(Inf, Inf))$integral The posterior density for \\(\\mu=0\\) and \\(\\sigma^2=0.5\\) is p.0.5/C=0.937. Neither interesting or particularly interpretable, and often we want to perform additional integration in order to compute quantities of interest. For example, imagine we wanted to know the probability that the parameters lay in the region of parameter space we were plotting, i.e. lay in the square \\(\\mu = (-2,2)\\) and \\(\\sigma^{2} = (0,5)\\). To obtain this probability we need to calculate the definite integral \\[\\int_{\\sigma^{2}=0}^{\\sigma^{2}=5} \\int_{\\mu=-2}^{\\mu=2} Pr(\\mu, \\sigma^{2} | {\\bf y})d\\mu d\\sigma^2\\] which requires integrating our likprior over the same limits and rescaling by \\(C\\): p.square &lt;- adaptIntegrate(likprior, y = Ndata$y, priorR = prior$R, priorB = prior$B, lower = c(-2, 0), upper = c(2, 5))$integral p.square/C ## [1] 0.9816286 While this is doable for simple problems like this, numerical integration for high-dimensional problems is often not feasible and MCMC provides a viable alternative. We can fit this model in \\(\\texttt{MCMCglmm}\\) pretty much in the same way as we did using glm: m1a.2 &lt;- MCMCglmm(y ~ 1, data = Ndata, prior = prior, thin = 1) The Markov chain is drawing random (but often correlated) samples from the joint posterior distribution (depicted by the red contours in Figure 2.6). The element of the output called Sol contains the posterior samples for the mean, and the element called VCV contains the posterior samples for the variance. We can produce a scatter plot: points(cbind(m1a.2$Sol, m1a.2$VCV)) Figure 2.7: The posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\). The black dots are samples from the posterior using MCMC, and the red contours are calculated by evaluating the posterior density on a grid of parameter values. The contours are normalised so that the posterior mode has a value of one. and we see that MCMCglmm is sampling the same distribution as the posterior distribution calculated on a grid of possible parameter values (Figure 2.7). A very nice property of MCMC is that we can calculate probabilities from the output without having to explicitly perform integration. Earlier we calculated the probability that the mean lay between \\(\\pm2\\) and the variance was less than 5 (\\(\\texttt{p.square/C}=\\) 0.982). Because MCMC has sampled the posterior distribution randomly, this probability will be equal to the expected probability that we have drawn an MCMC sample from the region. We can obtain an estimate of this by seeing what proportion of our actual samples lie in this square: prop.table(table(m1a.2$Sol &gt; -2 &amp; m1a.2$Sol &lt; 2 &amp; m1a.2$VCV &lt; 5)) ## ## FALSE TRUE ## 0.0196 0.9804 There is Monte Carlo error in the answer (0.980) but if we collect a large number of samples then this can be minimised. 2.4.1 Marginal Posterior Distribution The marginal distribution is often of primary interest in statistical inference, because it represents our knowledge about a parameter given the data: \\[Pr(\\sigma^{2} | {\\bf y}) = \\int Pr(\\mu, \\sigma^{2} | {\\bf y})d\\mu \\label{marg-eq} \\tag{2.2}\\] after averaging over any nuisance parameters, such as the mean in this case. Using MCMC, we can obtain the marginal distribution of the variance by simply evaluating the draws in VCV ignoring (averaging over) the draws in Sol: hist(m1a.2$VCV) abline(v = Best[&quot;var&quot;], col = &quot;red&quot;) Figure 2.8: Histogram of samples from the marginal distribution of the variance \\(Pr(\\sigma^{2} | {\\bf y})\\) using MCMC. The vertical line is the joint posterior mode, which differs slightly from the marginal posterior mode (the peak of the marginal distribution). In this example the marginal mode and the joint mode are very similar, although this is not necessarily the case and can depend on both the data and the prior. Section 2.6 covers properties of the inverse-Wishart prior in detail. 2.4.2 Credible Intervals 2.5 MCMC In order to be confident that \\(\\texttt{MCMCglmm}\\) has successfully sampled the posterior distribution it will be necessary to have a basic understanding of how MCMC works. The aim of MCMC is to sample parameter values from their posterior distribution, shown exactly (up to proportionality) in Figure 2.6. In all but the very simplest cases this distribution is not of a known form and we cannot sample from it directly by using functions such as rnorm. However we can set up a random walk in parameter space such that the chance a walker visits a particular set of parameter values is proportional to their posterior density. Importantly, this can done without needing to normalise the posterior density by \\(C\\). 2.5.1 Starting values First we need to initialise the chain and specify a set of parameter values from which the chain can start moving through parameter space. In practice, it’s generally a good idea to start the chain in region of relatively high probability. Although starting configurations can be set by the user using the start argument, in general the heuristic techniques used by \\(\\texttt{MCMCglmm}\\) seem to work quite well. We will denote the parameter values of the starting configuration (time \\(t=0\\)) as \\(\\mu_{t=0}\\) and \\({\\sigma^{2}}_{t=0}\\). There are several ways in which we can get the chain to move in parameter space, and the main techniques used in \\(\\texttt{MCMCglmm}\\) are Gibbs sampling and Metropolis-Hastings updates. To illustrate, it will be easier to turn the contour plot of the posterior distribution into a perspective plot (Figure 2.9). Figure 2.9: The posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\). This perspective plot is equivalent to the contour plot in Figure 2.6 but it has been normalised by \\(C\\) and is equal to, not just proportional to, the posterior density. 2.5.2 Metropolis-Hastings updates After initialising the chain we need to decide where to go next, and this decision is based on two rules. First we have to generate a candidate destination, and then we need to decide whether to go there or stay where we are. There are many ways in which we could generate candidate parameter values, and \\(\\texttt{MCMCglmm}\\) uses a well tested and simple method. A random set of coordinates are picked from a multivariate normal distribution that is centred on the initial coordinates \\(\\mu_{t=0}\\) and \\(\\sigma^{2}_{t=0}\\). We will denote this new set of parameter values as \\(\\mu_{new}\\) and \\(\\sigma^{2}_{new}\\). The question then remains whether to move to this new set of parameter values or remain at our current parameter values now designated as old \\(\\mu_{old}=\\mu_{t=0}\\) and \\(\\sigma^{2}_{old}=\\sigma^{2}_{t=0}\\). If the posterior probability for the new set of parameter values is greater, then the chain moves to this new set of parameters and the chain has successfully completed an iteration: (\\(\\mu_{t=1} = \\mu_{new}\\) and \\(\\sigma^{2}_{t=1}=\\sigma^{2}_{new}\\)). If the new set of parameter values has a lower posterior probability then the chain may move there, but not all the time. The probability that the chain moves to low lying areas, is determined by the relative difference between the old and new posterior probabilities. If the posterior probability for \\(\\mu_{new}\\) and \\(\\sigma^{2}_{new}\\) is 5 times less than the posterior probability for \\(\\mu_{old}\\) and \\(\\sigma^{2}_{old}\\), then the chain would move to the new set of parameter values 1 in 5 times. If the move is successful then we set \\(\\mu_{t=1} = \\mu_{new}\\) and \\(\\sigma^{2}_{t=1}=\\sigma^{2}_{new}\\) as before, and if the move is unsuccessful then the chain stays where it is (\\(\\mu_{t=1} = \\mu_{old}\\) and \\(\\sigma^{2}_{t=1}=\\sigma^{2}_{old}\\)). Note that we only need to know the posterior density up to proportionality to make these calculations (i.e we could use likprior directly without knowing \\(C\\)). Using these rules we can record where the chain has travelled and generate an approximation of the posterior distribution. Basically, a histogram of Figure 2.9. Why Metropolis-Hastings updates work can perhaps be more easily understood in terms of a simpler toy example. Imagine we had a strong prior such that only two sets of parameter values had positive posterior probability: Set A (\\(\\mu_A\\) an \\(\\sigma^2_A\\)) with a posterior probability 5 times that of Set B. If the chain is currently at Set A then the candidate parameter values will be Set B and vice-versa. Since we move from Set A to Set B 20% of the time and we move from Set B to Set A 100% of the time, the odds of being in Set A versus Set B is 1:0.2 which is exactly equal to their posterior odds of 5:1. 2.5.3 Gibbs Sampling Gibbs sampling is a special case of Metropolis-Hastings updating, and \\(\\texttt{MCMCglmm}\\) uses Gibbs sampling to update most parameters. In the Metropolis-Hastings example above, the Markov Chain was allowed to move in both directions of parameter space simultaneously. An equally valid approach would have been to set up two Metropolis-Hastings schemes where the chain was first allowed to move along the \\(\\mu\\) axis, and then along the \\(\\sigma^{2}\\) axis. In Figure 2.10 I have cut the posterior distribution of Figure 2.9 in half, and the edge of the surface facing left is the conditional distribution of \\(\\mu\\) given that \\(\\sigma^{2}=1\\): \\[Pr(\\mu |\\sigma^{2}=1, \\boldsymbol{\\mathbf{y}}).\\] Figure 2.10: The posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\), but only for values of \\(\\sigma^{2}\\) between 1 and 5, rather than 0 to 5 (Figure 2.9. The edge of the surface facing left is the conditional distribution of the mean when \\(\\sigma^{2}=1\\) (\\(Pr(\\mu | {\\bf y}, \\sigma^{2}=1)\\)). This conditional distribution follows a normal distribution. If we were using Metropolis-Hastings updates to sample \\(\\mu\\) we would need to evaluate this density, which may be much simpler than evaluating the full density. In fact for some cases, the equation that describes this conditional distribution can be derived despite the equation for the complete joint distribution of Figure 2.9 remaining unknown. When the conditional distribution of \\(\\mu\\) is known we can use Gibbs sampling. Lets say the chain at a particular iteration is located at \\(\\sigma^{2}=1\\). If we updated \\(\\mu\\) using a Metropolis-Hastings algorithm we would generate a candidate value and evaluate its relative probability compared to the old value. This procedure would take place in the slice of posterior facing left in Figure 2.10. However, because we know the actual equation for this slice we can just generate a new value of \\(\\mu\\) directly. This is Gibbs sampling. The slice of the posterior that we can see in Figure 2.10 actually has a normal distribution. Because of the weak prior this normal distribution has a mean close to the mean of \\(\\bf{y}\\) and a variance close to \\(\\frac{\\sigma^{2}}{n} = \\frac{1}{n}\\). Gibbs sampling can be much more efficient than Metropolis-Hastings updates, especially when high dimensional conditional distributions are known, as is typical in GLMMs. A technical description of the sampling schemes used by \\(\\texttt{MCMCglmm}\\) is given in the Chapter @ref(#technical), but is perhaps not important to know. 2.5.4 MCMC Diagnostics When fitting a model using \\(\\texttt{MCMCglmm}\\) the parameter values through which the Markov chain has travelled are stored and returned. The length of the chain (the number of iterations) can be specified using the nitt argument1 (the default is 13,000), and should be long enough so that the posterior approximation is valid. If we had known the joint posterior distribution in Figure 2.9 we could have sampled directly from the posterior. If this had been the case, each successive value in the Markov chain would be independent of the previous value after conditioning on the data, \\({\\bf y}\\), and a thousand iterations of the chain would have produced a histogram that resembled Figure 2.9 very closely. However, generally we do not know the joint posterior distribution of the parameters, and for this reason the parameter values of the Markov chain at successive iterations are usually not independent and care needs to be taken regarding the validity of the approximation. \\(\\texttt{MCMCglmm}\\) returns the Markov chain as mcmc objects, which can be analysed using the coda package. The function autocorr from the package \\(\\texttt{coda}\\) reports the level of non-independence between successive samples in the chain: autocorr(m1a.2$Sol) ## , , (Intercept) ## ## (Intercept) ## Lag 0 1.0000000000 ## Lag 1 0.0295393869 ## Lag 5 0.0069888310 ## Lag 10 0.0200379700 ## Lag 50 0.0009886787 autocorr(m1a.2$VCV) ## , , units ## ## units ## Lag 0 1.0000000000 ## Lag 1 0.2473917848 ## Lag 5 0.0082937460 ## Lag 10 -0.0132076992 ## Lag 50 -0.0007171886 The correlation between successive samples is low for the mean (0.030) but a bit high for the variance (0.247). When auto-correlation is high we effectively have fewer samples from the posterior than we have saved. The function effectiveSize, also from \\(\\texttt{coda}\\), reports the effective number of samples saved - if the autocorrelation could be made zero this would be the number of samples required to give the same precision on the posterior mean. For the variance, the effective sample size is 6248 quite a bit less than the number of stored posterior samples 10000. When the effective sample size is low the chain needs to be run for longer, and this can lead to storage problems for high dimensional problems. The argument thin can be passed to \\(\\texttt{MCMCglmm}\\) specifying the intervals at which the Markov chain is stored. In model m1a.2 we specified thin=1 meaning we stored every iteration (the default is thin=10). I usually aim to store 1,000-2,000 effective posterior samples with the autocorrelation between successive stored samples less than 0.1. The approximation obtained from the Markov chain is conditional on the set of parameter values that were used to initialise the chain. In many cases the first iterations show a strong dependence on the starting parametrisation, but as the chain progresses this dependence may be lost. As the dependence on the starting parametrisation diminishes the chain is said to converge and the argument burnin can be passed to MCMCped specifying the number of iterations which must pass before samples are stored. The default burn-in period is 3,000 iterations. Assessing convergence of the chain is notoriously difficult, but visual inspection and diagnostic tools such as gelman.diag often suffice. For difficult models, running several chains from different starting values and ensuring they have all converged on the same distribution is a good idea. plot(m1a.2$Sol) Figure 2.11: Summary plot of the Markov Chain for the intercept. The left plot is a trace of the sampled posterior, and can be thought of as a time-series. The right plot is a density estimate, and can be thought of a smoothed histogram approximating the posterior. On the left of Figure 2.11 is a time-series of the parameter as the MCMC iterates, and on the right is a posterior density estimate of the parameter (a smoothed histogram of the output). If the model has converged there should be no trend in the time-series. The equivalent plot for the variance is a little hard to see on the original scale, but on the log scale the chain looks good (Figure 2.12: plot(log(m1a.2$VCV)) Figure 2.12: Summary plot of the Markov Chain for the logged variance. The logged variance was plotted rather than the variance because it was easier to visualise. The left plot is a trace of the sampled posterior, and can be thought of as a time-series. The right plot is a density estimate, and can be thought of a smoothed histogram approximating the posterior. 2.6 Priors for Residual Variances \\(\\texttt{MCMCglmm}\\) uses an inverse-Wishart prior for the residual variance and so here will cover the properties of the scalar inverse-Wishart distribution and prior. Section 5.3 can be consulted for inverse-Wishart covariance matrices. For random effect (co)variances it is possible to use scaled non-central \\(F\\)-distribution priors which is strongly recommended (see Section 4.6). For a single variance, the inverse-Wishart prior is parameterised through the parameters \\(\\texttt{V}\\) and \\(\\texttt{nu}\\) in MCMCglmm. The MCMCglmm parameterisation of the inverse-Wishart is not standard but I find it intuitive: the prior information is equivalent to observing \\(\\texttt{nu}\\) residuals with variance \\(\\texttt{V}\\). Consequently, the distribution concentrates on \\(\\texttt{V}\\) as the degree of belief parameter, \\(\\texttt{nu}\\) increases. The distribution tends to be right skewed when \\(\\texttt{nu}\\) is not very large, with a mode of \\(\\texttt{V}\\frac{\\texttt{nu}}{\\texttt{nu}+2}\\) but a mean of \\(\\texttt{V}\\frac{\\texttt{nu}}{\\texttt{nu}-2}\\) (which is not defined for \\(\\texttt{nu}&lt;2\\)). Figure 2.13 plots the probability density functions holding \\(\\texttt{V}\\) equal to one but with \\(\\texttt{nu}\\) varying. Figure 2.13: Probability density function for a univariate inverse-Wishart with the variance at the limit set to 1 (\\(\\texttt{V}=1\\)) and varying degree of belief parameter (\\(\\texttt{nu}\\)). With \\(\\texttt{V}=1\\) these distributions are equivalent to inverse gamma distributions with shape and scale parameters set to \\(\\texttt{nu}\\)/2. For single variances the inverse-gamma is a special case of the inverse-Wishart2, and with \\(\\texttt{V}=1\\), the shape and scale of the inverse-gamma are both equal to \\(\\texttt{nu}/2\\). The inverse-gamma with shape and scale equal to 0.001 used to be commonly used. The motivation behind the prior was that making \\(\\texttt{nu}\\) small would result in a less influential prior because is was equivalent to only observing 0.2% (\\(\\texttt{nu}=0.002\\)) of a residual a priori. Setting \\(\\texttt{nu}=0\\) was avoided because this does not define a valid distribution. A probability distribution must integrate to one because a variable must have some value, and this condition is not met when setting \\(\\texttt{nu}=0\\). The prior distribution is said to be improper. In the example here, where \\(\\texttt{V}\\) is a single variance, the prior is only proper when \\(\\texttt{V}&gt;0\\) and \\(\\texttt{nu}&gt;0\\). Although improper priors do not specify valid prior distributions and therefore the Bayesian credentials of any model may be questionable, \\(\\texttt{MCMCglmm}\\) does allow them as they have some useful properties. 2.6.1 Improper Priors When improper priors are used their are two potential problems that may be encountered. The first is that if the data do not contain enough information, the posterior distribution itself may be improper, and any results obtained from \\(\\texttt{MCMCglmm}\\) will be meaningless. In addition, with proper priors there is a zero probability of a variance component being exactly zero but this is not necessarily the case with improper priors. This can produce numerical problems (trying to divide through by zero) and can also result in a reducible chain. A reducible chain is one which gets ‘stuck’ at some parameter value(s) and cannot escape. This is usually obvious from the mcmc plots but \\(\\texttt{MCMCglmm}\\) will often terminate before the analysis has finished with an error message of the form: ill-conditioned G/R structure: use proper priors ... However, improper priors do have some useful properties and in fact the default prior for the variances in has \\(\\texttt{nu}=0\\) (the value of \\(\\texttt{V}\\) is irrelevant). It is tempting to think that this prior is flat for the variance3, but it is in fact flat for the log-variance and therefore puts more weight on small values than a prior that is flat on the variance (Figure 2.14) - see Section 2.7. prior.m1a.3 &lt;- list(R = list(V = 1, nu = 0)) m1a.3 &lt;- MCMCglmm(y ~ 1, data = Ndata, thin = 1, prior = prior.m1a.3) Figure 2.14: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\sigma^{2})\\) in black, and an MCMC approximation for the posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\) in red. The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. An almost flat prior was used for the mean \\(Pr(\\mu)\\sim N(0, 10^8)\\) and a flat prior was used for the log-variance \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=1, \\texttt{nu}=0)\\)). Consequently, the mode of the marginal posterior distribution lies even below the ML estimate 2.15. Figure 2.15: An MCMC approximation for the marginal posterior distribution of the variance \\(Pr(\\sigma^{2} | {\\bf y})\\). A non-informative prior specification was used (\\(Pr(\\mu)\\sim N(0, 10^8)\\) and \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=0, \\texttt{nu}=0)\\)). The ML and REML estimates are plotted in blue and red,m respectively. Although inverse-Wishart distributions with negative degree of belief parameters are not defined, the resulting posterior distribution can be defined and proper if there is sufficient replication. When \\(\\texttt{V}=0\\) and \\(\\texttt{nu}=-1\\) we have a flat prior on the standard deviation over the interval \\((0,\\infty]\\). When \\(\\texttt{V}=0\\) and \\(\\texttt{nu}=-2\\) we have a flat prior on the variance. Since the default prior for the mean is normal with a very large variance (\\(10^8\\)) the prior for the mean is also essentially flat, resulting in a prior probability that is proportional to some constant for all possible parameter values. The posterior density in such cases is equal to the likelihood: \\[Pr(\\mu, \\sigma^{2} | {\\bf y}) \\propto Pr({\\bf y} | \\mu, \\sigma^{2}) \\label{fprior-eq} \\tag{2.3}\\] We can overlay the joint posterior distribution on the likelihood surface (Figure 2.16) and see that the two things are in close agreement, up to Monte Carlo error. prior.m1a.4 &lt;- list(R = list(V = 1e-16, nu = -2)) m1a.4 &lt;- MCMCglmm(y ~ 1, data = Ndata, thin = 1, prior = prior.m1a.4) Figure 2.16: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\sigma^{2})\\) in black, and an MCMC approximation for the posterior distribution \\(Pr(\\mu, \\sigma^{2} | {\\bf y})\\) in red. The likelihood has been normalised so that the maximum likelihood has a value of one, and the posterior distribution has been normalised so that the posterior mode has a value of one. Almost flat priors were used for the mean (\\(Pr(\\mu)\\sim N(0, 10^8)\\) and the variance \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=10^{-16}, \\texttt{nu}=-2)\\)) and so the posterior distribution is equivalent to the likelihood. Here, the joint posterior mode coincides with the ML estimates, as expected (Figure 2.16). In addition, the mode of the marginal distribution for the variance is equivalent to the REML estimator (See Figure 2.17). Indeed, the REML estimator can be seen as marginalising the mean under a flat prior, and for this reason is sometimes referred to as the marginal likelihood rather than the restricted likelihood (REML). Figure 2.17: An MCMC approximation for the marginal posterior distribution of the variance \\(Pr(\\sigma^{2} | {\\bf y})\\). An almost flat prior specification was used (\\(Pr(\\mu)\\sim N(0, 10^8)\\) and \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=10^{-16}, \\texttt{nu}=-2)\\)) and the REML estimator of the variance (red line) coincides with the marginal posterior mode. 2.7 Transformations Sometimes we would like to know the posterior distribution for some transformation of the parameters. For example, we may wish to obtain the posterior distribution for \\(\\log(\\sigma^2)\\). With MCMC this is easy - we can just transform the posterior samples (i.e. log(m1a.2$VCV)) and treat these as we would any other set of posterior samples. However, it is useful to understand how probabilities are expected to behave under these transformations. The key idea is that the probability of some function of \\(x\\), \\(f(x)\\), is equal to the probability of \\(x\\) divided by the Jacobian. When the transform only involves single parameters the Jacobian is \\(|df(x)/dx|\\) which for the log-transform is \\(1/x\\). For model m1a.3 we used an inverse-Wishart prior with \\(\\texttt{V=1}\\) and \\(\\texttt{nu=0}\\). The posterior density is plotted in Figure 2.14 and does not appear to coincide with the likelihood - there is more posterior density at small values of \\(\\sigma^2\\) However, this is because the posterior density is expressed for \\(\\sigma^2\\) and the prior is only flat in \\(log(\\sigma^2)\\). By multiplying the posterior density for \\(\\sigma^2\\) by \\(\\sigma^2\\) (i.e. dividing by \\(1/x\\) where \\(x\\) is \\(\\sigma^2\\)) we would obtain the joint density of \\(\\mu\\) and \\(log(\\sigma^2)\\). This would reduce the density for small values of \\(\\sigma^2\\) and increase the density for large values (since the density is scaled by \\(\\sigma^2\\)). Indeed, plotting the posterior distribution of \\(log(\\sigma^2)\\) shows this effect and the posterior is in agreement with the likelihood, as expected (Figure 2.18). Figure 2.18: Likelihood surface for the likelihood \\(Pr({\\bf y}|\\mu, \\log(\\sigma^{2}))\\) in black, and an MCMC approximation for the posterior distribution \\(Pr(\\mu, \\log(\\sigma^{2}) | {\\bf y})\\) in red. An almost flat prior was used for the mean \\(Pr(\\mu)\\sim N(0, 10^8)\\) and a flat prior was used for the log-variance \\(Pr(\\sigma^{2})\\sim IW(\\texttt{V}=1, \\texttt{nu}=0)\\). The double t is because I cannot spell.↩︎ The inverse gamma is a special case of the inverse-Wishart, although it is parametrised using \\(\\texttt{shape}\\) and \\(\\texttt{scale}\\), where \\(\\texttt{nu}=2\\ast\\texttt{shape}\\) and \\(\\texttt{V} = \\frac{\\texttt{scale}}{\\texttt{shape}}\\) (or \\(\\texttt{shape} = \\frac{\\texttt{nu}}{2}\\) and \\(\\texttt{scale} = \\texttt{V}\\frac{\\texttt{nu}}{2}\\)). There is no density function for the inverse-gamma in base R. However, the prior specification can be passed to the function \\(\\texttt{dprior}\\) to obtain the density: for example, dprior(1, prior=list(V=1, nu=0.002)).↩︎ Embarrassingly, I claimed that \\(\\texttt{V=1}\\) and \\(\\texttt{nu=0}\\) was flat in the original CourseNotes and I had forgotten to rescale the posterior density by the Jacobian when generating the equivalent contour plot to Figure 2.14 (which is computed on values of \\(\\textrm{log}(\\sigma^2)\\) for better results, and then transformed).↩︎ "],["glm.html", "3 Linear and Generalised Linear Models 3.1 Linear Model (LM) 3.2 Generalised Linear Model (GLM) 3.3 Poisson GLM 3.4 Overdispersion 3.5 Prediction in GLM 3.6 Binomial and Bernoulli GLM 3.7 Ordinal Data 3.8 Non-zero Binomial Data 3.9 Complete Separation", " 3 Linear and Generalised Linear Models 3.1 Linear Model (LM) A linear model is one in which unknown parameters are multiplied by (functions of) observed variables and then added together to give a prediction for the response variable. As an example, lets take the results from a Swedish experiment from the sixties: The experiment involved enforcing speed limits on Swedish roads on some days, but on other days letting everyone drive as fast as they liked. The response variable (y) is the number of accidents recorded. The experiment was conducted in 1961 and 1962 for 92 days in each year. As a first attempt we could specify the linear model: y ~ limit + year + day but what does this mean? 3.1.1 Linear Predictors The model formula defines a set of simultaneous (linear) equations \\[\\begin{array}{cl} E[y\\texttt{[1]}] &amp;=\\beta_{1}+\\beta_{2}(\\texttt{limit[1]==&quot;yes&quot;})+\\beta_{3}(\\texttt{year[1]==&quot;1962&quot;})+\\beta_{4}\\texttt{day[1]}\\\\ E[y\\texttt{[2]}] &amp;= \\beta_{1}+\\beta_{2}(\\texttt{limit[2]==&quot;yes&quot;})+\\beta_{3}(\\texttt{year[2]==&quot;1962&quot;})+\\beta_{4}\\texttt{day[2]}\\\\ \\vdots&amp;=\\vdots\\\\ E[y\\texttt{[184]}] &amp;= \\beta_{1}+\\beta_{2}(\\texttt{limit[184]==&quot;yes&quot;})+\\beta_{3}(\\texttt{year[184]==&quot;1962&quot;})+\\beta_{4}\\texttt{day[184]}\\\\ \\end{array} \\label{SE-eq} \\tag{3.1}\\] where the \\(\\beta\\)’s are the unknown coefficients to be estimated, and the variables in \\(\\texttt{this font}\\) are observed predictors. Continuous predictors such as day remain unchanged, but categorical predictors are expanded into a series of binary variables of the form ‘do the data come from 1961, yes or no?’, ‘do the data come from 1962, yes or no?’, and so on for as many years for which there are data. It is cumbersome to write out the equation for each data point in this way, and a more compact way of representing the system of equations is \\[ E[{\\bf y}] = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}} \\tag{3.2} \\] where \\({\\bf X}\\) is called a design matrix and contains the predictor information for all observations, and \\({\\boldsymbol{\\mathbf{\\beta}}} = [\\beta_{1}\\ \\beta_{2}\\ \\beta_{3}\\ \\beta_{4}]^{&#39;}\\) is the vector of parameters. Here, \\(E[{\\bf y}]\\) is a vector of the 184 expected values. X &lt;- model.matrix(y ~ limit + year + day, data = Traffic) X[c(1, 2, 184), ] ## (Intercept) limityes year1962 day ## 1 1 0 0 1 ## 2 1 0 0 2 ## 184 1 1 1 92 The binary predictors do the data come from 1961, yes or no? and there was no speed limit, yes or no? do not appear. These are the first factor levels of year and limit respectively, and are absorbed into the global intercept (\\(\\beta_{1}\\)) which is fitted by default in R. Hence the expected number of accidents for the four combinations (on day zero) are \\(\\beta_{1}\\) for 1961 with no speed limit, \\(\\beta_{1}+\\beta_{2}\\) for 1961 with a speed limit, \\(\\beta_{1}+\\beta_{3}\\) for 1962 with no speed limit and \\(\\beta_{1}+\\beta_{2}+\\beta_{3}\\) for 1962 with a speed limit. The simultaneous equations defined by Equation (3.2) cannot be solved directly because we do not know the left-hand side - expected values of \\(y\\). We only know the observed value, which we assume is distributed around the expected value with some error. In a normal linear model we assume that these errors (residuals) are normally distributed: \\[{\\bf y}-{\\bf X}{\\boldsymbol{\\mathbf{\\beta}}} = {\\bf e} \\sim N(0, \\sigma^{2}_{e}{\\bf I})\\] \\({\\bf I}\\) is a \\(184\\times 184\\) identity matrix. It has ones along the diagonal, and zeros in the off-diagonals. The zero off-diagonals imply that the residuals are uncorrelated, and the ones along the diagonal imply that they have the same variance (\\(\\sigma^{2}_{e}\\)). Thinking about the distribution of residuals is less helpful when we move on to GLM’s and so I prefer to think about the model in the form: \\[{\\bf y}\\sim N({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}, \\sigma^{2}_{e}{\\bf I})\\] and say the response is conditionally normal, with the conditioning on the model (\\({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}\\)). It is important to note that this is different from saying the response is normal. If having a speed limit had a very strong effect the (marginal) distribution of the response may be bimodal and far from normal, and yet by including speed-limit as a predictor, conditional normality may be achieved. We could use MCMCglmm to fit this model, but to connect better with what comes next, let’s use glm to estimate \\({\\bf \\beta}\\) and \\(\\sigma^{2}_{e}\\) assuming that the number of accidents follow a conditional normal distribution (the MCMCglmm syntax is identical): m2a.1 &lt;- glm(y ~ limit + year + day, data = Traffic) summary(m2a.1) ## ## Call: ## glm(formula = y ~ limit + year + day, data = Traffic) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.13111 1.45169 14.556 &lt; 2e-16 *** ## limityes -3.66427 1.35559 -2.703 0.00753 ** ## year1962 -1.34853 1.31121 -1.028 0.30511 ## day 0.05304 0.02355 2.252 0.02552 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 71.80587) ## ## Null deviance: 14128 on 183 degrees of freedom ## Residual deviance: 12925 on 180 degrees of freedom ## AIC: 1314.5 ## ## Number of Fisher Scoring iterations: 2 On day zero in 1961 in the absence of a speed limit we expect 21.1 accidents (the intercept). With a speed limit we expect 3.7 fewer accidents and we can quite confidently reject the null-hypothesis of no effect - particularly if we were willing to use a one-tailed test, which seems reasonable. There are 1.3 fewer accidents in 1962, although this could just be due to chance, and for every unit increase in \\(\\texttt{day}\\) the number of accidents is predicted to go up by 0.05. The \\(\\texttt{day}\\) variable is encoded as integers from 1 to 92 with the same \\(\\texttt{day}\\) in different years being comparable (for example, the same day of the week and roughly the same date). If \\(\\texttt{day}\\)’s are evenly spaced throughout the year the \\(\\texttt{day}\\) effect is roughly the effect of increasing calender date by four (365/92) days. The estimate of the residual variance, \\(\\sigma^2_e\\), is the dispersion parameter (71.8). Because the number of accidents are count data we might worry about the assumption of conditional normality, and indeed the residuals show the typical right skew: hist(resid(m2a.1)) Figure 3.1: Histogram of residuals from model m2a.1 which assumed they followed a Normal distribution. It’s not extreme, and the conclusions probably won’t change, but we could assume that the data follow some other distribution. 3.2 Generalised Linear Model (GLM) Generalised linear models extend the linear model to non-normal data. They are essentially the same as the linear model described above, except they differ in two aspects. First, it is not necessarily the mean response that is predicted, but some function of the mean response. This function is called the link function. For example, with a log link we are trying to predict the logged expectation: \\[\\textrm{log}(E[{\\bf y}]) = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}\\] or alternatively \\[E[{\\bf y}] = \\textrm{exp}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}})\\] where \\(\\textrm{exp}\\) is the inverse of the log link function- exponentiating. The second difference is that many distributions are single parameter distributions for which a variance does not need to be estimated because it can be inferred from the mean. For example, we could assume that the number of accidents are Poisson distributed, in which case we also make the assumption that the variance is equal to the expected value. Technically, GLM’s only apply to a restricted set of distributions (those in the exponential family) but \\(\\texttt{MCMCglmm}\\) can accommodate a range of GLM-like models for other distributions (see Table 11.1). 3.3 Poisson GLM For now we will concentrate on a Poisson GLM with log link (the default link function for the Poisson distribution): m2a.2 &lt;- glm(y ~ limit + year + day, family = poisson, data = Traffic) summary(m2a.2) ## ## Call: ## glm(formula = y ~ limit + year + day, family = poisson, data = Traffic) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.0467406 0.0372985 81.685 &lt; 2e-16 *** ## limityes -0.1749337 0.0355784 -4.917 8.79e-07 *** ## year1962 -0.0605503 0.0334364 -1.811 0.0702 . ## day 0.0024164 0.0005964 4.052 5.09e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 625.25 on 183 degrees of freedom ## Residual deviance: 569.25 on 180 degrees of freedom ## AIC: 1467.2 ## ## Number of Fisher Scoring iterations: 4 While the sign of the effects are comparable to that seen in the linear model, their numerical values are completely different and the significance of all effects has increased dramatically. Should we worry? The model is defined on the log scale and so to get back to the data scale we need to exponentiate. Exponentiating the intercept gives us the predicted number of accidents on day zero in 1961 without a speed limit: exp(m2a.2$coef[&quot;(Intercept)&quot;]) ## (Intercept) ## 21.04663 which is very close to the intercept in the linear model (21.131), which is reassuring. To get the prediction for the same day with a speed limit we need to add the \\(\\texttt{limityes}\\) coefficient exp(m2a.2$coef[&quot;(Intercept)&quot;] + m2a.2$coef[&quot;limityes&quot;]) ## (Intercept) ## 17.66892 With a speed limit there are expected to be 0.840 as many accidents than if there was no speed limit. This value can be more directly obtained: exp(m2a.2$coef[&quot;limityes&quot;]) ## limityes ## 0.8395127 and holds true for any given day in either year. The proportional change is identical because the model is linear on the log scale and \\(exp(\\beta+\\dots)=exp(\\beta)exp(\\dots)\\). There is not always a direct relationship with the corresponding coefficients from the linear model but we can reassure ourselves that the parameters have the same qualitative meaning. For example, for \\(\\texttt{day}\\) 0 in 1961 the linear model predicts a drop from 21.1 to 17.5 accidents when a speed limit is in place - around 0.83 as many accidents, comparable to that predicted in the log-linear model. So in terms of the reported coefficients, the linear model and the Poisson log-linear model are roughly consistent with each other. However, in terms of accurately quantifying the uncertainty in those coefficients the Poisson model has a serious problem - it is very over confident. 3.4 Overdispersion Most count data do not conform to a Poisson distribution because the variance in the response exceeds the expectation. In the summary to m2a.2 the ratio of the residual deviance to the residual degrees of freedom is 3.162 which means, roughly speaking, there is 3.2 times more variation in our response (after conditioning on the model) than what we expect. This is known as overdispersion and it is easy to see how it arises, and why it is so common. If the predictor data had not been available to us then the only model we could have fitted was one with just an intercept: m2a.3 &lt;- glm(y ~ 1, data = Traffic, family = &quot;poisson&quot;) summary(m2a.3) ## ## Call: ## glm(formula = y ~ 1, family = &quot;poisson&quot;, data = Traffic) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.07033 0.01588 193.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 625.25 on 183 degrees of freedom ## Residual deviance: 625.25 on 183 degrees of freedom ## AIC: 1517.2 ## ## Number of Fisher Scoring iterations: 4 for which the residual variance exceeds that expected by a factor of 3.5. Of course, the variability in the residuals must go up if there are factors that influence the number of accidents, but which we hadn’t measured. It’s likely that in most studies there are things that influence the response that haven’t been measured, and even if each thing has a small effect individually, in aggregate they can cause substantial overdispersion. 3.4.1 Multiplicative Overdispersion There are two ways of dealing with overdispersion. With glm the distribution name can be prefixed with quasi and a dispersion parameter estimated: m2a.4 &lt;- glm(y ~ limit + year + day, family = quasipoisson, data = Traffic) summary(m2a.4) ## ## Call: ## glm(formula = y ~ limit + year + day, family = quasipoisson, ## data = Traffic) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.046741 0.067843 44.909 &lt; 2e-16 *** ## limityes -0.174934 0.064714 -2.703 0.00753 ** ## year1962 -0.060550 0.060818 -0.996 0.32078 ## day 0.002416 0.001085 2.227 0.02716 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 3.308492) ## ## Null deviance: 625.25 on 183 degrees of freedom ## Residual deviance: 569.25 on 180 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 glm uses a multiplicative model of overdispersion and so the estimate of the dispersion parameter is roughly equivalent to how many times greater the variance is than expected, after taking into account the predictor variables. You will notice that although the parameter estimates have changed very little, the standard errors have gone up and the significance gone down. Overdispersion, if not dealt with, can result in extreme anti-conservatism. For example, the second lowest number of accidents (8) occurred on \\(\\texttt{day}\\) 91 of 1961 without a speed limit. Our model predicts this should have been the second worst day for accidents over the whole two years, and the probability of observing 8 or less accidents on this day is predicted to be approximately 3 in a 100,000: ppois(8, exp(m2a.2$coef[&quot;(Intercept)&quot;] + 91 * m2a.2$coef[&quot;day&quot;])) ## [1] 3.195037e-05 If we did not accommodate the overdispersion, anything additional we put in in the model that could potentially explain such an improbable occurrence would come out as significant even if in reality it wasn’t important. This is because there simply isn’t any flexibility in the null model to accommodate such occurrences. For example, if the extreme value happened to be associated with a particular level of a categorical predictor or happened to be associated with an extreme value of some continuous predictor, then the coefficients associated with these predictors may well come out as significant. However, under a more plausible null model the extreme observations may not be too surprising and there may be little support for the predictors having an effect on the response. A more plausible model, and one that we’ve alluded to, would be to allow the number of accidents to vary across sampling points due to unmeasured variables. This would allow the variation in the number of accidents to exceed the predicted mean based on the measured variables (the assumption of the standard Poisson). 3.4.2 Additive Overdispersion I believe that a model assuming all relevant variables have been measured or controlled for, should not be the default model, and so when you specify family=poisson in \\(\\texttt{MCMCglmm}\\), overdispersion is always dealt with4. However, \\(\\texttt{MCMCglmm}\\) does not use a multiplicative model, but an additive model. prior &lt;- list(R = list(V = 1, nu = 0.002)) m2a.5 &lt;- MCMCglmm(y ~ limit + year + day, family = &quot;poisson&quot;, data = Traffic, prior = prior, pl = TRUE) The element Sol contains the posterior distribution of the coefficients of the linear model, and we can plot their marginal distributions: Figure 3.2: MCMC summary plot for the coefficients from a Poisson glm (model m2a.5). Note that the posterior distribution for the year1962 spans zero, in agreement with the quasipoisson glm model, and that in general the estimates for the two models (and their uncertainty - see Section 2.4.2) are broadly similar: summary(m2a.4) ## ## Call: ## glm(formula = y ~ limit + year + day, family = quasipoisson, ## data = Traffic) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.046741 0.067843 44.909 &lt; 2e-16 *** ## limityes -0.174934 0.064714 -2.703 0.00753 ** ## year1962 -0.060550 0.060818 -0.996 0.32078 ## day 0.002416 0.001085 2.227 0.02716 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 3.308492) ## ## Null deviance: 625.25 on 183 degrees of freedom ## Residual deviance: 569.25 on 180 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 With additive overdispersion the linear predictor includes a ‘residual’, for which a residual variance is estimated (hence our prior specification). \\[E[{\\bf y}] = \\textrm{exp}({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf e})\\] At this point it will be handy to represent the linear model in a new way: \\[{\\bf l} = {\\boldsymbol{\\mathbf{\\eta}}}+{\\bf e}\\] where \\({\\bf l}\\) is a vector of latent variables (\\(\\textrm{log}(E[{\\bf y}])\\) in this case) and \\({\\boldsymbol{\\mathbf{\\eta}}}\\) is the usual symbol for the linear predictor (\\({\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}\\)). The data we observe are assumed to be Poisson variables with expectation equal to the exponentiated latent variables: \\[{\\bf y} \\sim Pois(\\textrm{exp}({\\bf l}))\\] Note that the latent variable does not exactly predict \\(y\\), as it would if the data were Normal, because there is additional variability in the Poisson process5. In the call to \\(\\texttt{MCMCglmm}\\) I specified pl=TRUE to indicate that I wanted to store the posterior distributions of the latent variables (also known as the liabilities). This is not usually necessary and can require a lot of memory (we have 1000 posterior samples for each of the 182 data points). However, as an example we can obtain the posterior mean residual for data point 92 which is the data from \\(\\texttt{day}\\) 92 in 1961 when there was no speed limit: lat92 &lt;- m2a.5$Liab[, 92] # posterior distribution of the 92nd latent variable (liability) eta92 &lt;- m2a.5$Sol[, &quot;(Intercept)&quot;] + m2a.5$Sol[, &quot;day&quot;] * Traffic$day[92] # posterior distribution of X\\beta for the 92nd observation resid92 &lt;- lat92 - eta92 # posterior distribution of e for the 92nd observation mean(resid92) ## [1] -0.1341791 # posterior mean of e for the 92nd observation This particular observation has a negative expected residual indicating that the probability of getting injured was less than expected for this particular realisation of that \\(\\texttt{day}\\) in that year without a speed limit. If that combination of predictors (\\(\\texttt{day}\\)=92, \\(\\texttt{year}\\)=1961 and \\(\\texttt{limit}\\)=\\(\\texttt{no}\\)) could be repeated it does not necessarily mean that the actual number of accidents would always be less than expected, because it would follow a Poisson distribution with a mean equal to exp(lat92) (21.974). Like residuals in a standard linear model, the residuals are assumed to be independently and normally distributed with an expectation of zero and an estimated variance. If the residual variance was zero then \\({\\bf e}\\) would be a vector of zeros and the model would conform to the standard Poisson GLM. However, the posterior distribution of the residual variance is located well away form zero: plot(m2a.5$VCV) Figure 3.3: MCMC summary plot for the residual (units) variance from a Poisson glm (model m2a.5). The residual variance models any overdispersion, and a residual variance of zero would imply that the response conforms to a standard Poisson. 3.5 Prediction in GLM To get the expected number of accidents for the 92nd observation we simply exponentiated the latent variable: exp(lat92). However, it is important to realise that this is the expected number had the residual been exactly equal to the observed residual for that observation (resid92): we are calculating the expected number conditional on the set of unmeasured variables that affected that particular realisation of \\(\\texttt{day}\\) 92 in 1961 without a speed limit. When calculating a prediction we usually aim to average over these residuals (or random effects - see 4.2) since we would like to know what the average response would be for observations made on a \\(\\texttt{day}\\) of type 92 in 1961 without a speed limit. On the log-scale the expectation is simply the linear predictor (\\(\\eta\\)): \\[ log(E_e[y]) = E_e[l] = E_e[\\eta+e] = \\eta+E_e[e]=\\eta \\] since the residuals have zero expectation (here I have subscripted the expectation with the variable we are averaging over). The predict function can be applied to MCMCglmm objects and if we specify type=\"terms\" we get the prediction of the link scale - the log scale in this case: predict(m2a.5, type = &quot;terms&quot;)[92] ## [1] 3.224037 which is equal to the posterior mean of eta92 obtained earlier. We can see this visually in Figure 3.4 where I have plotted the distribution of the latent variable on a \\(\\texttt{day}\\) of type 92 in 1961 without a speed limit. Figure 3.4: The predicted distribution for the average number of accidents on the log scale for a \\(\\texttt{day}\\) of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor (\\(\\eta=\\)) with a variance of \\(\\sigma^{2}_e\\). As a consequence the mean, median and mode of the distribution are equal to the linear predictor on the log scale. To get the prediction on the data scale (i.e. in terms of the actual expected number of accidents) it is tempting to think we could just calculate exp(eta92). However, this is the expected number of accidents had the residual been exactly zero. If we wish to average over the residuals we require: \\[ E_e[y] = E_e[\\textrm{exp}(l)] = E_e[\\textrm{exp}(\\eta+e)] \\] and because exponentiation is a non-linear function this average will deviate from \\(\\textrm{exp}(\\eta)\\) (Figure 3.5. Figure 3.5: The predicted distribution for the average number of accidents on the data scale for a \\(\\texttt{day}\\) of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor (\\(\\eta\\)) with a variance of \\(\\sigma^{2}_e\\) (see 3.4). However when transforming to the data scale (by exponentiating) the symmetry is lost and the different measures of central tendency do not coincide. Since the residuals are normal on the log scale, the distribution on the data scale is log-normal and so analytical solutions exist for the mean, mode and median. To obtain predictions on the data scale we can specify type=\"response\" (the default) when using predict: predict(m2a.5)[92] ## [1] 26.48956 which is slightly greater than exp(eta92) (25.183). For all link-functions, the median value on the data scale can be easily calculated by taking the inverse-link transform of the linear predictor. However, obtaining the mean and mode is often more challenging than it is for log-link, and numerical integration or approximations are required. The predict function is returning a single number for observation 92 yet the model object contains 1,000 samples from the posterior distribution of all model parameters. This is because the predict function returns the posterior mean of the predicted value. Since we have the complete posterior distribution we can also place a 95% credible interval on the prediction (see Section 2.4.2): predict(m2a.5, interval = &quot;confidence&quot;)[92, ] ## fit lwr upr ## 26.48956 23.27675 30.10716 3.5.1 Posterior Predictive Distribution In some cases we would like to visualise or summarise aspects of the predictive distribution other than the mean. The complete predictive distribution is hard to work with, but the simulate function allows you to draw samples from the predictive distribution. The default is to generate a sample using a random draw from the posterior distribution, resulting in a draw from what is known as the posterior predictive distribution: ypred &lt;- simulate(m2a.5) We can use these simulated values to characterise any aspect of the predictive distribution we want. For example, we can obtain quantiles and compare them to the quantiles of the actual data to see how well the model captures aspects of the observed marginal distribution: qqplot(ypred, Traffic$y) abline(0, 1) Figure 3.6: qq-plot of the posterior predictive distribution and the data distribution Not too bad, although the predictive distribution perhaps has greater support for extreme values than are observed. If we merely wish to know the interval in which some specified percentage of the data are predicted to lie, we can also use the predict function but with interval=\"prediction\". By default the 95% (highest posterior density) interval is calculated using as many simulated samples as there are saved posterior samples. For the 92nd observation the prediction interval is predict(m2a.5, interval = &quot;prediction&quot;)[92, ] ## fit lwr upr ## 26.696 10.000 47.000 Note that the reported mean (fit) differs from that returned by interval=\"confidence\" due to Monte Carol error only. 3.6 Binomial and Bernoulli GLM The general concepts introduced for the Poisson GLM extend naturally to Binomial data, albeit with a different link function. However, it is worth spending a little time exploring a Binomial GLM as I think overdispersion, and how we deal with it, is easier to understand with binomial data. However, we’ll also see how the ‘residuals’ defined earlier for capturing overdispersion complicate the analysis of Bernoulli data and result in MCMcglmm using a non-standard parameterisation. The Binomial distribution has two parameters - the number of trials \\(n\\) and the probability of success, \\(p\\). In a Binomial GLM the number of trials is assumed known leaving only \\(p\\) to be estimated from the number of trials that are ‘successes’ or ‘failures’. When family=\"binomial\" is specified (or equivalently family=\"multinomial2\"6) MCMCglmm uses the standard link function for the Binomial - the logit link - and the logit probability of success is modelled as \\[log\\left(\\frac{p}{1-p}\\right) = l = \\eta+e\\] The logit transform takes a probability and turns it into a log odds ratio. If we want to get back to the probability we use the inverse of the logit transform: \\[p = \\frac{exp(l)}{1+exp(l)}\\] The logit link is actually the quantile function for the logistic distribution and so is available as the function qlogis. The inverse of a quantile function is a cumulative distribution function and so the inverse-logit transform is plogis. To introduce the Binomial GLM we will analyse some data I collected on how grumpy my colleagues look. I took two photos (\\(\\texttt{photo}\\)) of 22 people (\\(\\texttt{person}\\)) working in the Institute of Evolution and Ecology, Edinburgh. In one photo the person was happy and in the other they were grumpy (\\(\\texttt{type}\\)). 122 respondents gave a score between 1 and 10 indicating how grumpy they thought each person looked in each photo (with 10 being the most grumpy). data(Grumpy) Grumpy[c(1:3, 44), ] ## y l5 g5 type photo person age ypub ## 1 4.032787 101 21 grumpy 4511 ally_p 38 13 ## 2 3.081967 113 9 happy 4512 ally_p 38 13 ## 3 7.885246 15 107 grumpy 4521 darren_o 38 16 ## 44 3.798319 105 14 happy 4516 laura_r 34 10 \\(\\texttt{y}\\) gives the average score given by the 122 respondents. The number of respondents giving a photo a score of five or less (\\(\\texttt{l5}\\)) or more than five (\\(\\texttt{g5}\\)) is also recorded in addition to the person’s age (\\(\\texttt{age}\\)) and a proxy for how long they had been academia - the number of years since they published their first academic paper (\\(\\texttt{ypub}\\)). Here, we will model the probability of getting a grumpy score greater than five as a function of whether the person was happy or grumpy and how long they had been in academia. As with glm, successes should be in the first column of the response, and failures in the second: mbinom.1 &lt;- MCMCglmm(cbind(g5, l5) ~ type + ypub, data = Grumpy, family = &quot;binomial&quot;, pl = TRUE) summary(mbinom.1) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 5377.999 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 1.348 0.7597 1.947 1000 ## ## Location effects: cbind(g5, l5) ~ type + ypub ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.72025 -1.67538 0.05147 1000 0.094 . ## typehappy -1.28167 -1.97027 -0.59820 1000 &lt;0.001 *** ## ypub 0.02064 -0.00613 0.05175 1105 0.156 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model coefficients are most easily interpreted after exponentiating as they then give the proportional change in the odds ratio. The odds of having a score greater than five is exp(-1.282)=0.278 times lower when happy, as expected. The odds increases by a factor exp(0.021)=1.021 for each year in academia. When the coefficient is small in magnitude like this, you can get a rough estimate by looking directly at the coefficient: 0.021 roughly translates into a 2.1% increase and -0.021 would translate into a 2.1% decrease7. The \\(\\texttt{units}\\) (residual) variance is also large with credible intervals that are far from zero. 3.6.1 Overdispersion As with overdispersion in the Poisson model, this excess variation can be attributed to predictors that are not included in the model but cause the probability of success to vary over observations (photos in this case). For example, the 3rd and 25th observation have the same values for every predictor Grumpy[c(3, 25), ] ## y l5 g5 type photo person age ypub ## 3 7.885246 15 107 grumpy 4521 darren_o 38 16 ## 25 5.319672 73 49 grumpy 4527 craig_w 38 16 and so in the absence of overdispersion these parameters would result in a predicted probability of success of \\[p = \\textrm{plogis}(\\beta_{\\texttt{(Intercept)}}+\\beta_{\\texttt{typehappy}}\\times0+\\beta_{\\texttt{ypub}}\\times16)=0.404 \\label{plogis-eq} \\tag{3.3}\\] Given there were 122 respondents (trials) we can calculate the two values between which the number of successes is expected to fall 95 % of the time. qbinom(c(0.025, 0.975), size = 122, prob = p) ## [1] 39 60 While photo \\(\\texttt{4527}\\) is well within the range with 49 successes, the number of respondents giving photo \\(\\texttt{4521}\\) is substantially higher (107) - it’s an outlier. In Figure 3.7 the two photos are shown and it is clear why their underlying probabilities may deviate from that predicted. Most obviously the two photos are of different people and people vary in how grumpy they look. Since we have two photos per person we could (and should) estimate \\(\\texttt{person}\\) effects, however this is best done by treating these effects as random, which we will cover later, in Section 4.1. Even if \\(\\texttt{person}\\) effects were fitted there’s also likely to be a whole host of observation-level (photo-specific) effects that are not captured in the model such as whether the person had their eyes closed or was wearing a dreary grey fleece. Figure 3.7: Photo 4521 (left) and photo 4527 (right). For the predictors fitted in model mbinom.1, these photos have the same values (\\(\\texttt{type}=\\texttt{grumpy}\\) and \\(\\texttt{ypub}=16\\) years) If we include the ‘residuals’ when calculating the predicted probability for these two photos we can see that indeed their probabilities are quite different: mean(plogis(mbinom.1$Liab[, 3])) ## [1] 0.8621967 # predicted probability for photo 4521 mean(plogis(mbinom.1$Liab[, 25])) ## [1] 0.4001446 # predicted probability for photo 4527 3.6.2 Prediction When calculating the predicted probability in the absence of the residuals in Equation (3.3) I was careful to say that the prediction assumed an absence of overdispersion. However, when overdispersion is present we need to average over the distribution of the residuals in order to get an average. As we saw in the log-linear Poisson model, because the inverse-link function (plogis) is non-linear the average of \\(E_e[\\texttt{plogis}(\\eta+e)]\\) is different from \\(\\texttt{plogis}(E_e[\\eta+e])=\\texttt{plogis}(\\eta)\\). Unlike the Poisson log-linear model this expectation cannot be calculated analytically and the predict function by default numerically evaluates the integral: \\[\\int_l \\texttt{plogis}(l)f_N(l | \\eta, \\sigma^2_e)dl\\] where \\(f_N\\) is the probability density function of the normal. For each of the 44 observations this is done 1,000 times (the number of saved posterior samples) to get the posterior mean prediction and can be very slow (it is actually faster to fit the model). The number reported by predict is \\(np\\) rather than \\(p\\) and so to get the predicted probability for a photo of someone who has grumpy and had been publishing for 16 years we can get the prediction for the 3rd observation and divide by the number of trials (122): predict(mbinom.1)[3]/122 ## [1] 0.4243498 A little different from \\(\\texttt{plogis}(\\eta)=\\) 0.404. For the binomial with logit link, analytical approximations have been developed in Diggle et al. (2004) and McCulloch and Searle (2001) which are considerably faster and reasonably accurate: predict(mbinom.1, approx = &quot;diggle&quot;)[3]/122 ## [1] 0.4210936 predict(mbinom.1, approx = &quot;mcculloch&quot;)[3]/122 ## [1] 0.4255396 3.6.3 Bernoulli GLM Bernoulli data are a special case of the Binomial in which the number of trials is equal to one and so either a success or a failure is observed. To explore a Brenoulli model we can take our average Grumpy scores and simply dichotomise them into whether the average score was five or less or more than five: Grumpy$mean5 &lt;- Grumpy$y &gt; 5 If we fit a binomial model to these data in MCMCglmm it has exactly the same form as before (although a single column of outcomes can be passed). But importantly, for Bernoulli data there is no information to estimate the residual variance. This does not necessarily mean that variation in the probability of success across observations is absent, only that we can’t estimate it. For example, imagine we took 100 people who had been publishing for 16 years and took a photograph of them when they were grumpy. Let’s say the probability that the mean score for such photos exceeded 5 was 0.5. If the probability for all photos was exactly 0.5 (i.e. the probability of success did not vary over observations) then we expect 50 success and 50 failures across our observations. However, imagine the case where the probability of success was 100% for 50 photos and 0% for 50 photos (i.e. the probability of success varies considerably over observations). We would also expect 50 success and 50 failures, and so the distribution of successes with and without variation in the underlying probability would be identical. In the absence of information most software sets the ‘residual’ variance to zero (i.e. the probability of success dose not vary over observations), but it is important to understand that this is a convenient but arbitrary choice. Given this, it is desirable that any conclusions drawn from the model do not depend on this arbitrary choice. Worryingly, both the location effects (fixed and random) and variance components are completely dependent on the magnitude of the residual variance. MCMCglmm allows the user to fix the residual variance at a value of their choice, but unfortunately a value of zero results in a chain that will not mix and so I usually fix the residual variance to one8: prior.mbern.1 = list(R = list(V = 1, fix = 1)) mbern.1 &lt;- MCMCglmm(mean5 ~ type + ypub, family = &quot;binomial&quot;, data = Grumpy, prior = prior.mbern.1) However, it would have been equally valid to fixed the residual variance at three: prior.mbern.2 = list(R = list(V = 3, fix = 1)) mbern.2 &lt;- MCMCglmm(mean5 ~ type + ypub, family = &quot;binomial&quot;, data = Grumpy, prior = prior.mbern.2) and if we compare the MCMC traces for the coefficients we can see that we are sampling different posterior distributions (Figure 3.8). plot(mcmc.list(mbern.1$Sol, mbern.2$Sol), density = FALSE) Figure 3.8: MCMC trace for coefficients of a Bernoulli GLM from two models (mbern.1 in black and mbern.2 in red). The data and model structure are identical but in mbern.1 the residual variance was set to one and in mbern.2 the residual variance was set to three. The data provide no information about the residual variance. Should we worry? Not really. The two models give almost identical predictions (Figure 3.9). plot(predict(mbern.1), predict(mbern.2)) abline(0, 1) Figure 3.9: Predicted probabilities from a Bernoulli GLM from two models. The data and model structure are identical but in mbern.1 the residual variance was set to one and in mbern.2 the residual variance was set to three. The data provide no information about the residual variance. We just have to be careful about how we express the results. Stating that the typehappy coefficient is -2.852 (the posterior mean estimate from mbern.1) is meaningless without putting it in the context of the assumed residual variance (one). Although the Diggle et al. (2004) approximation is less accurate than that in McCulloch and Searle (2001) we can use it rescale the estimates by the assumed residual variance (we’ll call it \\(\\sigma^{2}_{\\texttt{units}}\\)) in order to obtain the posterior distributions of the parameters under the assumption that the actual residual variance (we’ll call it \\(\\sigma^{2}_{e}\\)) is equal to some other value. For location effects the posterior distribution needs to be multiplied by \\(\\sqrt{\\frac{1+c^{2}\\sigma^{2}_{e}}{1+c^{2}\\sigma^{2}_{\\texttt{units}}}}\\) where \\(c=16\\sqrt{3}/15\\pi\\). If obtain estimates under the assumption that \\(\\sigma^{2}_{e}=0\\) and we see the posterior distributions of the coefficients are very similar from the two models (Figure 3.10). c2 &lt;- ((16 * sqrt(3))/(15 * pi))^2 rescale.2 &lt;- mbern.1$Sol * sqrt(1/(1 + c2 * 1)) rescale.3 &lt;- mbern.2$Sol * sqrt(1/(1 + c2 * 3)) plot(mcmc.list(as.mcmc(rescale.2), as.mcmc(rescale.3)), density = FALSE) Figure 3.10: MCMC trace for rescaled coefficients of a Bernoulli GLM from two models (mbern.1 in black and mbern.2 in red). The data and model structure are identical but in mbern.1 the residual variance was set to one and in mbern.2 the residual variance was set to three. However, the coefficients have been rescaled using the Diggle et al. (2004) approximation such that they represent what the coefficients would be if the residual variance was zero. The data provide no information about the residual variance. In addition, the posterior distributions are centred on the ML estimates obtained by glm which implicitly assumes \\(\\sigma^2_e=0\\) (Figure 3.11). Figure 3.11: Posterior distributions for rescaled coefficients of a Bernoulli GLM (mbern.2). The rescaling gives approximate (but accurate) posterior distributions had the residual variance been set to zero, rather than three. The red lines indicate estimates from glm that implicitly assumes the residual variance is zero and the blue lines indicate the unscaled posterior means. 3.6.4 Probit link The inverse-logit function is the cumulative distribution function for the logistic distribution. It makes sense that a cumulative distribution function for a continuous distribution that can take any value would serve as a good inverse link function for a probability: it takes any value between plus and minus infinity and squeezes it to be between zero and one. While the logit is the most common link function for binomial data, the probit link and complementary log-log (cloglog) link are also widely used. They are the cumulative distribution functions for the unit normal and standard Gumbel distributions respectively. In Figure 3.12 we can see how the probability changes as a function of a covariate (\\(x\\)) for the different links (with intercepts and slopes chosen so that the functions are matched at the origin). Figure 3.12: Predicted probabilities as a function of covariate \\(x\\) using the inverse logit (black), inverse probit (red) and inverse complementary log-log link functions. For each link the intercept and slope were chosen such that when \\(x= 0\\) the function equals 0.5 and has a derivative of one. We can see that the link functions generate rather similar predicted probabilities, particularly the logit and probit links. For Bernoulli data, another way to conceptualise these link functions is in terms of threshold models. Imagine the case where we have managed to set the non-identifiable residual variance to zero and all residuals \\(e\\) are zero and can be omitted. The probability of success is then given by the inverse-link of \\(\\eta\\). For probit link this would be the probability of getting a value less than \\(\\eta\\) from the unit normal (the grey area in the left panel of Figure 3.13). Equivalently, it is the probability of getting a value greater than 0 from a normal with a mean equal to \\(\\eta\\) and a standard deviation of one (the grey area in the right panel of Figure 3.13). Figure 3.13: Probability density function for \\(\\epsilon\\) (left) or \\(\\eta+\\epsilon\\) (right) where \\(\\epsilon\\) is normal with mean zero and a standard deviation of one. Applying pnorm to \\(\\eta\\) gives the shaded area on the left and is the probability of success using probit link. The shaded area on the right gives the same probability and is the chance of getting a value greater than zero from a normal with mean \\(\\eta\\) and a standard deviation of one. If we think explicitly about the normal deviates that underpin these probability calculations (we’ll call them \\(\\epsilon\\)), then in the left panel \\(\\epsilon\\) falls below the ‘threshold’ \\(\\eta\\) with the required probability or in the right panel, \\(\\eta+\\epsilon\\) falls above the ‘threshold’ zero with the required probability. Since \\(\\epsilon\\) is unit-normal we can equate \\(\\eta\\) with \\(e\\) and \\(\\eta+\\epsilon\\) with \\(l\\) and apply the inverse link function \\(\\mathbf{1}_{\\{l&gt;0\\}}\\). This function outputs 1 (a success) if \\(l&gt;0\\) and a failure otherwise (as in the right panel of Figure 3.13). This is implemented as family=threshold in MCMCglmm, and if the residual variance (i.e. the variance of \\(\\epsilon\\) or \\(e\\)) is fixed at one corresponds exactly to standard probit regression9. prior.mbinom.4 = list(R = list(V = 1, fix = 1)) mbinom.4 &lt;- MCMCglmm(mean5 ~ type + ypub, family = &quot;threshold&quot;, data = Grumpy, prior = prior.mbinom.4) Unfortunately the same trick can’t be used for other link functions because the \\(\\epsilon\\)’s’ cannot be equated with \\(e\\)’s because they come from different distributions. In some ways the probit link is a natural link function for models that contain random effects, which are also usually assumed to be normal. However, the downside is that the coefficients in a probit model do not have a direct interpretation like they do in a logit model. Nevertheless, both models give very similar predictions and are unlikely to be statistically distinguishable in the vast mean5 of cases (Figure 3.14). Figure 3.14: Predicted probabilities from a Bernoulli GLM from two models with the same model structure. However, mbern.1 uses a logit link with a residual variance set to one and in mbinom.4 uses a standard probit link. 3.7 Ordinal Data Thinking about Bernoulli GLM’s in terms of thresholds provides a natural way of thinking about how we could model categorical data that falls into a natural ordering. For example, rather than dichotomising our outcome into those that had a mean score greater than, or less than, five, lets place the observations into three categories: (1, 4], (4, 6], (6, 10]. Grumpy$categories &lt;- as.numeric(cut(Grumpy$y, c(1, 4, 6, 10))) Rather than just having a single threshold at zero, we can imagine adding another threshold that chops the distribution of \\(\\eta+e=l\\) into three regions, and hence probabilities. To make things simple we will just fit an intercept only model (so all observations have the same value of \\(\\eta\\)) as this will be easier to visualise: prior.mordinal = list(R = list(V = 1, fix = 1)) mordinal &lt;- MCMCglmm(categories ~ 1, data = Grumpy, family = &quot;threshold&quot;, prior = prior.mordinal) The output of mordinal gives the intercept (\\(\\eta\\) in this case) as before but also the additional threshold (cutpoint - stored as CP in the model object): summary(mordinal) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 92.2748 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 1 1 1 0 ## ## Location effects: categories ~ 1 ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 0.4892 0.1356 0.9025 607 0.012 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Cutpoints: ## ## post.mean l-95% CI u-95% CI eff.samp ## cutpoint.traitcategories.1 1.322 0.8801 1.775 435.8 As we did in the right panel of Figure 3.13, we can draw this model with the estimated threshold (\\(\\gamma\\)) included (Figure 3.15). Figure 3.15: Probability density function for \\(l=\\eta+e\\) where \\(e\\) is normal with mean zero and a standard deviation of one. \\(\\eta=\\) 0.489 and was obtained from the model mordinal. The distribution is ‘cut’ into three regions by the fixed threshold at zero and the estimated threshold (\\(\\gamma\\)) at 1.322. The shaded areas correspond to the probabilities of observing the three (ordered) outcomes. The shaded areas give the probability for each category and the posterior mean probabilities can be easily calculated: mean(pnorm(0, mordinal$Sol)) ## [1] 0.3158146 mean(pnorm(mordinal$CP, mordinal$Sol) - pnorm(0, mordinal$Sol)) ## [1] 0.4768311 mean(1 - pnorm(mordinal$CP, mordinal$Sol)) ## [1] 0.2073543 These correspond closely to the observed frequencies in the data: table(Grumpy$categories)/44 ## ## 1 2 3 ## 0.3181818 0.4772727 0.2045455 3.8 Non-zero Binomial Data The final distribution that we will cover in this Chapter is something I have called the Non-zero Binomial which can be fitted using family=\"nzbinom\". It was implemented for a specific application in which \\(n\\) (\\(\\texttt{number}\\)) bumblebees were pooled and assayed for the presence of the Acute Bee Paralysis virus (Pascall et al. 2018). If the assay came back positive then at least one bee in the pool was infected and the outcome was recorded as a ‘success’ - otherwise non of the bees in the pool were infected and we have a ‘failure’ (\\(\\texttt{infected}\\)). data(ABPvirus) ABPvirus[c(1:2, 98:99), ] ## species number infected ## 1 B.pascuorum 10 0 ## 2 B.pascuorum 10 0 ## 98 B.lucorum 2 1 ## 99 B.lucorum 10 0 The observations are essentially censored, and while we cover censoring more generally in Chapter 9, the Non-zero Binomial is perhaps best covered here. The probability of a success for the Non-zero Binomial (which we will designate as \\(P\\)) is \\(1-(1-p)^n\\) where \\(p\\) is the probability that an individual bumblebee was infected. If the number of bumblebees in a pool varies over observations, family=\"nzbinom\" is a useful tool and the linear model is defined for the logit transform of \\(p\\), as in the standard binomial. In the standard Bernoulli model the residual variance could not be estimated and was fixed at some value. Perhaps surprisingly, there is some information to estimate the residual variance (overdispersion) in a Non-zero Binomial model as long as \\(n\\) varies over pools. However, the amount of information is so small that it is probably safest to fix the residual variance at some non-zero value (I use one as a convention) rather than risk sampling very high values of the residual variance that can cause numerical issues. We will ignore which species of bumblebee the observations were made on, as these are best treated as random effects (Chapter 4) mnzbinom &lt;- MCMCglmm(cbind(infected, number) ~ 1, family = &quot;nzbinom&quot;, data = ABPvirus, prior = list(R = list(V = 1, fix = 1))) summary(mnzbinom) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 131.4134 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 1 1 1 0 ## ## Location effects: cbind(infected, number) ~ 1 ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -3.088 -3.489 -2.656 510 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The median probability of an individual bumblebee being infected is low: \\(\\texttt{plogis}(eta)=\\) 0 such that the expected probability of at least one infection is small for the lowest pool-size (\\(\\texttt{number}=\\) 2) and is still well below one in the largest pool size (\\(\\texttt{number}=\\) 11) - see Figure 3.16. Figure 3.16: Probability of seeing at least one infection (\\(P\\)) in pools of varying size when the probability of an individual bumblebee being infected (\\(p\\)) is constant. The solid black line is the posterior mean and the shaded area is the 95% credible interval. The estimated median probability of an individual bumblebee being infected is0.044and the mean probability is0.044. If the data set was larger and/or variation in the pool size was greater, there may be sufficient information to justify trying to estimate the residual variance, rather than fixing it to one. To see where the information comes from first imagine that you have information on samples all with a pool-size of 1. You could jut average you the resulting zeros and ones to get an estimate of \\(P_1\\) where the subscript designates the size of the pool for which the probability of at least one infection is calculated. Even if the probability of infection varied over individuals, the average probability \\(E[p]\\) is equal to \\(P_1\\), as we saw when we determined why variation in the probability cannot be estimated with standard Bernoulli data. Let’s then imagine we move to pool sizes of 2. \\(P_2\\) (the probability of at least one success in a pool of 2) is then \\(E[1-(1-p)^2]\\). If there is no variation in \\(p\\) then \\(P_2=1-(1-P_1)^2\\) since \\(p\\) is a constant and equal to \\(P_1\\). However, when there is variation in \\(p\\) the non-linearity in the function (i.e. \\((1-p)^2\\)) means that \\(E[1-(1-p)^2]\\) will deviate from \\(1-(1-E[p])^2=1-(1-P_1)^2\\): we expect the the number of successful pools of size 2 to be less than that predicted from estimating \\(p\\) from pools of size 110. Consequently, the rate at which \\(P\\) asymptotes with increasing pool-size (as seen in Figure 3.16) provides information about how much variation in \\(p\\) exists. 3.9 Complete Separation One potential issue that can occur in GLM is something known as complete separation or the extreme category problem. While it can occur for any distribution which is discrete, it is most commonly seen when the response is Bernoulli. It occurs when the predictors perfectly predict the outcome. For example, in Section 3.6.3 we generated Bernoulli data for each photo by assessing whether the mean score was greater than or less than five. Photo \\(\\texttt{type}\\) is a very good predictor of the outcome, but not perfect: table(mean5 = Grumpy$mean5, type = Grumpy$type) ## type ## mean5 grumpy happy ## FALSE 9 19 ## TRUE 13 3 However, if we removed the three observations for which the mean grumpy score was greater than five despite the person being happy, then \\(\\texttt{type}\\) perfectly predicts the outcome. If we fit a probit model to these data, but using glm, a very strange thing happens: data.cs &lt;- subset(Grumpy, !(mean5 == TRUE &amp; type == &quot;happy&quot;)) mcs &lt;- glm(mean5 ~ type, family = binomial(link = probit), data = data.cs) summary(mcs) ## ## Call: ## glm(formula = mean5 ~ type, family = binomial(link = probit), ## data = data.cs) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.2299 0.2698 0.852 0.394 ## typehappy -5.9798 359.8415 -0.017 0.987 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 51.221 on 40 degrees of freedom ## Residual deviance: 29.767 on 39 degrees of freedom ## AIC: 33.767 ## ## Number of Fisher Scoring iterations: 17 The coefficient for \\(\\texttt{typehappy}\\) now has a huge standard error and the p-value is close to one, despite being a very good predictor of the outcome. If we assess significance using Fisher’s exact test we get: fisher.test(table(data.cs$mean5, data.cs$type)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(data.cs$mean5, data.cs$type) ## p-value = 2.977e-05 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.0000000 0.2098429 ## sample estimates: ## odds ratio ## 0 A very significant result, as expected. The model fitted using \\(\\texttt{MCMCglmm}\\) also behaves oddly with very poor mixing (Figure 3.17). prior.mcs.2 = list(R = list(V = 1, fix = 1)) mcs.2 &lt;- MCMCglmm(mean5 ~ type, family = &quot;threshold&quot;, data = data.cs, prior = prior.mcs.2) plot(mcs.2$Sol) Figure 3.17: MCMC summary plots for the intercept and \\(\\texttt{typehappy}\\) effect in a binary GLM (mcs.2) with probit link. For \\(\\texttt{happy}\\) photos all 19 observations are failures (the mean grumpy score is less than 5) and we have complete separation. A normal prior with large variance was used for the model coefficients. This strange behaviour occurs because the ML estimate for the probability of success is zero for \\(\\texttt{happy}\\) photos. On the probit scale this translates into \\(-\\infty\\) and so the ML estimate of the difference between \\(\\texttt{happy}\\) and \\(\\texttt{grumpy}\\) photos (the \\(\\texttt{typehappy}\\) effect) will also be \\(-\\infty\\). Consequently, with a flat prior on the \\(\\texttt{typehappy}\\) effect the posterior distribution is improper. The default prior for model coefficients in \\(\\texttt{MCMCglmm}\\) is not flat - they are normal with a large variance (\\(10^8\\)), but even so this diffuse prior can cause problems if there is complete (or near complete) separation. To see this, think about an intercept-only model. A diffuse prior on the probit scale puts a lot of density on very large positive or negative values and so puts a lot of density close to zero and one on the probability scale (see Figure 3.12). If the likelihood prevents the posterior from reaching these extremes then the prior has little influence because it is largely flat outside of the extremes (it is U-shaped). However, with complete separation the likelihood is indeed placing a lot of density at these extreme values and the prior holds the posterior at these values. If, on the other hand, we had specified the prior on the intercept to be normal with a mean of zero and a variance of one then the prior on the probability scale would be flat (since the inverse-link function is the cumulative density function for the unit normal). If a logit-link had been used then there is no normal prior that would result in flatness on the probability scale, and having a mean of zero and a variance of \\(\\pi^2/3\\) (the variance of a logistic distribution) is a close as you can get if the residual variance had been zero (3.18). Figure 3.18: Prior density on the probability of success in an intercept-only Bernoulli GLM. The solid black line is for a standard probit link and a normal prior on the intercept with zero mean and a variance of one. The solid red line is for a standard logit link (with the residual variance set to zero) and a normal prior on the intercept with zero mean and a variance of one \\(\\pi^2/3\\). The dashed lines are for when the variance is set to 100 (logit-link) or \\(g^2 100\\approx\\) 39.3 (probit link) - see Section 3.9.1. 3.9.1 The Gelman, Jakulin, et al. (2008) prior Ideally we would like to extend this idea so that we can define priors for coefficients associated with categorical and continuous predictors, not just the intercept. We would also like these priors to work well in the presence of variation caused by the addition of random effects (Chapter 4) and (depending on the family specified) a non-zero residual variance. What follows is very fiddly and a short summary and workflow can be found at the end. For accommodating additional variance in the intercept-only model we can use a prior variance that is the sum of the link variance and the model-defined variance \\(v\\). In the Bernoulli models we’ve fitted \\(v\\) is just the residual variance which has been fixed at one. For the logit-link, the link variance is \\(\\pi^2/3\\) which would result in a prior variance of \\(1+\\pi^2/3\\). For the threshold link, the link variance is zero resulting in a prior variance of \\(1\\) (i.e. the standard probit where the residual variance is set to zero and the link variance is one). With random effects \\(v\\) may not be known a priori and a judicious choice about it’s magnitude would need to be made. For models that contain predictors, Gelman, Jakulin, et al. (2008) suggested a prior for logit-link models where \\(v=0\\). They suggested that the input variables should be standardised prior to model fitting and that the the associated coefficients should have independent Cauchy priors with a scale of 10 for the (new) intercept and a scale of 2.5 for the remaining coefficients. The motivation behind this prior is that the average probability, defined by the new intercept, can have a wide range (\\(10^{-9}\\) \\(-\\) 1-\\(10^{-9}\\)). This is still rather U-shaped on the probability scale and in many applications a lower scale may perform better (Figure 3.18). The prior on the remaining coefficients imply that extremely large changes in probability between categorical predictors or per 0.5 standard deviations of a continuous predictor have low probability: for changes greater than ten on the logit scale, the prior density is 0.84. This combination of standardisation and prior naturally penalises higher order interactions, but it’s properties when random effects were fitted (or the residual variance was non-zero) were not explored. Personally, I don’t like generic rescaling of inputs - I like my coefficients to have meaningful units. I would like to know how much my pot-belly is expected to grow per pistachio nut, not per standard deviation of pistachio nut consumption in the data set the dietician happened to have analysed. However, rather than standardising the inputs prior to model fitting we can fit them in their original form and place a prior on the coefficients that is equivalent to that recommended by Gelman, Jakulin, et al. (2008) had we rescaled the inputs. Although the Cauchy prior can probably be specified by treating the coefficients as random, here we will use a normal distribution with the same scales. This will penalise large coefficients more than the Cauchy prior. To accommodate non-zero \\(v\\), we can multiply the scales recommended by Gelman, Jakulin, et al. (2008) by \\(\\sqrt{1/(1+c^2\\texttt{v})}\\) such that the same prior interpretation can be given (approximately) to the marginal effects of the predictors (see earlier). A change of one logit is neither equivalent or proportional to a change of one probit. For example, going from -1 to -2 on the logit scale we move from a probability of 0.27 to 0.12 but on the probit scale a change from -1 to -2 results in a change of probability from 0.16 to 0.02. Around zero (i.e a probability of 0.5 in both cases) the change in probability per logit is 0.25 and the change in probability per probit is 0.399. Consequently, we can multiply the scales recommended by Gelman, Jakulin, et al. (2008) by \\(g=\\) 0.25 / 0.399=dlogis(0)/dnorm(0)=\\(\\sqrt{2\\pi}/4\\) to achieve approximate equivalence. Note this was done when determining the regression coefficients to produce Figure 3.12 and the two prior densities diverge as the probability moves away from 0.5 (see Figure 3.18 also). When additional variance is present we can multiply the original scales by \\(\\sqrt{g^2/\\texttt{v}}\\) when family=\"threshold\" is used. The function gelman.prior takes the (fixed effect) model formula as its first argument and together with the data.frame the model is to be fitted to (passed in the argument data) generates the prior specification (\\(\\texttt{mu}\\) and \\(\\texttt{V}\\)) for the model parameters, had the inputs been scaled as in Gelman, Jakulin, et al. (2008). The argument intercept.scale specifies the scale of the normal (standard deviation) for the intercept and the argument coef.scale specifies the scale of the normal for the remaining parameters - both defined as if inputs were scaled11. By default these scales are set to one. This is all quite confusing and here are the main points and a practical workflow. Gelman, Jakulin, et al. (2008) recommended standardising inputs prior to logistic regression and using a prior that allows a wide range for the average probability of success but penalises large effects associated with predictors. The function gelman.prior helps set up a prior similar to that recommended by Gelman, Jakulin, et al. (2008) but a) using a normal in place of a Cauchy and b) not requiring the inputs to be standardised prior to analysis. To gelman.prior you should pass your model formula (fixed in MCMCglmm) and data that will be used in the analysis. You also need to decide on a prior scale for the mean (intercept.scale) and the coefficients of the standardised inputs (coef.scale). To achieve a prior as close to that recommended by Gelman, Jakulin, et al. (2008) you can use \\(10\\sqrt{1/(1+c^2\\texttt{v})}\\) (family=\"binomial\") or \\(10\\sqrt{g^2/\\texttt{v}}\\) (family=\"threshold\") for intercept.scale and \\(2.5\\sqrt{1/(1+c^2\\texttt{v})}\\) (family=\"binomial\") or \\(2.5\\sqrt{g^2/\\texttt{v}}\\) (family=\"threshold\") for coef.scale. \\(c=16\\sqrt{3}/15\\pi\\) and g=\\(\\sqrt{2\\pi}/4.\\) The output of gelman.prior can be passed directly as the fixed effect prior (i.e. prior=list(B=gelman.prior(...)). We can try this out for the model we fitted to the data set with complete separation: g &lt;- sqrt(2 * pi)/4 prior.mcs.3 = list(B = gelman.prior(~type, data = data.cs, coef.scale = g * 2.5, intercept.scale = g * 10), R = list(V = 1, fix = 1)) mcs.3 &lt;- MCMCglmm(mean5 ~ type, family = &quot;threshold&quot;, data = data.cs, prior = prior.mcs.3) plot(mcs.3$Sol) Figure 3.19: MCMC summary plots for the intercept and \\(\\texttt{type}\\) effect in a binary GLM (mcs.3) with probit link. For \\(\\texttt{happy}\\) photos all 19 observations are failures (the mean grumpy score is less than 5) and we have complete separation. A prior was set up using gelman.prior that penalises very large differences between \\(\\texttt{happy}\\) and \\(\\texttt{grumpy}\\) photos. and we can see that the chain now looks well behaved (Figure 3.19). We can calculate the posterior distribution for the probability of success for \\(\\texttt{happy}\\) photos, where we had complete separation (19 failures and 0 successes). If we compare this posterior distribution with that from the model that used the diffuse (default) prior, we can see that all the density is not concentrated at very low density 3.20. This seems reasonable given that zero successes and 19 failures has a reasonable probability (0.135)even when the probability of success is 0.1. Figure 3.20: Posterior distribution for the probability of success for \\(\\texttt{happy}\\) photos, where there was complete separation (19 failures and 0 successes). For model mcs.3 (left) a prior was set up using gelman.prior that penalised large differences between \\(\\texttt{happy}\\) and \\(\\texttt{grumpy}\\) photos. For model mcs.2 (right) the default diffuse prior was used. References "],["ranef.html", "4 Random effects 4.1 Generalised Linear Mixed Model (GLMM) 4.2 Prediction with Random Effects 4.3 Overdispersed Binomial as a Bernoulli GLMM 4.4 Intra-class Correlations 4.5 Underdispersion 4.6 Priors for Random Effect Variances 4.7 Prior Generators 4.8 Priors on Functions of Variances 4.9 Fixed or Random?", " 4 Random effects In some cases we may have measured variables whose effects we would like to treat as random. Often the distinction between fixed and random is given by example: things like city, species, individual and vial are random, but sex, treatment and age are not. Or the distinction is made using rules of thumb: if there are few factor levels and they are interesting to other people they are fixed. However, this doesn’t really confer any understanding about what it means to treat something as fixed or random, and doesn’t really allow judgements to be made for variables in which the rules of thumb seem to contradict each other. Similarly, these ‘explanations’ don’t give any insight into the fact that all effects are technically random in a Bayesian analysis. Random effect models are often expressed as an extension of Equation (3.2): \\[E[{\\bf y}] = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}{\\bf u} \\label{MM} \\tag{4.1}\\] where \\({\\bf Z}\\) is a design matrix like \\({\\bf X}\\), and \\({\\bf u}\\) is a vector of parameters like \\({\\boldsymbol{\\mathbf{\\beta}}}\\). However, at this stage there is simply no distinction between fixed and random effects. We could combine the design matrices (\\({\\bf W} = [{\\bf X}, {\\bf Z}]\\)) and combine the vectors of parameters (\\(\\boldsymbol{\\theta} = [{\\boldsymbol{\\mathbf{\\beta}}}^{&#39;}, {\\bf u}^{&#39;}]^{&#39;}\\)) to get: \\[E[{\\bf y}] = {\\bf W}\\boldsymbol{\\theta} \\label{MM2} \\tag{4.2}\\] which is identical to Equation (4.1). So if we don’t need to distinguish between fixed and random effects at this stage, when should we distinguish between them, and what distinguishes them? When we treat an effect as random we believe that the coefficients have some distribution around a mean of zero; often we assume they are normal12 and that they are independent (represented by an identity matrix) and identically distributed with variance \\(\\sigma^{2}_{u}\\): \\[{\\bf u} \\sim N({\\bf 0}, {\\bf I}\\sigma^{2}_{u})\\] \\(\\sigma^{2}_{u}\\) is a parameter of the model which we estimate, in addition to \\({\\bf u}\\). In a Bayesian analysis we would also assign \\(\\sigma^{2}_{u}\\) a prior, and \\(\\sigma^{2}_{u}\\) is often called a hyper-parameter with an associated hyper-prior. Fixed effects in a frequentist analysis are not assigned a distribution, but we can understand this in terms of the limit to the normal distribution \\[\\boldsymbol{\\beta} \\sim N({\\bf 0}, {\\bf I}\\sigma^{2}_{\\beta})\\] as \\(\\sigma^{2}_{\\beta}\\) tends to infinity. In a Bayesian setting we would call this a flat improper prior. In practice, we often use diffuse proper priors in Bayesian analyses. For example, the default in \\(\\texttt{MCMCglmm}\\) is to set \\(\\sigma^{2}_{\\beta}=10^8\\). Then, \\(\\boldsymbol{\\beta}\\) are technically random - they are assigned a distribution - but I find it useful to retain the frequentist terminology ‘fixed’. The only difference then is that the ‘fixed’ effects are assigned a prior distribution with a variance that is defined by the user-specified prior (\\(\\sigma^{2}_{\\beta}\\) - which is often set to be large) and the ‘random’ effects are assigned a prior distribution with a variance that is estimated (\\(\\sigma^{2}_{u}\\) - which could be large, but also zero). That is the distinction between fixed and random effects. The difference really is that simple, but it takes a long time and a lot of practice to understand what this means in practical terms, and why working with random effects can be a very powerful way of modelling data. To get a feel for why we might want to fit an effect as random or not, lets work through an example before moving on to model fitting. In Section 3.6 we analysed binomial data where 122 respondents had looked at 44 photographs of people and given them a ‘grumpy score’ of more than five (a success) or less than five (a failure). If, instead of 122 respondents, there had been a zillion respondents, we could use the average proportion of success for each photo as a nearly perfect estimates of their probabilities of success. The variance of these near-perfect estimates could serve as a reasonable estimate of the variance in photo effects. If the probabilities were all clustered tightly around 0.5: 0.505, 0.501, 0.499 and so on, then variance would be estimated to be small. Let’s then imagine that we obtained a \\(45^\\textrm{th}\\) photograph but by this point the respondents were so bored I managed to only recruit a single person who gave the photo a score greater than five - a success. Since we only have one observation for this photo the average proportion of success would be one. Do you think the best estimate of the probability of success for the \\(45^\\textrm{th}\\) photograph is then 1.000? I think you wouldn’t: you would use the knowledge that you have gained from the other photos and say that it is more likely that if you had managed to recruit more respondents you would have got a roughly even split of success and failures. You have used common sense, treated the photo effects as random, and shrunk photo 45’s effect towards the average because the variance (\\(\\sigma^2_u\\)) was small and we have a strong prior. If we had treated the photo effects as fixed, we believe that the only information regarding a photo’s value comes from data associated with that particular photo, and the estimate of photo 45’s probability would have been one. When we treat an effect as random, we also use the information that comes from data associated with that particular photo (obviously), but we weight that information by what the data associated with other photos tell us about the likely values that the effect could take - through the parameter \\(\\sigma^2_u\\). What if the probabilities weren’t all clustered tightly around 0.5, but took on values 0.500, 0.998, 0.002, 0.327 …? The variance \\(\\sigma^2_u\\) would be larger and the prior information for our \\(45^\\textrm{th}\\) photo would be weaker: perhaps we got a success because the underlying probability was 0.998, but a single success would also not be very surprising if the underlying probability was 0.500, or even 0.327. We might then be happy that our best estimate of the probability of success for the \\(45^\\textrm{th}\\) photograph was close to one, although with such weak prior information (large \\(\\sigma^2_u\\)) the uncertainty would remain large. When the motivation for treating an effect as random is explained this way, it is hard to come up with a reason why you wouldn’t treat all effects as random. However, you have to consider how much information is in a given data set to estimate \\(\\sigma^2_u\\), which we will cover in Section 4.9. 4.1 Generalised Linear Mixed Model (GLMM) In Section 3.6, the binomial model we fitted only contained fixed effects, as specified in the fixed argument to MCMCglmm (fixed=cbind(g5,l5)~type+ypub). No random effects were fitted, although ‘residuals’ were fitted as default to absorb any overdispersion. Residuals are random effects for which we estimate a variance - the hyperparameter, \\(\\sigma^2_e\\) - and when used with Binomial or Poisson responses are commonly referred to as observation-level random effects. Since there is a one-to-one correspondence between observation and photo in this data set, \\(\\sigma^2_e\\) is equivalent to the \\(\\sigma^2_u\\) discussed above (although \\(\\sigma^2_e\\) refers to the variance on the logit scale rather the probability scale used implicitly above). We saw that the probability of success varied greatly across photos (model mbinom.1) but we also noted that some of this variation may be due to the person being photographed and we could tease apart the effect of person from the specifics of the photo since each person was photographed twice - once when happy and once when grumpy. \\(\\texttt{person}\\) has 22 levels and you are probably not interested in knowing the grumpy score of someone you didn’t know - \\(\\texttt{person}\\) effects seem to satisfy the rule of thumb often used to decide that they should be treated as random. The random effect model is specified through the argument random and for simple effects as these we simply put the name of the corresponding column (\\(\\texttt{person}\\)) in the model formula. We will also specify inverse-Wishart priors for both the residual variance and the variance of the \\(\\texttt{person}\\) effects (see Section 2.6) although scaled non-central \\(F\\)-distribution priors are recommended for random-effect variances (see Section 4.6): prior.mbinom.2 = list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1, nu = 0.002))) mbinom.2 &lt;- MCMCglmm(cbind(g5, l5) ~ type + ypub, random = ~person, data = Grumpy, family = &quot;binomial&quot;, pr = TRUE, prior = prior.mbinom.2) summary(mbinom.2) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 5376.535 ## ## G-structure: ~person ## ## post.mean l-95% CI u-95% CI eff.samp ## person 0.8571 0.008145 1.644 744.3 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 0.5662 0.2551 1.03 757.7 ## ## Location effects: cbind(g5, l5) ~ type + ypub ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.69340 -1.71089 0.32701 1000 0.170 ## typehappy -1.28947 -1.73468 -0.80224 1000 &lt;0.001 *** ## ypub 0.01961 -0.01583 0.05956 1000 0.292 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can see that the between-person variance is comparable to the residual (across-photo within-person) variance although the credible intervals on both variances is wide. \\(\\texttt{MCMCglmm}\\) does not store the posterior distribution of the random effects by default, as there may be a lot of them and they are often not of interest. However, since I specified pr=TRUE, the whole of \\(\\boldsymbol{\\theta}\\) is stored rather than just \\({\\boldsymbol{\\mathbf{\\beta}}}\\). In Section 3.6 we saw that photo 4521 and photo 4527, despite having the same fixed effect prediction (\\(\\texttt{type}\\) = \\(\\texttt{grumpy}\\), \\(\\texttt{ypub}\\) = 16 years), had quite different probabilities of success, with the posterior mean probabilities being 0.862 and 0.400 respectively. What we didn’t know is whether this divergence in probability was due to the person being photographed or some property of the photo. 4.2 Prediction with Random Effects If we use the \\(\\texttt{predict}\\) method on our model the default is to not only marginalise the residuals, but also to marginalise any other random effects. If we predict the probability of success for these two photos they are identical, because we are calculating the expectation based on \\({\\bf X}{\\boldsymbol{\\beta}}\\) only: predict(mbinom.2)[c(3, 25), ]/122 ## 3 25 ## 0.4268878 0.4268878 The \\(\\texttt{predict}\\) method (and \\(\\texttt{simulate}\\) method) for \\(\\texttt{MCMCglmm}\\) includes the argument \\(\\texttt{marginal}\\) which by default takes the \\(\\texttt{random}\\) argument used to fit the model. If we want to obtain a prediction that includes (some of) the random effects we can remove the corresponding term from the formula passed to \\(\\texttt{marginal}\\). Since we only have one random term, which we like to include in the prediction, \\(\\texttt{marginal}\\) is empty: predict(mbinom.2, marginal = NULL, interval = &quot;confidence&quot;)[c(3, 25), ]/122 ## fit lwr upr ## 3 0.6305977 0.4239065 0.8337487 ## 25 0.3926134 0.2243962 0.5917725 It seems that some of the divergence in probability is due to the person being photographed: our best estimate is that if we had taken many photos of \\(\\texttt{darren_o}\\) when grumpy 63.1 % of people would have scored him above five on the grumpy scale, but for \\(\\texttt{craig_w}\\) it would be lower (39.3 %). The 95% credible (confidence) intervals on each are wide however, and a formal comparison (on the logit scale) gives a 95% credible interval that overlaps zero: HPDinterval(mbinom.2$Sol[, &quot;person.darren_o&quot;] - mbinom.2$Sol[, &quot;person.craig_w&quot;]) ## lower upper ## var1 -0.1010677 2.502953 ## attr(,&quot;Probability&quot;) ## [1] 0.95 4.3 Overdispersed Binomial as a Bernoulli GLMM The \\(\\texttt{Grumpy}\\) data set aggregates the scores of the 122 respondents into a single binomial response for each photograph. However, we could imagine disaggregating the data such that each respondent for each photograph gets a Bernoulli response with a success if they gave a particular photo a score greater than five. The disaggregated data (FullGrumpy) have \\(122\\times 44 = 5,368\\) observations (although a few respondents did not assess all photos). data(FullGrumpy) head(FullGrumpy, 3) ## y type photo person age ypub respondent student ## 1 9 grumpy 4511 ally_p 38 13 1 NO ## 2 8 grumpy 4511 ally_p 38 13 2 NO ## 3 3 grumpy 4511 ally_p 38 13 3 NO \\(\\texttt{y}\\) is now the score each respondent \\(\\texttt{respondent}\\) gave each \\(\\texttt{photo}\\) (rather than the average score for each \\(\\texttt{photo}\\) in Grumpy). In addition, we have the respondent-level information \\(\\texttt{student}\\) which can be either \\(\\texttt{YES}\\) or \\(\\texttt{NO}\\). We will turn each persons score into the Bernoulli response FullGrumpy$g5 &lt;- FullGrumpy$y &gt; 5 and fit the model prior.mbinom.3 = list(R = list(V = 1, fix = 1), G = list(G1 = list(V = 1, nu = 0.002), G2 = list(V = 1, nu = 0.002))) mbinom.3 &lt;- MCMCglmm(g5 ~ type + ypub, random = ~person + photo, data = FullGrumpy, family = &quot;binomial&quot;, prior = prior.mbinom.3) summary(mbinom.3) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 5332.549 ## ## G-structure: ~person ## ## post.mean l-95% CI u-95% CI eff.samp ## person 1.046 0.0005014 2.173 308.1 ## ## ~photo ## ## post.mean l-95% CI u-95% CI eff.samp ## photo 0.8689 0.3223 1.717 210.6 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 1 1 1 0 ## ## Location effects: g5 ~ type + ypub ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.84769 -1.97064 0.42422 886.5 0.162 ## typehappy -1.49164 -2.13927 -0.94842 749.9 &lt;0.001 *** ## ypub 0.02431 -0.02036 0.06655 1000.0 0.240 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The random effects deal with any variation in the probability of success across photos and are exactly comparable to the residuals of the binomial model mbinom.2. It is therefore surprising that the posterior distributions for both the variance in \\(\\texttt{person}\\) effects and the variance in \\(\\texttt{photo}\\)/residual effects appear to be different between the two models, as do the fixed effects. This is a peculiarity of logit-link models in \\(\\texttt{MCMC}\\) and wouldn’t be seen in \\(\\texttt{family=&quot;threshold&quot;}\\) models that implements the standard probit link. In the binomial model mbinom.2 we implicitly assumed that the probability of success did not vary across respondents within photos - this was an assumption, and one that cannot be tested. In the Bernoulli model mbinom.3 we explicitly assumed that the probability of success varied across respondents within photos, and the variance on the logit-scale was one. Since Bernoulli data provide no information about observation-level variability either (or most likely, neither) assumption could be true but we have no way of knowing (Section 3.6.3). As we saw with fixed effect coefficients in Bernoulli GLM, stating that the variances in photo effects is 0.869 is meaningless in a Bernoulli GLMM without putting it in the context of the assumed residual variance. The standard approach - what I refer to as the standard logit model - is to assume the residual variance is zero. While I think this is a good standard, this is prohibited in \\(\\texttt{MCMCglmm}\\) because the chain will not mix. But as we saw with the fixed effects, we can rescale the variances by needs to be multiplied by \\(1/(1+c^{2}\\sigma^{2}_{\\texttt{units}})\\) where \\(c=16\\sqrt{3}/15\\pi\\) and \\(\\sigma^{2}_{\\texttt{units}}\\) is our assumed residual variance, which is one (Figure 4.1). c2 &lt;- ((16 * sqrt(3))/(15 * pi))^2 rescale.VCV.2 &lt;- mbinom.2$VCV colnames(rescale.VCV.2)[2] &lt;- &quot;photo&quot; rescale.VCV.3 &lt;- mbinom.3$VCV[, c(&quot;person&quot;, &quot;photo&quot;)]/(1 + c2) plot(mcmc.list(as.mcmc(rescale.VCV.2), as.mcmc(rescale.VCV.3)), density = FALSE) Figure 4.1: MCMC traces for the estimated variances in \\(\\texttt{person}\\) and \\(\\texttt{photo}\\) effects from a Bernoulli GLMM (model mbinom.3) of individual data (red) and a Binomial GLMM (model mbinom.2) where all data for a photo have been aggregated into a single Binomial response (black). The posterior distribution of the variances from the Bernoulli GLMM have been rescaled to what would be observed if the residual variance was zero (rather than one). 4.4 Intra-class Correlations A more common approach, however, is to express the variances as intra-class correlations where we take the variance of interest and express it as a proportion of the total. For example, for the \\(\\texttt{person}\\) effects, the intra-class correlation would be \\[ICC = \\frac{\\sigma^2_{\\texttt{person}}}{\\sigma^2_{\\texttt{person}}+\\sigma^2_{\\texttt{photo}}+\\sigma^2_{\\texttt{units}}+\\pi^2/3}\\] where the \\(\\pi^2/3\\) appears because we have used the logit link and this is the link variance (the variance of the unit logistic). ICC.2 &lt;- mbinom.2$VCV/(rowSums(mbinom.2$VCV) + pi^2/3) colnames(ICC.2)[2] &lt;- &quot;photo&quot; ICC.3 &lt;- mbinom.3$VCV[, c(&quot;person&quot;, &quot;photo&quot;)]/(rowSums(mbinom.3$VCV) + pi^2/3) plot(mcmc.list(as.mcmc(ICC.2), as.mcmc(ICC.3)), density = FALSE) Figure 4.2: MCMC traces for the estimated intra-class correlation for \\(\\texttt{person}\\) and \\(\\texttt{photo}\\) effects from a Bernoulli GLMM (model mbinom.3) of individual data (red) and a Binomial GLMM (model mbinom.2) where all data for a photo have been aggregated into a single Binomial response (black). If we had used \\(\\texttt{family=&quot;threshold&quot;}\\) this would be omitted because the link variance is zero as we are already working on the … scale13 Pierre de Villemereuil’s \\(\\texttt{QGLMM}\\) package. The eagle-eyed will have noticed that although the trace plots for the rescaled variance/intra-class correlation for the \\(\\texttt{person}\\) effects look identical between the two models, the trace plots for the \\(\\texttt{photo}\\) effects look slightly different with the posterior from model mbinom.3 (the Bernoulli GLMM in red) appearing to have more density at higher values. However, this difference is due to Monte Carlo error, which is quite high for the variance estimates because the autocorrelation in the chain is moderate. The reported effective sample size in the model summary gives some indication of this - for example in the Bernoulli model the effective sample size for the variance in \\(\\texttt{photo}\\) effects is 211, quite a bit less than the 1,000 samples saved (Section 2.5.4). We can see this more clearly if we just plot the traces for Bernoulli model (Figure 4.3). Figure 4.3: MCMC trace for the variances in \\(\\texttt{person}\\) and \\(\\texttt{photo}\\) effects from a Bernoulli GLMM (model mbinom.3). Two things are apparent from Figure 4.3. Autocorrelation is present - this is not surprising: for each iteration of the MCMC chain the random effects are Gibbs sampled conditional on their variance in the previous iteration, and then conditional on the updated random effects the variances are then Gibbs sampled (Section 2.5). This will invariably lead to autocorrelation. Second, the trace for the variance in \\(\\texttt{person}\\) effects appears to intermittently get ‘stuck’ at values close to zero. In part, this reflects the mechanics of the Gibbs sampling, but it is also a consequence of the inverse-Wishart prior used which has a sharp peak in density at small values (Section 2.6). When the variance in \\(\\texttt{person}\\) effects gets ‘stuck’ at zero, the variance in \\(\\texttt{photo}\\) effects appears to get ‘stuck’ at high values. This is because the data provide strong support for the combined effect of \\(\\texttt{photo}\\) and \\(\\texttt{person}\\) being large, but contain less information about their separate effects. Consequently, if the \\(\\texttt{person}\\) variance drops to zero, the \\(\\texttt{photo}\\) variance increases to compensate. In this example, the effects described above are quite subtle, and simply running the chain for longer would probably suffice. However, a better general strategy would be to employ parameter expansion and use scaled non-central \\(F\\) priors. 4.5 Underdispersion Underdispersion in GLMM is likely to create problems. While (G)LMM are parameterised in terms of variances, it is possible - and advisable - to think about them as modelling the covariances between observations belonging to the same group. In the context of the data collected on photos, underdispersion would arise if there were negative covariances in the probability of success between the Bernoulli trials of each photo that we aggregate into the binomial response. Since the variance parameters are really covariance parameters, the best estimate for the units ‘variance’ in the binomial would be negative. However, because GLMM are parameterised in terms of variances, negative values are prohibited and the best estimate would lie on the boundary at zero. This can also impact the estimate of the variances for other random effects - because the total ‘variance’ is negative, positive values of the variances at other levels are disfavoured. I don’t see this as an issue with GLMM, but an issue with the choice of distribution. The binomial arises from independent trials with a fixed probability. When underdispersion occurs, we are envisaging that the trials are no longer independent. When they were dependent but positively correlated we could accommodate the resulting overdispersion by letting the probability of success vary over photos. When they are dependent but negatively correlated, and therefore underdispersed, we can’t use this trick - the data are simply not binomially distributed and we can’t model our way out of that. Similarly, the Poisson distribution arises as the distribution of the number of events that occur in a fixed amount of time if they appear independently and at constant rate. If events are dependent but positively correlated, we can accommodate this overdisperion by allowing the rate to vary over observations. If events are dependent but negatively correlated we can’t use this trick - the data are simply not Poisson distributed. Family sizes are a good example. You have a kid. You have another kid for them to play with. You’re tired and broke and so stop having kids. Whereas a Poisson process is memoryless, a parent isn’t, and we would need a distribution that allowed for that. 4.6 Priors for Random Effect Variances Parameter expansion is an algorithmic trick for speeding up the mixing and convergence of the MCMC chain. An unintended but useful side-effect of parameter expansion is that it can allow a wider class of prior distributions while still permitting Gibbs sampling (Chapter 2). In order to explore parameter expansion, and the associated \\(F\\) prior for random-effect variances, we will work with a model and data-set where the issues noted for model mbinom.3 are much more obvious - the Schools example discussed in Gelman (2006). schools &lt;- data.frame(school = letters[1:8], estimate = c(28.39, 7.94, -2.75, 6.82, -0.64, 0.63, 18.01, 12.16), ve = c(14.9, 10.2, 16.3, 11, 9.4, 11.4, 10.4, 17.6)^2) head(schools) ## school estimate ve ## 1 a 28.39 222.01 ## 2 b 7.94 104.04 ## 3 c -2.75 265.69 ## 4 d 6.82 121.00 ## 5 e -0.64 88.36 ## 6 f 0.63 129.96 The response variable estimate is the relative effect of Scholastic Aptitude Test coaching programs in 8 schools and Gelman (2006) focusses on the variance in school effects. Since we only have a single estimate per school there will be a one-to-one mapping between \\(\\texttt{school}\\) effects and the residual. In most cases this would result in the variance in school effects being confounded with the residual variance. Here, however, we have been gifted the residual (within school) variance (\\(\\texttt{ve}\\)) which varies from school to school. In reality, these residual variances are actually estimates and we might wish to factor in this additional complication, but for now we will ignore this complexity and come back to it in Chapter 9. First, lets fit the inverse-Wishart prior we have been using up to now with \\(\\texttt{V}=1\\) and \\(\\texttt{nu}=0.002\\). This prior is equivalent to an inverse-gamma prior with a shape and scale of 0.001 (Section 2.6): prior.mschool.1 &lt;- list(R = list(V = diag(schools$ve), fix = 1), G = list(G1 = list(V = 1, nu = 0.002))) mschool.1 &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior.mschool.1) The model contains an argument we haven’t seen before: \\(\\texttt{rcov}\\). In all previous analyses we used the default ~units which fits a set of independent and identically distributed residuals with a single variance (\\(\\sigma^2_{\\texttt{units}}\\)) to be estimated. \\(\\texttt{MCMCglmm}\\) allows this assumption to be relaxed, and Chapter 5 is dedicated to this subject. Here, we will simply note that we have assigned each school a residual variance (the corresponding element of \\(\\texttt{ve}\\)) and fixed it at this value, leaving only the intercept and the variance in school effects (\\(\\sigma^2_{\\texttt{school}}\\)) to be estimated. The MCMC trace for \\(\\sigma^2_{\\texttt{school}}\\) looks dreadful (Figure 4.4). Figure 4.4: MCMC trace for the variance in \\(\\texttt{school}\\) effects from model mschool.1 in which an inverse-Wishart prior was used with \\(\\texttt{V=1}\\) and \\(\\texttt{nu=0.002}.\\) The autocorrelation in the chain is evident and there appears to be a lot of posterior density at zero. These are separate issues. Autocorrelation in the chain is due to algorithmic inefficiencies in sampling the posterior distribution, whereas a lot of posterior density near zero reflects the combined information coming from the data and coming from the prior (Chapter 2). Certainly, some posterior distributions are harder to sample from than others, and the efficiency of the MCMC algorithm may decrease when the posterior is close to zero. But if the chain can be run long enough that these inefficiencies are not consequential, situations where the posterior has ‘too much’ density near zero indicate potential problems with the prior, not algorithmic problems. 4.6.1 \\(F\\) and folded-\\(t\\) priors For the inverse-Wishart prior we specified the parameters \\(\\texttt{V}\\) and \\(\\texttt{nu}\\). The parameters \\(\\texttt{alpha.mu}\\) and \\(\\texttt{alpha.V}\\) can also be specified in the prior, and if \\(\\texttt{alpha.V}\\) is non-zero then parameter expansion is used. These additional parameters specify a prior for the variance, \\(\\sigma^2_{\\texttt{school}}\\), that is a non-central scaled F-distribution with the numerator degrees-of-freedom set to one. We can also think about this prior in terms of the standard deviation, \\(\\sigma_{\\texttt{school}}\\), which in some ways is more natural, since it is in the same units as the response. The prior distribution for the standard deviation is a folded scaled non-central \\(t\\). As the length of their names suggest, these two distributions are quite complicated. Regrettably, this complication is exacerbated in \\(\\texttt{MCMCglmm}\\) by specifying the prior through the distribution of the parameter expansion working parameters (See Section ??). I will leave the full relationship between the prior specification and the \\(F\\) and \\(t\\) distributions to the footnote14, and introduce two simplifications here. First, these distributions have three free parameters, yet the prior specification has four. Without loss of generality we will use \\(\\texttt{V}=1\\) throughout. Then, \\(\\texttt{alpha.V}\\) specifies the scale of the \\(F\\) prior for the variance and \\(\\sqrt{\\texttt{alpha.V}}\\) specifies the scale of the \\(t\\) prior for the standard deviation. Second, since the non-central forms of these distributions are rarely - if ever - used as priors, we will set the non-centrality parameter to zero via \\(\\texttt{alpha.mu}=0\\). For the standard deviation prior this results in the added simplification that the folded-\\(t\\) becomes the half-\\(t\\) (essentially a \\(t\\) with the negative values missing). We are then left with two-parameter distributions with the scale set by \\(\\texttt{alpha.V}\\) and the (denominator) degrees-of-freedom, \\(\\texttt{nu}\\). Before discussing the properties of the \\(F\\) and half-\\(t\\) priors on the posterior distributions of variances and standard deviations, respectively, let’s confirm that parameter expansion does indeed increase efficiency independent of the prior being used. In Section 2.6 we saw that an improper inverse-Wishart distribution with \\(\\texttt{V}=0\\) and \\(\\texttt{nu}=-1\\) is flat for the standard deviation. A half-\\(t\\) with \\(\\texttt{nu}=-1\\) is also an improper flat prior on the standard deviation irrespective of what is specified for the other parameters. Let’s fit these flat improper priors with and without parameter expansion: prior.nopx &lt;- list(R = list(V = diag(schools$ve), fix = 1), G = list(G1 = list(V = 1e-16, nu = -1))) m.nopx &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior.nopx) # flat prior on the school standard deviation - no parameter expansion prior.px &lt;- list(R = list(V = diag(schools$ve), fix = 1), G = list(G1 = list(V = 1e-16, nu = -1, alpha.mu = 0, alpha.V = 1000^2))) # note I have set V=1e-16 rather than V=1: with nu=-1, V does not influence the # prior m.px &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior.px) # flat prior on the school standard deviation - with parameter expansion We can see that these two models are sampling from the same posterior (Figure 4.5) but the efficiency of the algorithm is greater when parameter expansion is used, with an effective posterior sample size of 877 rather than 404. Figure 4.5: MCMC traces for the standard deviation in \\(\\texttt{school}\\) effects. In black is the trace for model m.nopx in which an improper inverse-Wishart prior was used for the variance with \\(\\texttt{V=0}\\) and \\(\\texttt{nu=-1}\\). In red is the trace for model m.px in which an improper \\(F_{1,-1}\\) prior was used with \\(\\texttt{nu=-1}\\) and $\\(\\texttt{alpha.V}=1000^2\\). Both priors are flat for the standard deviation but m.px employs parameter expansion. While parameter expansion usually results in more efficient sampling of the posterior, to justify its use, it is important that it also allows us to specify priors with sensible properties. Those with Bayesian scruple may baulk at the use of an improper prior. However, if we specify \\(\\texttt{nu=1}\\) (a half-\\(t_1\\) or half-Cauchy) we can specify a proper prior that becomes flat for the standard deviation as we increase the scale. Let’s fit a proper half-Cauchy with a scale that is large (relative to the data) - \\(\\sqrt{\\texttt{alpha.V}}=1000\\). prior.mschool.2 &lt;- list(R = list(V = diag(schools$ve), fix = 1), G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000^2))) # half-Cauchy prior on the standard-deviation with scale 1000 mschool.2 &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior.px) # flatish prior on the school standard deviation Overlaying the trace on those obtained under flat improper priors shows that the posterior distributions for \\(\\sigma_\\texttt{school}\\) are indistinguishable (Figure 4.6). Figure 4.6: MCMC traces for the standard deviation in \\(\\texttt{school}\\) effects. In black is the trace for model m.nopx in which an improper inverse-Wishart prior was used for the variance with \\(\\texttt{V=0}\\) and \\(\\texttt{nu=-1}\\). In red is the trace for model m.px in which an improper \\(t_{-1}\\) prior was used for the standard deviation with \\(\\texttt{alpha.V}\\) set to \\(1000^2\\). Both priors are flat for the standard deviation but m.px employs parameter expansion. In green is the trace for model mschool.2 in which a proper half-Cauchy (\\(t_{1}\\)) with a scale of \\(1000\\) was used. This prior is almost flat for the standard deviation over the range of values that have reasonable support. Is a flat(ish) prior on the standard deviation more sensible than the inverse-Wishart prior on the variance we have been using previously (\\(\\texttt{V}=1\\) and \\(\\texttt{nu}\\)=0.002)? The short answer is yes. Setting \\(\\texttt{nu}\\) to be small in the inverse-Wishart is motivated by the idea that having few prior degrees of freedom provides a diffuse prior on the variance. However, as \\(\\texttt{nu}\\) becomes smaller the prior only becomes flat for the log of the variance, and there is a nasty spike at small values for both the variance (Section 2.6) and the standard deviation. However, in some cases, as in the Schools example, the amount of information provided by the data may be so small that a flat prior on the standard deviation might be to vague. In such cases, reducing the scale has been advised, and Gelman (2006) used a scale of 25 for this example: prior.mschool.3 &lt;- list(R = list(V = diag(schools$ve), fix = 1), G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 25^2))) mschool.3 &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior.mschool.3) When reducing the scale, some care has to be taken that the scale is aligned with the scale of the data. For example, the common ‘default’ recommendation of setting the scale to 5 might prove problematic in this example where the standard deviation of the data exceeds one by some margin: prior.mschool.4 &lt;- list(R = list(V = diag(schools$ve), fix = 1), G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 5^2))) mschool.4 &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior.mschool.4) The default in brms (Bürkner 2017) is to use a half-\\(t\\) with 3 degrees of freedom and a scale of 2.5. The tails of this distribution are less ‘heavy’ than the Cauchy and in conjunction with the reduced scale penalise large values more strongly. prior.mschool.5 &lt;- list(R = list(V = diag(schools$ve), fix = 1), G = list(G1 = list(V = 1, nu = 3, alpha.mu = 0, alpha.V = 2.5^2))) mschool.5 &lt;- MCMCglmm(estimate ~ 1, random = ~school, rcov = ~idh(school):units, data = schools, prior = prior.mschool.5) When comparing these priors visually we see that they are quite different, despite all having some history as default and/or recommended priors (Figure 4.7). Figure 4.7: Prior probability densities for the standard deviation of \\(\\texttt{school}\\) effects used in the \\(\\texttt{mschool}\\) models \\(\\texttt{1-5}\\). The scale of the half-Cauchy and half-t distributions are given after the colon. Note the inverse-gamma is the prior for the variance - not the standard deviation. With so little information coming from the data, these priors also have considerable influence on the posterior (Figure 4.8). When \\(\\texttt{V}\\)=1 and \\(\\texttt{nu}\\)=0.002, small values are strongly favoured, and this is also true to a lesser extent for the half-\\(t_3\\) prior with low scale. As we up the scale and/or reduce the degree-of-freedom to one (the half-Cauchy) the prior becomes flatter and the resulting posteriors are more similar. Figure 4.8: Posterior probability densities for the standard deviation of \\(\\texttt{school}\\) effects used in the \\(\\texttt{mschool}\\) models \\(\\texttt{1-5}\\). The priors used in these models are given in the legend with the scale of the half-Cauchy and half-t distributions following the colon. Note the inverse-gamma is the prior for the variance - not the standard deviation. Figure 4.8 is anxiety-inducing - which prior to use? While I am reluctant to give general advise, in my own work I tend to use a half-Cauchy with a large scale: often \\(\\sqrt{1000}\\) (alpha.V=1000) depending on the scale of the data. However, I try to avoid collecting and analysing data that contains as little information as seen in this example (but see Section 4.9). When this is the case, shifts in the posterior distribution under different priors are often subtle. However, for variances or standard deviations that have support close to zero the spike of the inverse-Wishart/inverse-gamma distribution can cause problems, and for this reason I usually avoid it. The added bonus is faster mixing under parameter expansion. While the prior for the residual variance is restricted to the the inverse-Wishart/inverse-gamma distribution in MCMCglmm (Section 2.6) the data usually contains so much information for the residual variance that prior sensitivity is limited. For models where the random-effect specification defines a (co)variance matrix rather than a scalar variance, see Section 5.3. 4.7 Prior Generators Even when a user knows which prior they would like to use, it can be rather fiddly generating the prior list, especially when there are many random terms, some of which define (co)variance matrices. In order to simplify the process of prior specification a set of prior generator functions are available in versions &gt;4.0. These functions \\(\\texttt{IW}\\) (inverse-Wishart), \\(\\texttt{IG}\\) (inverse-gamma) and \\(\\texttt{F}\\) (central \\(F\\) with 1 numerator degree of freedom) take two arguments specifying the parameters of the prior distribution to be used for the variances. The function \\(\\texttt{tSD}\\) can also be used which specifies a half-\\(t\\)-distribution for the standard deviation. The functions come with default values, but these can be overridden (Table ??). Note that a set parameters can always be chosen such that \\(\\texttt{IW}=\\texttt{IG}\\) and \\(\\texttt{F}=\\texttt{tSD}\\). The difficult topic of prior specification for (co)variance matrices is covered in Section 5.3 and I’ll leave the documentation of the prior generator functions in this context to then. Table 4.1: Table of prior generator functions for specifying the (marginal) prior of a variance (\\(\\texttt{IW}\\), \\(\\texttt{IG}\\) and \\(\\texttt{F}\\)) or a standard deviation (\\(\\texttt{tSD}\\)). The two parameters for each distribution are given together with their defaults. Function Distribution Parameter1 Parameter2 IW Inverse-Wishart V=1 nu=0.002 IG Inverse-gamma shape=0.001 scale=0.001 F F with df1=1 df2=1 scale=1000 tSD Half-t (for SD) df=1 scale=\\(\\sqrt{1000}\\) The prior generator functions can be used to specify the prior for a particular term, or all \\(\\texttt{G}\\) and \\(\\texttt{R}\\) terms. For example, for model mbinom.3 we specified priors for the single residual variance and the two random effect variances: prior.mbinom.3 = list(R = list(V = 1, fix = 1), G = list(G1 = list(V = 1, nu = 0.002), G2 = list(V = 1, nu = 0.002))) If we wished to use a central \\(F_{1,1}\\) prior with a scale of a \\(1000\\) for the variance in the first random term, the long-hand prior would look like: prior.mbinom.3 = list(R = list(V = 1, fix = 1), G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000), G2 = list(V = 1, nu = 0.002))) However, we can also specify this prior as prior.mbinom.3 = list(R = list(V = 1, fix = 1), G = list(G1 = F(1, 1000), G2 = list(V = 1, nu = 0.002))) If we wished to also use the \\(F\\) prior for the second random term we could either use prior.mbinom.3 = list(R = list(V = 1, fix = 1), G = list(G1 = F(1, 1000), G2 = F(1, 1000))) or more compactly as: prior.mbinom.3 = list(R = list(V = 1, fix = 1), G = F(1, 1000)) The prior generators can also be used for the residual variance(s) but because we need to specify additional arguments (\\(\\texttt{fix}\\) in this case) we have to stick with the long-hand version. 4.8 Priors on Functions of Variances 4.8.1 Intra-class Correlation 4.9 Fixed or Random? At the start of this Chapter I highlighted the central decision that needs to be made when deciding if an effect should be random or fixed: should a vague prior for the effect (or flat prior in a frequentist analysis) be specified entirely by the analyst (fixed) or should the prior be updated using all data that has relevant information (random)? When put this way, treating something as random would always seem preferable. However, sometimes there is so little relevant information that the benefit of treating an effect as random is outweighed by the cost of the increased complexity. In my experience, when there is a lack of relevant information - the information required to estimate the random-effect variance, \\(\\sigma^2_u\\) - it is almost always because there are only a few levels of the predictor, not because the replication per level is small. This is why people often use the rule-of-thumb that if there are few factor levels (\\(&lt;5\\)) then we should treat the effects as fixed. In this scenario, even if we had infinite replication at each level, we would still only have fewer than five observations from which to estimate the variance. In such cases a second rule-of-thumb is often satisfied: if the levels are interesting to other people, they are fixed. However, when the two rules-of-thumb are in conflict, people often agonise about the choice. If your data set is of an admirable size, I would argue that it often doesn’t matter what you choose. If you have few levels of a predictor, that probably means you have a lot of replication per level. In these situations, the amount of information coming from observations from that level overwhelm any prior information and so it doesn’t matter whether you say the prior information is vague (fixed) or try and update the prior information from all observations (random). In my own field, year is often a good example. A field project has ran for less than five years but each year a lot data is collected. Should the year effects be treated as fixed or random? Well there are few levels, suggesting fixed, but the effect of year 2014 (for example) isn’t particularly interesting, suggesting random. People sometimes claim fixed because years haven’t been sampled at random, but this argument shows a deep misunderstanding of what a random effect is. ‘random’ isn’t referring to years being sampled at random, but referring to the fact that we would like to treat year effects as random variables coming from a distribution. If I had observations from many years I would certainly fit year effects as random. However, with few years and a lot of data per year I would treat them as fixed. Let’s consider a data-set I collected and that I use for teaching: load(url(&quot;https://github.com/jarrodhadfield/sda/raw/master/data/BTtarsus.rda&quot;)) head(BTtarsus) ## tarsus_mm bird_id sex year nest_orig nest_rear day_hatch ## 1 17.2 L298904 F 2011 11_A9 11_A9 0 ## 2 17.6 L298903 M 2011 11_A9 11_A9 0 ## 3 16.2 L298905 F 2011 11_82 11_A9 0 ## 4 17.0 L298901 M 2011 11_82 11_A9 0 ## 5 17.3 L298900 M 2011 11_A9 11_A9 1 ## 6 16.1 L298902 M 2011 11_82 11_A9 1 \\(\\texttt{tarsus_mm}\\) are the lengths of the tarsus bone in Blue Tit chicks and \\(\\texttt{year}\\) is the year (2011-2014) in which the chicks hatched and I measured them. The remaining variables will not be relevant here. The key point is that \\(\\texttt{year}\\) only has four levels but the number of birds with tarsus measurements is high in each year (between 593 and 854). Let’s fit two models, one where year effects are fixed and one where they are random: BTtarsus$year &lt;- as.factor(BTtarsus$year) prior.myear.fixed = list(R = list(V = 1, nu = 0.002)) myear.fixed &lt;- MCMCglmm(tarsus_mm ~ year, data = BTtarsus, prior = prior.myear.fixed) prior.myear.random = list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000))) myear.random &lt;- MCMCglmm(tarsus_mm ~ 1, random = ~year, data = BTtarsus, pr = TRUE, prior = prior.myear.random) If we plot the posterior distribution for the estimated year means we can see that they are extremely similar (Figure 4.9). Figure 4.9: MCMC traces for the estimated year means when year effects are treated as fixed (black) or random (red). Because the amount of replication per year is high the posterior distributions are very similar With a flatish prior on the standard deviation of year effects (half-Cauchy with scale \\(\\sqrt{1000}\\)) the posterior distribution is, for those familiar with Blue Tit tarsus lengths, hopeless uncertain (Figure 4.10). A standard deviation of 0.5 has some support. This implies that some years we expect some seriously stumpy Blue Tits and in other years we expect some seriously leggy Blue Tits. But a standard deviation of 0.05 also has some support and this implies that tarsus length hardly varies from year-to-year. In such cases we might as well admit that we cannot estimate the between year variability with sufficient precision even though we can infer with precision the effect of the handful of years we have measurements for. When this is the case, we may as well treat the effects as fixed. Figure 4.10: Posterior distribution for the standard deviation of year effects from model prior.myear.random. References "],["cat-int.html", "5 Categorical Random Interactions 5.1 Variance Structures 5.2 Linking Functions 5.3 Priors for Covariance Matrices", " 5 Categorical Random Interactions In Chapter 4 we looked at a simple random-effect specification where a single effect is associated with each level of a categorical predictor and the effects are assumed to be identically and independently distributed. In the next three chapters we will cover more complicated random-effect specifications. In this Chapter we will start by exploring models where an interaction is formed between two categorical predictors, one which (usually) has few levels with main effects fitted as fixed and the other which (usually) has many levels and would normally be fitted as random. We will also cover related models where the random effects associated with two or more sets of categorical predictors (including residual effects) are correlated. This includes multi-membership models as a special case. It is probably easiest to start with an example. The data set \\(\\texttt{sigma}\\) contains data on how well fruit flies (Drosophila melanogaster) transmit sigma virus to their offspring (Carpenter et al. 2012). data(sigma) head(sigma) ## infected not_infected virus line ## 1 2 38 USA 391A ## 2 0 8 USA 391A ## 3 3 50 France 392A ## 4 17 48 France 556A ## 5 10 16 France 556A ## 6 1 14 Greece 392A The number of flies in a vial that are infected with the virus (\\(\\texttt{infected}\\)) or not (\\(\\texttt{not_infected}\\)) are recorded. The mean number of flies per vial is 29 although the range is large (1 - 96 ). All the flies in a vial are the offspring of parents infected with one of five strains (\\(\\texttt{strain}\\)) of sigma virus, a virus that is transmitted to offspring through eggs and sperm. The parents belong to one of 67 genetically distinct lines (\\(\\texttt{line}\\)). Let’s start with a simple binomial GLMM that we covered in Chapter 4, but to keep things manageable we’ll only analyse data from three strains (\\(\\texttt{France}\\), \\(\\texttt{Greece}\\) and \\(\\texttt{Spain}\\), named after their country of origin). Note that we will specify the prior using prior generator functions (Section 4.7) and so this specification can be recycled in all models. See Section @ref(VCVprior-r–sec) for why we have specified a scaled-\\(F_{1,2}\\) distribution rather than the scaled-\\(F_{1,1}\\) distribution used when estimating scalar variances (Section 4.6). sigma_small &lt;- subset(sigma, virus %in% c(&quot;France&quot;, &quot;Greece&quot;, &quot;Spain&quot;)) prior.sigma = list(R = IW(1, 0.002), G = F(2, 1000)) m.sigma.1 &lt;- MCMCglmm(cbind(infected, not_infected) ~ virus, random = ~line, data = sigma_small, family = &quot;binomial&quot;, prior = prior.sigma) summary(m.sigma.1) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 16739.41 ## ## G-structure: ~line ## ## post.mean l-95% CI u-95% CI eff.samp ## line 3.954 2.328 5.649 850.3 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 2.686 2.25 3.099 661.5 ## ## Location effects: cbind(infected, not_infected) ~ virus ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.66374 -1.22050 -0.08718 717.3 0.022 * ## virusGreece -1.73961 -2.10652 -1.39823 1000.0 &lt;0.001 *** ## virusSpain 0.53186 0.16149 0.89971 1000.0 0.004 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For the base-line virus, \\(\\texttt{France}\\), the estimate of the median probability of infection is 0.34, obtained by applying the plogis function to the intercept (Section 3.6.2). However, the viruses have substantially different infection rates with the median odds of infection for \\(\\texttt{Greece}\\) being only 0.18 times that of \\(\\texttt{France}\\), whereas \\(\\texttt{Spain}\\) has an odds of infection 1.73 times greater (obtained by exponentiating the coefficients). There is also substantial variation across lines in how well they transmit the virus (posterior mean for \\(\\sigma^2_\\texttt{line}\\)= 3.95) and substantial overdispersion (\\(\\sigma^2_\\texttt{units}\\)= 2.69). However, this model contains the implicit assumption that the increased odds of infection associated with a line is constant over viruses. For example, the first level of \\(\\texttt{line}\\) is \\(\\texttt{101A}\\). If this line’s effect is \\(u^{(\\texttt{line})}_1\\) then the linear predictor for all observations of that line will include \\(u^{(\\texttt{line})}_1\\) and the odds of infection is expected to change by a factor \\(\\textrm{exp}(u^{(\\texttt{line})}_1)\\) irrespective of the virus. However, we may expect there to be genetic interactions between the virus and the fly such that the infectivity of a line differs depending on the virus. We could add a simple interaction between line and variance and treat these (\\(67\\times 3 = 201\\)) interaction effects as random: m.sigma.2 &lt;- MCMCglmm(cbind(infected, not_infected) ~ virus, random = ~line + virus:line, data = sigma_small, family = &quot;binomial&quot;, prior = prior.sigma) summary(m.sigma.2) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 16731.28 ## ## G-structure: ~line ## ## post.mean l-95% CI u-95% CI eff.samp ## line 3.615 2.043 5.274 315.9 ## ## ~virus:line ## ## post.mean l-95% CI u-95% CI eff.samp ## virus:line 0.6537 0.2215 1.09 679.3 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 2.317 1.907 2.725 741.8 ## ## Location effects: cbind(infected, not_infected) ~ virus ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.6611 -1.2598 -0.0904 1000 0.020 * ## virusGreece -1.7384 -2.1970 -1.2575 1000 &lt;0.001 *** ## virusSpain 0.5568 0.1203 1.0217 1000 0.016 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 While the variance in the interaction effects (\\(\\sigma^2_\\texttt{virus:line}=\\) 0.65) is substantially smaller than the main \\(\\texttt{line}\\) effects, the lower 95% credible interval is well removed from zero suggesting that interactions are present. This model still has implicit assumptions baked into it about how lines vary in their infectivity for a given virus, and how lines covary in their infectivity between viruses. To avoid complications later, I will - with some abuse of notation - call the main \\(\\texttt{line}\\) effects in this model \\(u^{(\\texttt{m-line})}\\) and their variance \\(\\sigma^2_\\texttt{m-line}\\), with a posterior mean of 3.61. The aggregate effect of \\(\\texttt{line}\\) \\(i\\) infected with \\(\\texttt{virus}\\) \\(j\\) is then given by: \\[ u^{(\\texttt{line})}_{ij} = u^{(\\texttt{m-line})}_i+u^{(\\texttt{virus:line})}_{ij}\\] If we took the linear predictor for two observations made on the same line in the same virus they would both share the same two random effects and therefore have the same \\(u^{(\\texttt{line})}_{ij}\\). Consequently, the covariance in their linear predictor is the variance of \\(u^{(\\texttt{line})}\\): \\(\\sigma^2_\\texttt{m-line}+\\sigma^2_\\texttt{virus:line}\\). Since these two variances are assumed constant over viruses, we are implicitly assuming that the variance between-lines is the same for all viruses. If we took the linear predictor for two observations made on the same line but in different viruses they would both share the same \\(u^{(\\texttt{m-line})}\\) effect but have different interaction effects (\\(u^{(\\texttt{virus:line})}\\)). Consequently, the covariance in their linear predictor is the variance of \\(u^{(\\texttt{m-line})}\\) only: \\(\\sigma^2_\\texttt{m-line}\\). Again, \\(\\sigma^2_\\texttt{m-line}\\) is assumed constant over viruses and so this implies that the between-line covariance is also constant: the covariance between lines infected with \\(\\texttt{France}\\) versus \\(\\texttt{Spain}\\) is the same as the covariance between lines infected with \\(\\texttt{France}\\) versus \\(\\texttt{Greece}\\) or \\(\\texttt{Spain}\\) versus \\(\\texttt{Greece}\\). This constancy is also reflected in the between-line correlation: \\[r_{\\texttt{line}}= \\frac{\\sigma^2_\\texttt{m-line}}{\\sigma^2_\\texttt{m-line}+\\sigma^2_\\texttt{virus:line}}\\] which has a posterior mean of 0.84 and a relatively tight 95% credible interval of 0.73 - 0.95. Since \\(\\sigma^2_\\texttt{m-line}\\) is a variance, and therefore constrained to be positive, we are also implicitly assuming that the covariance and correlation are both constant and positive: a line that shows greater infectivity with \\(\\texttt{France}\\) is not expected, on average, to have reduced infectivity with \\(\\texttt{Spain}\\). We can think of summarising this information in a \\(3\\times 3\\) between-line covariance matrix (the meaning of the colours will become apparent soon): \\[{\\bf V}_{{\\color{red}{\\texttt{line}}}}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{\\color{blue}{\\texttt{France}}}&amp;\\sigma_{\\color{blue}{\\texttt{France}, \\texttt{Spain}}}&amp;\\sigma_{\\color{blue}{\\texttt{France}, \\texttt{Greece}}}\\\\ \\sigma_{\\color{blue}{\\texttt{France}, \\texttt{Spain}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Spain}}}&amp;\\sigma_{\\color{blue}{\\texttt{Spain}, \\texttt{Greece}}}\\\\ \\sigma_{\\color{blue}{\\texttt{France}, \\texttt{Greece}}}&amp;\\sigma_{\\color{blue}{\\texttt{Spain}, \\texttt{Greece}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Greece}}}\\\\ \\end{array} \\right]\\] The diagonal elements gives us the variance in \\(\\texttt{line}\\) effects for each virus and the off-diagonal elements gives us the covariance in \\(\\texttt{line}\\) effects between pairs of viruses. This covariance matrix has 6 (co)variances, but in model m.sigma.2 we assumed that these (co)variances could be parametrised using only two parameters: all diagonal elements are equal to \\(\\sigma^2_\\texttt{m-line}+\\sigma^2_\\texttt{virus:line}\\) and all off-diagonal elements are equal to \\(\\sigma^2_\\texttt{m-line}\\). If we wish to relax this assumption we need to understand how variance functions, such as \\(\\texttt{us}\\) and \\(\\texttt{idh}\\), work. 5.1 Variance Structures We could refit our first model m.sigma.2 using the random effect specifications: random = ~us(1):line or random = ~idh(1):line and this would give exactly the same answer as the model specified by ~line. The term inside the brackets is a model formula and is interpreted exactly how you would interpret any R formula expect the intercept is only fitted if it is explicitly defined (as here - remember a 1 in a formula stands in for ‘intercept’). These formula are therefore fitting an intercept which is interacted with the random effects. We can get a representation of the interaction for the first few levels of line (101A, 109A, 129A, 142A, 153A): \\[ \\begin{array}{c|rrrrrc} &amp;{\\color{red}{\\texttt{101A}}}&amp;{\\color{red}{\\texttt{109A}}}&amp;{\\color{red}{\\texttt{129A}}}&amp;{\\color{red}{\\texttt{142A}}}&amp;{\\color{red}{\\texttt{153A}}}&amp;\\dots\\\\ \\hline {\\color{blue}{\\texttt{(1)}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{101A}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{109A}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{129A}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{142A}}}&amp;{\\color{blue}{\\texttt{(1)}}}.{\\color{red}{\\texttt{153A}}}&amp;\\dots\\\\ \\end{array} \\] Across the top, we have the original \\(\\texttt{line}\\) effects in red, and along the side we have the term defined by the variance structure formula (just the intercept in this case). The interaction forms a new set of factors. Although they have different names from the original \\(\\texttt{line}\\) effects, it is clear that there is a one to one mapping between the original and the new factor levels and the models are therefore equivalent. For more complex interactions this is not the case. For example, if we fit \\(\\texttt{virus}\\) in the variance structure model (i.e. us(virus):line or idh(virus):line) we get15: \\[\\begin{array}{c|rrrrrc} &amp;{\\color{red}{\\texttt{101A}}}&amp;{\\color{red}{\\texttt{109A}}}&amp;{\\color{red}{\\texttt{129A}}}&amp;{\\color{red}{\\texttt{142A}}}&amp;{\\color{red}{\\texttt{153A}}}&amp;\\dots\\\\ \\hline {\\color{blue}{\\texttt{France}}}&amp;{\\color{blue}{\\texttt{France}}}.{\\color{red}{\\texttt{101A}}}&amp;{\\color{blue}{\\texttt{France}}}.{\\color{red}{\\texttt{109A}}}&amp;{\\color{blue}{\\texttt{France}}}.{\\color{red}{\\texttt{129A}}}&amp;{\\color{blue}{\\texttt{France}}}.{\\color{red}{\\texttt{142A}}}&amp;{\\color{blue}{\\texttt{France}}}.{\\color{red}{\\texttt{153A}}}&amp;\\dots\\\\ {\\color{blue}{\\texttt{Spain}}}&amp;{\\color{blue}{\\texttt{Spain}}}.{\\color{red}{\\texttt{101A}}}&amp;{\\color{blue}{\\texttt{Spain}}}.{\\color{red}{\\texttt{109A}}}&amp;{\\color{blue}{\\texttt{Spain}}}.{\\color{red}{\\texttt{129A}}}&amp;{\\color{blue}{\\texttt{Spain}}}.{\\color{red}{\\texttt{142A}}}&amp;{\\color{blue}{\\texttt{Spain}}}.{\\color{red}{\\texttt{153A}}}&amp;\\dots\\\\ {\\color{blue}{\\texttt{Greece}}}&amp;{\\color{blue}{\\texttt{Greece}}}.{\\color{red}{\\texttt{101A}}}&amp;{\\color{blue}{\\texttt{Greece}}}.{\\color{red}{\\texttt{109A}}}&amp;{\\color{blue}{\\texttt{Greece}}}.{\\color{red}{\\texttt{129A}}}&amp;{\\color{blue}{\\texttt{Greece}}}.{\\color{red}{\\texttt{142A}}}&amp;{\\color{blue}{\\texttt{Greece}}}.{\\color{red}{\\texttt{153A}}}&amp;\\dots\\\\ \\end{array}\\] which creates three times as many random effects, one associated with observations for each \\(\\texttt{virus}\\) for each each \\(\\texttt{line}\\). 5.1.1 \\(\\texttt{idh}\\) Variance Structure The different variance functions make different assumptions about how the effects are distributed within and across rows. First, we may want to allow the variance in the effects to be different for each row of factors; i.e. does \\(\\texttt{line}\\) explain different amounts of variation depending on the \\(\\texttt{virus}\\). We can fit this model using the idh function: m.sigma.3 &lt;- MCMCglmm(cbind(infected, not_infected) ~ virus, random = ~idh(virus):line, data = sigma_small, family = &quot;binomial&quot;, prior = prior.sigma) In the simpler models we have fitted so far, each random effect term (terms separated by + in the random formula) specified a single variance (e.g. \\(\\sigma^2_\\texttt{virus:line}\\)) and the prior specification was relatively simple and covered in Sections 2.6 and 4.6. Prior specifications for covariance matrices are much trickier and are covered in Section 5.3. For now, simply note that we have recycled the same prior generator function that we used in the simpler models, despite the the prior specification requiring \\(3\\times 3\\) matrices. If we take a look at the model summary: summary(m.sigma.3) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 16741.54 ## ## G-structure: ~idh(virus):line ## ## post.mean l-95% CI u-95% CI eff.samp ## virusFrance.line 3.980 2.005 6.307 599.3 ## virusGreece.line 4.214 2.181 6.830 496.5 ## virusSpain.line 3.687 2.093 5.750 639.6 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 2.314 1.923 2.722 697.1 ## ## Location effects: cbind(infected, not_infected) ~ virus ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.69857 -1.26637 -0.08768 399.0 0.018 * ## virusGreece -1.62506 -2.44844 -0.77185 1000.0 0.004 ** ## virusSpain 0.62382 -0.10118 1.48195 570.2 0.116 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 we see that three parameters are summarised for ~idh(virus):line: the variance in line effects for each virus. While the \\(\\texttt{idh}\\) variance function allows the variances to be different across the viruses it assumes that \\(\\texttt{line}\\) effects for one virus are independent of the \\(\\texttt{line}\\) effects for a different virus. In terms of our coloured table of effects \\(\\texttt{idh}\\) assumes effects in different rows are independently distributed, but it relaxes the assumption that they are identically distributed - they can have different variances. We can represent this structure in terms of a \\(3\\times3\\) covariance matrix: \\[{\\bf V}_{{\\color{red}{\\texttt{line}}}}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{\\color{blue}{\\texttt{France}}}&amp;0&amp;0\\\\ 0&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Spain}}}&amp;0\\\\ 0&amp;0&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Greece}}}\\\\ \\end{array} \\right]\\] We can extract the posterior means for each variance and place them into a matrix: Vline.idh &lt;- diag(colMeans(m.sigma.3$VCV)[1:3]) colnames(Vline.idh) &lt;- rownames(Vline.idh) &lt;- c(&quot;France&quot;, &quot;Spain&quot;, &quot;Greece&quot;) Vline.idh ## France Spain Greece ## France 3.979824 0.000000 0.000000 ## Spain 0.000000 4.214103 0.000000 ## Greece 0.000000 0.000000 3.687059 Because the \\(\\texttt{line}\\) effects are assumed to be multivariate normal we can also represent their distribution in terms of an ellipsoid (you can rotate the image): plotsubspace(Vline.idh, axes.lab = TRUE) Widget 5.1: Ellipsoid that circumscribes 95% of the expected \\(\\texttt{line}\\) effects as estimated in model m.sigma.3. This can be thought of as a scatter plot of the \\(\\texttt{line}\\) effects between each virus, if the \\(\\texttt{line}\\) effects could be directly measured. Because the covariances of the \\(\\texttt{line}\\) effects between each virus were set to zero, and the variance of the \\(\\texttt{line}\\) effects are quite similar for each virus, the ellipsoid is almost spherical. Although we have allowed the variance in \\(\\texttt{line}\\) effects to be different across the viruses, we have used the default specification for the residual structure (rcov=~units). \\(\\texttt{MCMCglmm}\\) augments the original data-frame with a column called \\(\\texttt{units}\\) which is a factor that has unique levels for each row. This is covered in greater depth when discussing multi-response models (Chapter 7) but for the single-response model used here we can simply think of \\(\\texttt{units}\\) as indexing residuals. Having a constant residual variance for each virus but allowing the between-line variance to vary is somewhat dangerous: if the residual variances did vary then this may be reflected to some degree in the estimates of the between-line variances. Allowing the residual variances to also vary overs viruses is straightforward: m.sigma.4 &lt;- MCMCglmm(cbind(infected, not_infected) ~ virus, random = ~idh(virus):line, rcov = ~idh(virus):units, data = sigma_small, family = &quot;binomial&quot;, prior = prior.sigma) summary(m.sigma.4) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 16739.59 ## ## G-structure: ~idh(virus):line ## ## post.mean l-95% CI u-95% CI eff.samp ## virusFrance.line 4.107 1.953 6.485 373.3 ## virusGreece.line 4.259 2.278 6.863 517.6 ## virusSpain.line 3.752 1.946 5.575 622.3 ## ## R-structure: ~idh(virus):units ## ## post.mean l-95% CI u-95% CI eff.samp ## virusFrance.units 1.760 1.193 2.411 580.1 ## virusGreece.units 2.515 1.709 3.311 855.9 ## virusSpain.units 2.654 1.927 3.358 999.0 ## ## Location effects: cbind(infected, not_infected) ~ virus ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.68250 -1.26166 -0.09061 1000 0.014 * ## virusGreece -1.68520 -2.48699 -0.75300 1000 &lt;0.001 *** ## virusSpain 0.59606 -0.17928 1.46033 1146 0.156 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As with the variance in \\(\\texttt{line}\\) effects, the residual variances don’t appear to be dramatically different across viruses. 5.1.2 \\(\\texttt{us}\\) Variance Structure The \\(\\texttt{idh}\\) structure allowed the variance in \\(\\texttt{line}\\) effects to be different for different viruses, but it assumed that the effect of a line for one virus is uncorrelated with the effect of that line in another virus. However, the strong correlation (0.84) in \\(\\texttt{line}\\) effects estimated from model m.sigma.2 suggests that this assumption is unlikely to be reasonable. We can relax this assumption by using the us function which estimates the fully parameterised matrix seen earlier: m.sigma.5 &lt;- MCMCglmm(cbind(infected, not_infected) ~ virus, random = ~us(virus):line, rcov = ~idh(virus):units, data = sigma_small, family = &quot;binomial&quot;, prior = prior.sigma) summary(m.sigma.5) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 16734.1 ## ## G-structure: ~us(virus):line ## ## post.mean l-95% CI u-95% CI eff.samp ## virusFrance:virusFrance.line 4.043 2.179 5.942 560.2 ## virusGreece:virusFrance.line 3.621 2.018 5.732 413.6 ## virusSpain:virusFrance.line 3.011 1.639 4.666 688.7 ## virusFrance:virusGreece.line 3.621 2.018 5.732 413.6 ## virusGreece:virusGreece.line 4.996 2.374 7.843 332.0 ## virusSpain:virusGreece.line 3.494 1.759 5.394 439.0 ## virusFrance:virusSpain.line 3.011 1.639 4.666 688.7 ## virusGreece:virusSpain.line 3.494 1.759 5.394 439.0 ## virusSpain:virusSpain.line 3.878 2.154 5.756 778.4 ## ## R-structure: ~idh(virus):units ## ## post.mean l-95% CI u-95% CI eff.samp ## virusFrance.units 1.767 1.155 2.416 715.0 ## virusGreece.units 2.491 1.811 3.323 643.8 ## virusSpain.units 2.627 1.981 3.302 799.8 ## ## Location effects: cbind(infected, not_infected) ~ virus ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.625816 -1.181138 -0.057590 1000.0 0.014 * ## virusGreece -1.839004 -2.341864 -1.271311 838.3 &lt;0.001 *** ## virusSpain 0.504100 0.004488 1.018975 1000.0 0.046 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The rows displayed for the G-structure correspond to the 9 elements of the \\(3\\times 3\\) covariance matrix (displayed column-wise). As before, we can arrange the posterior mean estimates in to a matrix Vline.us &lt;- matrix(colMeans(m.sigma.5$VCV)[1:9], 3, 3) colnames(Vline.us) &lt;- rownames(Vline.us) &lt;- c(&quot;France&quot;, &quot;Spain&quot;, &quot;Greece&quot;) Vline.us ## France Spain Greece ## France 4.042588 3.620713 3.011193 ## Spain 3.620713 4.995818 3.493868 ## Greece 3.011193 3.493868 3.878277 We can see that the covariances are large, and like the variances, relatively constant. If we visualise this distribution we can see it is quite different from that obtained from model \\(\\texttt{m.sigma.4}\\) (or model \\(\\texttt{m.sigma.3}\\)) where the covariances are assumed to be zero. plotsubspace(Vline.us, axes.lab = TRUE) Widget 5.2: Ellipsoid that circumscribes 95% of the expected \\(\\texttt{line}\\) effects as estimated in model \\(\\texttt{m.sigma.5}\\). This can be thought of as a scatter plot of the \\(\\texttt{line}\\) effects between virus, if the \\(\\texttt{line}\\) effects could be directly measured. The correlations of the \\(\\texttt{line}\\) effects between viruses are large, and the variance in \\(\\texttt{line}\\) effects are roughly equal in magnitude across viruses. Consequently the orientation of the major axis of the ellipsoid is tending towards \\(45^{o}\\) relative to the figure axes. We can use the function posterior.cor to convert the posterior samples of the (co)variance matrix into posterior samples of the correlation matrix. Doing so we can see that the estimated correlations are indeed large and relatively invariant. rline.us &lt;- matrix(colMeans(posterior.cor(m.sigma.5$VCV[, 1:9])), 3, 3) colnames(rline.us) &lt;- rownames(rline.us) &lt;- c(&quot;France&quot;, &quot;Spain&quot;, &quot;Greece&quot;) rline.us ## France Spain Greece ## France 1.0000000 0.8102356 0.7627693 ## Spain 0.8102356 1.0000000 0.7983696 ## Greece 0.7627693 0.7983696 1.0000000 In this particular example, it looks like the simpler two-parameter model implemented in model m.sigma.2 may do a good job of representing the covariance matrix: Vline.2 &lt;- matrix(mean(m.sigma.2$VCV[, &quot;line&quot;]), 3, 3) diag(Vline.2) &lt;- diag(Vline.2) + mean(m.sigma.2$VCV[, &quot;virus:line&quot;]) colnames(Vline.2) &lt;- rownames(Vline.2) &lt;- c(&quot;France&quot;, &quot;Spain&quot;, &quot;Greece&quot;) Vline.2 ## France Spain Greece ## France 4.268622 3.614878 3.614878 ## Spain 3.614878 4.268622 3.614878 ## Greece 3.614878 3.614878 4.268622 and we can see this if we visualise the two distributions simultaneously: plotsubspace(Vline.us, Vline.2, axes.lab = TRUE, shadeCA = FALSE) Widget 5.3: Ellipsoids that circumscribes 95% of the expected \\(\\texttt{line}\\) effects as estimated in model \\(\\texttt{m.sigma.5}\\) where all (co)variance parameters are estimated (red wireframe) and model \\(\\texttt{m.sigma.2}\\) where the variances across viruses, and the correlations between viruses, are assumed constant (solid blue). Note that when using a \\(\\texttt{us}\\) structure for the \\(\\texttt{line}\\) effects in model \\(\\texttt{m.sigma.6}\\) we retained the \\(\\texttt{idh}\\) structure for the residuals. Since the flies in each vial (indexed by \\(\\texttt{units}\\) since they correspond to a row of the data-frame) are all exposed to the same virus, the residual covariances between viruses cannot be estimated and so can be set to zero. In Chapter 7 we will look at multi-response models where this generally isn’t the case. 5.1.3 Other Variance Structures In some cases, additional restrictions need to be placed on a covariance matrix. In Section 3.6.3 we used the prior argument \\(\\texttt{fix}\\) to force the residual variance to be fixed at a specified value in a Bernoulli GLM. When a variance structure defines a (co)variance matrix, \\(\\texttt{fix}\\) specifies a diagonal element of the matrix that splits the matrix into four quadrants. For example, imagine we have the \\(5\\times 5\\) matrix specified in the \\(\\texttt{V}\\) element of the prior: \\[\\texttt{V}= \\left[ \\begin{array}{cc|ccc} 1&amp;0.5&amp;0.5&amp;0&amp;0\\\\ 0.5&amp;1&amp;0.5&amp;0&amp;0\\\\ \\hline 0.5&amp;0.5&amp;2&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;2&amp;1\\\\ 0&amp;0&amp;0&amp;1&amp;3\\\\ \\end{array} \\right]\\] If \\(\\texttt{fix}=3\\) then the matrix is split into four quardants with the lower right quadrant starting on the 3rd diagonal element. This quadrant will be fixed at the value specified in \\(\\texttt{V}\\) but all remaining (co)variance parameters will be estimated (if a \\(\\texttt{us}\\) structure is used). Sometimes constraints are required that can not be achieved by using \\(\\texttt{fix}\\) and cannot be expressed in terms of the (re)parameterisations discussed above. Table 5.1 provides a summary of available variance functions and \\(\\texttt{ante}\\) structures - which give the greatest flexibility - are covered in Section 6.5). Table 5.1: Different random effect specifications in \\(\\texttt{MCMCglmm}\\) (with equivalent \\(\\texttt{lmer}\\) syntax when possible) with fixed parameters in red and estimated parameters in black. \\(\\texttt{virus}\\) is a factor with three levels so the resulting covariance and correlation matrices are \\(3\\times 3\\). For the cors(virus):line structure, the prior specification had fix=2 which forces the last diagonal block (starting from position 2,2) to be a correlation matrix rather than a covariance matrix. \\(\\texttt{fix}\\) can also be specified for other structures but then it fixes the last diagonal block to what ever is specified in the \\(\\texttt{V}\\) element of the prior. \\(\\texttt{ante}\\) structures are not covered here but in Section 6.5 MCMCglmm lmer nparameters Variance Correlation line (1|line) 1 \\(\\left[\\begin{array}{ccc}V&amp;V&amp;V\\\\V&amp;V&amp;V\\\\V&amp;V&amp;V\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;{\\color{red}{1}}&amp;{\\color{red}{1}}\\\\{\\color{red}{1}}&amp;{\\color{red}{1}}&amp;{\\color{red}{1}}\\\\{\\color{red}{1}}&amp;{\\color{red}{1}}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) us(virus):line (virus|line) 6 \\(\\left[\\begin{array}{ccc}V_{1,1}&amp;C_{1,2}&amp;C_{1,3}\\\\C_{2,1}&amp;V_{2,2}&amp;C_{2,3}\\\\C_{3,1}&amp;C_{3,2}&amp;V_{3,3}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;r_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) virus:line (1|virus:line) 1 \\(\\left[\\begin{array}{ccc}V&amp;{\\color{red}{0}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;V&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{0}}&amp;V\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;{\\color{red}{0}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{1}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{0}}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) line+virus:line (1|line)+(1|virus:line) 2 \\(\\left[\\begin{array}{ccc}V_1+V_2&amp;V_1&amp;V_1\\\\V_1&amp;V_1+V_2&amp;V_1\\\\V_1&amp;V_1&amp;V_1+V_2\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r&amp;r\\\\r&amp;{\\color{red}{1}}&amp;r\\\\r&amp;r&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) idh(virus):line 3 \\(\\left[\\begin{array}{ccc}V_{1,1}&amp;{\\color{red}{0}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;V_{2,2}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{0}}&amp;V_{3,3}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;{\\color{red}{0}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{1}}&amp;{\\color{red}{0}}\\\\{\\color{red}{0}}&amp;{\\color{red}{0}}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) corg(virus):line 3 \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;r_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;r_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) corgh(virus):line 3 \\(\\left[\\begin{array}{ccc}{\\color{red}{V_{1,1}}}&amp;r_{1,2}{\\color{red}{\\sqrt{V_{1,1}V_{2,2}}}}&amp;r_{1,3}{\\color{red}{\\sqrt{V_{1,1}V_{3,3}}}}\\\\r_{2,1}{\\color{red}{\\sqrt{V_{2,2}V_{1,1}}}}&amp;{\\color{red}{V_{2,2}}}&amp;r_{2,3}{\\color{red}{\\sqrt{V_{2,2}V_{3,3}}}}\\\\r_{3,1}{\\color{red}{\\sqrt{V_{3,3}V_{1,1}}}}&amp;r_{3,2}{\\color{red}{\\sqrt{V_{3,3}V_{2,2}}}}&amp;{\\color{red}{V_{3,3}}}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;r_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) cors(virus):line 4 \\(\\left[\\begin{array}{ccc}V_{1,1}&amp;C_{1,2}&amp;C_{1,3}\\\\C_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\C_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) \\(\\left[\\begin{array}{ccc}{\\color{red}{1}}&amp;r_{1,2}&amp;r_{1,3}\\\\r_{2,1}&amp;{\\color{red}{1}}&amp;r_{2,3}\\\\r_{3,1}&amp;r_{3,2}&amp;{\\color{red}{1}}\\\\ \\end{array}\\right]\\) 5.2 Linking Functions In some models we would like to estimate covariances between sets of random effects (or random effects and residuals) in a way that is conceptually very similar to that discussed in Section 5.1 using variance structures. However, the way that we would like the (co)variances to be set up cannot be achieved by forming interactions. This happens when the predictors associated with the two or more sets of random effects are in different columns of the data frame, despite sharing the same set of levels, and we need some way to link them. Again, this is perhaps best illustrated through an example. 5.2.1 \\(\\texttt{str}\\): covariances between random terms My mum is a great mum. She’s also a super grandmother to my children, and I wonder whether these two things are correlated?16 Let’s say we could measure the well-being for many children over several generations, and importantly we have multiple children from the same mother so we can estimate \\(\\texttt{mother}\\) effects. As importantly, some of these mothers go onto to be grandmothers of their children’s children. If we also have measures of well-being for these children we can also estimate \\(\\texttt{grandmother}\\) effects. However, it’s not clear how we could specify a covariance between these effects in the same way that we did using variance structures. Our hypothetical data-frame may look like this: ## y mother grandmother ## 1 -2.612746 q u ## 2 -1.200558 h c ## 3 2.101922 r k ## 4 1.374406 b x ## 5 -4.339486 u i ## 6 -1.358210 o t with \\(\\texttt{y}\\) being the children’s well-being score and the letters being the names of the child’s mother and grandmother. I have ensured that the \\(\\texttt{mother}\\) and \\(\\texttt{grandmother}\\) columns have the same factor levels using the \\(\\texttt{levels}\\) argument of the function \\(\\texttt{as.factor}\\). It is not necessary that all mothers appear as grandmothers, or vice versa, but of course some mothers must appear as grandmothers if we wish to estimate the correlation in their effects on a child’s well-being. The linking function \\(\\texttt{str}\\) links two or more sets of random effects and assumes a fully unstructured covariance matrix for their distribution. It may help to visualise the random effects that the term str(mother+grandmother) is specifying: \\[\\begin{array}{c|rrrrc} &amp;{\\color{red}{\\texttt{a}}}&amp;{\\color{red}{\\texttt{b}}}&amp;{\\color{red}{\\texttt{c}}}&amp;{\\color{red}{\\texttt{d}}}&amp;\\dots\\\\ \\hline {\\color{blue}{\\texttt{mother}}}&amp;{\\color{blue}{\\texttt{mother}}}.{\\color{red}{\\texttt{a}}}&amp;{\\color{blue}{\\texttt{mother}}}.{\\color{red}{\\texttt{b}}}&amp;{\\color{blue}{\\texttt{mother}}}.{\\color{red}{\\texttt{c}}}&amp;{\\color{blue}{\\texttt{mother}}}.{\\color{red}{\\texttt{d}}}&amp;\\dots\\\\ {\\color{blue}{\\texttt{grandmother}}}&amp;{\\color{blue}{\\texttt{grandmother}}}.{\\color{red}{\\texttt{a}}}&amp;{\\color{blue}{\\texttt{grandmother}}}.{\\color{red}{\\texttt{b}}}&amp;{\\color{blue}{\\texttt{grandmother}}}.{\\color{red}{\\texttt{c}}}&amp;{\\color{blue}{\\texttt{grandmother}}}.{\\color{red}{\\texttt{d}}}&amp;\\dots\\\\ \\end{array}\\] We can seen that this is conceptually identical to our sigma virus example but rather than specifying the classifying variable inside \\(\\texttt{idh()}\\) or \\(\\texttt{us()}\\), as we could for \\(\\texttt{virus}\\), the classifying variable is actually the column names of the terms specified in \\(\\texttt{str()}\\) . prior.str = list(R = IW(1, 0.002), G = F(1, 1000)) m.str &lt;- MCMCglmm(y ~ 1, random = ~str(mother + grandmother), data = data.nana, prior = prior.str) The model output shows that a \\(2\\times 2\\) covariance matrix has been estimated for the \\(\\texttt{mother}\\)/\\(\\texttt{grandmother}\\) effects. summary(m.str) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 2081.763 ## ## G-structure: ~str(mother + grandmother) ## ## post.mean l-95% CI u-95% CI eff.samp ## mother.mother 2.2118 1.0797 3.8169 819.6 ## grandmother.mother 0.1213 -0.4134 0.7509 1093.0 ## mother.grandmother 0.1213 -0.4134 0.7509 1093.0 ## grandmother.grandmother 0.6925 0.2314 1.2873 1000.0 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 3.455 3.039 3.929 1000 ## ## Location effects: y ~ 1 ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 0.05767 -0.61336 0.70451 1000 0.85 When simulating the data I assumed the variance in mother effects was 2, the variance in grandmother effects was 1 and the covariance between them was 0.5 (giving a correlation of 0.35). The data set contained few (grand)mothers and so the estimates are uncertain, but the credible intervals overlap the true values. 5.2.2 \\(\\texttt{mm}\\): multi-membership models Using \\(\\texttt{str}\\) allowed us to link random effects associated with categorical predictors appearing in different columns of our data frame. The effects defined by different columns were allowed to have different variances (the variance in mother effects was different from the variance in grandmother effects) and the correlation was estimated. In some cases we would like to treat the effects defined by the different columns as the same effects when their levels agree. In the \\(\\texttt{str}\\) example we did not want to do this because being a mother is not the same thing as being a grandmother. However, imagine that each of our children has five friends: ## y friend.1 friend.2 friend.3 friend.4 friend.5 ## 1 -1.3268672 L G M B J ## 2 1.2606695 M T N G X ## 3 -2.6914660 E A Y B T ## 4 -0.1975649 Q A I L J ## 5 0.4686364 S B Q E X ## 6 -2.6165464 M D K O N Friend B appears as a friend to multiple children and we would like to model that friend’s effect on the focal child’s well-being irrespective of whether B appears as \\(\\texttt{friend.1}\\), \\(\\texttt{friend.2}\\) … or \\(\\texttt{friend.5}\\) as the numbering is arbitrary. The linking function \\(\\texttt{mm}\\) allows us to specify this. prior.mm = list(R = IW(1, 0.002), G = F(1, 1000)) m.mm &lt;- MCMCglmm(y ~ 1, random = ~mm(friend.1 + friend.2 + friend.3 + friend.4 + friend.5), data = data.friends, prior = prior.mm) summary(m.mm) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 1842.966 ## ## G-structure: ~mm(friend.1 + friend.2 + friend.3 + friend.4 + friend.5) ## ## post.mean l-95% CI u-95% CI ## friend.1+friend.2+friend.3+friend.4+friend.5 0.1395 0.05475 0.2269 ## eff.samp ## friend.1+friend.2+friend.3+friend.4+friend.5 796 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 2.235 1.917 2.495 729.4 ## ## Location effects: y ~ 1 ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.2512 -0.9245 0.4907 1000 0.484 The variance in \\(\\texttt{friend}\\) effects was set to 0.1 in the simulated data. While small compared to the simulated residual variance (2) the aggregate effect of all friends is five times this: 0.5. Note that if children varied in how many friends they had, the number of \\(\\texttt{friend.#}\\) columns can be set to the maximum and children with fewer friends than the maximum can have \\(\\texttt{NA}\\) in any redundant columns. For example, lets say the second child in our data-frame only had four friends - \\(\\texttt{friend}\\) G is not, in fact, a friend of this child. We would then just set this cell to \\(\\texttt{NA}\\): head(data.friends) ## y friend.1 friend.2 friend.3 friend.4 friend.5 ## 1 -1.3268672 L G M B J ## 2 1.2606695 M T N &lt;NA&gt; X ## 3 -2.6914660 E A Y B T ## 4 -0.1975649 Q A I L J ## 5 0.4686364 S B Q E X ## 6 -2.6165464 M D K O N When fitting the model, \\(\\texttt{MCMCglmm}\\) will issue the warning: missing values in random predictors but this can be ignored. 5.2.3 \\(\\texttt{covu}\\): covariances between random and residual terms Occasionally, we need to link a set of random effects with a set of residuals. This most commonly occurs in multi-response models (Chapter 7) when we have repeated measures for one response but only single measures for another response. I will leave the main discussion of this topic to then (Section 7.4). However, to give some idea of how this works, imagine that the final random term in the random formula defines the set of random effects \\([{\\bf u}^{(1)}, {\\bf u}^{(2)}, {\\bf u}^{(3)}]\\) (for example the \\(\\texttt{virus}\\) by \\(\\texttt{line}\\) effects discussed above). Imagine also that the first residual term in the rcov formula defines the residuals \\([{\\bf e}^{(1)}, {\\bf e}^{(2)}]\\). By specifying \\(\\texttt{covu=TRUE}\\) in the prior for the first residual term a \\(5\\times 5\\) covariance matrix is estimated rather than a \\(3\\times 3\\) covariance matrix for the random effects and a \\(2\\times 2\\) covariance matrix for the residuals. 5.2.4 \\(\\texttt{theta_scale}\\): scaled linear predictor Occasionally, we need to force a common relationship between two sets of (multiple) random effects. As with \\(\\texttt{covu}\\) models, this most commonly occurs in multi-response models (Chapter 7) and so will be covered in detail there (Section ??). For now, we can express the idea compactly by imagining two sets random effects \\(\\left\\{{\\bf u}^{(1)}, {\\bf u}^{(2)}\\right\\}\\) and \\(\\left\\{{\\bf u}^{(3)}, {\\bf u}^{(4)}\\right\\}\\). The \\(\\texttt{theta_scale}\\) argument allows the user to force the constraint \\(\\left\\{{\\bf u}^{(3)}, {\\bf u}^{(4)}\\right\\}\\)=\\(\\left\\{b{\\bf u}^{(1)}, b{\\bf u}^{(2)}\\right\\}\\) with \\(b\\) estimated. Fixed effects can also be added to the set to be scaled. for example, \\(\\left\\{{\\bf \\beta}^{(B)}, {\\bf u}^{(3)}, {\\bf u}^{(4)}\\right\\}\\)=\\(\\left\\{b{\\bf \\beta}^{(A)}, b{\\bf u}^{(1)}, b{\\bf u}^{(2)}\\right\\}\\). 5.3 Priors for Covariance Matrices When fitting a \\(k\\) dimensional (co)variance matrix, \\(\\texttt{V}\\) and \\(\\texttt{alpha.V}\\) must specify \\(k\\) dimensional (co)variance matrices and \\(\\texttt{alpha.mu}\\) must specify a vector of length \\(k\\). The degrees-of-freedom, \\(\\texttt{nu}\\), however, remains scalar. By using prior generators to specify the prior (Section 4.7) we hid this complexity. If we want to understand exactly what the prior generators are specifying in this context we cannot avoid this complexity. In addition, if we want to add arguments such as \\(\\texttt{fix}\\) or \\(\\texttt{covu}\\) to the prior, prior generators cannot be used. What follows is important, but also hard. For those short of time, patience or expertise, I would recommend using the prior generator F(2,1000) for random-effect covariance matrices. However, you may need to adjust the scale if the standard deviation of the data differs greatly from one (Section 4.6). 5.3.1 Marginal Priors for Variances The joint distribution of all \\(k(k-1)/2\\) elements of the covariance matrix is hard to characterise or visualise (Tokuda et al. 2025). We will start by exploring a simpler problem - given some prior specification for a \\(k\\) dimensional (co)variance matrix what is the marginal prior distribution of each variance? Luckily, the marginal distributions have the same form as they have in the scalar case but with some rescaling of parameters. We will designate the parameters of the marginal distribution using asterisks. The marginal distribution is characterised by the scalar parameters \\(\\texttt{V}^{\\ast}\\) and \\(\\texttt{nu}^{\\ast}\\), and if \\(F\\) priors are used, then \\(\\texttt{alpha.mu}^{\\ast}\\) and \\(\\texttt{alpha.V}^{\\ast}\\). The properties of these (marginal) distributions are fully covered in Sections 2.6 and 4.6, respectively. Before continuing, it should be pointed out that what follows in the next paragraph does not hold when an \\(\\texttt{idh}\\) structure is used. \\(\\texttt{idh}\\) structures set all correlations to zero and simply estimate a set of \\(k\\) variances, where \\(k\\) is the number of parameters specified by the \\(\\texttt{idh}\\) model. For example, in the sigma virus example we used idh(virus):line and \\(k=3\\) since there were three levels of \\(\\texttt{virus}\\). The prior specification still requires \\(k\\times k\\) (\\(3\\times 3\\)) matrices for \\(\\texttt{V}\\) and \\(\\texttt{alpha.V}\\) but the off-diagonal elements are ignored, and the prior for the \\(i^\\textrm{th}\\) variance is simply \\(\\texttt{V}^{\\ast}=\\texttt{V[i,i]}\\), \\(\\texttt{nu}^\\ast=\\texttt{nu}\\), \\(\\texttt{alpha.mu}^\\ast=\\texttt{alpha.mu[i]}\\) and \\(\\texttt{alpha.V}^\\ast=\\texttt{alpha.V[i,i]}\\)17. Forcing all variances to have the same degrees-of-freedom might be too restrictive, but at least with \\(\\texttt{idh}\\) structures this can be relaxed using an alternative parameterisation18. This luxury isn’t afforded to other variance structures for which covariances or correlation are estimated. For structures where a full covariance matrix is to be inferred (for example by using a \\(\\texttt{us}\\) structure) a useful result is that the prior for the \\(i^\\textrm{th}\\) variance has parameters \\(\\texttt{V}^{\\ast}=\\frac{\\texttt{nu}}{\\texttt{nu}^{\\ast}}\\texttt{V[i,i]}\\), \\(\\texttt{nu}^{\\ast}=\\texttt{nu}-k+1\\), \\(\\texttt{alpha.mu}^\\ast=\\texttt{alpha.mu[i]}\\) and \\(\\texttt{alpha.V}^\\ast=\\texttt{alpha.V[i,i]}\\). Consequently, we can choose a \\(\\texttt{V}\\) and \\(\\texttt{nu}\\) that allows us to specify one of the priors discussed in Sections 2.6 and 4.6 for a single variance. For example, in Section 2.6 we saw that an inverse-gamma prior with a shape and scale of 0.001 is equivalent to the scalar inverse-Wishart with \\(\\texttt{V}^{\\ast}=1\\) and \\(\\texttt{nu}^{\\ast}=0.002\\). Consequently, we can place this prior on the \\(i^\\textrm{th}\\) variance by setting \\(\\texttt{nu}=k-1+0.002\\) such that \\(\\texttt{nu}^{\\ast}=0.002\\). Having \\(\\texttt{V[i,i]}=\\texttt{nu}^{\\ast}/\\texttt{nu}=0.002/(k-1+0.002)\\) then results in \\(\\texttt{V}^{\\ast}=1\\). An improper prior that is flat for the variance requires \\(\\texttt{V}^{\\ast}=0\\) and \\(\\texttt{nu}^{\\ast}=-2\\) which can be achieved by setting \\(\\texttt{nu}=k-3\\) and \\(\\texttt{V[i,i]}=0\\). In Section 4.6 we noted that scaled \\(F\\) priors for variances (scaled folded-\\(t\\) priors for standard deviations) can have better properties than the inverse-Wishart. As in Section 4.6 we can set \\(\\texttt{alpha.mu}\\) to be a vector of zeros and work with the scaled central \\(F\\) and scaled half-\\(t\\). The marginal distribution for the \\(i^\\textrm{th}\\) variance then follows a \\(F_{1, \\texttt{nu}^{\\ast}}\\) distribution with scale equal to \\(\\texttt{V}^{\\ast}*\\texttt{alpha.V[i,i]}\\). We can therefore chose \\(\\texttt{V}\\), \\(\\texttt{nu}\\) and \\(\\texttt{alpha.V}\\) to specify one of the priors discussed in Section 2.6 for a single variance or standard deviation. For example, by setting \\(\\texttt{nu}=k\\) we have \\(\\texttt{nu}^{\\ast}=1\\) generating a scaled \\(F_{1,1}\\) prior for the variance (or half-Cauchy for the standard deviation). We noted in Section 4.6 that the prior specification in \\(\\texttt{MCMCglmm}\\) is redundant and we can set the scale by setting \\(\\texttt{V}\\) to one and controlling the scale through \\(\\texttt{alpha.V}\\). This can be achieved in the multivariate case by setting \\(\\texttt{V[i,i]}=1/k\\) such that \\(\\texttt{V}^{\\ast}=1\\) and then altering \\(\\texttt{alpha.V[i,i]}\\) as before. The rules for rescaling can be hard to remember and fiddly to implement. In order to simplify the process of prior specification, priors can also be specified using the prior generator functions detailed in Section 4.7. When the random term defines a (co)variance matrix, these prior generators specify a prior for which the marginal prior distributions for the variance have the specified distribution. In all cases the off-diagonal elements of \\(\\texttt{V}\\) and \\(\\texttt{alpha.V}\\) are set to zero. If we want to see the prior specification generated by a prior generator we can use the function resolve_prior which also requires the dimension of the (co)variance matrix (\\(\\texttt{k}\\)) and the type of variance structure (\\(\\texttt{vtype}\\)). For example, to have scaled-\\(F_{1,1}\\) marginal priors for the variances with a scale of 1,000, the specification for a \\(3\\times 3\\) unstructured (co)variance matrix is: resolve_prior(F(1, 1000), k = 3, vtype = &quot;us&quot;) ## $V ## [,1] [,2] [,3] ## [1,] 0.3333333 0.0000000 0.0000000 ## [2,] 0.0000000 0.3333333 0.0000000 ## [3,] 0.0000000 0.0000000 0.3333333 ## ## $nu ## [1] 3 ## ## $alpha.mu ## [1] 0 0 0 ## ## $alpha.V ## [,1] [,2] [,3] ## [1,] 1000 0 0 ## [2,] 0 1000 0 ## [3,] 0 0 1000 5.3.2 Marginal Priors for Covariances and Correlations Above, we looked at the prior specifications that resulted in marginal priors for the variances which are well understood. What do the marginal priors for the covariances and correlations look like under these prior specifications? While the marginal distribution for a covariance does not have an easy form, the marginal distribution of a correlations for an inverse-Wishart with diagonal \\(\\texttt{V}\\) is a beta distribution transformed to the interval \\([-1, 1]\\) (Box and Tiao 1973; Barnard, McCulloch, and Meng 2000). The shape and scale of the beta are equal to \\((\\texttt{nu}-k+1)/2\\) which is the marginal degrees-of-freedom \\(\\texttt{nu}^{\\ast}\\) used in the prior generator function19. The beta is flat when \\(\\texttt{nu}^{\\ast}=2\\) since the shape=scale=1. As \\(\\texttt{nu}^{\\ast}\\) becomes smaller the distribution becomes more and more U-shaped with a lot of prior density close to -1 and 1. As \\(\\texttt{nu}^{\\ast}\\) increases beyond \\(2\\) this distribution becomes more constrained around zero (Figure 5.1). Since the working parameters in parameter expansion cancel for the correlation, the distribution of the correlations is the same as that under the inverse-Wishart (although the distribution of the covariances will differ). Figure 5.1: Prior marginal density for a correlation when \\(\\texttt{V}\\) is diagonal. The density is a beta distribution over the interval \\([-1,1]\\) with shape=scale=\\(\\texttt{nu}^{\\ast}/2\\). \\(\\texttt{nu}^{st}/2\\) is the marginal degrees of freedom for a variance: \\(\\texttt{nu}-k+1\\). With simple random effects models, where only variances need to be estimated, I usually use a scaled \\(F_{1,1}\\) prior with large scale. In the past, I also specified priors for covariance matrices that had a marginal \\(F_{1,1}\\) prior. However, since this implies \\(\\texttt{nu}^{\\ast}\\)=1 (and \\(\\texttt{nu}=k\\)) this may push the posterior correlation away from zero, especially if the data have support for correlations that are large in magnitude. Using an \\(F_{1,2}\\) marginal prior for the variance is flat for the correlation but it does mean we’ve switched from a half-Cauchy (half-\\(t_1\\)) to a half-\\(t_2\\) for the standard deviation. However, with a large scale (\\(\\sqrt{1000}=\\) 31.6 for the standard deviation), this is unlikely to have a large effect (Figure 5.2). Figure 5.2: Marginal prior densities for the standard deviation using the prior generator functions \\(\\texttt{F(1,1000)}\\), \\(\\texttt{F(2,1000)}\\) and \\(\\texttt{F(3,1000)}\\). \\(\\texttt{F(2,1000)}\\) results in a flat prior for the correlation. Note that the legend refers to these in terms of the marginal density for the standard deviations (ie. half-\\(t\\)) and that the half-\\(t_1\\) is the Cauchy. Also, while the x-axis runs from zero to 31, the posterior distributions for the standard deviation in \\(\\texttt{line}\\) effects lie in the range 1.2-3.5. By doing this, the figure is directly comparable to Figure 4.7. Let’s rerun model m.sigma.5 with an \\(F_{1,1}\\) prior of the same scale to see what happens: prior.sigma.6 = list(R = IW(1, 0.002), G = F(1, 1000)) m.sigma.6 &lt;- MCMCglmm(cbind(infected, not_infected) ~ virus, random = ~us(virus):line, rcov = ~idh(virus):units, data = sigma_small, family = &quot;binomial&quot;, prior = prior.sigma.6) The variance in \\(\\texttt{line}\\) effects over viruses remain largely unchanged Figure 5.3: MCMC traces for the variance in \\(\\texttt{line}\\) effects for the three viruses when the marginal prior for the variances are scaled (1000) \\(F_{1,2}\\) (black - model \\(\\texttt{m.sigma.5}\\)) or \\(F_{1,1}\\) (red- model \\(\\texttt{m.sigma.6}\\)) distributions. The \\(F_{1,2}\\) prior is flat for the correlation in \\(\\texttt{line}\\) effects across viruses. The correlations in \\(\\texttt{line}\\) effects between viruses are nudged a little further from zero under the new prior but the effect is rather subtle (there’s also some auto-correlation in chains so they should be run for longer than the number of default iterations). Figure 5.4: MCMC traces for the correlation in \\(\\texttt{line}\\) effects between the three viruses when the marginal prior for the variances are scaled (1000) $\\(F_{1,2}\\) (black - model \\(\\texttt{m.sigma.5}\\)) or \\(F_{1,1}\\) (red- model \\(\\texttt{m.sigma.6}\\)) distributions. The \\(F_{1,2}\\) prior is flat for the correlation in \\(\\texttt{line}\\) effects across viruses. In contrast, altering the degrees-of-freedom in the inverse-Wishart (ie. not using parameter-expanded \\(F\\)-priors) to achieve a better prior distribution for the correlation can have a much stronger influence on the variances and standard deviations 5.5. Figure 5.5: Marginal prior densities for the standard deviation using the prior generator functions \\(\\texttt{IW(1,0.002)}\\), \\(\\texttt{IW(1,1)}\\) and \\(\\texttt{IW(1,2)}\\), \\(\\texttt{IW(1,3)}\\) and \\(\\texttt{IW(1,10)}\\). \\(\\texttt{IW(1,2)}\\) results in a flat prior for the correlation. Note that the legend refers to these in terms of the marginal density for the variance (ie. inverse-gamma). Also, while the x-axis runs from zero to 31, the posterior distributions for the standard deviation in \\(\\texttt{line}\\) effects lie in the range 1.2-3.5. By doing this, the figure is directly comparable to Figure 4.7. 5.3.3 Full joint prior We have concentrated on the marginal priors, since these are relatively easy to understand and visualise. However, by concentrating on the marginal distributions we may miss aspects of the joint prior distribution that is important. Characterising the joint distribution is hard, particular under parameter expansion where the full probability density function probably doesn’t have a closed form solution. However, we can simulate from these priors using the function rprior and use the visualisation tools in the library \\(\\texttt{VisCov}\\) (Tokuda et al. 2025). prior.line &lt;- resolve_prior(prior.sigma$G, k = 3, vtype = &quot;us&quot;) VCV &lt;- rprior(2000, prior = prior.line) # simulate from prior for line (co)variance matrix mat &lt;- apply(VCV, 1, matrix, nrow = 3, simplify = FALSE) # coerce to list CovPlotData = VisCov(&quot;User defined distribution&quot;, param = list(prob = 0.5, mat = mat)) Figure 5.6: Visualisation of the prior distribution used for the \\(3\\times 3\\) (co)variance matrix of \\(\\texttt{line}\\) effects from model \\(\\texttt{m.sigma.5}\\) where a parameter expanded prior was used with marginal \\(F{1,2}\\) priors on the variances with a scale of a 1,000 (half-\\(t_2\\) prior on the standard deviation with scale \\(\\sqrt{1000}\\approx 32\\)). The prior is flat for the correlations. See Tokuda et al. (2025) for discussion of \\(\\texttt{VisCov}\\) plots. We can also visualise the inverse-Wishart prior with \\(\\texttt{nu}=4\\) which is flat for the correlations since \\(\\texttt{nu}^{\\ast}=2\\). We can see the dependencies in the posterior are stronger, particularly between the correlations and the standard deviations (Figure 5.7). prior.IW &lt;- resolve_prior(IW(1, 2), k = 3, vtype = &quot;us&quot;) # inverse-Wishart prior with marginal inverse-gamma(1,1) prior on variances VCV &lt;- rprior(2000, prior = prior.IW) # simulate from prior for line (co)variance matrix mat &lt;- apply(VCV, 1, matrix, nrow = 3, simplify = FALSE) # coerce to list CovPlotData = VisCov(&quot;User defined distribution&quot;, param = list(prob = 0.5, mat = mat)) Figure 5.7: Visualisation of the prior distribution for a \\(3\\times 3\\) (co)variance matrix using an inverse-Wishart distribution with \\(\\texttt{n}=4\\) and \\(\\texttt{V}={\\bf I}\\frac{1}{2}\\). This prior is flat for the correlations (\\(\\texttt{nu}^{\\ast}=2\\)) but the marginal prior for the variances is inverse-gamma with shape and scale equal to one. See Tokuda et al. (2025) for discussion of \\(\\texttt{VisCov}\\) plots. References "],["cont-int.html", "6 Continuous Random Interactions 6.1 Random Regression 6.2 Heterogeneous (Residual) Variances 6.3 Autocorrelation 6.4 Variance stabilisation 6.5 Antedependence and Autoregressive Models 6.6 User-defined Design Matrices 6.7 Splines 6.8 Penalised Signal Regression", " 6 Continuous Random Interactions In Chapter 5 we saw how we could define a set of random effects by specifying a linear model within a variance function and then interacting this term with a categorical predictor. In that Chapter, we used the random term ~us(virus):line in order to model \\(\\texttt{virus}\\) by \\(\\texttt{line}\\) interactions. Since the variance function model involved a categorical predictor (virus), and the intercept is omitted by default, it was not really necessary to fully engage with the idea that the variance function is taking a model formula - most users familiar with covariance matrices will be happy that ~us(virus):line specifies the covariance matrix: \\[{\\bf V}_{{\\color{red}{\\texttt{line}}}}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{\\color{blue}{\\texttt{France}}}&amp;\\sigma_{\\color{blue}{\\texttt{France}, \\texttt{Spain}}}&amp;\\sigma_{\\color{blue}{\\texttt{France}, \\texttt{Greece}}}\\\\ \\sigma_{\\color{blue}{\\texttt{France}, \\texttt{Spain}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Spain}}}&amp;\\sigma_{\\color{blue}{\\texttt{Spain}, \\texttt{Greece}}}\\\\ \\sigma_{\\color{blue}{\\texttt{France}, \\texttt{Greece}}}&amp;\\sigma_{\\color{blue}{\\texttt{Spain}, \\texttt{Greece}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Greece}}}\\\\ \\end{array} \\right]\\] However, we are also free to use continuous covariates in the variance function model, or even a mixture of continuous and categorical predictors. Although the resulting covariance matrix is interpreted in the same way, it can be less intuitive. 6.1 Random Regression As an example, we’ll use a longitudinal data set on chicken growth (See Figure 6.1). data(ChickWeight) head(ChickWeight) ## Grouped Data: weight ~ Time | Chick ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 The data consist of body weights (weight) for 50 chicks (Chick) measured up to 12 times over a 3 week period. The variable Time is the number of days since hatching, and Diet is a four level factor indicating the type of protein diet the chicks received. The data are plotted in Figure 6.1. Figure 6.1: Weight data of 50 chicks from hatching until three weeks old. Chicks in the first two rows were fed on $ exttt{Diet 1}$, and chicks in rows 3 to 5 were fed on $ exttt{Diet 2}$ to $ exttt{4}$, respectively. Growth curves tend to be sigmoidal and so one of the non-linear growth curves such as the Gompertz or logistic may fit the data well. However, these can be tricky to use and an alternative is to try and capture the form of the curve using polynomials. We’ll start with a quadratic function at the population level and and fit chick effects as random20. prior.weight &lt;- list(R = IW(1, 0.002), G = F(2, 10000)) mweight.1 &lt;- MCMCglmm(weight ~ Time + I(Time^2) + Diet:Time, random = ~Chick, data = ChickWeight, pr = TRUE, prior = prior.weight) I can hear the rule-of-thumbers squeal - I’ve fitted an interaction with \\(\\texttt{Diet}\\), but no ‘main’ effect. Let’s think about the fixed effect part of the model. For the reference \\(\\texttt{Diet}\\) (\\(\\texttt{Diet 1}\\)) the linear model is \\[E[\\texttt{weight} | \\texttt{Diet}=1] = \\beta_0+\\beta_1\\texttt{Time} + \\beta_{2}\\texttt{Time}^{2}\\] At hatching (\\(\\texttt{Time}\\)=0) the expected weight of a chick is \\(\\beta_0\\). The slope, \\(\\beta_1\\), is a rate because it is in units of grams per day - it’s a growth rate. The quadratic term, \\(\\beta_2\\), is in units of grams per day\\(^{2}\\) and represents acceleration, or more intuitively, the rate of change in the growth rate in units [grams per day] per day. At hatching, the growth rate is \\(\\beta_1\\) but at other values of \\(\\texttt{Time}\\) the growth rate is \\(\\beta_1+\\beta_{2}\\texttt{Time}\\)21. For a non-reference \\(\\texttt{Diet}\\) (for example, \\(\\texttt{Diet 2}\\)) the model is \\[E[\\texttt{weight} | \\texttt{Diet}=\\texttt{2}] = \\beta_0+\\beta_1\\texttt{Time} + \\beta_{2}\\texttt{Time}^{2}+\\beta_{3}\\texttt{Time}\\] where \\(\\beta_{3}\\) is the interaction effect, which will be labelled as \\(\\texttt{Diet2:Time}\\) in the model output. The equation can be more easily understood with a little rearrangement: \\[E[\\texttt{weight} | \\texttt{Diet}=\\texttt{2}] = \\beta_0+\\left(\\beta_1+\\beta_3\\right)\\texttt{Time} + \\beta_{2}\\texttt{Time}^{2}\\] At hatching the model predicts chicks on different diets to have the same expected weight (\\(\\beta_0\\)). This is why I intentionally dropped the ‘main’ effect: at hatching the diet is yet to have an effect on the chick’s weight and since chicks were randomly assigned to a treatment they should be equivalent at \\(\\texttt{Time}\\) 0. We could test this if we wished, but I’m prepared to trust the experimenters. After hatching, however, we may expect the diet to cause the weights of the chicks to diverge because it alters their growth rate. The difference in growth rate between \\(\\texttt{Diet 2}\\) and \\(\\texttt{Diet 1}\\) is \\(\\beta_{3}\\). We’ve assumed that the effect of \\(\\texttt{Time}^{2}\\) is constant over diets. This means that if the growth rates differ between the treatment groups, that difference remains constant: the growth rate on \\(\\texttt{Diet 2}\\) at some value of \\(\\texttt{Time}\\) is \\(\\beta_1+\\beta_3+\\beta_{2}\\texttt{Time}\\), and consistently differs from \\(\\texttt{Diet 1}\\) by an amount \\(\\beta_\\texttt{3}\\) at all values of \\(\\texttt{Time}\\). Let’s have a look at the model summary: summary(mweight.1) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 5398.374 ## ## G-structure: ~Chick ## ## post.mean l-95% CI u-95% CI eff.samp ## Chick 575.9 335.2 813.1 1000 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 610.5 529.4 679.2 1000 ## ## Location effects: weight ~ Time + I(Time^2) + Diet:Time ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 38.09190 29.37737 46.23461 1000 &lt;0.001 *** ## Time 3.66880 2.38609 4.84156 1000 &lt;0.001 *** ## I(Time^2) 0.14632 0.09254 0.19570 1100 &lt;0.001 *** ## Time:Diet2 1.80958 0.99523 2.48771 1000 &lt;0.001 *** ## Time:Diet3 4.46028 3.79161 5.28554 1000 &lt;0.001 *** ## Time:Diet4 2.94754 2.20819 3.70829 1140 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hopefully the output is now familiar. At hatching the chicks weigh approximately 38 grams. On the reference diet the chicks are growing at a rate of 3.7 grams per day at the beginning (\\(\\texttt{Time}=0\\)) but as they get older they start to grow faster (the coefficient associated with \\(\\texttt{Time}^2\\) is positive). All diets seem to increase growth rate compared to \\(\\texttt{Diet 1}\\). While the explanation of how to interpret the fixed effects was perhaps laboured, it should be remembered that fixed and random effects are completely equivalent in how they enter the linear predictor (Chapter 4). If the explanation of the fixed effect part of the model is understood, then nothing new needs to be learned when we try to understand the chick effects. If we consider \\(\\texttt{Chick}\\) \\(i\\) fed on the reference diet, its expected weight is: \\[E[\\texttt{weight} | \\texttt{Diet}=1, \\texttt{Chick}=i] = \\beta_0+\\beta_1\\texttt{Time} + \\beta_{2}\\texttt{Time}^{2}+u^{(0)}_i\\] where \\(u^{(0)}_i\\) is the random chick effect. The model assumes that chicks deviate consistently in their weight across time, but chicks do not deviate in their growth rate. Since we saved the random chick effects (pr=TRUE) we can obtain the predictions for each chick by specifying marginal=NULL (Section 4.2): pweight.1 &lt;- predict(mweight.1, marginal = NULL) and we can add them to the \\(\\texttt{xyplot}\\) plot as black lines (Figure 6.2). Figure 6.2: Weights of each chick as a function of age (points). The mean growth curve was fitted as a quadratic function of \\(\\texttt{Time}\\) with random chick intercepts (model mweight.1). The predicted weights are shown as lines. The predictions don’t look too bad (but see Section ??), although you will notice that for some chicks (e.g. 7, 13 and 35) the slope of the predicted growth seems either too shallow, or too steep. As we did with the \\(\\texttt{Diet}\\) by \\(\\texttt{Time}\\) interaction, we could also allow our chicks to deviate in their growth rates and fit the model: \\[E[\\texttt{weight} | \\texttt{Diet}=1, \\texttt{Chick}=i] = \\beta_0+(\\beta_1+u^{(1)}_i)\\texttt{Time} + \\beta_{2}\\texttt{Time}^{2}+u^{(0)}_i \\label{rr-eq} \\tag{6.1}\\] where \\(u^{(1)}_i\\) is how much \\(\\texttt{Chick}\\) \\(i\\)’s growth rate differs from the average (\\(\\beta_1\\) for \\(\\texttt{Diet 1}\\)). This type of model is often referred to as a random-regression model. To fit such a model we can replace Chick in the random formula with the term us(1+Time):Chick. The linear model inside the variance function has two parameters, an intercept (1 - which he have to specify explicitly22) and a regression slope associated with Time. Consequently, the interaction with \\(\\texttt{Chick}\\) defines the set of coefficients: \\[\\begin{array}{c|rrrrrc} &amp;{\\color{red}{\\texttt{Chick1}}}&amp;{\\color{red}{\\texttt{Chick2}}}&amp;{\\color{red}{\\texttt{Chick3}}}&amp;\\dots\\\\ \\hline {\\color{blue}{\\texttt{(Intercept)}}}&amp;{\\color{blue}{\\texttt{(Intercept)}}}.{\\color{red}{\\texttt{Chick1}}}&amp;{\\color{blue}{\\texttt{(Intercept)}}}.{\\color{red}{\\texttt{Chick2}}}&amp;{\\color{blue}{\\texttt{(Intercept)}}}.{\\color{red}{\\texttt{Chick3}}}&amp;\\dots\\\\ {\\color{blue}{\\texttt{Time}}}&amp;{\\color{blue}{\\texttt{Time}}}.{\\color{red}{\\texttt{Chick1}}}&amp;{\\color{blue}{\\texttt{Time}}}.{\\color{red}{\\texttt{Chick2}}}&amp;{\\color{blue}{\\texttt{Time}}}.{\\color{red}{\\texttt{Chick3}}}&amp;\\dots\\\\ \\end{array}\\] Each chick now has an intercept and a slope, as in Equation (6.1), and because we have used the us variance structure we are estimating the \\(2\\times2\\) matrix: \\[{\\bf V}_{{\\color{red}{\\texttt{Chick}}}}= \\left[ \\begin{array}{cc} \\sigma^{2}_{\\color{blue}{\\texttt{(Intercept)}}}&amp;\\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}}}\\\\ \\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Time}}}\\\\ \\end{array} \\right]\\] \\(\\sigma^{2}_{\\color{blue}{\\texttt{(Intercept)}}}\\) is the amount of variation in intercepts between chicks, and \\(\\sigma^{2}_{\\color{blue}{\\texttt{Time}}}\\) is the amount of variation in the regression slopes (the growth rates) between chicks. Note that our first model could also have been fitted using us(1):Chick (rather than just Chick) which may make it clearer why the \\(\\texttt{Chick}\\) effects in that model could be called intercepts. Let’s fit the random regression model: mweight.2 &lt;- MCMCglmm(weight ~ Time + I(Time^2) + Diet:Time, random = ~us(1 + Time):Chick, data = ChickWeight, pr = TRUE, prior = prior.weight) summary(mweight.2) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 4544.498 ## ## G-structure: ~us(1 + Time):Chick ## ## post.mean l-95% CI u-95% CI eff.samp ## (Intercept):(Intercept).Chick 146.00 77.176 219.94 1000 ## Time:(Intercept).Chick -37.70 -55.904 -22.19 1000 ## (Intercept):Time.Chick -37.70 -55.904 -22.19 1000 ## Time:Time.Chick 11.67 7.511 16.88 1000 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 133.1 115.5 149.6 984.1 ## ## Location effects: weight ~ Time + I(Time^2) + Diet:Time ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 37.8003 33.8645 41.7408 1000 &lt;0.001 *** ## Time 4.5223 3.3090 5.6356 1000 &lt;0.001 *** ## I(Time^2) 0.1294 0.1074 0.1547 1000 &lt;0.001 *** ## Time:Diet2 1.2520 -0.1042 2.5516 1129 0.070 . ## Time:Diet3 1.8390 0.3580 3.2278 1000 0.016 * ## Time:Diet4 2.8644 1.4744 4.2199 1000 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There seems to be substantial variation in growth rate between chicks after controlling for the diet treatment. The posterior mean of the slope variance is 11.67 which in terms of a standard deviation is 3.42 - larger than the effects of \\(\\texttt{Diet}\\). Again, we can get the predicted weights from this model pweight.2 &lt;- predict(mweight.2, marginal = NULL) and we can see that the fit is better (See Figure 6.3). Figure 6.3: Weights of each chick as a function of age (points). Predictions are shown for models where the mean growth curve was fitted as a quadratic function of \\(\\texttt{Time}\\) with random chick intercepts (black lines - model mweight.1) or random chick intercepts and slopes (red lines - model mweight.2). In theory we could fit higher degree random regressions (data and prior permitting). For example, fitting random=~us(1+Time+I(Time^2)):Chick would allow the quadratic term for each Chick to deviate from the population mean and we would be estimating the \\(3\\times3\\) covariance matrix: \\[{\\bf V}_{{\\color{red}{\\texttt{Chick}}}}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{\\color{blue}{\\texttt{(Intercept)}}}&amp;\\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}}}&amp;\\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}^{2}}}\\\\ \\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Time}}}&amp;\\sigma_{\\color{blue}{\\texttt{Time}, \\texttt{Time}^{2}}}\\\\ \\sigma_{\\color{blue}{\\texttt{(Intercept)}, \\texttt{Time}^2}}&amp;\\sigma_{\\color{blue}{\\texttt{Time}, \\texttt{Time}^{2}}}&amp;\\sigma^{2}_{\\color{blue}{\\texttt{Time}^2}}\\\\ \\end{array} \\right]\\] 6.2 Heterogeneous (Residual) Variances One under-appreciated facet of random regression models is that when a polynomial of degree \\(n\\) is fitted, the variance in the response is predicted to change as a polynomial of degree \\(n+1\\). In our random-intercept slope model (mweight.2) the variance in \\(\\texttt{weight}\\) due to the random terms is \\[ \\begin{array}{rl} \\textrm{Var}(\\texttt{weight}) =&amp;\\sigma^2_{\\texttt{(Intercept)}}+2\\texttt{Time}\\sigma_{\\texttt{(Intercept)}, \\texttt{Time}}+\\texttt{Time}^2\\sigma^2_{\\texttt{Time}}\\\\ \\end{array} \\label{Vrr-eq} \\tag{6.2} \\] Since we specified a 1st degree polynomial of of \\(\\texttt{Time}\\) at the level of \\(\\texttt{Chick}\\) (an intercept and slope) the variance is a 2nd degree polynomial of \\(\\texttt{Time}\\) (it has a quadratic term, with coefficient \\(\\sigma^2_{\\texttt{Time}}\\)). The function buildV returns the predicted variance of observations (and the covariances if diag=FALSE) after conditioning on the fixed effect model. As with the predict function, there is also the option to condition on some of the random effects (i.e. treat them as constants) by excluding them from the argument passed to marginal (see Section 4.2). However, the default is not to condition on them and so we can get the total predicted variance for each observation due to chick effects and the residuals. The residual variance in our model is constant, so the total variance is obtained by simply adding \\(\\sigma^2_{\\textrm{units}}\\) to Equation (6.2). predv.weight.2 &lt;- buildV(mweight.2) The predicted variance goes up dramatically with \\(\\texttt{Time}\\) (red points in Figure 6.4). In contrast, model (mweight.1), where chicks do not vary in their growth rate, assumes that the variance is constant with \\(\\texttt{Time}\\) (black points in Figure 6.4). Figure 6.4: Predicted variance in weight due to random effects from model mweight.2. The two models are dramatically different in how they predict the variance changes with \\(\\texttt{Time}\\). This important difference isn’t really apparent from comparing their predictions with the data (Figure 6.3. However, if we obtain the 95% prediction intervals for the two models (the interval within which we expect to see 95% of observations) pred.weight.1 &lt;- predict(mweight.1, interval = &quot;prediction&quot;) pred.weight.2 &lt;- predict(mweight.2, interval = &quot;prediction&quot;) and overlay these on the data for all chicks plotted simultaneously, we see that our first model - that assumes constant variance - has serious problems (black lines in Figure 6.5). Initially, it predicts far more variability than observed, but after around \\(\\texttt{Time}\\) 12 it predicts too little variability. The random slope model (red lines) does much better, although it still predicts too much variability initially, particularly at hatching. Figure 6.5: Chick weights plotted against time with predicted mean weight (solid lines) and 95% predictions intervals (dashed line). The black line is for a random intercept model (mweight.1) that implicitly assumes the variance is constant over \\(\\texttt{Time}\\). The red line is for a random intercept-slope model (mweight.2) that allows the variance (across chicks) to change as a quadratic function of \\(\\texttt{Time}\\). At this point we may be fairly chuffed with our model - not only does it seem to predict the expected response well (Figure 6.3), but in terms of the variance in the response, it’s a massive improvement over the intercept-only model (Figure 6.5). Indeed, the \\(\\texttt{ChickWeight}\\) data has become something of a classic for illustrating the power of random regression models. However, a number of issues remain with the model and it’s interpretation. We will address these issues in the remainder of this section and the next. The only way that our random intercept-slope model (model mweight.2) can accommodate an increase in the variance over \\(\\texttt{Time}\\) is through the parameters of the (co)variance matrix of chick effects (Equation (6.2)). Consequently, a non-zero \\(\\sigma^2_\\texttt{Time}\\) may indicate that chick’s vary in their growth rate but it may also just be capturing increases in variance due to some other mechanism. Perhaps the evidence for a non-zero \\(\\sigma^2_\\texttt{Time}\\) is coming solely from how the variance in weight changes as a function of \\(\\texttt{Time}\\) not from assessing whether weights from the same chick are correlated? To see this, let’s take the \\(\\texttt{ChickWeight}\\) data frame and permute \\(\\texttt{Chick}\\) labels within each \\(\\texttt{Time}\\) category. ChickWeightPerm &lt;- ChickWeight ChickWeightPerm$Chick &lt;- unlist(with(ChickWeight, tapply(Chick, Time, sample))) The \\(\\texttt{Chick}\\) labels in this new data frame do not link multiple observations from the same chick, but let’s fit the same random regression model that we have fitted previously: mweight.perm &lt;- MCMCglmm(weight ~ Time + I(Time^2) + Diet:Time, random = ~us(1 + Time):Chick, data = ChickWeightPerm, pr = TRUE, prior = prior.weight) Given the \\(\\texttt{Chick}\\) labels are essentially meaningless, we might expect a model fitted to these permuted data would find no evidence for variation in chick intercepts or slopes. However, if we look at the posterior distributions for the variance in intercepts and slopes we see that there is very strong evidence that chicks vary in the growth rate (Figure 6.6). Figure 6.6: Posterior distributions for the variances in chick intercepts and slopes. The model was fitted to a data set where chick labels at each time-point were randomised. This is quite disconcerting - our model tells us that chicks systematically differ in their growth rate, yet we’ve jumbled the chick labels up in a way that weights from the same chick must be completely uncorrelated. How do we know that something similar isn’t happening with our original model? One option would be to allow the residual variance to change as a function of \\(\\texttt{Time}\\) and so we can be sure that the information for estimating the random effect part of the model is really coming from the repeated nature of the observations. Equation (6.2) gave use the expected variance due to \\(\\texttt{Chick}\\) effects as a function of \\(\\texttt{Time}\\). We can use this idea to allow the residual variance to increase with the square of some covariate. Although I won’t discuss it here, \\(\\texttt{covu}\\) models (Section 7.4) could be used to allow the residual variance to change as a polynomial function of the predictor with arbitrary degree. Lets add the term us(Time):units to the random part of our model: mweight.3 &lt;- MCMCglmm(weight ~ Time + I(Time^2) + Diet:Time, random = ~us(1 + Time):Chick + us(Time):units, data = ChickWeight, pr = TRUE, prior = prior.weight, longer = 10) summary(mweight.3) ## ## Iterations = 30001:129901 ## Thinning interval = 100 ## Sample size = 1000 ## ## DIC: 3155.614 ## ## G-structure: ~us(1 + Time):Chick ## ## post.mean l-95% CI u-95% CI eff.samp ## (Intercept):(Intercept).Chick 8.736 0.5156 19.9539 471.8 ## Time:(Intercept).Chick -5.844 -11.7612 -0.8111 321.4 ## (Intercept):Time.Chick -5.844 -11.7612 -0.8111 321.4 ## Time:Time.Chick 5.037 2.8220 7.6839 490.5 ## ## ~us(Time):units ## ## post.mean l-95% CI u-95% CI eff.samp ## Time:Time.units 1.431 1.031 1.926 215.6 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 8.321 0.458 15.45 320 ## ## Location effects: weight ~ Time + I(Time^2) + Diet:Time ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 40.43255 39.06057 41.47525 837.9 &lt;0.001 *** ## Time 3.31023 2.36158 4.27471 499.1 &lt;0.001 *** ## I(Time^2) 0.19681 0.17217 0.22287 393.0 &lt;0.001 *** ## Time:Diet2 1.20502 0.07702 2.43222 891.6 0.038 * ## Time:Diet3 2.04874 0.82236 3.31371 669.7 0.006 ** ## Time:Diet4 2.60724 1.39079 3.76669 1000.0 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model mixes quite poorly, so I’ve ran it for 10 times longer than the default using the argument longer23. For a real analysis I would increase it even further, but the qualitative features of the posterior can be reliably assessed. The variance associated with us(Time):units lies roughly between one and two, and is certainly not zero. The (co)variances of the \\(\\texttt{Chick}\\) effects have dramatically decreased compared to the model where us(Time):units was not fitted (model mweight.2). What is the term us(Time):units fitting? The term units is factor with a unique level for each row of the data-frame. When we fit ~units (the default argument to rcov) we are simply fitting residuals. The term us(Time):units is fitting a slope effect for each level of units (note we have not included the intercept in the variance function). Let’s call the residual for observation \\(i\\), as specified by ~units, as \\(e^{(0)}_i\\) (rather than the usual \\(e_i\\)). Let’s call the random effect \\(i\\) specified by the term us(Time):units as \\(e^{(1)}_i\\) since it is the slope for observation \\(i\\) with respect to \\(\\texttt{Time}\\). Then, \\[\\sigma^2_e = Var(e^{(0)}+e^{(1)}\\texttt{Time})=\\sigma^2_{\\texttt{units}}+\\texttt{Time}^{2}\\sigma^2_{\\texttt{Time:units}}\\] This has the same form as Equation (6.2) although note that that it misses a term \\(\\sigma_{\\texttt{Time:units}, \\texttt{units}}\\) and so the residual variance, \\(\\sigma^2_e\\), is forced to increase as \\(\\texttt{Time}\\) gets larger.24 The reason that the (co)variances of the \\(\\texttt{Chick}\\) effects has decreased is because our new model attributes part of the increase in variance over \\(\\texttt{Time}\\) to things other than variation in growth rate. The model does a good job at predicting how the variance increases with \\(\\texttt{Time}\\), and in particular predicts the variance at hatching much better than our previous model (blue lines in Figure 6.7). Figure 6.7: Chick weights plotted against time with predicted mean weight (solid lines) and 95% predictions intervals (dashed line). The red line is for a random intercept-slope model (mweight.2) that allows the variance to across chicks to change as a quadratic function of \\(\\texttt{Time}\\). The blue line is also for a random intercept-slope model (mweight.3) but the residual variance has been allowed to change with the square of \\(\\texttt{Time}\\). We can also look at how well the predicted means align with each chick’s growth curve. However, to make a fair comparison with other models we should marginalise us(Time):units so that the prediction doesn’t contain part of our ‘residual’ (\\(e^{(1)}\\texttt{Time}\\)): pweight.3 &lt;- predict(mweight.3, marginal = ~us(Time):units) If we overlay these predictions on those from the model with constant residual variance, we can see that the predictions for the mean are rather similar (Figure 6.8). Figure 6.8: Weights of each chick as a function of age (points). Predictions are shown for models where the mean growth curve was fitted as a quadratic function of \\(\\texttt{Time}\\) with random chick intercept and slopes. The residual variance is either assumed constant (red lines - model mweight.2) or to change with square of \\(\\texttt{Time}\\) (blue lines - model mweight.3). By allowing the residual variance to change as a function of \\(\\texttt{Time}\\) our estimate of the variation in growth rate (\\(\\sigma^\\texttt{Time}\\)) has dropped from 11.7 to 5 - a major change in interpretation. Is this change in interpretation justified? I would argue that in terms of predicting the mean, the model that assumed constant residual variance actually does slightly better than our new model. Both models also show autocorrelation in their residuals - we see runs of data-points lying consistently above, or consistently below, the prediction lines (for example \\(\\texttt{Chick 43}\\)). If we plot the residuals from our predictions, these patterns are much more obvious (Figure 6.9). Figure 6.9: Weights of each chick as a function of age (points). Predictions are shown for models where the mean growth curve was fitted as a quadratic function of \\(\\texttt{Time}\\) with random chick intercept and slopes. The residual variance is either assumed constant (red lines - model mweight.2) or to change with square of \\(\\texttt{Time}\\) (blue lines - model mweight.3). We are in a tricky situation. The model with non-constant residual variance does a better job at catching the patterns in the data. If it did not, the variance associated with the term us(Time):units should be zero, and it is clearly not. However, when we focus on specific aspects of the data (e.g. the predicted means) it does a slightly poorer job. The central issue here is that both models have their inadequacies, and without fixing them it might be hard to draw very firm conclusions. While dissatisfying, it is the reality, and previous random-regression tutorials that have used the \\(\\texttt{ChickWeight}\\) data set as an example rarely touch on the difficulties and subtleties of the approach. However, as is often the case, in Section 6.4 we will see that the log-transform can come to our rescue. First, let’s return to the issue of autocorrelation. 6.3 Autocorrelation In Figure 6.9 we plotted a time series for the residuals of each chick. Neighbouring residuals were much more likely to have the same sign than a random pair of residuals, indicating positive autocorrelation in the residuals over short time scales: two observations close in time are more similar to each other than our model predicts. Conversely, those time series where the middle residuals fall below the zero line tend to have terminal residuals that fall above the line, and vice versa: at longer time scales we see negative autocorrelation in the residuals and our observations are less similar to each other than our model predicts. A way to visualise these issues more clearly is to think about the correlations we see in our data between observations made on the same chicks at different times, and compare these to what the model predicts. In the previous section I gave the equation for the how the variance due to the \\(\\texttt{Chick}\\) effects changes over \\(\\texttt{Time}\\) (Equation (6.2)). This was a special case of the more general equation for the covariance in \\(\\texttt{weight}\\) between ages \\(\\texttt{Time}_1\\) and \\(\\texttt{Time}_2\\) due to \\(\\texttt{Chick}\\) effects:25 \\[ \\begin{array}{rl} \\textrm{Cov}(\\texttt{weight}_1, \\texttt{weight}_2)=&amp;\\textrm{Cov}(u^{(0)}+u^{(1)}\\texttt{Time}_1, u^{(0)}+u^{(1)}\\texttt{Time}_2)\\\\ =&amp;\\sigma^2_{\\texttt{(Intercept)}}+(\\texttt{Time}_1+\\texttt{Time}_2)\\sigma_{\\texttt{(Intercept)}, \\texttt{Time}}+\\texttt{Time}_1\\texttt{Time}_2\\sigma^2_{\\texttt{Time}}\\\\ \\end{array} \\label{Crr-eq} \\tag{6.3} \\] Because we can conceptualise the random regression in terms of the covariance between observations, rather than in terms of intercepts, slopes, and so on, random regression models are sometimes known as covariance function models. We can use the function \\(\\texttt{buildV}\\) to obtain the expected (co)variances in our data due to the random effects we choose to marginalise and the residuals. The default is to marginalise all random effects which will give us the covariances due to both chick effects and residuals: theoreticalC.3 &lt;- buildV(mweight.3, diag = FALSE) Note that we need to specify diag=FALSE. This returns an \\(n\\times n\\) (sparse) covariance matrix for the data-frame, where \\(n\\) is the number of observations (578 in this case). Since we are assuming that the covariance structure for each chick is the same, we can just extract the covariance structure for the first chick (which has a complete time series of 12 observations)26. We can also turn the \\(12\\times 12\\) covariance matrix into a correlation matrix since we have already assessed how well the model does at predicting the scale of the observations (Figure 6.7). theoreticalC.3 &lt;- cov2cor(theoreticalC.3[1:12, 1:12]) This is a matrix of expected correlations in weights at different ages after controlling for the fixed effect part of the model. To obtain the observed correlations, we need to calculate the correlation in the deviations of the chick’s weights from their predicted values. Note that these predicted values should be based on the fixed effect part of the model only, and the chick effects need to be marginalised (the default for \\(\\texttt{predict}\\)). ChickWeight$marginal.resid &lt;- c(ChickWeight$weight - predict(mweight.3)) I will refer to these as marginal residuals to distinguish them from the residual plotted in Figure 6.9 which include the chick effects in the prediction. We can reshape \\(\\texttt{ChickWeight}\\) into wide format where each row is a chick and the marginal residuals at different ages are columns: chick.wide &lt;- reshape(ChickWeight[, c(&quot;marginal.resid&quot;, &quot;Chick&quot;, &quot;Time&quot;)], v.names = &quot;marginal.resid&quot;, idvar = &quot;Chick&quot;, timevar = &quot;Time&quot;, direction = &quot;wide&quot;) head(chick.wide[, 1:5]) ## Chick marginal.resid.0 marginal.resid.2 marginal.resid.4 marginal.resid.6 ## 1 1 1.5674476 3.159771 2.1776546 -3.3789029 ## 13 2 -0.4325524 1.159771 1.1776546 4.6210971 ## 25 3 2.5674476 -8.840229 -1.8223454 -0.3789029 ## 37 4 1.5674476 1.159771 -0.8223454 -0.3789029 ## 49 5 0.5674476 -5.840229 -8.8223454 -7.3789029 ## 61 6 0.5674476 1.159771 2.1776546 6.6210971 From this we can calculate the observed \\(12\\times 12\\) correlation in the marginal residuals. empiricalC &lt;- cor(chick.wide[, -1], use = &quot;pairwise.complete.obs&quot;) Note that we needed to specify use=\"pairwise.complete.obs\" because some chicks have missing values at later times, presumably because they died - an issue I will return to in Section ??. The theoretical and observed correlations are shown in blue and black respectively (Widget 6.1) Widget 6.1: In black (wire-frame) are the correlations in the marginal residuals of weight over \\(\\texttt{Time}\\). In blue (solid) is the predicted correlations from a model with random chick intercept and slopes and a residual variance that is assumed to change with the square of \\(\\texttt{Time}\\). See code persp3d(x = unique(ChickWeight$Time), y = unique(ChickWeight$Time), z = empiricalC, xlab = &quot;Time 1&quot;, ylab = &quot;Time 2&quot;, zlab = &quot;correlation&quot;, aspect = c(1, 1, 0.5), col = &quot;black&quot;, front = &quot;lines&quot;, back = &quot;lines&quot;, diagonal = &quot;back&quot;) persp3d(x = unique(ChickWeight$Time), y = unique(ChickWeight$Time), z = theoreticalC.3, add = TRUE, col = &quot;blue&quot;) The diagonal ridge has a height of one: observations are perfectly correlated with themselves. Close to the ridge, the observed correlations are larger than that predicted (the red wire-frame is above the blue surface) particularly at later ages. This indicates that the autocorrelation is stronger than our model predicts over short time-scales. As we move further away from the ridge the observed correlations becomes smaller than our predicted correlations indicating that the autocorrelation is weaker than our model predicts at medium time-scales. Finally, the model is very poor at predicting the correlation between the weight at hatching and the weights at all other values of \\(\\texttt{Time}\\): the observed correlation starts off strong and eventually becomes weakly negative, whereas the predicted correlation quickly drops to very negative values. Three dimensional plots can be confusing. We can also plot the correlations between the marginal residuals at a specific time-point (\\(\\texttt{Time 1}\\)) and all others. These plots are sometimes referred to as variograms. In Figure 6.10 we have the empirical correlations as black circles and the theoretical correlations as either red lines (model mweight.2 with constant variance) or blue lines (model mweight.3 as shown in Widget 6.1). The three panels show the correlations between the marginal residuals at each of the 12 time-points against the weight at hatching (\\(\\texttt{Time 1}=0\\) - left), the mid-point (\\(\\texttt{Time 1}=10\\) - middle) or the end (\\(\\texttt{Time 1}=21\\) - right). The discrepancies between the model and the data are quite striking, particularly for the hatching weights (\\(\\texttt{Time 1}=0\\)) where the model predicts very strong negative correlations which are much more modest in reality. In addition, in all panels there is a discontinuity for the theoretical correlations when \\(\\texttt{Time 1}=\\texttt{Time 2}\\), whereas empirically, the correlations go much more smoothly to one as \\(\\texttt{Time 1}=\\texttt{Time 2}\\). This discontinuity is due to the residual terms us(Time):units and units which contribute to the variance (i.e. the covariance when \\(\\texttt{Time 1}=\\texttt{Time 2}\\)), but not the covariance/correlations between different values of \\(\\texttt{Time}\\) (i.e. the covariance when \\(\\texttt{Time 1}\\neq\\texttt{Time 2}\\)). In the geostatistical literature this is often referred to as the ‘nugget effect’ and is visible in Widget 6.1 by viewing from the origin. Figure 6.10: Correlations between the marginal residuals for weight at each of the 12 time-points against the marginal residuals at hatching (\\(\\texttt{Time 1}=0\\) - left), the mid-point (\\(\\texttt{Time 1}=10\\) - middle) or the end (\\(\\texttt{Time 1}=21\\) - right). The lines are the theoretical predictions from a random-intercept slope model (red) and a random-intercept slope model that allows the residual variance to change with \\(\\texttt{Time}\\) (blue). In my experience, random-regression models are rarely conceptualised in terms of covariance functions. This makes sense - the model is constructed in terms of process, and thinking about variation in growth rate (i.e \\(\\sigma^2_\\texttt{Time}\\)) is much more intuitive than thinking about how variation in growth rate translates into patterns of correlation in the data. Nevertheless, if a random regression model does not capture the correlations in the data well, then any conclusions from the model have to be taken with caution. When people think directly about modelling patterns of correlation in the data, they are often thinking about auto-regressive models. These models often do a better job at modelling any correlation structure in the data, and we will cover them in Section 6.5. First, lets see if we can solve some of our problems more easily. 6.4 Variance stabilisation The variance in chick weight increases dramatically with \\(\\texttt{Time}\\) and it requires effort to understand how this aspect of the data is captured by random-regression models. We also saw that while modelling the change in variance accurately improved our models ability to capture general features of the data, it did compromise our ability to predict some aspects, such as the predicted means. In addition to the variance, the mean chick weight also increased dramatically in time, but we managed to model this fairly intuitively by fitting \\(\\texttt{Time}\\) and \\(\\texttt{Time}^2\\) in the fixed effect formula. Let’s start by plotting the means and standard deviations for each \\(\\texttt{Time}\\) (Figure 6.11). Figure 6.11: Standard deviation of chick weight against the mean over \\(\\texttt{Time}\\) A remarkably tight and linear relationship. A linear relationship between the standard deviation and the mean, or the variance and the square of the mean, is often the hall-mark of a multiplicative process - chick don’t add \\(x\\) grams to their weight per unit time, they increase their weight be \\(x\\)% per unit time. Log transforming the outcome of a multiplicative process makes it additive and very often reduces, or even eliminates, any relationship between the mean and variance. Let’s fit our three models that have random intercepts (mlweight.1), random intercepts and slopes (mlweight.2) and random intercepts and slopes and with the residual variance allowed to increase with \\(\\texttt{Time}\\) (mlweight.3). prior.lweight &lt;- list(R = IW(1, 0.002), G = F(2, 1000)) mlweight.1 &lt;- MCMCglmm(log(weight) ~ Time + I(Time^2) + Diet:Time, random = ~Chick, data = ChickWeight, prior = prior.lweight) mlweight.2 &lt;- MCMCglmm(log(weight) ~ Time + I(Time^2) + Diet:Time, random = ~us(1 + Time):Chick, data = ChickWeight, prior = prior.lweight) mlweight.3 &lt;- MCMCglmm(log(weight) ~ Time + I(Time^2) + Diet:Time, random = ~us(1 + Time):Chick + us(Time):units, data = ChickWeight, prior = prior.lweight, longer = 10) If we look at the model summary for our most complex model: summary(mlweight.3) ## ## Iterations = 30001:129901 ## Thinning interval = 100 ## Sample size = 1000 ## ## DIC: -1363.942 ## ## G-structure: ~us(1 + Time):Chick ## ## post.mean l-95% CI u-95% CI eff.samp ## (Intercept):(Intercept).Chick 0.0047459 0.0021679 0.0072349 1000 ## Time:(Intercept).Chick -0.0008225 -0.0014196 -0.0002922 1000 ## (Intercept):Time.Chick -0.0008225 -0.0014196 -0.0002922 1000 ## Time:Time.Chick 0.0004528 0.0002712 0.0006633 1000 ## ## ~us(Time):units ## ## post.mean l-95% CI u-95% CI eff.samp ## Time:Time.units 9.101e-06 3.058e-10 1.641e-05 1000 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 0.004024 0.003018 0.005304 1000 ## ## Location effects: log(weight) ~ Time + I(Time^2) + Diet:Time ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 3.689762 3.665121 3.711355 1000 &lt;0.001 *** ## Time 0.097980 0.089050 0.106964 1000 &lt;0.001 *** ## I(Time^2) -0.001813 -0.001992 -0.001667 1000 &lt;0.001 *** ## Time:Diet2 0.017027 0.002913 0.030224 1000 0.014 * ## Time:Diet3 0.027384 0.013780 0.043057 1000 &lt;0.001 *** ## Time:Diet4 0.031457 0.017899 0.046060 1000 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 we can see that there is good evidence for variation in both intercepts and slopes. However, the variance associated with \\(\\texttt{us(Time):units}\\), which allows the residual variance to increase with \\(\\texttt{Time}^2\\), is very small. Using the work-flow used in previous sections, we can first look to see how well these models capture any change in the mean and variance in weight over \\(\\texttt{Time}\\) (Figure 6.12). Figure 6.12: Log chick weights plotted against time with predicted mean weight (solid lines) and 95% predictions intervals (dashed line). The black line is for a random intercept model (mlweight.1) that implicitly assumes the variance is constant over \\(\\texttt{Time}\\). The red line is for a random intercept-slope model (mlweight.2) that allows the variance across chicks to change as a quadratic function of \\(\\texttt{Time}\\). The blue line is also for a random intercept-slope model (mlweight.3) but the residual variance has been allowed to change with the square of \\(\\texttt{Time}\\). While the log-transform hasn’t stabilised the variance with respect to \\(\\texttt{Time}\\) or the mean, the change in variance is much less dramatic than it was on the artithemtic scale. The model with only random intercepts assumes constant variance which is not well supported. The two random intercept-slope models give almost identical predictions and capture the change in variance reasonably well, although the variance at hatching remains smaller than the model predicts. The two models are very similar because model mlweight.3 predicts that the increase in variance with \\(\\texttt{Time}\\) can be attributed almost completely to variation in growth rate (\\(\\sigma^2_\\texttt{Time:units}\\) is very small) unlike our conclusions from the model on the raw weights, mweight.3. Allowing the residual variance to change as a function of \\(\\texttt{Time}\\) has hardly any impact on our estimate of the variance in growth rate (\\(\\sigma^2_\\texttt{Time}\\)) - it has only decreased by 2.4%, rather than the 56.9% decrease we saw on the arithmetic scale. The random intercept-slope models also do a better job at capturing the correlations in the data with a reasonable alignment between the empirical correlations and the estimated correlation function (Figure ??). Widget 6.2: In black (wire-frame) are the correlations in the marginal residuals of log weight over \\(\\texttt{Time}\\). In blue (solid) is the predicted correlations from a model with random chick intercept and slopes and a residual variance that is assumed to change with the square of \\(\\texttt{Time}\\). Looking at the correlations across a subset of time points also shows reasonable alignment, and highlights that allowing the residual variance to change with \\(\\texttt{time}\\) (the blue lines) is unnecessary on the log-scale (Figure ??). Figure 6.13: Correlations between the marginal residuals for log weight at each of the 12 time-points against the marginal residuals at hatching (\\(\\texttt{Time 1}=0\\) - left), the mid-point (\\(\\texttt{Time 1}=10\\) - middle) or the end (\\(\\texttt{Time 1}=21\\) - right). The lines are the theoretical predictions from a random-intercept slope model (red) and a random-intercept slope model that allows the residual variance to change with \\(\\texttt{Time}\\) (blue). Despite the improvement, there still remains some discrepancy between the observed and predicted correlations, so let’s see if we can improve the model. 6.5 Antedependence and Autoregressive Models Antedependence models offer a flexible way of modelling relationships between time-ordered variables and include autoregressive models as a special case. They can be applied to (time) ordered random effects or ordered residuals, but lets focus on the residual structure here. In the previous section, we designated \\(\\texttt{units}\\) effects as \\(e\\)’s and superscripted them with \\((0)\\) or \\((1)\\) depending on whether it was the intercept (specified through rcov=~units) or the slope (specified through random=~us(Time):units). Here we will break with this notation and use the notation \\(e^{(1)}\\), \\(e^{(2)}\\) … \\(e^{(T)}\\) to designate the residuals for a set of \\(T\\) observations belonging to a time-series. For our analyses of chick weight, \\(T=12\\). The time series is initiated at \\(t=1\\) and we will say \\[e^{(1)} = \\epsilon^{(1)}\\] where \\(\\epsilon^{(1)}\\) is drawn from a normal distribution with a mean of zero and a variance \\(\\sigma^2_{\\epsilon^{(1)}}\\). With one observation we’ve simply redefined the residual, which isn’t very interesting. However, for the second residual the model is: \\[e^{(2)} = \\beta_{2|1}e^{(1)}+\\epsilon^{(2)}\\] where \\(\\beta_{2|1}\\) is the regression coefficient of \\(e^{(2)}\\) on \\(e^{(1)}\\) and \\(\\epsilon^{(2)}\\) is drawn from a normal distribution with a mean of zero and a variance \\(\\sigma^2_{\\epsilon^{(2)}}\\). We can regress the third residual on the previous residuals to obtain \\[e^{(3)} = \\beta_{3|1}e^{(1)}+\\beta_{3|2}e^{(2)}+\\epsilon^{(3)}\\] and we can follow the same rules for the remaining residuals. Essentially, we are building a series of nested multiple regressions, where each variable is regressed on all previous values. I’ll refer to the regression coefficients as lag-coefficients. The residual variances of these regressions - the variances of the \\(\\epsilon\\)’s - I’ll refer to as the innovation variances as they add new variation into the time series. We can represent the antedependence model as a path diagram, but to keep things manageable we’ll just look at the first six time points and only represent a subset of all possible lag coefficients (Figure 6.14). Figure 6.14: Path diagram of an antedependence model for a set of six residuals (\\(e\\)). The \\(\\epsilon\\)’s are ‘innovations’ specific to each residual. The curved arrows represent the effect of a residual on a future residual with strength determined by the lag coefficient. lag-1 coefficients are in black, lag-2 coefficients are in red and lag-3 coefficients are in green. Higher order lag coefficients are not shown. Each curved arrow represents one of the lag coefficients. For \\(T\\) variables there are \\(T(T-1)/2\\) lag-coefficients and \\(T\\) innovations variances. If we put no constraints on these parameters and estimate all \\(T(T+1)/2\\) of them, we would be estimating a fully unstructured covariance matrix, as specified by us, albeit using a different parameterisation. The trick, of course, is to impose sensible constraints. One obvious set of constraints would be to set some of the lag coefficients to zero. In \\(\\texttt{MCMCglmm}\\), any individual lag coefficient can be set to zero in the prior. However, it is more common to set a group of coefficients to zero. I have coloured the paths in Figure 6.14 by their order. The black paths are the lag-1 coefficients - they represent the effect of the previous value on the current one. The red paths are the lag-2 coefficients - they represent the effect of the previous value but one on the current one. The blue and green paths are the lag-3 and lag-4 coefficients, respectively. The most usual constraint is to only retain the low order lag coefficients. For example, in a first-order antedependence model the lag-1 coefficients are estimated, but higher order coefficients are set to zero. In a second-order antedependence model the lag-1 and lag-2 coefficients are retained, and so on. To fit antedependence structures in MCMCglmm we need to use the variance function \\(\\texttt{ante}\\) followed by the antedependence order of the model. For example, to fit a 2nd order antedependence model to the chick residuals, we could use rcov=~ante2(as.factor(Time)):Chick, which would fit the model: We can also impose further constraints by specifying that coefficients of the same lag are all the same. This can achieved by putting a \\(\\texttt{c}\\) (for constant) before the antedependence order when specifying the variance structure. For example, \\(\\texttt{antec2}\\) fits a 2nd order antedependence where only two lag-parameters need to be estimated: the lag-1 and lag-2 coefficients (\\(\\beta_{1}\\) and \\(\\beta_{2}\\)) , respectively: By default, the innovation variances for each residual are allowed to take on different values. We can also specify that we would like the innovation variances to be time-homogeneous so that we just estimate a single innovation variance, \\(\\sigma^2_{\\epsilon}\\), common to all residuals. This can achieved by putting a \\(\\texttt{v}\\) after the antedependence order when specifying the variance structure. For example, \\(\\texttt{antec2v}\\) fits a 2nd order antedependence with three parameters - \\(\\beta_{1}\\), \\(\\beta_{2}\\) and \\(\\sigma^2_{\\epsilon}\\). The model is time-homogeneous in its parameters. For example, the lag-1 coefficients are all \\(\\beta_{1}\\) irrespective of whether we are looking at \\(e^{(1)}\\)’s affect on \\(e^{(2)}\\), or \\(e^{(4)}\\)’s affect on \\(e^{(5)}\\). For those not familiar with antedependence models, two misconceptions are possible when considering an antedependence model of the form \\(\\texttt{antec2v}\\). The first misconception is that because a 2nd order model has been fitted, any residuals that are more than two time units apart are uncorrelated. This isn’t the case. Consider the pair of residuals \\(e^{(1)}\\) and \\(e^{(4)}\\) for which there is no direct path since they are at lag-3. Their covariance is: \\[ \\textrm{Cov}(e^{(1)}, e^{(4)})=(\\beta_{4|3}\\beta_{3|2}\\beta_{2|1}+\\beta_{4|3}\\beta_{3|1}+\\beta_{4|2}\\beta_{2|1})\\sigma^2_{\\epsilon^{(1)}} \\] The three products of lag-coefficients represent the three possible (indirect) paths that connect \\(e^{(1)}\\) to \\(e^{(4)}\\) and cause them to be correlated. When we assume the lag-coefficients are time homogeneous this simplifies to: \\[ \\textrm{Cov}(e^{(1)}, e^{(4)})=(\\beta_{1}^3+2\\beta_{1}\\beta_{2})\\sigma^2_{\\epsilon} \\] but these calculations can still be a bit hairy when the order of the model is high and are best calculated in matrix form27. The second possible misconception is that because the parameters are time-homogeneous, the distribution of residuals must also be time-homogeneous. If the lag-coefficients are not too large then the distribution of residuals will eventually become time-homogeneous, but it takes time for them to ‘forget’ their initial state. As an example, in Figure 6.15 I have plotted the variances (left) and the lag-1 correlations (right) as a function of time when \\(\\beta_1=0.5\\), \\(\\beta_2=0.25\\) and \\(\\sigma^2_{\\epsilon}=1\\). The equilibrated (co)variances are shown as solid lines in Figure 6.15 and are often referred to as the stationary (co)variances. Figure 6.15: The residual variance (left) and the correlation between residuals and their predecessors (right) as a function of time for a 2nd-order antedependence model with time-homogeneous parameters (\\(\\beta_1=0.5\\), \\(\\beta_2=0.25\\) and \\(\\sigma^2_{\\epsilon}=1\\)). That was a lot of abstract concepts. Lets fit a first order antedependence model to the residuals in our model of log chick weights. The details of the prior specification can be found in Section 6.5.2. prior.lweight.4 &lt;- list(R = list(V = diag(1), nu = 0.002, beta.mu = 0, beta.V = 10000), G = F(2, 1000)) mlweight.4 &lt;- MCMCglmm(log(weight) ~ Time + I(Time^2) + Diet:Time, random = ~us(1 + Time):Chick, rcov = ~antec1v(as.factor(Time)):Chick, data = ChickWeight, prior = prior.lweight.4) summary(mlweight.4) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: -1653.614 ## ## G-structure: ~us(1 + Time):Chick ## ## post.mean l-95% CI u-95% CI eff.samp ## (Intercept):(Intercept).Chick 9.280e-05 7.253e-12 0.0003487 1150.5 ## Time:(Intercept).Chick 4.739e-06 -1.595e-04 0.0001722 593.2 ## (Intercept):Time.Chick 4.739e-06 -1.595e-04 0.0001722 593.2 ## Time:Time.Chick 3.774e-04 1.729e-04 0.0006103 286.5 ## ## R-structure: ~antec1v(as.factor(Time)):Chick ## ## post.mean l-95% CI u-95% CI eff.samp ## iv.Chick 0.003169 0.00277 0.003565 726.0 ## b1.Chick 0.961113 0.90766 1.018135 575.6 ## ## Location effects: log(weight) ~ Time + I(Time^2) + Diet:Time ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 3.713980 3.699639 3.732040 900.8 &lt;0.001 *** ## Time 0.093557 0.083217 0.103216 1000.0 &lt;0.001 *** ## I(Time^2) -0.001670 -0.001883 -0.001431 1131.0 &lt;0.001 *** ## Time:Diet2 0.017056 0.002319 0.033409 1000.0 0.034 * ## Time:Diet3 0.030164 0.015311 0.044834 1000.0 &lt;0.001 *** ## Time:Diet4 0.023738 0.006780 0.037589 1000.0 0.006 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the summary28 we now see that the variance in intercepts has become very small and the innovation variance is around two-thirds the estimate we obtained of the variance in intercepts in the model without antedependent residuals (mlweight.3). The lag-1 coefficient is also very large and overlaps one. Let’s see how well this new model captures the changes in mean and variance over time (Figure 6.16). Figure 6.16: Chick log weights plotted against time with predicted mean weight (solid lines) and 95% predictions intervals (dashed line). The blue line is for a random intercept-slope model (mlweight.2) that allowed the residual variance to increase with the square of $ exttt{Time}$. The green line is for a random intercept-slope model (mlweight.2) with residuals modelled using a 1st order antedependence structure It looks like the antedependence model (in green) does a better job at predicting the reduced variation in hatching compared to the model where the residual variation is allowed to change with \\(\\texttt{Time}\\) squared (in blue). However, the predicted variances at later ages are rather similar. The predicted correlation structure, however, looks slightly worse than the previous model (Figure ??). Widget 6.3: In black (wire-frame) are the correlations in the marginal residuals of log weight over \\(\\texttt{Time}\\). In green (solid) is the predicted correlations from a model with random chick intercept and slopes and a 1st order antedependence structure for the residuals. and this is perhaps more obvious when we look at the a select subset of correlations (Figure (fig:ar-2d-l7)). Sadly, the antedependence model has not improved our model’s ability to capture the correlations in the data, and has primarily improved the estimate of the variance at hatching. 6.5.1 Autoregressive Models When the parameters of an antedependence model are time-homogenous the model is often referred to as autoregressive. In the statistical literature it is usual to also assume that the model has reached stationarity by the first census point. For time-series that have natural zero points, like the chicken growth data, this is unlikely. However, for data that can be considered a sample from a longer time-series, assuming stationarity seems reasonable. When I refer to autoregressive models, I am implicitly assuming the series is already at stationarity. In \\(\\texttt{MCMCglmm}\\), only first-order autoregressives are implemented and can be specified as an \\(\\texttt{ar1}\\) variance structure. This can seen as a special case of a \\(\\texttt{antec1}\\) structure but with the two constraints. First, the lag-1 coefficient is constrained to be between \\(-1\\) and \\(1\\), ensuring that the antedependce model has an equilibrium. Second, the innovation variance for the first census point is constrained to be \\(\\sigma^2_{\\epsilon^{(1)}}=\\sigma^2_{\\epsilon}/(1-\\beta_1)^{2}\\), whereas all remaining census points have an innovation variance of \\(\\sigma^2_\\epsilon\\). Since \\(\\sigma^2_\\epsilon/(1-\\beta_1)^{2}\\) is the equilibrium variance this ensures that all residuals have the same variance \\(\\sigma^2_e = \\sigma^2_\\epsilon/(1-\\beta_1)^{2}\\) and the correlation between time points separated by \\(\\tau\\) time points is \\(\\beta^{\\tau}_1\\). As with antedependence models the model object contains the posterior samples of the (co)variance matrix but the model summary reports the variance \\(\\sigma^2_e\\) (rather than the innovation variance \\(\\sigma^2_\\epsilon\\)) and \\(\\beta_1\\). 6.5.2 Priors in Antedependence models For antedependence models, priors are placed on the innovation variances and the lag-coefficients. The prior for the innovation variances are specified in the same way that priors are specified for variances in an idh structure (see Section 5.3). The prior for the lag-coefficients, like the prior for the fixed effects, is multivariate normal with a specified mean vector (\\(\\texttt{beta.mu}\\)) and a specified (co)variance matrix (\\(\\texttt{beta.V}\\)). The default prior for the lag-coefficients are also the same as for the fixed effects with a zero mean vector and a diagonal (co)variance matrix with large variances (\\(10^{10}\\)). Understanding what prior this places on the covariance matrix is challenging, and currently, the best option is probably to assess this graphically using simulations from the prior. prior &lt;- list(V = diag(1), nu = 1, alpha.mu = 0, alpha.V = 1000, beta.mu = 0, beta.V = 10) VCV &lt;- rprior(2000, prior = prior, vtype = &quot;antec1v&quot;, k = 3) 6.6 User-defined Design Matrices \\(\\texttt{MCMCglmm}\\) generally uses \\(\\texttt{model.matrix}\\), or a native equivalent, when evaluating a model formula to obtain the design matrices. Sometimes R formula syntax is not flexible enough to generate the appropriate design matrix. However, if a random-effect design matrix can be generated, it is possible to fit the model by exploiting the \\(\\texttt{idv}\\) variance structure. However, the parameters defined by the formula in the \\(\\texttt{idv}\\) structure should not be interacted with another factor. Consider the term ~idv(z1+z2+z3+z4):1 and let’s set up the table of random effects as we have before: \\[\\begin{array}{c|c} &amp;{\\color{red}{\\texttt{(Intercept)}}}\\\\ \\hline\\\\ {\\color{blue}{\\texttt{z1}}}&amp;{\\color{blue}{\\texttt{z1}}}.{\\color{red}{\\texttt{(Intercept)}}}\\\\ {\\color{blue}{\\texttt{z2}}}&amp;{\\color{blue}{\\texttt{z2}}}.{\\color{red}{\\texttt{(Intercept)}}}\\\\ {\\color{blue}{\\texttt{z3}}}&amp;{\\color{blue}{\\texttt{z3}}}.{\\color{red}{\\texttt{(Intercept)}}}\\\\ {\\color{blue}{\\texttt{z4}}}&amp;{\\color{blue}{\\texttt{z4}}}.{\\color{red}{\\texttt{(Intercept)}}}\\\\ \\end{array}\\] An \\(\\texttt{idv}\\) structure, like an \\(\\texttt{idh}\\) structure, assumes no correlation between effects on different rows: the \\(\\texttt{id}\\) in \\(\\texttt{idh}\\) stands for identity matrix. The \\(\\texttt{h}\\) in \\(\\texttt{idh}\\) stands for heterogeneous and so each row is allowed a different variance. Clearly, with only one effect per row estimating a variance is not feasible. However, with \\(\\texttt{idv}\\) the variances are assumed homogeneous and so only a single variance (the \\(\\texttt{v}\\) in \\(\\texttt{idv}\\)) is estimated that is common to all effects. The (four) regression coefficient are then treated exactly like simple random effects: identically and independently distributed with zero mean and a variance to be estimated. When there are many random effects, writing ~idv(z1+z2+z3+z4+...z999+z1000):1 is going to be painful to write and read. However, you can simply including the design matrix as a matrix in the data-frame and fit it inside the \\(\\texttt{idv}\\) structure. The multi-membership model fitted in Section 5.2.2 is a case in point. In that example, each child had five friends and we would like to fit \\(\\texttt{friend}\\) as a random effect that predicts the child’s well-being. We can take a look at the random effect design matrix for that model: ## A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ## 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 ## 3 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 ## 4 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ## 5 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 ## 6 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 The row for each child has five ones indicating their friends. This design matrix cannot be created using a standard R formula and the multi-membership linking function ~mm(freind.1+freind.2+freind.3+freind.4+freind.5) was used to create the design matrix. However, if we had the incidence matrix, we could place it in the data-frame and fit an identical model: data.friends$Z &lt;- Z m.mm2 &lt;- MCMCglmm(y ~ 1, random = ~idv(Z), data = data.friends, prior = prior.mm) Note that there is no need to explicitly interact the \\(\\texttt{idv}\\) structure with an intercept (in fact \\(\\texttt{MCMCglmm}\\) will complain if you try). In this example, the predictor variables (\\(\\texttt{freind}\\)) are categorical, but there is nothing to stop design matrices from being formed using continuous predictors, as we’ll see in the next section on splines. 6.7 Splines The function \\(\\texttt{smspline}\\) in \\(\\texttt{lmeSplines}\\) can be used to generate design matrices for penalised splines that are transformed such that the random effects are expected to be identically and independently distributed with unknown variance. Using the ideas discussed above for user-specified design matrices (Section 6.6) we can use an \\(\\texttt{idv}\\) structure to fit the spline. However, rather than generating the design matrix outside of \\(\\texttt{MCMCglmm}\\) and adding it to the data-frame, we can simply use the \\(\\texttt{smspline}\\) function inside \\(\\texttt{idv}\\). For example, we can fit a penalised cubic spline to one of the test data sets in \\(\\texttt{lmeSplines}\\): spline.prior = list(R = IW(1, 0.002), G = F(1, 1000)) m.spline &lt;- MCMCglmm(y ~ time, random = ~idv(smspline(time)), data = smSplineEx1, pr = TRUE, prior = spline.prior) The random effects need to be saved (pr=TRUE) because these are the coefficients of the spline. Similarly, when making a prediction we want to include the spline coefficients in the prediction, rather than marginalising them (Section 4.2). p.slpine &lt;- predict(m.spline, marginal = NULL) The model wiggles its way through the data, as splines do … plot(y ~ time, data = smSplineEx1) lines(p.slpine ~ smSplineEx1$time) Figure 6.17: Data and prediction from a penalised cubic spline fitted to the example data \\(\\texttt{smSplineEx1}\\) from \\(\\texttt{lmeSplines}\\) 6.8 Penalised Signal Regression Note that I specified the prior using a prior generator function (Sections 2.6 and 5.3) but I upped the scale to 10,000 from the usual 1,000 because of the scale of the data. However, using a scale of 1,000 generates an almost identical posterior in this instance.↩︎ Taking the derivative of \\(E[y | \\texttt{Diet}=1]\\) with respect to \\(\\texttt{Time}\\) gives us the rate.↩︎ Remember that a global intercept is not fitted by default for variance structure models, and the model formula is essentially ~virus-1. To add the global intercept, us(1+virus):line could be fitted but this can be harder to interpret because the effects are then France, Spain-France and Greece-France. If a \\(\\texttt{us}\\) structure is fitted, the two models are equivalent reparameterisations of each other although the priors have to be modified accordingly. This is not the case if the variance function is \\(\\texttt{idh}\\). In this case the virus-specific variances are allowed to vary as before, but a constant covariance equal to \\(\\sigma^{2}_{\\color{blue}{\\texttt{France}}}\\) is also assumed↩︎ longer takes a positive integer and multiplies the arguments nitt, thin and burnin (which in this example were at there defaults, 13,000, 10 and 3,000)↩︎ covu models can overcome this constraint, or we could fit the reverse of \\(\\texttt{Time}\\) if we wanted a negative relationship (i.e. a covariate which is zero at day 21, one at day 20, two at day 19 and so on).↩︎ In this formula, \\(\\texttt{Time}\\) is fixed and the random effects are random variables: the covariance is over chicks. The general rules for obtaining a covariance are:a) The covariance between sums is the sum of the covariances between their summands. For example \\(\\textrm{Cov}(a+b, c+d) = \\textrm{Cov}(a, b)+\\textrm{Cov}(a, c)+\\textrm{Cov}(b, c)+\\textrm{Cov}(b, d)\\).b) The covariance between products, for example \\(\\textrm{Cov}(ab, cd)\\), is often a nightmare unless only one of the variables on each side is random and the remainder are constants. For example, if \\(a\\) and \\(c\\) are constants and \\(b\\) and \\(d\\) are random \\(\\textrm{Cov}(ab, cd)=ac\\textrm{Cov}(b, d)\\).c) The covariance between a constant and a random variable is zero.d) And finally, \\(\\textrm{Var}(b)=\\textrm{Cov}(b, b)\\).↩︎ To do this by hand, we need to add the covariances up due to the three terms in the model: us(1+Time):Chick and us(Time):units in the random formula, and units in \\(\\texttt{rcov}\\). For us(1+Time):Chick, subset the random effect design matrix that is relevant for the \\(i\\)th \\(\\texttt{Chick}\\) effects. Lets call this \\({\\bf Z}_i\\) which will have 12 rows (one for each observation) and two columns. The first column will be all ones (since it is the predictor for the intercept) and the second column will contain the 12 unique values of \\(\\texttt{Time}\\). The covariance due to chick effects is then \\({\\bf Z}_i{\\bf V}_\\texttt{Chick}{\\bf Z}_i^{\\top}\\) where \\({\\bf V}_\\texttt{Chick}\\) is the (co)variance matrix of intercepts and slopes. To this, we need to add the (co)variance matrix due to the us(Time):units. Again, subset the random effect design matrix that is relevant for the 12 \\(\\texttt{units}\\) effects associated with \\(\\texttt{Chick}\\) \\(i\\). Again, the matrix \\({\\bf Z}_u\\) has 12 rows, one for each observation. However, it has 12 columns since the there is a separate \\(\\texttt{units}\\) effect for each observation. \\({\\bf Z}_u\\) is diagonal, with the diagonals being the 12 unique values of \\(\\texttt{Time}\\). The covariance due to us(Time):units is \\({\\bf Z}_u\\sigma^2_\\texttt{Time:units}{\\bf Z}_u^{\\top}\\). Finally, we need to add the covariance due to units which is \\({\\bf I}\\sigma^2_\\texttt{units}\\) since we are assuming the residuals are independent with constant variance \\(\\sigma^2_\\texttt{units}\\) (\\({\\bf I}\\) is a \\(12\\times 12\\) identity matrix). These operations follow from the rules outlined in Section ?? and the footnote above[^cov].↩︎ To calculate the (co)variance matrix of the residuals (\\({\\bf V}_e\\)) we can form two matrices: \\({\\bf V}_{\\epsilon}\\) is a diagonal matrix with the innovation variances along the diagonal. \\({\\bf B}\\) is a matrix with \\(b_{ij}=\\beta_{i|j}\\). \\({\\bf V}_e\\)= \\(({\\bf I}-{\\bf B})^{-1}{\\bf V}_\\epsilon ({\\bf I}-{\\bf B})^{-\\top}\\). If we want to do the reverse operation where we have \\({\\bf V}_e\\) and would like to know the innovation variances and lag-coefficients↩︎ in versions &lt;4.0 the estimate covariance matrix was reported in the model summary. However, for large matrices (\\(12\\times 12\\) here) the output was overwhelming. Now, the innovation variance(s) are reported followed by the lag-coefficient(s) at successive lags. If you use \\(\\texttt{antev=TRUE}\\) in the call to \\(\\texttt{summary}\\) the covariance matrix will be reported. The model object still stores the output of the covariance matrix, and from this the antedependent parameters can be obtained using the function \\(\\texttt{posterior.ante}\\)↩︎ "],["multi.html", "7 Multi-response Models 7.1 Multi-response Non-Gaussian Models 7.2 Multi-response Bernoulli Models 7.3 Wide versus Long Format 7.4 Covariances between random and residual terms (\\(\\texttt{covu}\\)) 7.5 Scaled linear predictors: \\(\\texttt{theta_scale}\\) 7.6 Multinomial Models 7.7 Zero-inflated Models 7.8 Hurdle Models 7.9 Zero-altered Models", " 7 Multi-response Models So far we have only fitted models to a single response variable, and even then, each response variable came from a distribution that only required one location parameter to be estimated, such as the mean of the Poisson or the probability of the binomial. In this section we will first cover multi-response models and then move on to models of multi-parameter distributions. Since they are much less widely used than single-response models, let’s start by motivating why anyone would want to use them. Imagine we knew how much money 200 people had spent on their holiday and on their car in each of four years, and we want to know whether a relationship exists between the two. A simple correlation would be one possibility, but then how do we control for the repeated measures? An often used solution to this problem is to choose one variable as the response (lets say the amount spent on a car) and have the other variable as a predictor (the amount spent on a holiday) for which a fixed effect is estimated. The choice is essentially arbitrary, highlighting the belief that any relationship between the two types of spending maybe in part due to unmeasured variables, rather than being completely causal. In practice does this matter? Let’s imagine there was only one unmeasured variable: disposable income. There are repeatable differences between individuals in their disposable income, but also some variation within individuals across the four years. Likewise, people vary in what proportion of their disposable income they are willing to spend on a holiday versus a car, but this also changes from year to year. We can simulate some toy data to get a feel for the issues: id&lt;-gl(200,4) # 200 people recorded four times av_wealth&lt;-rlnorm(200, 0, 1) ac_wealth&lt;-rlnorm(800, log(av_wealth[id]), 1/4) # expected disposable incomes + some year to year variation av_ratio&lt;-rbeta(200,10,10) ac_ratio&lt;-rbeta(800, 3*(av_ratio[id]), 3*(1-av_ratio[id])) # expected proportion spent on car + some year to year variation y.car&lt;-ac_wealth*ac_ratio # disposable income * proportion spent on car y.hol&lt;-ac_wealth*(1-ac_ratio) # disposable income * proportion spent on holiday Spending&lt;-data.frame(y.hol=log(y.hol), y.car=log(y.car), id=id) A simple model suggests the two types of spending (on the log-scale) are positively related: mspending.1 &lt;- MCMCglmm(y.car ~ y.hol, data = Spending) summary(mspending.1) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 2634.549 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 1.57 1.416 1.733 1000 ## ## Location effects: y.car ~ y.hol ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -0.6410 -0.7455 -0.5447 1000 &lt;0.001 *** ## y.hol 0.3129 0.2485 0.3785 1000 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This conclusion seems to be supported by just looking at a scatter plot of the two variables (Figure 7.1). Figure 7.1: Money spent on car versus money spent on holiday (both logged) with the regression line from a simple regression (model mspending.1) An obvious problem with the model is that we have repeated measures (the spending habits of each individual have been recorded for each of four years) and yet we haven’t dealt with any possible non-independence. We can remedy this by fitting id effects as random: mspending.2 &lt;- MCMCglmm(y.car ~ y.hol, random = ~id, data = Spending) summary(mspending.2) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 1922.517 ## ## G-structure: ~id ## ## post.mean l-95% CI u-95% CI eff.samp ## id 1.674 1.313 2.045 1000 ## ## R-structure: ~units ## ## post.mean l-95% CI u-95% CI eff.samp ## units 0.5132 0.4589 0.5733 1131 ## ## Location effects: y.car ~ y.hol ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) -1.1801 -1.3688 -0.9852 1000 &lt;0.001 *** ## y.hol -0.2762 -0.3447 -0.2111 1053 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Strangely, and in contradiction to the scatter plot, the model suggests that spending more on a holiday means less is spent on a car - the regression slope is negative. If I hadn’t looked at the raw data, I would probably report the negative relationship and move on. But I have looked at the raw data and simply reporting a negative relationship without caveats makes me feel uneasy. Lets proceed with a multi-response model of the problem to see what is going on. The two responses are passed as a matrix using cbind(), and the rows of this matrix are indexed by the reserved variable units, and the columns by the reserved variable trait. It is useful to think of a new data frame where the response variables have been stacked column-wise and the other predictors duplicated accordingly. Below is the original data frame on the left (Spending) and the stacked data frame on the right when cbind(y.hol, y.car) is passed as the response: \\[\\begin{array}{cc} \\begin{array}{cccc} &amp;{\\color{blue}{\\texttt{y.hol}}}&amp;{\\color{blue}{\\texttt{y.car}}}&amp;\\texttt{id}\\\\ {\\color{red}{\\texttt{1}}}&amp;\\texttt{0.058057}&amp;\\texttt{1.475391}&amp;\\texttt{1}\\\\ {\\color{red}{\\texttt{2}}}&amp;\\texttt{0.763508}&amp;\\texttt{0.759238}&amp;\\texttt{1}\\\\ \\vdots&amp;\\vdots&amp;\\vdots\\\\ {\\color{red}{\\texttt{800}}}&amp;\\texttt{-2.304347}&amp;\\texttt{0.774078}&amp;\\texttt{200}\\\\ \\end{array}&amp; \\Longrightarrow \\begin{array}{ccccc} &amp;\\texttt{y}&amp;{\\color{blue}{\\texttt{trait}}}&amp;\\texttt{id}&amp;{\\color{red}{\\texttt{units}}}\\\\ 1&amp;\\texttt{0.058057}&amp;{\\color{blue}{\\texttt{y.hol}}}&amp;\\texttt{1}&amp;{\\color{red}{\\texttt{1}}}\\\\ 2&amp;\\texttt{0.763508}&amp;{\\color{blue}{\\texttt{y.hol}}}&amp;\\texttt{1}&amp;{\\color{red}{\\texttt{2}}}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 800&amp;\\texttt{-2.304347}&amp;{\\color{blue}{\\texttt{y.hol}}}&amp;\\texttt{200}&amp;{\\color{red}{\\texttt{800}}}\\\\ 801&amp;\\texttt{1.475391}&amp;{\\color{blue}{\\texttt{y.car}}}&amp;\\texttt{1}&amp;{\\color{red}{\\texttt{1}}}\\\\ 802&amp;\\texttt{0.759238}&amp;{\\color{blue}{\\texttt{y.car}}}&amp;\\texttt{1}&amp;{\\color{red}{\\texttt{2}}}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 1600&amp;\\texttt{0.774078}&amp;{\\color{blue}{\\texttt{y.car}}}&amp;\\texttt{200}&amp;{\\color{red}{\\texttt{800}}}\\\\ \\end{array} \\end{array} \\label{multi-eq} \\tag{7.1}\\] From this we can see that fitting a multi-response model is a direct extension to how we fitted models with categorical random interactions (Chapter 5): prior.mspending.3 &lt;- list(R = IW(1, 0.002), G = F(2, 1000)) mspending.3 &lt;- MCMCglmm(cbind(y.hol, y.car) ~ trait - 1, random = ~us(trait):id, rcov = ~us(trait):units, data = Spending, prior = prior.mspending.3, family = c(&quot;gaussian&quot;, &quot;gaussian&quot;)) The only real difference is that we must now specify the distribution for each response in \\(\\texttt{family}\\). While the interpretation of the model is identical to that covered in Chapter 5, it does take a little time to get used to working with the new categorical factor, \\(\\texttt{trait}\\). I have fitted a fixed \\(\\texttt{trait}\\) effect so that the two types of spending can have different intercepts. I usually suppress the global intercept (-1) for these types of models so the second coefficient is not the difference between the intercept for the first level of \\(\\texttt{trait}\\) (y.hol) and the second level (y.car) but the actual trait specific intercepts. Note that the levels of \\(\\texttt{trait}\\) are ordered as they appear in the response (\\(\\texttt{y.hol}\\) then \\(\\texttt{y.car}\\) in this instance). A \\(2\\times2\\) covariance matrix is estimated for the random term where the diagonal elements are the variance in consistent individual (\\(\\texttt{id}\\)) effects for each type of spending. The off-diagonal is the covariance between these effects which if positive suggests that people that consistently spend more on their holidays consistently spend more on their cars. A \\(2\\times2\\) residual covariance matrix is also fitted. In Section5.1.1 we fitted heterogeneous error models using idh():units which made sense for single-response models because each level of unit was specific to a particular observation and so any covariances could not be estimated. In multi-response models this is not the case because both traits have often been measured on the same observational unit and so the covariance can be measured. In the context of this example a positive covariance would indicate that in those years an individual spent a lot on their car they also spent a lot on their holiday. Let’s take a look at the model summary: summary(mspending.3) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 4011.721 ## ## G-structure: ~us(trait):id ## ## post.mean l-95% CI u-95% CI eff.samp ## traity.hol:traity.hol.id 1.0040 0.7859 1.264 1000 ## traity.car:traity.hol.id 0.8399 0.6449 1.040 1000 ## traity.hol:traity.car.id 0.8399 0.6449 1.040 1000 ## traity.car:traity.car.id 1.1357 0.8967 1.404 1000 ## ## R-structure: ~us(trait):units ## ## post.mean l-95% CI u-95% CI eff.samp ## traity.hol:traity.hol.units 0.7258 0.6525 0.8182 1000.0 ## traity.car:traity.hol.units -0.3025 -0.3649 -0.2497 1000.0 ## traity.hol:traity.car.units -0.3025 -0.3649 -0.2497 1000.0 ## traity.car:traity.car.units 0.6231 0.5573 0.6940 909.4 ## ## Location effects: cbind(y.hol, y.car) ~ trait - 1 ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## traity.hol -0.9060 -1.0823 -0.7671 1000 &lt;0.001 *** ## traity.car -0.9268 -1.0699 -0.7610 1000 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 we can see that the between-individual covariance (\\(\\texttt{traity.car:traity.hol.id}\\)) is strongly positive but the with-individual covariance (\\(\\texttt{traity.car:traity.hol.unit}\\)) is strongly negative. With a single predictors, a regression is defined as the covariance between the response and the predictor divided by the variance in the predictor29. We can therefore obtain the coefficients of a regression of \\(\\texttt{y.car}\\) on \\(\\texttt{y.hol}\\) at the level of both \\(\\texttt{id}\\) and \\(\\texttt{units}\\)30: id.regression &lt;- mspending.3$VCV[, &quot;traity.car:traity.hol.id&quot;] # covariance between individuals id.regression &lt;- id.regression/mspending.3$VCV[, &quot;traity.hol:traity.hol.id&quot;] # regression across individual units.regression &lt;- mspending.3$VCV[, &quot;traity.car:traity.hol.units&quot;] # covariance within individuals units.regression &lt;- units.regression/mspending.3$VCV[, &quot;traity.hol:traity.hol.units&quot;] # regression within individuals Conceptually, the regression at the level of \\(\\texttt{id}\\) is a regression of average expenditures across people, whereas the regression at the level of \\(\\texttt{units}\\) is a regression of yearly expenditures within individuals. We can compare these two regression with those that we got from the single response models that did (mspending.2) or did not (mspending.1) fit \\(\\texttt{id}\\) effects (Figure 7.2). Figure 7.2: MCMC summary plot of the coefficient from a regression of car spending on holiday spending in black. The red and green traces are from a model where the regression coefficient is estimated at two levels: within an individual (blue) and across individuals (red). The relationship between the two types of spending is in part mediating by a third unmeasured variable, disposable income. The regression coefficients differ substantially at the within individual (blue) and between individual (red) levels, and neither is entirely consistent with the regression coefficient from the single response models (black). The process by which we generated the data gives rise to this phenomenon - large variation between individuals in their disposable income means that people who are able to spend a lot on their holiday can also afford to spend a lot on their holidays (hence positive covariation between \\(\\texttt{id}\\) effects). However, a person that spent a large proportion of their disposable income in a particular year on a holiday, must have less to spend that year on a car (hence negative residual (within year) covariation). When fitting the simpler single-response models we make the assumption that the effect of spending money on a holiday directly effects how much you spend on a car. If this relationship was purely causal then the regression coefficients at the level of \\(\\texttt{id}\\) and \\(\\texttt{units}\\) would have the same expectation, and the simpler model would be justified. For example, we could simulate data where the expected car expenditure depends directly on holiday expenditure (using a regression coefficient of -0.3) with some variation around this due to between and within-individual effects: Spending$y.car2 &lt;- Spending$y.hol * -0.3 + rnorm(200, 0, 1)[Spending$id] + rnorm(800, 0, sqrt(2)) We can fit the univariate and multivariate models to these data, and compare the regression coefficients as we did before. Figure 7.3 shows that the regression coefficients are all similar and a value of -0.3 has a reasonably high posterior probability. Figure 7.3: MCMC trace plot of the coefficient from a regression of car spending on holiday spending in black. The red and blue traces are from a model where the regression coefficient is estimated at two levels: within an individual (blue) and across individuals (red). In simulated data the relationship between the two types of spending is causal and the regression coefficients have the same expectation. However, the posterior standard deviation from the simple regression is smaller because information from the two different levels is pooled. However, it should be noted that the posterior standard deviation is smaller in the simpler model because the more strict assumptions have allowed us to pool information across the two levels to get a more precise answer. This is one of the downsides of multi-response models - if the regressions at each level are the same we can get a more precise estimate using a standard single-response model. The other major benefit of the single-response model is that we only have to worry whether the conditional distribution of the response variable is modelled well (\\(\\texttt{y.car}\\) in this case) . In a multi-response model, we have to consider whether the model for the joint distribution of all responses is doing a good job. 7.1 Multi-response Non-Gaussian Models Model specification for multi-response models does not depend on whether the response variables are Gaussian or not. However, there is an important difference between models involving non-Gaussian variables and those involving only Gaussian variables. For Gaussian responses, the linear predictor is defined as \\[\\boldsymbol{\\eta} = E[{\\bf y}|{\\bf X}\\boldsymbol{\\beta}+{\\bf Z}{\\bf u}]\\] and any observation-level deviations from this expectation appear as \\(\\texttt{units}\\) effects: \\[{\\bf e} = {\\bf y}-\\boldsymbol{\\eta}\\] For non-Gaussian data the linear predictor is defined \\[\\boldsymbol{\\eta} = E[{\\bf l}|{\\bf X}\\boldsymbol{\\beta}+{\\bf Z}{\\bf u}+{\\bf e}]\\] where \\({\\bf l}\\) is a vector of latent variables (Section 3.4.2). Here the \\(\\texttt{units}\\) effects appear inside the linear predictor and model any overdispersion with respect to the named distribution. The ‘residual’ due to the named distribution is then \\[{\\bf y} -\\textrm{link}^{-1}(\\boldsymbol{\\eta})\\] Consequently, with non-Gaussian data the covariances are set up in terms of the underlying parameters of the distribution (on the link scale) not in terms of the response directly. This is not always appropriate. Let’s take the example of the Sweedish road accident data analysed in Chapter 3. We saw that the number of accidents per day can be modelled using an overdispersed Poisson. Let’s say we also had data on how much money car insurance companies paid out each day. A sensible way of modelling insurance pay-outs would to treat it as Gaussian and include the number of accidents as a covariate. However, if we analysed the accident and insurance pay-out data in a multi-response model, we would be measuring the covariance between insurance pay-outs and the expected number of accidents per day (\\(l\\)). Insurance companies don’t pay settlements to hypothetical accidents but actual accidents (\\(\\texttt{y}\\)) and so the multi-response model is questionable. In contrast, let’s say we also had data on how icy the road was on each day. Here, I would be quite happy to say that iciness determines the expected number of accidents per day (\\(l\\)) but the actual number of accidents (\\(\\texttt{y}\\)) will vary around this expectation. However, even here care still needs to be taken. In Section 3.4 we saw that overdispersion arises if there are unmeasured variables that affect the response of interest. If iciness (\\(\\texttt{ice}\\)) was an important predictor of the number of accidents, then including it as a covariate in a single-response model of the number of accidents would bring the overdsipsersion, and hence \\(\\sigma^2_\\texttt{units}\\), down. In a multi-response model we can obtain the shift in the \\(\\texttt{units}\\) variance had we done this. The \\(\\texttt{units}\\) variance for road accidents (\\(\\sigma^2_\\texttt{traity.unit}\\)) would be equivalent to the units variance in a single-response model without iciness as a covariate. However, \\(\\sigma^2_\\texttt{traity.unit}-\\sigma^2_\\texttt{traity:traitice.unit}/\\sigma^2_\\texttt{traitice.unit}\\) is equivalent to the units variance in the single-response model had iciness been fitted31. Let’s call this \\(\\sigma^2_\\texttt{traity|ice.unit}\\) since it is the \\(\\texttt{units}\\) variance in \\(\\texttt{y}\\) after conditioning on \\(\\texttt{ice}\\). If, in the single-response model, the number of accidents had become underdispersed when adding \\(\\texttt{ice}\\), the best estimate of \\(\\sigma^2_\\texttt{traity|ice.unit}\\) is negative (Section 4.5). However, because covariance matrices are constrained to be positive-definite, \\(\\sigma^2_\\texttt{traity|ice.unit}\\) is constrained to be positive and we would see that the estimate of \\(\\sigma^2_\\texttt{traity|ice.unit}\\) is at the boundary (zero) and the residual correlation between the two responses (\\(\\sigma_\\texttt{traity:traitice.unit}/\\sigma_\\texttt{traity.unit}\\sigma_\\texttt{traitice.unit}\\)) is at 1 or -132. In practice this rarely happens, but when it does I would argue that it arises because the number of accidents, \\(\\texttt{y}\\), is incompatible with a Poisson distribution and alternative distribution should be sought (Section 4.5). 7.2 Multi-response Bernoulli Models As in single-response models, Bernoulli responses require some thought because the \\(\\texttt{units}\\) variance is not identifiable in the likelihood. However, the \\(\\texttt{units}\\) correlation between a Bernoulli response and other responses (including other Bernoulli responses) is identifiable. To explore multi-response Bernoulli models we will use longitudinal data collected on patients with primary biliary cirrhosis from the Mayo Clinic. The data are available from the \\(\\texttt{survival}\\) package data(pbc) head(pbcseq[, c(&quot;id&quot;, &quot;age&quot;, &quot;sex&quot;, &quot;ascites&quot;, &quot;bili&quot;, &quot;hepato&quot;)]) ## id age sex ascites bili hepato ## 1 1 58.76523 f 1 14.5 1 ## 2 1 58.76523 f 1 21.3 1 ## 3 2 56.44627 f 0 1.1 1 ## 4 2 56.44627 f 0 0.8 1 ## 5 2 56.44627 f 0 1.0 1 ## 6 2 56.44627 f 0 1.9 1 The data consist of 1945 records from 312 patients (\\(\\texttt{id}\\)). The age and sex of the patient were recorded and whether they suffered from ascites (\\(\\texttt{ascites}\\)) and hepatomegaly or enlarged liver (\\(\\texttt{hepato}\\)). The serum concentration of bilirunbin was also recorded (\\(\\texttt{bili}\\)). We will consider two multi-response models. 7.2.1 Bernoulli-Gaussian First, we will simultaneously model log(\\(\\texttt{bili}\\)) as Gaussian and \\(\\texttt{ascites}\\) as Bernoulli with threshold link (probit). As we will see, in multi-response models, modelling Bernoulli responses as \\(\\texttt{family=&quot;threshold&quot;}\\) has some nice advantages. prior.pbc1 &lt;- list(R = list(V = diag(2), nu = 1.002, fix = 2), G = F(2, 1000)) m.pbc1 &lt;- MCMCglmm(cbind(log(bili), ascites) ~ trait - 1 + trait:(age + sex), random = ~us(trait):id, rcov = ~us(trait):units, data = pbcseq, family = c(&quot;gaussian&quot;, &quot;threshold&quot;), prior = prior.pbc1, longer = 10) One difference from the previous model on car and holiday expenditure is that I’ve added some predictors to the fixed effect model. Interacting \\(\\texttt{trait}\\) with age+sex fits an \\(\\texttt{age}\\) effect for each trait and a \\(\\texttt{sex}\\) effect for each trait. If age+sex had not been interacted with \\(\\texttt{trait}\\) the effects of the predictors are assumed to be the same for the two traits. It is also possible to specify response-specific fixed-effect models using the function \\(\\texttt{at.level}\\). For example, \\(\\texttt{at.level(trait, 1):(age+sex)+at.level(trait, 2):(sex)}\\) would fit an age and sex effect for \\(\\texttt{trait}\\) 1 (log(\\(\\texttt{bili}\\))) and a sex effect for \\(\\texttt{trait}\\) 2 (\\(\\texttt{ascites}\\))33. The main difference, however, from the previous model of two Gaussian responses is that fix=2 has been added to the prior specification. For a \\(2\\times 2\\) covariance matrix this simply fixes the second variance (the \\(\\texttt{units}\\) variance for the Bernoulli trait, \\(\\texttt{ascites}\\)) at whatever is specified in \\(\\texttt{V}\\) (1 in this case)34. However, the \\(\\texttt{units}\\) variance for the Gaussian trait, and the \\(\\texttt{units}\\) covariance between the Gaussian and Bernoulii trait, are still estimated. summary(m.pbc1) ## ## Iterations = 30001:129901 ## Thinning interval = 100 ## Sample size = 1000 ## ## DIC: 4333.734 ## ## G-structure: ~us(trait):id ## ## post.mean l-95% CI u-95% CI eff.samp ## traitbili:traitbili.id 1.0291 0.8707 1.2147 810.6 ## traitascites:traitbili.id 0.6457 0.4465 0.8523 1000.0 ## traitbili:traitascites.id 0.6457 0.4465 0.8523 1000.0 ## traitascites:traitascites.id 0.8721 0.4709 1.2919 808.9 ## ## R-structure: ~us(trait):units ## ## post.mean l-95% CI u-95% CI eff.samp ## traitbili:traitbili.units 0.3197 0.2982 0.3421 912.9 ## traitascites:traitbili.units 0.2673 0.2205 0.3113 1000.0 ## traitbili:traitascites.units 0.2673 0.2205 0.3113 1000.0 ## traitascites:traitascites.units 1.0000 1.0000 1.0000 0.0 ## ## Location effects: cbind(log(bili), ascites) ~ trait - 1 + trait:(age + sex) ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## traitbili 1.371771 0.592765 2.075652 1000.0 &lt;0.001 *** ## traitascites -3.060259 -4.055639 -2.021530 1000.0 &lt;0.001 *** ## traitbili:age -0.004100 -0.016049 0.006663 1000.0 0.464 ## traitascites:age 0.027939 0.012933 0.042015 573.4 0.002 ** ## traitbili:sexf -0.446576 -0.851819 -0.069735 1000.0 0.032 * ## traitascites:sexf 0.033496 -0.468967 0.494346 1000.0 0.898 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can see that the covariance between the traits is strongly positive both across and within patients. Patients with constitutively high concentrations of bilirunbin are prone to ascites, and periods when a patient has high concentrations of bilirunbin they are more prone to ascites. Looking at the correlations can give a better feel for the magnitude of the associations. Figure 7.4: Posterior distributions for the correlations between log(\\(\\texttt{bili}\\)) and the presence of ascites (on the latent scale). In the model with two Gaussian responses we also thought about the association between the two traits in terms of regression. If we follow the same recipe for the Bernoulii response (\\(\\texttt{ascites}\\)) regressed on the Gaussian response (log(\\(\\texttt{bili}\\))) we end up with regression coefficients had we fitted log(\\(\\texttt{bili}\\)) as a predictor and also the true expected log(\\(\\texttt{bili}\\)) of each patient (which of course we don’t know, but constitute the \\(\\texttt{id}\\) effects for that trait). The change in the risk of \\(\\texttt{acites}\\) when increasing a patient’s log(\\(\\texttt{bili}\\)) by one unit is greater than the difference in risk between two patient’s that on average differ in log(\\(\\texttt{bili}\\)) by one unit (Figure 7.5). Figure 7.5: MCMC trace plot of the coefficient from a probit regression of ascites presence (\\(\\texttt{ascites}\\)) on log serum concentration of bilirunbin (\\(\\texttt{bili}\\)). The red and blue traces are from a model where the regression coefficient is estimated at two levels: within an individual (blue) and across individuals (red). Note that this conclusion is opposite to the one you might draw from looking at the correlations. Why? The reason is that the proportion of variation that is within patients (Section 4.4) is quite different for the two traits: for log(\\(\\texttt{bili}\\)) the posterior mean is 0.24 and for \\(\\texttt{acites}\\) it is 0.54. The within-patient effects for log(\\(\\texttt{bili}\\)) have proportionally little variation yet they still strongly correlate with the within-patient \\(\\texttt{acites}\\) effects which have proportionally greater variation. We can also more formally assess whether the within-patient (\\(\\texttt{units}\\)) regression is stronger than the between-patient regression: preg.id &lt;- m.pbc1$VCV[, &quot;traitbili:traitascites.id&quot;]/m.pbc1$VCV[, &quot;traitbili:traitbili.id&quot;] preg.units &lt;- m.pbc1$VCV[, &quot;traitbili:traitascites.units&quot;]/m.pbc1$VCV[, &quot;traitbili:traitbili.units&quot;] HPDinterval(preg.units - preg.id) ## lower upper ## var1 -0.02136848 0.4267755 ## attr(,&quot;Probability&quot;) ## [1] 0.95 The 95% credible interval overlaps zero, just. If we consider the opposite regression - the Gaussian response (log(\\(\\texttt{bili}\\))) on the Bernoulii response (\\(\\texttt{ascites}\\)) things aren’t quite as straight forward. In a single response-model the actual presence or not of \\(\\texttt{ascites}\\) would be fitted, yet in the multi response-model the association is at the level of the latent variable. As noted above for car accidents and insurance payouts, there is an important distinction between measuring an association directly on the data scale versus the latent scale. However, when Bernoulli models are conceptualised in terms of threshold models we can make this distinction disappear. If we designate the Gaussian response as \\(y\\) and the Bernoulli response as \\(x\\) (0 or 1) with associated latent variable \\(l\\), we can calculate the two expectations \\[E[y|x=1]=E[y | l&gt;0]\\] and \\[E[y|x=0]=E[y | l&lt;0]\\] The difference between these expectations is the coefficient you would obtain by fitting the Bernoulli variable (\\(\\texttt{ascites}\\)) as a predictor in a model for the Gaussian response (log(\\(\\texttt{bili}\\))). The difference is given by \\[E[y|x=1]-E[y|x=0]=\\frac{f_N(\\alpha)\\sigma^2_{y,l}}{F_N(\\alpha)(1-F_N(\\alpha))\\sigma_{l}}\\] where \\(f_N\\) and \\(F_N\\) are the density and cumulative density functions for the unit normal, and \\(\\alpha = \\mu_l/\\sigma_{l}\\)35. We can visualise what we have done. Figure 7.6 plots the 95% prediction interval for the latent variable associated with \\(\\texttt{ascites}\\) presence (x-axis) and log(\\(\\texttt{bili}\\)) (y-axis) assuming the mean is at the central black point (this is the posterior mean prediction for the data). Conditioning on the posterior mean estimates of \\({\\bf V}_{\\texttt{id}}\\) and \\({\\bf V}_{\\texttt{units}}\\), the predictive distribution is multivariate normal with covariance matrix \\({\\bf V}_{\\texttt{id}}+{\\bf V}_{\\texttt{units}}\\) and so the 95% prediction interval is an ellipse. When the latent variable exceeds zero (vertical dashed line) the \\(y=1\\) (\\(\\texttt{ascites}\\) is present). Consequently, the blue shaded area is the region of the predictive distribution for individuals with \\(\\texttt{ascites}\\) and the white region of the ellipse is for individuals without \\(\\texttt{ascites}\\). The expectation of \\(log(\\texttt{bili})\\) for these two groups are plotted as arrows. Figure 7.6: Representation of a Gaussian (\\(log(\\texttt{bili})\\)) and Bernoulli (\\(\\texttt{ascites}\\)) multi-response model. The ellipse is the 95% prediction interval for the Gaussian trait and the Bernoulli latent variable (on the probit scale). Observations to the right of the threshold (vertical dashed line) are successes and have \\(\\texttt{ascites}\\) and those to the left do not. The means of the Gaussian variable in these two groups are plotted as arrows. We can also plot the MCMC trace of the difference, and we can see that the difference is large and certainly not zero. Figure 7.7: MCMC trace plot of the regression coefficient of log serum concentration of bilirunbin (\\(\\texttt{bili}\\)) on ascites presence (\\(\\texttt{ascites}\\)) as obtained from a multi-response model. 7.2.2 All Bernoulli We can also consider a bivariate model of the two Bernoulli variables \\(\\texttt{ascites}\\) and \\(\\texttt{hepato}\\). The model set-up is almost identical, although we need to add the constraint that the residual variances of both traits are not identifiable in the likelihood but the residual correlation is: we need o restrict the residual covariance matrix to a residual correlation matrix. Replacing the \\(\\texttt{us}\\) variance structure with \\(\\texttt{corg}\\) achieves this. The prior specification only requires a degree-belief parameter \\(\\texttt{nu}\\) which results in a beta distribution for each correlation with shape and scale equal to \\((\\texttt{nu}-k+1)/2\\). Since \\(k=2\\), I set \\(\\texttt{nu}=3\\) which results in a flat prior for the correlation. Note that specifying the marginal prior F(2,1000) for the (co)variance matrix of \\(\\texttt{id}\\) effects also results in \\(\\texttt{nu}=3\\) (See Section 5.3.2). prior.pbc2 &lt;- list(R = list(V = diag(2), nu = 3), G = F(2, 1000)) m.pbc2 &lt;- MCMCglmm(cbind(hepato, ascites) ~ trait - 1 + trait:(age + sex), random = ~us(trait):id, rcov = ~corg(trait):units, data = pbcseq, family = c(&quot;threshold&quot;, &quot;threshold&quot;), prior = prior.pbc2, longer = 10) On the latent scale we see a reasonably strong correlation between the two outcomes: summary(m.pbc2) ## ## Iterations = 30001:129901 ## Thinning interval = 100 ## Sample size = 1000 ## ## DIC: ## ## G-structure: ~us(trait):id ## ## post.mean l-95% CI u-95% CI eff.samp ## traithepato:traithepato.id 1.8991 1.3529 2.525 1000 ## traitascites:traithepato.id 0.9516 0.6336 1.296 1000 ## traithepato:traitascites.id 0.9516 0.6336 1.296 1000 ## traitascites:traitascites.id 1.0250 0.5998 1.520 849 ## ## R-structure: ~corg(trait):units ## ## post.mean l-95% CI u-95% CI eff.samp ## traithepato:traithepato.units 1.0000 1.00000 1.0000 0 ## traitascites:traithepato.units 0.2441 0.08559 0.4155 1000 ## traithepato:traitascites.units 0.2441 0.08559 0.4155 1000 ## traitascites:traitascites.units 1.0000 1.00000 1.0000 0 ## ## Location effects: cbind(hepato, ascites) ~ trait - 1 + trait:(age + sex) ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## traithepato 0.377939 -0.699259 1.422358 1000 0.492 ## traitascites -3.051553 -4.090771 -1.907037 1000 &lt;0.001 *** ## traithepato:age 0.007085 -0.009985 0.023202 1000 0.408 ## traitascites:age 0.024262 0.007002 0.038508 1000 0.004 ** ## traithepato:sexf -0.577105 -1.112530 -0.013496 1000 0.044 * ## traitascites:sexf 0.118815 -0.391260 0.657897 1000 0.658 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If we wish, we can also characterise the relationship in terms of a contingency table. The model defines a multivariate normal distribution of latent variables around the fixed-effect prediction, and the probability of falling in any of the four quadrants defined by the origin can be calculated. We can imagine a population of individuals all of which have the same expected latent variables (the weighted average of the male and female means at the average age, where the weights are the frequencies of males and females in the data frame): mu &lt;- c(0, 0) nobs &lt;- nrow(pbcseq) # number of obseravtions mu[1] &lt;- mean(predict(m.pbc2, type = &quot;terms&quot;)[1:nobs]) # predicted mean for hepato latent variable mu[2] &lt;- mean(predict(m.pbc2, type = &quot;terms&quot;)[nobs + 1:nobs]) # predicted mean for ascites latent variable Around these means the latent variables for a particular individual at a particular time (as a vector are) \\({\\bf u}_i+{\\bf e}_{ij}\\). These vectors are a drawn from a multivariate normal with zero mean and (co)variance \\({\\bf V}_{\\texttt{id}}+{\\bf V}_{\\texttt{units}}\\): V &lt;- matrix(colMeans(m.pbc2$VCV[, 1:4] + m.pbc2$VCV[, 4 + 1:4]), 2, 2) # posterior mean covariance matrix Figure 7.8 gives a visual representation of this distribution, and the quadrants that correspond to different outcomes. Figure 7.8: Representation of a bivariate Bernoulli model. The solid point represents the means of the two Bernoulli latent variables and the ellipse is the 95% prediction interval for the latent variables. The pair of latent variable lie in one of the four quadrants corresponding to the observed outcome (white: hepato=0 and ascites=0, pink: 1/0, blue: 0/1 and purple: 1/1). The raw contingency table for the data is: prop.table(table(hepato = pbcseq$hepato, ascites = pbcseq$ascites)) ## ascites ## hepato 0 1 ## 0 0.49016481 0.01594896 ## 1 0.41998937 0.07389686 The \\(\\texttt{pmnorm}\\) function in the package \\(\\texttt{mnorm}\\) calculates the cumulative density function for the multivariate normal, and we can use this to calculate the four probabilities. I’ve done this in a rather long winded way so that the code could be extended to ordinal data that falls into more than two categories (Section 3.7): n &lt;- 2 m &lt;- 2 P &lt;- matrix(NA, n, m) thresh1 &lt;- c(-Inf, 0, Inf) thresh2 &lt;- c(-Inf, 0, Inf) for (i in 1:n) { for (j in 1:m) { lower &lt;- c(thresh1[i], thresh2[j]) upper &lt;- c(thresh1[i + 1], thresh2[j + 1]) P[i, j] &lt;- mnorm::pmnorm(lower, upper, mean = mu, sigma = V)$prob } } We don’t expect the predicted probabilities to perfectly match what is observed since the data are unbalanced (individuals vary in how many times they are observed) and individuals vary in their sex and age but our prediction is for a population of individuals that are in some way average. Nevertheless, the predicted probabilities are close: P ## [,1] [,2] ## [1,] 0.4335509 0.01500272 ## [2,] 0.4573340 0.09411244 7.3 Wide versus Long Format In the multi-response models we have fitted so far, all responses have been observed for every unit of observation. For example, there were no cases where a patient was assessed for \\(\\texttt{ascites}\\) but \\(\\texttt{bili}\\) was not measured. If there had been missing observations, these could have been recorded as \\(\\texttt{NA}\\) and \\(\\texttt{MCMCglmm}\\) will treat them as unknown observations to be sampled and averaged over. If the amount of missingness is low then this is generally not an issue. However, if there is a lot of missing data then updating the missing observations can be slow and result in poor mixing. In these cases it is usually best to store the data in long-format and remove the missing values 36. When fitting multi-response models to long-format data it needs to be in a format shown in Equation (7.1). There needs to be two columns in the data-frame: \\(\\texttt{family}\\) specifying the distribution for each observation and \\(\\texttt{trait}\\) which has factors indexing the observation type. In such cases the \\(\\texttt{family}\\) argument to \\(\\texttt{family}\\) should be \\(\\texttt{NULL}\\). In the next section we will fit a bivariate model to log(\\(\\texttt{bili}\\)) and the categorical variable \\(\\texttt{status}\\). \\(\\texttt{status}\\) has three levels indicating whether the final outcome for the patient was censored (0) had a liver transplant (1) or died (2). Let’s reshape the data and add the necessary columns: pbcseq_long &lt;- tidyr::pivot_longer(pbcseq, cols = c(status, bili), values_to = &quot;y&quot;, names_to = &quot;trait&quot;, cols_vary = &quot;slowest&quot;) # merge status and bili columns into a single column: y # trait is the column indicating if y belongs to status or bili pbcseq_long$trait&lt;-as.factor(pbcseq_long$trait) pbcseq_long$family &lt;- dplyr::recode(pbcseq_long$trait, bili = &quot;gaussian&quot;, status = &quot;threshold&quot;) # family specifies the distribution type for the two sets of observations. 7.4 Covariances between random and residual terms (\\(\\texttt{covu}\\)) In the previous section we generated a long-format data-frame \\(\\texttt{pbcseq_long}\\) with the response variable \\(\\texttt{y}\\) being associated with two traits: \\(\\texttt{status}\\) with three levels and then the continuous variable \\(\\texttt{bili}\\). In reality, \\(\\texttt{status}\\) is not a repeat-measure trait, it is the final outcome (censored, transplant or dead) duplicated across all of the patients records. Consequently we should only retain a single record: remove&lt;-with(pbcseq_long, duplicated(id) &amp; trait==&quot;status&quot;) # cols_vary = &quot;slowest&quot; in pivot_longer means status records are followed by bili # Then, `remove` is TRUE for all status observations except the first for each id. pbcseq_long&lt;-pbcseq_long[-which(remove),] # rows of data-frame for first patient subset(pbcseq_long[,c(&quot;y&quot;, &quot;trait&quot;, &quot;family&quot;, &quot;id&quot;)], id==1) ## # A tibble: 3 × 4 ## y trait family id ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2 status threshold 1 ## 2 14.5 bili gaussian 1 ## 3 21.3 bili gaussian 1 In addition lets log transform \\(\\texttt{bili}\\) and turn \\(\\texttt{status}\\) into a 2-level outcome dead (1) or not (0): pbcseq_long &lt;- pbcseq_long %&gt;% mutate(y = if_else(trait == &quot;bili&quot;, log(y), y)) pbcseq_long &lt;- pbcseq_long %&gt;% mutate(y = if_else(trait == &quot;status&quot;, as.numeric(y == 2), y)) If we wish to model death and \\(\\texttt{bili}\\) simultaneously, the Bernoulli-Gaussian model covered in Section 7.2.1 seems appropriate. However, since we only have a single-record of \\(\\texttt{status}\\) for each patient, it doesn’t make sense to fit \\(\\texttt{id}\\) effects for \\(\\texttt{status}\\) as they are not identifiable from the \\(\\texttt{unit}\\) effects (which themselves have non-identifiable variance because \\(\\texttt{status}\\) is Bernoulli). However, \\(\\texttt{bili}\\) is repeat-measure and so it does make sense to fit \\(\\texttt{id}\\) effects for this traits. In addition, allowing the \\(\\texttt{id}\\) effects for \\(\\texttt{bili}\\) to be correlated with the \\(\\texttt{unit}\\) effects of \\(\\texttt{status}\\) also seems reasonable - perhaps the long-term concentration of bilirunbin dictates whether a patient will live or not. In Section 5.2 we covered ways in which we could link two (or more) sets of random effects and estimate their covariance matrix. Here, we need to link a set of random effects with a set of residuals. \\(\\texttt{MCMCglmm}\\) allows the set of random effects appearing in the final random term of the random specification to be correlated with the set of residuals appearing in the first residual term of the rcov specification. The linking is specified by adding a covu=TRUE to the prior specification for the first residual term. prior.pbc_long&lt;-list(R=list(R1=list(V=diag(2),nu=3, covu=TRUE, fix=2), R2=IW(1, 0.002))) m.pbc_long&lt;-MCMCglmm(y~trait-1+at.level(trait, &quot;bili&quot;):(age+sex)-1+at.level(trait, &quot;status&quot;):sex, random=~us(at.level(trait, &quot;bili&quot;)):id, rcov=~us(at.level(trait, &quot;status&quot;)):id+us(at.level(trait, &quot;bili&quot;)):units, data=pbcseq_long, family=NULL, prior=prior.pbc_long) The model specification requires a bit of unpacking. trait-1 fits intercepts for both traits. at.level(trait, \"bili\"):(age+sex) fits an age effect and a sex effect to (log) \\(\\texttt{bili}\\) and at.level(trait, \"status\"):sex fits a sex effect for \\(\\texttt{status}\\). random=~us(at.level(trait, \"bili\")):id fits \\(\\texttt{id}\\) effects for \\(\\texttt{bili}\\). This works because at.level(trait, \"bili\") defines a vector of zero’s (if \\(\\texttt{trait}=\\texttt{status}\\)) and one’s (if \\(\\texttt{trait}=\\texttt{bili}\\)) for which random slopes are defined (see Chapter 6). Consequently, when \\(\\texttt{trait}=\\texttt{status}\\) the model is \\(0\\times u =0\\) and when \\(\\texttt{trait}=\\texttt{bili}\\) the model is \\(1\\times u =u\\), where \\(u\\) is the \\(\\texttt{id}\\) effect. Similarly, the first part of the residual specification us(at.level(trait, \"status\")):id defines a complementary set of residuals for \\(\\texttt{status}\\). This works because for \\(\\texttt{status}\\) there is only one observation per level of \\(\\texttt{id}\\) and so the specification satisfies the condition for a residual: the effect must be unique to an observation. Using us(at.level(trait, \"status\")):units would also have satisfied this condition, but the levels in the residual specification need to correspond to those in the random term for them to be linked, hence \\(\\texttt{id}\\) was used rather than \\(\\texttt{units}\\). Finally, we have a second residual component that defines the residuals for \\(\\texttt{bili}\\): us(at.level(trait, \"bili\")):units. The prior for the first residual term (\\(\\texttt{R1}\\)) contains the argument covu=TRUE indicating that covariances between the set of residuals and the set of random effects defined by the final random term are present. The prior for the resulting (\\(2\\times 2\\)) covariance matrix is also specified here and the prior for the final random term (the only random term in this model) is omitted. Note that the variance structure is that specified by the residual term (\\(\\texttt{us}\\) in this instance). We have fixed the second variance in this covariance matrix to one, since these are the \\(\\texttt{unit}\\) effects for a Bernoulli trait and so the variance isn’t identified. Using \\(\\texttt{nu=3}\\) places a flat prior on the correlation (Section 5.3.2) but the marginal prior for the variance of the \\(\\texttt{id}\\) effects on \\(\\texttt{bili}\\) is inverse-Wishart with \\(\\texttt{V=1.5}\\) and \\(\\texttt{nu=2}\\). Not ideal, but it is not possible to use parameter-expansion (Section 4.6) with \\(\\texttt{covu}\\) structures. summary(m.pbc_long) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 3890.949 ## ## G-structure: ~us(at.level(trait, &quot;bili&quot;)):id ## ## G-R structure below ## ## R-structure: ~us(at.level(trait, &quot;status&quot;)):id ## ## post.mean l-95% CI ## at.level(trait, &quot;bili&quot;).id:at.level(trait, &quot;bili&quot;).id 1.048 0.8714 ## at.level(trait, &quot;status&quot;).id:at.level(trait, &quot;bili&quot;).id 0.683 0.5624 ## at.level(trait, &quot;bili&quot;).id:at.level(trait, &quot;status&quot;).id 0.683 0.5624 ## at.level(trait, &quot;status&quot;).id:at.level(trait, &quot;status&quot;).id 1.000 1.0000 ## u-95% CI eff.samp ## at.level(trait, &quot;bili&quot;).id:at.level(trait, &quot;bili&quot;).id 1.2256 828.9 ## at.level(trait, &quot;status&quot;).id:at.level(trait, &quot;bili&quot;).id 0.7957 1132.5 ## at.level(trait, &quot;bili&quot;).id:at.level(trait, &quot;status&quot;).id 0.7957 1132.5 ## at.level(trait, &quot;status&quot;).id:at.level(trait, &quot;status&quot;).id 1.0000 0.0 ## ## ~us(at.level(trait, &quot;bili&quot;)):units ## ## post.mean l-95% CI ## at.level(trait, &quot;bili&quot;):at.level(trait, &quot;bili&quot;).units 0.319 0.2966 ## u-95% CI eff.samp ## at.level(trait, &quot;bili&quot;):at.level(trait, &quot;bili&quot;).units 0.3388 1000 ## ## Location effects: y ~ trait - 1 + at.level(trait, &quot;bili&quot;):(age + sex) - 1 + at.level(trait, &quot;status&quot;):sex ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## traitbili 2.22043 1.56302 2.86707 1112 &lt;0.001 *** ## traitstatus -0.20370 -0.34516 -0.04858 1000 0.006 ** ## at.level(trait, &quot;bili&quot;):age -0.01884 -0.02855 -0.00846 1000 0.004 ** ## at.level(trait, &quot;bili&quot;):sexf -0.56767 -0.93859 -0.22381 1093 &lt;0.001 *** ## sexm:at.level(trait, &quot;status&quot;) 0.73497 0.29862 1.16791 1000 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is a strong positive association between the expected long-term value of (log) \\(\\texttt{bili}\\) and whether a patient dies before the end of the study. Figure 7.9 shows the posterior distribution for the correlation in \\(\\texttt{id}\\) effects. Figure 7.9: Posterior distribution of the correlation between patient effects on (log) \\(\\texttt{bili}\\) and residual \\(\\texttt{status}\\) (death) from model \\(\\texttt{m.pbc_long}\\). A more usual way to fit this type of model is to use a single-response for death and simply fit the average of each patient’s (log) \\(\\texttt{bili}\\) as a predictor. pbcseq_status &lt;- subset(pbcseq_long, trait == &quot;status&quot;) pbcseq_status$mean.bili &lt;- with(pbcseq, tapply(log(bili), id, mean))[pbcseq_status$id] m.pbc_status &lt;- MCMCglmm(y ~ mean.bili, data = pbcseq_status, family = NULL, prior = list(R = list(V = 1, fix = 1))) In Figure 7.10 the regression coefficient from the multi-response model (in black) is compared to that from the single-response model (in red) and we can see that the multi-response model gives a regression coefficient that is greater in magnitude37. Figure 7.10: Posterior distributions for the probit regression of \\(\\texttt{status}\\) (death) on a patient’s (log) \\(\\texttt{bili}\\) value. The black trace is obtained from a multi-response model and the predictor is the expected value of log \\(\\texttt{bili}\\) for each patient, whereas the red trace is obtained from a single-response model and the predictor is the sample mean of log \\(\\texttt{bili}\\) for each patient. The reason for this pattern is that the coefficient in the single-response model is attenuated because of the measurement error in the expected (log) \\(\\texttt{bili}\\). If \\(r\\) is the correlation between the true value of a predictor and its measured value, we expect the regression using measured values to be equal to the regression obtained from the true values multiplied by \\(r^2\\) (see Chapter 9). However, the attenuation is small in this example because on average \\(\\texttt{bili}\\) has been measured 6 times per patient and the within-patient variability is quite low - the intra-class correlation for log \\(\\texttt{bili}\\) from the multi-response model is 0.77. Together the \\(r^2\\) is predicted to be 0.95 (\\(\\sigma^2_\\texttt{bili.id}/(\\sigma^2_\\texttt{bili.id}+\\sigma^2_\\texttt{bili.units}/6)\\)) - almost identical to the attenuation seen. In addition to dealing with the attenuation, the multi-response model also makes more efficient use of the data when the design is unbalanced (some patients only have a single measurement whereas others have up to 16) and the posterior standard deviation of the regression coefficient is 0.06 from the multi-response model but 0.08 from the standard single response model. 7.5 Scaled linear predictors: \\(\\texttt{theta_scale}\\) In the previous sections of this Chapter we’ve often thought about multi-response models in terms of regression. In some cases, the regression coefficient was different at different levels. For example, in Section 7.2.1 the regression within patients differed from the regression across patients. In other cases, the regression coefficient was only fitted for specific levels. For example, in Section 7.4 the regression was only fitted across patients not within. Although we parameterised the model in terms of unstructured covariance matrices (\\(\\texttt{us}\\)) we could have fitted the same model parameterised in terms of regression coefficients and innovation variances using antedependence structures (see Section 6.5). Sometimes, however, we would like a model where the regression is held constant across two or more levels but is allowed to be different (or zero) at others. The \\(\\texttt{theta_scale}\\) argument in \\(\\texttt{MCMCglmm}\\) allows two sets of parameters to be fitted that differ by a common scaling factor. For example, we may have two vectors of random effects \\(\\{{\\bf u}_1 {\\bf u}_2\\}\\) that appear in the linear predictor of one set of observations and would like to fit the effects \\(\\{\\theta_s{\\bf u}_1 \\theta_s{\\bf u}_2\\}\\) for another set of observations. By fitting the first set of effects for one trait (the predictor) and the second set of effects to the other trait (the response) we can hold the regression constant (\\(\\theta_s\\)) at the two levels defined by the random effects. # library(mlmRev) data(star) 7.6 Multinomial Models In the previous sections of this chapter we covered multi-response models where each response comes from a single parameter distribution. \\(\\texttt{MCMCglmm}\\) also allows some distributions that are multi-parameter such as the multinomial and a range of zero-flated distributions. The syntax for fitting these models is similar to that for multi-response models with \\(\\texttt{trait}\\) indexing the different parameters of the distribution and \\(\\texttt{units}\\) indexing the observation. We will start with multinomial models where the response is the number of counts in two or more nominal categories. When the number of categories is two we have the binomial which was covered in Section 3.6. In binomial models we condition on the total number of counts and the model is parameterised in terms of the probability of success, with failure being the base-line category. In multinomial models we also condition on the total number of counts and model the probability of belonging to each category compared to a base-line category. In both cases, it makes sense that the model is parameterised in terms of \\(K-1\\) comparisons since if we know \\(K-1\\) probabilities the final probability is known since the sum of all probabilities must equal one. As with binomial models, the number in each category can be passed as columns using cbind and then the base-line category is that associated with the final column. Alternatively, if the total number of counts is one, the response can be passed as a single vector of categories for each observation and then the base-line category is the first factor level of the response. In both cases, a logit-link is used. To explore multinomial models we will analyse data collected by the Alaska Science Center on the prey items of seabirds breeding on Middleton Island in the Gulf of Alaska - see here. We will restrict ourselves to data obtained on the chicks of the wonderful Tufted Puffin: data(tufted_puffin) head(tufted_puffin) ## year month Capelin Herring Pollock SandLance Other ## 1 1978 Jul 0 0 0 1 0 ## 2 1978 Aug 0 0 0 1 0 ## 3 1978 Aug 0 0 0 6 0 ## 4 1978 Aug 0 0 0 1 0 ## 5 1978 Aug 0 0 0 1 0 ## 6 1978 Aug 0 0 0 1 0 Each row corresponds to one of 565 samples with their collection year and month recorded. The following four columns are the number of Capelin (\\(\\texttt{Capelin}\\)), Pacific herring (\\(\\texttt{Herring}\\)), Walleye pollock (\\(\\texttt{Pollock}\\)) and Pacific sand lance (\\(\\texttt{SandLance}\\)) in each sample. An additional 40 prey taxa were also recorded and their combined counts per sample are in the final column (\\(\\texttt{Other}\\)). Figure 7.11 plots the number of prey items for 150 random samples. Figure 7.11: Number of prey items of different types in 150 randomly selected samples from Tufted Puffin chicks. The tick-marks on the x-axis delimit samples taken in the same year, and within years, samples are ordered by the proportion of prey items that are Pacific sand lance. In terms of numbers, Capelin and the Pacific sand lance are the most common prey items, with ‘Other’ being the rarest colMeans(tufted_puffin[, -c(1:2)]) ## Capelin Herring Pollock SandLance Other ## 1.7238938 1.1522124 0.6141593 1.6300885 0.5150442 However, there is tremendous variation in the total number of prey items per sample and it looks as if those samples with many prey items are dominated by Capelin. The multinomial ignores (conditions on) the total number of prey items and essentially works with the proportions of each prey item, as shown in Figure 7.12 for the 150 random samples. Figure 7.12: Proportion of prey items that are of each type in 150 randomly selected samples from Tufted Puffin chicks. The tick-marks on the x-axis delimit samples taken in the same year, and within years, samples are ordered by the proportion of prey items that are Pacific sand lance. Once we control for the total number of prey items we see that Capelin actually makes up the lowest proportion of the diet and Pacific sand lance dominates: total_counts &lt;- rowSums(tufted_puffin[, -c(1:2)]) colMeans(tufted_puffin[, -c(1:2)]/total_counts) ## Capelin Herring Pollock SandLance Other ## 0.1200594 0.1316400 0.1449124 0.4062647 0.1971235 It is also apparent from Figure 7.12 (and Figure 7.11) that samples from the same year seem to have very similar compositions. Although less obvious, it also looks like there will be overdispersion - even within years, samples seem to vary in composition more than you would expect from multinomial sampling alone. In Figure 7.13 I have sampled from the multinomial with the probabilities given above and the observed total number of prey items for each of the 150 samples plotted. It is clear that the simulated data are much less structured than the real data shown in Figure 7.12. Figure 7.13: Simulated proportion of prey items that are of each type assuming that the probability of a prey item is constant over the samples. The tick-marks on the x-axis delimit samples taken in the same year, and within years, samples are ordered by the proportion of prey items that are Pacific sand lance. Let’s fit a multinomial model to these data, using \\(\\texttt{Other}\\) as the base-line category. We will also fit fully unstructured, \\(4\\times 4\\), covariance matrices at the level of year and sample (\\(\\texttt{unit}\\)): prior.tufted &lt;- list(R = IW(1, 2), G = F(2, 1000)) m.tufted &lt;- MCMCglmm(cbind(Capelin, Herring, Pollock, SandLance, Other) ~ trait - 1, data = tufted_puffin, random = ~us(trait):year, rcov = ~us(trait):units, family = &quot;multinomial&quot;, longer = 20, prior = prior.tufted, pr = TRUE) With two \\(4\\times 4\\) covariance matrices, the model summary is pretty overwhelming. To get a feel for the broad patterns, I have summarised the two matrices in terms of correlations (upper-triangle), variances (diagonals) and covariances (lower-triangle) in Table 7.1. The posterior means together with the 95% credible intervals (in square brackets) are reported. Table 7.1: Posterior mean and 95% credible intervals for the variances (diagonal), covariances (lower-triangle) and correlations (upper-triangle) in year effects (top) and sample effects (bottom) from model \\(\\texttt{m.tufted}\\) Capelin Herring Pollock SandLance \\({\\bf V}_\\texttt{year}\\) Capelin 63.63 [13.58-141.35] 0.09 [-0.40-0.55] 0.21 [-0.29-0.64] -0.13 [-0.54-0.32] Herring 3.95 [-21.33-31.63] 38.06 [10.77-79.25] -0.16 [-0.57-0.29] 0.37 [-0.02-0.73] Pollock 5.90 [-12.16-20.04] -3.53 [-14.44-7.44] 12.79 [3.18-26.06] -0.23 [-0.61-0.18] SandLance -4.21 [-21.24-10.62] 9.66 [-2.66-25.76] -3.39 [-10.99-3.62] 17.54 [6.09-32.76] \\({\\bf V}_\\texttt{units}\\) Capelin 60.09 [22.95-99.82] -0.15 [-0.60-0.24] -0.09 [-0.60-0.42] -0.02 [-0.46-0.36] Herring -4.53 [-18.84-8.02] 17.13 [9.46-26.71] -0.01 [-0.45-0.44] 0.68 [0.48-0.84] Pollock -1.27 [-9.11-7.63] 0.08 [-3.67-4.12] 4.41 [1.72-7.23] 0.02 [-0.51-0.50] SandLance -0.38 [-12.84-13.70] 11.30 [5.72-18.83] 0.30 [-4.12-4.43] 15.85 [9.02-23.29] There seems to be substantial variation, both between years and between samples, in the linear predictors for each prey item. Although credible intervals are wide, the generally pattern seems to be that the between year variation exceeds the between sample variation. Correlations between the linear predictors for different prey items also have wide credible intervals, but in general they seem fairly small. The exception to this is between the \\(\\texttt{Herring}\\) and \\(\\texttt{SandLance}\\) predictors, where the correlation is moderately positive, particularly at the level of samples (\\(\\texttt{units}\\)). Does the model fit the data? We can generate a draw from the posterior predictive distribution for the specific years sampled using the predict function and specifying \\(\\texttt{marginal=NULL}\\): sim_tufted &lt;- simulate(m.tufted, marginal = NULL) This is a vector of counts of length \\(n(K-1)\\) where the first \\(n\\) values are the counts (in this example) for Capelin and the second set of \\(n\\) values are for Herring and so on. The counts for \\(\\texttt{Other}\\) are not returned but can be obtained by taking the difference between the total number of counts for each observation and the sum of the counts across the other \\(K-1\\) categories. If we plot the quantiles of the observed number of counts against the quantiles of the posterior prediction for the number of counts we see a very strong 1:1 relationship (Figure 7.14). Figure 7.14: qq-plot of prey item counts from a posterior predictive simulation of model \\(\\texttt{m.tufted}\\) (x-axis) versus the empirical quantiles (y-axis). However, this is rather misleading. The strong relationship may simply be driven by variation in the total number of counts per sample - something that is not modelled, but conditioned on. For this reason, I usually focus on the proportions when assessing the adequacy of the model. The qq-plot of the proportions shows a slightly noisier relationship, but still, it looks like the the marginal distribution of the data is captured well by the model (Figure 7.15). Figure 7.15: qq-plot of prey item proportions from a posterior predictive simulation of model \\(\\texttt{m.tufted}\\) (x-axis) versus the empirical quantiles (y-axis). While these qq-plots are good for assessing whether the model captures broad aspects of the data distribution, they don’t really provide insight as to whether the finer patterns have been captured well - for example, if changes in the composition of the samples over time has been adequately modelled. In Figure 7.16 I have plotted the proportion of each prey item for 150 random chicks as obtained from the posterior simulation (top) and compared it to the actual diet compositions (bottom). We can see, at least qualitatively, that the model seems to be picking up the structuring due to year rather well. Within years, we do not expect the posterior simulation to match the observation perfectly, since we are drawing a sample for a new chick within that year (the predict function always marginalises residual \\(\\texttt{unit}\\) effects). However, if the model is doing a good job the distribution of compositions within years should look reasonable. Although hard to judge, there appears to be no major discrepancies - the model does predict that samples tend to be dominated by a single prey item even if the dominating prey item varies to some degree across samples within a year. Figure 7.16: A posterior predictive simulation (top) using model \\(\\texttt{m.tufted}\\) for the proportion of prey items that are of each type in 150 randomly selected samples. On the bottom are the observed proportion of prey items. The tick-marks on the x-axis delimit samples taken in the same year, and within years, samples are ordered by the proportion of prey items that are Pacific sand lance. Note that \\(\\texttt{unit}\\) effects are marginalised in the posterior predictive simulation and so we do not expect an exact correspondence between samples: only the distributions within and across years should match. While the pictures are pretty, we need to go beyond them to get a full understanding of what the model is telling us. With \\(K=5\\) categories we have \\(K-1=4\\) latent variables (traits) that are the log-odds ratio of observing a particular prey item versus the base-line prey item, in this case \\(\\texttt{Other}\\). Abbreviating the prey types by their initial, the latent variables for sample \\(i\\) in year \\(j\\) are \\[ \\begin{array}{rcl} l^{(\\texttt{C-O})}_{ij} =&amp; \\textrm{log}\\left(\\frac{Pr(\\texttt{C}_{ij})}{Pr(\\texttt{O}_{ij})}\\right)&amp;= \\beta^{(\\texttt{C-O})}_0+u^{(\\texttt{C-O})}_{j}+e^{(\\texttt{C-O})}_{ij}\\\\ l^{(\\texttt{H-O})}_{ij} =&amp; \\textrm{log}\\left(\\frac{Pr(\\texttt{H-O}_{ij})}{Pr(\\texttt{O}_{ij})}\\right)&amp;= \\beta^{(\\texttt{H-O})}_0+u^{(\\texttt{H-O})}_{j}+e^{(\\texttt{H-O})}_{ij}\\\\ l^{(\\texttt{P-O})}_{ij} =&amp; \\textrm{log}\\left(\\frac{Pr(\\texttt{P}_{ij})}{Pr(\\texttt{O}_{ij})}\\right)&amp;= \\beta^{(\\texttt{P-O})}_0+u^{(\\texttt{P-O})}_{j}+e^{(\\texttt{P-O})}_{ij}\\\\ l^{(\\texttt{S-O})}_{ij} =&amp; \\textrm{log}\\left(\\frac{Pr(\\texttt{S}_{ij})}{Pr(\\texttt{O}_{ij})}\\right)&amp;= \\beta^{(\\texttt{S-O})}_0+u^{(\\texttt{S-O})}_{j}+e^{(\\texttt{S-O})}_{ij}\\\\ \\end{array} \\] where the \\(\\beta_0\\) are trait-specific intercepts, the \\(u\\) are trait-specific year effects and and the \\(e\\) are trait-specific sample (\\(\\texttt{unit}\\)) effects that capture overdispersion. The effects for each trait are really comparisons against the base-line category, \\(\\texttt{Other}\\), and I have used a notation that makes this explicit, since it is easy to forget. Nevertheless, it needs to be kept in mind. For example, let’s imagine that Capelin and Herring appear in the diet completely independently of each other and so knowing there are many Capelin in a particular year is not informative about the abundance of Herring. It might then be tempting to assume that two sets of year effects, \\(u^{(\\texttt{C-O})}\\) and \\(u^{(\\texttt{H-O})}\\) are uncorrelated. However, they may be correlated through their shared dependence on \\(\\texttt{Other}\\) prey items. To understand this, I often find it easier to think about the multinomial as a multivariate Poisson which is then followed by normalisation. The ratio of the probabilities, for example of Capelin versus \\(\\texttt{Other}\\), is also equal to the ratio of expected counts: \\[\\frac{Pr(\\texttt{C}_{ij})}{Pr(\\texttt{O}_{ij})} = \\frac{n_{ij}Pr(\\texttt{C}_{ij})}{n_{ij}Pr(\\texttt{O}_{ij})}=\\frac{E[n^{(\\texttt{C})}_{ij}]}{E[n^{(\\texttt{0})}_{ij}]}\\] where \\(n_{ij}\\) is the total number of counts across all categories: \\(n_{ij}=n^{(\\texttt{C})}_{ij}+n^{(\\texttt{H})}_{ij}+n^{(\\texttt{P})}_{ij}+n^{(\\texttt{S})}_{ij}+n^{(\\texttt{O})}_{ij}\\). Sine a Poisson GLM with log-link involves linear models of the log expectation, we can interpret each parameter in the multinomial model as the difference in parameters from a bivariate Poisson model. For example, if we have log-linear models for the Capelin and \\(\\texttt{Other}\\) counts: \\[ \\textrm{log}\\left(E[n^{(\\texttt{C})}_{ij}]\\right) = \\beta^{(\\texttt{C})}_0+u^{(\\texttt{C})}_{j}+e^{(\\texttt{C})}_{ij}\\\\ \\] \\[ \\textrm{log}\\left(E[n^{(\\texttt{O})}_{ij}]\\right) = \\beta^{(\\texttt{O})}_0+u^{(\\texttt{O})}_{j}+e^{(\\texttt{O})}_{ij}\\\\ \\] then the parameters of the multinomial model are \\(\\beta^{(\\texttt{C-O})}_0 = \\beta^{(\\texttt{C})}_0-\\beta^{(\\texttt{O})}_0\\), \\(u^{(\\texttt{C-O})}_{j}=u^{(\\texttt{C})}_{j}-u^{(\\texttt{O})}_{j}\\) and \\(e^{(\\texttt{C-O})}_{ij}=e^{(\\texttt{C})}_{ij}-e^{(\\texttt{O})}_{ij}\\). The assumption that the abundance of Capelin in a particular year is independent of the abundance of Herring implies that \\(COV(u^{(\\texttt{C})}, u^{(\\texttt{H})})=0\\). However, the covariance of the year effects in the multinomial is (for Capelin and Herring): \\[ \\begin{array}{rl} COV(u^{(\\texttt{C-O})}, u^{(\\texttt{H-O})})=&amp;COV(u^{(\\texttt{C})}-u^{(\\texttt{O})}, u^{(\\texttt{H})}-u^{(\\texttt{O})})&amp; \\\\ =&amp;COV(u^{(\\texttt{C})}, u^{(\\texttt{H})})-COV(u^{(\\texttt{C})}, u^{(\\texttt{O})})-COV(u^{(\\texttt{H})}, u^{(\\texttt{O})}) +VAR(u^{(\\texttt{O})})&amp; \\\\ \\end{array} \\] and would remain non-zero even if \\(COV(u^{(\\texttt{C})}, u^{(\\texttt{H})})=0\\). If the abundances of Capelin, Herring and \\(\\texttt{Other}\\) are all independent, the covariance simplifies to \\(VAR(u^{(\\texttt{O})})\\) reflecting the shared dependence of the two log-odds on the base-line category, \\(\\texttt{Other}\\). Similarly, the variance of year effects in the multinomial is (for Capelin): \\[ \\begin{array}{rl} VAR(u^{(\\texttt{C-O})})=&amp;VAR(u^{(\\texttt{C})})+VAR(u^{(\\texttt{O})})-2COV(u^{(\\texttt{C})}, u^{(\\texttt{O})})\\\\ \\end{array} \\] Even if the abundance of Capelin and \\(\\texttt{Other}\\) are independent across years, the variance in the log-odds depends on the year variance of both categories: \\(VAR(u^{(\\texttt{C})})+VAR(u^{(\\texttt{O})})\\). When we fitted multi-response models previously we naturally assumed that a diagonal covariance matrix somehow represents a null - the two traits are independent of each other. However, the above results suggest that we may need to modify our null when thinking about multinomial models. If we think that the log abundances of different prey items are independent across years but have the same yearly variation, then our null for the covariance matrix of multinomial year effects would have \\(2\\sigma^2_{u}\\) along the diagonal and \\(\\sigma^2_{u}\\) on the off-diagonals. Looking at the estimates when we fitted a fully unstructured covariance matrix for the year effects (Table 7.1) suggest this model may not perform that well. While the between-year variances have large credible intervals, and so a common variance of around 20 may not be too bad, a common covariance of 10 (correlation of 0.5) looks too large. Nevertheless, we can fit this model and assess the fit. We can represent the covariance structure of the year effects in this model as \\({\\bf V}_{\\texttt{year}} = \\sigma^2_{u}({\\bf I}+{\\bf J})\\) where \\({\\bf I}\\) and \\({\\bf J}\\) are \\(4\\times 4\\) identity and unit matrices, respectively38. To fit a year covariance matrix of this form we can use an \\(\\texttt{idv}\\) variance structure with model formula trait+at.set(trait, 1:(K-1))39: m.tuftedb &lt;- MCMCglmm(cbind(Capelin, Herring, Pollock, SandLance, Other) ~ trait - 1, data = tufted_puffin, random = ~idv(~trait + at.set(trait, 1:4)):year, rcov = ~us(trait):units, family = &quot;multinomial&quot;, longer = 20, prior = prior.tufted, pr = TRUE) The posterior mean and 95% credible interval of \\(\\sigma^2_u\\) is 12.94 [7.40-19.62]. In the fully unstructured model (\\(\\texttt{m.tufted}\\)) the average of the four between-year variances halved was 16.5 and the average of the six between-year covariances was 2.4 (see Table 7.1). The estimate of \\(\\sigma^2_u\\) can be seen as a compromise estimate of these ten parameters. To assess whether this simpler model does a good job at capturing the differences between years we can decide on a suitable metric and compare the metric calculated on the observed data with that calculated on posterior predictive simulations. One possible metric would be to calculate the mean proportions of Capelin, Herring, Pollock and Sand lance across samples for each year and then take the variance in these year means for each prey item or the correlation between year means for pairs of prey items. Relating this metric to the parameters of the model is not straight forward since the sample means for each year will, in part, reflect between sample variability, particularly in years where replication is low. In addition, the model works with log-probabilities, yet the described metric works with raw proportions since these are often zero and logging isn’t an option. Despite the metric’s limitations, I would still expect it to reveal major problems with the specified year effects. Figure 7.17: The mean proportion of each prey item across samples in a given year were calculated for one thousand posterior predictive simulations from model \\(\\texttt{m.tuftedb}\\). This histograms represent the posterior predictive distributions for the variances of the yearly means (diagonal) or the correlations in yearly means between prey items. The red lines are the values calculated from the data. The posterior predictive simulations suggest that the model is not too disastrous. The posterior distributions for the covariances are centred a little above the observed values, and the posterior distributions for the variances are concentrated a little below the observed values. Nevertheless, the discrepancies are not extreme, and even for Capelin, the posterior predictive distribution includes higher variances than that observed. We can also fit a model which retains the assumption of independence but allows the amount of yearly variation in abundance to differ across categories. The (co)variance in multinomial year effects would then be \\({\\bf V}_{\\texttt{year}} = {\\bf D}+\\sigma^2_{\\texttt{O}}{\\bf J}\\) where \\({\\bf D}\\) is a diagonal matrix with four variances to be estimated in addition to \\(\\sigma^2_{\\texttt{O}}\\). This can be achieved using the model structure idh(trait):year+year. The first term fits independent effects for each of the four traits but allows them to have different variances which can interpreted as \\(VAR(u^{(\\texttt{C})})\\), \\(VAR(u^{(\\texttt{H})})\\), \\(VAR(u^{(\\texttt{P})})\\) and \\(VAR(u^{(\\texttt{S})})\\) from the Poisson model. The second term fits a year effect common to all traits, which can interpreted as \\(VAR(u^{(\\texttt{O})})\\) from the Poisson model. m.tuftedc &lt;- MCMCglmm(cbind(Capelin, Herring, Pollock, SandLance, Other) ~ trait - 1, data = tufted_puffin, random = ~idh(trait):year + year, rcov = ~us(trait):units, family = &quot;multinomial&quot;, longer = 20, prior = prior.tufted, pr = TRUE) The posterior means and 95% credible intervals for the year variances are displayed in table 7.2. Again, the credible intervals are wide and a common variance of around 20 may not be too bad for all categories except \\(\\texttt{Other}\\). The yearly variance for \\(\\texttt{Other}\\) is considerably smaller, which is to be expected since this represents the covariance between year effects in the fully unstructured model which were consistently less than half the variances. A reduced between-year variance for the \\(\\texttt{Other}\\) category seems biologically reasonable: since this category lumps 40 prey items which are likely to show some degree of independence in their abundances across years. Table 7.2: Posterior mean and 95% credible intervals for the variances from model \\(\\texttt{m.tuftedc}\\). Note that the variance labelled \\(\\texttt{Other}\\) is the variance of the main year effects fitted as ~year. Posterior Mean l-95% CI u-95% CI Capelin 57.43 12.65 123.53 Herring 29.45 5.10 62.91 Pollock 11.06 2.72 23.11 SandLance 14.02 5.12 27.91 Other 1.55 0.00 5.32 If we perform the same posterior predictive check that we did for the previous model we can see it does a good job (Figure 7.18). Figure 7.18: The mean proportion of each prey item across samples in a given year were calculated for one thousand posterior predictive simulations from model \\(\\texttt{m.tuftedc}\\). This histograms represent the posterior predictive distributions for the variances of the yearly means (diagonal) or the correlations in yearly means between prey items. The red lines are the values calculated from the data. We could also consider a similar model for the sample \\(\\texttt{unit}\\) effects: m.tuftedd &lt;- MCMCglmm(cbind(Capelin, Herring, Pollock, SandLance, Other) ~ trait - 1, data = tufted_puffin, random = ~idh(trait):year + year + units, rcov = ~idh(trait):units, family = &quot;multinomial&quot;, longer = 20, prior = prior.tufted, pr = TRUE) The magnitude of the units variances show similar patterns to the year variances, with \\(\\texttt{Other}\\) being small and Capelin being large (Table 7.3). Table 7.3: Posterior mean and 95% credible intervals for the variances from model \\(\\texttt{m.tuftedd}\\). Note that the variance labelled \\(\\texttt{Other}\\) is the variance of the main year and unit effects fitted as ~year and ~units respectively. year units Capelin 75.95 [15.70-162.35] 82.48 [34.52-137.49] Herring 33.22 [7.32-73.18] 11.29 [5.42-17.93] Pollock 12.45 [3.17-25.32] 4.28 [0.62-8.42] SandLance 12.95 [3.90-25.01] 9.78 [5.41-15.48] Other 1.69 [0.00-5.75] 3.07 [0.00-6.06] Since model \\(\\texttt{m.tuftedd}\\) simplifies the covariance structure of the unit effects compared to previous models, it makes sense to assess the fit of the model by looking at unit-level patterns. To do this, we can simply calculate the proportion of each prey item in a sample and then calculate the variance in these proportions , or the correlation in proportions for pairs of prey items. Figure 7.19 plots the posterior predictive distributions of these quantities and compares them to their observed values. Figure 7.19: The histograms represent the posterior predictive distributions for the variances of the proportion of each prey item in a sample (diagonal) or the correlations in the proportion of prey items in a sample. The red lines are the values calculated from the data. Generally, the model seems to predict these quantities well, although the correlation between Herring and Sand lance is at the extreme end of that seen in the posterior predictive simulations. This is evidenced in the fully unstructured model where the correlation is estimated to be more positive than all other correlations (Table 7.1). 7.7 Zero-inflated Models Each datum in a zero-inflated model is associated with two latent variables. The first latent variable is associated with the named distribution and the second latent variable is associated with zero inflation. I’ll work through a zero-inflated Poisson (ZIP) model to make things clearer. As the name suggests, a ZIP distribution is a Poisson distribution with extra zero’s. The observed zeros are modelled as a mixture distribution of zero’s originating form the Poisson process and zero’s arising through zero-inflation. It is the probability (on the logit scale) that a zero is from the zero-inflation process that we aim to model with the second latent variable. The likelihood has the form: \\[\\begin{array}{rl} Pr(y=0) =&amp; \\texttt{plogis}(l_{2})+\\texttt{plogis}(-l_{2})\\ast \\texttt{dpois}(0, \\texttt{exp}(l_{1}))\\\\ Pr(y | y&gt;0) =&amp; \\texttt{plogis}(-l_{2})\\ast \\texttt{dpois}(y, \\texttt{exp}(l_{1}))\\\\ \\end{array}\\] \\(\\texttt{pscl}\\) fits zero-inflated models very well through the zeroinfl function, and I strongly recommend using it if you do not want to fit random effects. To illustrate the syntax for fitting ZIP models in MCMCglmm I will take one of their examples: data(&quot;bioChemists&quot;, package = &quot;pscl&quot;) head(bioChemists) ## art fem mar kid5 phd ment ## 1 0 Men Married 0 2.52 7 ## 2 0 Women Single 0 2.05 6 ## 3 0 Women Single 0 3.75 6 ## 4 0 Men Married 1 1.18 3 ## 5 0 Women Single 0 3.75 26 ## 6 0 Women Married 2 3.59 2 art is the response variable - the number of papers published by a Ph.D student - and the remaining variables are to be fitted as fixed effects. Naively, we may expect zero-inflation to be a problem given 30% of the data are zeros, and based on the global mean we only expect around 18%. table(bioChemists$art == 0) ## ## FALSE TRUE ## 640 275 ppois(0, mean(bioChemists$art)) ## [1] 0.1839859 As with binary models we do not observe any residual variance for the zero-inflated process, and in addition the residual covariance between the zero-inflation and the Poisson process cannot be estimated because both processes cannot be observed in a single data point. To deal with this I’ve fixed the residual variance for the zero-inflation at 1, and the covariance is set to zero using the idh structure. Setting V=diag(2) and nu=0.002 we have the inverse-gamma prior with shape=scale=0.001 for the residual component of the Poisson process which captures overdispersion: prior.m5d.1 = list(R = list(V = diag(2), nu = 0.002, fix = 2)) m5d.1 &lt;- MCMCglmm(art ~ trait - 1 + at.level(trait, 1):fem + at.level(trait, 1):mar + at.level(trait, 1):kid5 + at.level(trait, 1):phd + at.level(trait, 1):ment, rcov = ~idh(trait):units, data = bioChemists, prior = prior.m5d.1, family = &quot;zipoisson&quot;) As is often the case the parameters of the zero-inflation model mixes poorly (See Figure 7.20 especially when compared to equivalent hurdle models (See Section 7.8). Poor mixing is often associated with distributions that may not be zero-inflated but instead overdispersed. Figure 7.20: Posterior distribution of fixed effects from model m5d.1 in which trait 1 (\\(\\texttt{art}\\)) is the Poisson process and trait 2 (\\(\\texttt{zi.art}\\)) is the zero-inflation. The model would have to be run for (much) longer to say something concrete about the level of zero-inflation but my guess would be it’s not a big issue, given the probability is probably quite small: quantile(plogis(m5d.1$Sol[, 2]/sqrt(1 + c2))) ## 0% 25% 50% 75% 100% ## 0.004678307 0.009205072 0.010996518 0.014144546 0.058293082 7.7.1 Posterior predictive checks Another useful check is to fit the standard Poisson model and use posterior predictive checks to see how many zero’s you would expect under the simple model: prior.m5d.2 = list(R = list(V = diag(1), nu = 0.002)) m5d.2 &lt;- MCMCglmm(art ~ fem + mar + kid5 + phd + ment, data = bioChemists, prior = prior.m5d.2, family = &quot;poisson&quot;, saveX = TRUE) nz &lt;- 1:1000 oz &lt;- sum(bioChemists$art == 0) for (i in 1:1000) { pred.l &lt;- rnorm(915, (m5d.2$X %*% m5d.2$Sol[i, ])@x, sqrt(m5d.2$VCV[i])) nz[i] &lt;- sum(rpois(915, exp(pred.l)) == 0) } Figure 7.21 shows a histogram of the posterior predictive distribution of zero’s (nz) from the model compared to the observed number of zeros (oz). The simpler model seems to be consistent with the data, suggesting that a ZIP model may not be required. Figure 7.21: Posterior predictive distribution of zeros from model m5d.2 with the observed number in red. 7.8 Hurdle Models Hurdle models are very similar to zero-inflated models but they can be used to model zero-deflation as well as zero-inflation and seem to have much better mixing properties in \\(\\texttt{MCMCglmm}\\). As in ZIP models each datum in the hurdle model is associated with two latent variables. However, whereas in a ZIP model the first latent variable is the mean parameter of a Poisson distribution the equivalent latent variable in the hurdle model is the mean parameter of a zero-truncated Possion distribution (i.e. a Poisson distribution without the zeros observed). In addition the second latent variable in a ZIP model is the probability that an observed zero is due to zero-inflation rather than the Poisson process. In hurdle models the second latent variable is simply the probability (on the logit scale) that the response variable is zero or not. The likelihood is: \\[\\begin{array}{rl} Pr(y=0) =&amp; \\texttt{plogis}(l_{2})\\\\ Pr(y | y&gt;0) =&amp; \\texttt{plogis}(-l_{2})\\ast \\texttt{dpois}(y, \\texttt{exp}(l_{1}))/(1-\\texttt{ppois}(0, \\texttt{exp}(l_{1})))\\\\ \\end{array}\\] To illustrate, we will refit the ZIP model (m5d.1) as a hurdle-Poisson model. m5d.3 &lt;- MCMCglmm(art ~ trait - 1 + at.level(trait, 1):fem + at.level(trait, 1):mar + at.level(trait, 1):kid5 + at.level(trait, 1):phd + at.level(trait, 1):ment, rcov = ~idh(trait):units, data = bioChemists, prior = prior.m5d.1, family = &quot;hupoisson&quot;) Plotting the Markov chain for the equivalent parameters that were plotted for the ZIP model shows that the mixing properties are much better (compare Figure 7.20 with Figure 7.22). Figure 7.22: Posterior distribution of fixed effects from model m5d.3 in which trait 1 (\\(\\texttt{art}\\)) is the zero-truncated Poisson process and trait 2 (\\(\\texttt{hu.art}\\)) is the binary trait zero or non-zero. The interpretation of the model is slightly different. Fitting just an intercept in the hurdle model implies that the proportion of zeros observed across different combinations of those fixed effects fitted for the Poisson process is constant. Our 95% credible intervals for this proportion is (See section ??): c2 &lt;- (16 * sqrt(3)/(15 * pi))^2 HPDinterval(plogis(m5d.3$Sol[, 2]/sqrt(1 + c2))) ## lower upper ## var1 0.2669255 0.3255151 ## attr(,&quot;Probability&quot;) ## [1] 0.95 and we can compare this to the predicted number of zero’s from the Poisson process if it had not been zero-truncated: HPDinterval(ppois(0, exp(m5d.3$Sol[, 1] + 0.5 * m5d.3$VCV[, 1]))) ## lower upper ## var1 0.1359227 0.3528413 ## attr(,&quot;Probability&quot;) ## [1] 0.95 The credible intervals largely overlap, strongly suggesting a standard Poisson model would be adequate. However, our prediction for the number of zero’s that would arise form a non-truncated Poisson process only involved the intercept term. This prediction therefore pertains to the number of articles published by single women with no young children who obtained their Ph.D’s from departments scoring zero for prestige (phd) and whose mentors had published nothing in the previous 3 years. Our equivalent prediction for men is a little lower HPDinterval(ppois(0, exp(m5d.3$Sol[, 1] + m5d.3$Sol[, 3] + 0.5 * m5d.3$VCV[, 1]))) ## lower upper ## var1 0.09385229 0.2910297 ## attr(,&quot;Probability&quot;) ## [1] 0.95 suggesting that perhaps the number of zero’s is greater than we expected for this group. However, this may just be a consequence of us fixing the proportion of zero’s to be constant across these groups. We can relax this assumption by fitting a separate term for the proportion of zeros for men: m5d.4 &lt;- MCMCglmm(art ~ trait - 1 + at.level(trait, 1:2):fem + at.level(trait, 1):mar + at.level(trait, 1):kid5 + at.level(trait, 1):phd + at.level(trait, 1):ment, rcov = ~idh(trait):units, data = bioChemists, prior = prior.m5d.1, family = &quot;hupoisson&quot;) which reveals that although this proportion is expected to be (slightly) smaller: HPDinterval(plogis((m5d.4$Sol[, 2] + m5d.4$Sol[, 4])/sqrt(1 + c2))) ## lower upper ## var1 0.2312673 0.3072587 ## attr(,&quot;Probability&quot;) ## [1] 0.95 the proportion of zeros expected for men is probably still less than what we expect from a non-truncated Poisson process for which the estimates have changed very little: HPDinterval(ppois(0, exp(m5d.4$Sol[, 1] + m5d.4$Sol[, 3] + 0.5 * m5d.4$VCV[, 1]))) ## lower upper ## var1 0.07276588 0.248731 ## attr(,&quot;Probability&quot;) ## [1] 0.95 This highlights one of the disadvantages of hurdle models. If explanatory variables have been fitted that affect the expectation of the Poisson process then this implies that the proportion of zero’s observed will also vary across these same explanatory variables, even in the absence of zero-inflation. It may then be necessary to fit an equally complicated model for both processes even though a single parameter would suffice in a ZIP model. However, in the absence of zero-inflation the intercept of the zero-inflation process in a ZIP model is \\(-\\infty\\) on the logit scale causing numerical and inferential problems. An alternative type of model are zero-altered models. 7.9 Zero-altered Models Zero-altered Poisson (ZAP) models are identical to Poisson-hurdle models except a complementary log-log link is used instead of the logit link when modelling the proportion of zeros. However for reasons that will become clearer below, the zero-altered process (za) is predicting non-zeros as opposed to the ZIP and hurdle-Poisson models where it is the number of zeros. The likelihood is: \\[\\begin{array}{rl} Pr(y=0) =&amp; 1-\\texttt{pexp}(\\texttt{exp}(l_{2}))\\\\ Pr(y | y&gt;0) =&amp; \\texttt{pexp}(\\texttt{exp}(l_{2}))\\ast \\texttt{dpois}(y, \\texttt{exp}(l_{1}))/(1-\\texttt{ppois}(0, \\texttt{exp}(l_{1})))\\\\ \\end{array}\\] since the inverse of the complementary log-log transformation is the distribution function of the extreme value (log-exponential) distribution. It happens that \\(\\texttt{ppois}(0,\\texttt{exp}(l)) = \\texttt{dpois}(0,\\texttt{exp}(l)) = 1-\\texttt{pexp}(\\texttt{exp}(l))\\) so that if \\(l = l_{1} = l_{2}\\) then the likelihood reduces to: \\[\\begin{array}{rl} Pr(y=0) =&amp; \\texttt{dpois}(0,\\texttt{exp}(l))\\\\ Pr(y | y&gt;0) =&amp; \\texttt{dpois}(y, \\texttt{exp}(l))\\\\ \\end{array}\\] which is equivalent to a standard Poisson model. We can then test for zero-flation by constraining the overdispersion to be the same for both process using a trait by units interaction in the R-structure, and by setting up the contrasts so that the zero-altered regression coefficients are expressed as differences from the Poisson regression coefficients. When this difference is zero the variable causes no zero-flation, when it is negative it causes zero-inflation and when it is positive it causes zero-deflation: m5d.5 &lt;- MCMCglmm(art ~ trait * (fem + mar + kid5 + phd + ment), rcov = ~trait:units, data = bioChemists, family = &quot;zapoisson&quot;) summary(m5d.5) ## ## Iterations = 3001:12991 ## Thinning interval = 10 ## Sample size = 1000 ## ## DIC: 3038.879 ## ## R-structure: ~trait:units ## ## post.mean l-95% CI u-95% CI eff.samp ## trait:units 0.3644 0.2541 0.4827 50.15 ## ## Location effects: art ~ trait * (fem + mar + kid5 + phd + ment) ## ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 0.336746 -0.002718 0.644566 272.8 0.044 * ## traitza_art -0.527132 -1.109418 0.010963 189.4 0.058 . ## femWomen -0.201442 -0.367876 -0.044271 350.2 0.006 ** ## marMarried 0.078166 -0.118030 0.262194 262.6 0.402 ## kid5 -0.125775 -0.264035 -0.005271 199.9 0.066 . ## phd 0.017631 -0.068209 0.096252 307.5 0.692 ## ment 0.019415 0.012902 0.027132 500.3 &lt;0.001 *** ## traitza_art:femWomen 0.048930 -0.220809 0.288344 216.1 0.720 ## traitza_art:marMarried 0.141315 -0.171269 0.475263 201.6 0.376 ## traitza_art:kid5 -0.080008 -0.266962 0.128955 181.7 0.424 ## traitza_art:phd 0.007655 -0.137684 0.129547 227.9 0.928 ## traitza_art:ment 0.028197 0.013272 0.044048 140.1 &lt;0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 we can see from this that the more papers a mentor produces, the more zero-deflation (or conversely the less papers a mentor produces, the more zero-inflation). If the model has &gt;2 responses and you want to regress response \\(\\texttt{i}\\) effects on the remainder (i.e. a multiple regression), the coefficients can be obtained as solve(V[-i,-i], V[i,-i]) where \\(\\texttt{V}\\) is the (co)variance matrix of effects.↩︎ We could also have parameterised the model directly as a regression using \\(\\texttt{ante1(trait):id}\\) \\(\\texttt{ante1(trait):units}\\) - see Section 6.5.↩︎ If we had an additional covariate, let’s say \\(\\texttt{rain}\\), we can also obtain the \\(\\texttt{units}\\) variance had we fitted both \\(\\texttt{rain}\\) and \\(\\texttt{ice}\\). Define \\({\\bf c}\\) as the \\(2\\times 1\\) vector of residual covariances between road accidents and the weather variables and \\({\\bf V}\\) as the \\(2\\times 2\\) residual covariance matrix for the weather variables. Then \\(\\sigma^2_\\texttt{traity|weather.unit} = \\sigma^2_\\texttt{traity.unit}-{\\bf c}^{\\top}{\\bf V}^{-1}{\\bf c}\\).↩︎ For a \\(2\\times 2\\) covariance matrix, positive-definitness requires both variances to be positive and the correlation to lie between -1 and 1. For covariance matrices of larger dimension, the conditions for positive-definitness are more complicated. Perhaps the simplest definition is that all eigenvalues are positive. The function \\(\\texttt{posterior.evals}\\) returns the posterior distribution of the \\(k\\) eigenvalues given the posterior samples of a \\(k\\) dimensional covariance matrix (obtained for the appropriate \\(k^2\\) columns of the \\(\\texttt{VCV}\\) element of the \\(\\texttt{MCMCglmm}\\) model object.)↩︎ \\(\\texttt{at.level}\\) takes the name of categorical variable together with a vector of specified levels. It creates an \\(n\\times k\\) incidence matrix where \\(n\\) is the length of the categorical variable and \\(k\\) the number of specified levels. The matrix element \\(ij\\) has a one if the categorical variable for observation \\(i\\)is of level \\(j\\), and zero otherwise. When \\(\\texttt{at.level}\\) is used in a model formula, and interacted with other terms, it restricts those terms to only having effects when the observations are associated with the \\(k\\) specified levels. For example, imagine a categorical variable \\(\\texttt{fac}\\) with three levels \\(\\texttt{a}\\), \\(\\texttt{b}\\), and \\(\\texttt{c}\\) and a continuous variable \\(\\texttt{x}\\). \\(\\texttt{at.level(fac, c(&quot;a&quot;, &quot;c&quot;)):x}\\) would fit two effects - an effect of \\(x\\) when \\(\\texttt{fac}=\\texttt{a}\\) and an effect of \\(x\\) when \\(\\texttt{fac}=\\texttt{c}\\). \\(\\texttt{at.set}\\) is similar although it creates an \\(n\\times 1\\) incidence matrix where element \\(i\\) is one if the categorical variable for observation \\(i\\) is any of the specified levels. The term \\(\\texttt{at.set(fac, c(&quot;a&quot;, &quot;c&quot;)):x}\\) would then fit one effect - the effect of \\(x\\) when \\(\\texttt{fac}\\) is equal to \\(\\texttt{a}\\) or \\(\\texttt{c}\\).↩︎ For more complicated models a (co)variance matrix may be estimated for a particular random component rather than just a single variance as here. In such cases, V is a matrix. The value at which (part of) the (co)variance matrix is fixed at is determined by V. Any elements of the covariance matrix in rows and/or columns equal to or greater than fix are fixed. In the case of a single variance fix=1 simply fixes the variance (element 1,1 of the (co)variance matrix) at whatever is specified in V (one in this example).↩︎ Obtaining this expression is involved. We can express the expected value of \\(y\\) given \\(l\\) is less than, or greater than, some value \\(c\\) using the law of total expectation \\[E[y | l&gt;c] = E[E[y|l] | l&gt;c]\\] and \\[E[y | l&lt;c] = E[E[y|l] | l&lt;c]\\] \\(E[y|l]\\) is the mean of a conditional normal which has a well known form: \\(\\mu_y+\\frac{\\sigma_{y,l}}{\\sigma^2_{l}}(l-\\mu_l)\\). The expectation of this when \\(l&gt;c\\) is then \\(\\mu_y+\\frac{\\sigma_{y,l}}{\\sigma^2_{l}}(E[l|l&gt;c]-\\mu_l)\\). \\(E[l|l&gt;c]\\) can be obtained using the inverse Mills ratio: \\[E[l|l&gt;c] = \\mu_l + \\sigma_{l}\\frac{f_N(\\alpha)}{(1 - F_N(\\alpha))}\\] and when \\(l&lt;c\\) we have \\[E[l|l&lt;c] = \\mu_l - \\sigma_{l}\\frac{f_N(\\alpha)}{F_N(\\alpha)}\\] where \\(f_N\\) and \\(F_N\\) are the density and cumulative density functions for the unit normal, and \\(\\alpha = \\mu_l/\\sigma_{l}\\). Code for obtaining \\(E[y | l&gt;c]\\) and \\(E[y | l&gt;c]\\) can be found here condE &lt;- function(mu, V, c = 0, greater = TRUE) { # A bivariate normal with mean vector mu and covariance matrix V is # assumed. condE give expectation of 1st variable given 2nd variable is &gt; # c (greater=TRUE) or &lt; c (greater=FALSE) if (nrow(V) != ncol(V) | nrow(V) != 2) { stop(&quot;V should be a 2x2 matrix&quot;) } if (length(mu) != 2) { stop(&quot;mu should be length 2&quot;) } alpha &lt;- (c - mu[2])/sqrt(V[2, 2]) if (greater) { lambda &lt;- dnorm(alpha)/(1 - pnorm(alpha)) # inverse Mills ratio expectation &lt;- mu[2] + sqrt(V[2, 2]) * lambda } else { lambda &lt;- dnorm(alpha)/pnorm(alpha) expectation &lt;- mu[2] - sqrt(V[2, 2]) * lambda } # Expectation that E[l|l&gt;0] or E[l|l&lt;0] expectation &lt;- mu[1] + (expectation - mu[2]) * V[1, 2]/V[2, 2] # Expectation of x given l|l&gt;0. # The whole thing simplifies to mu[1] \\pm lambda*V[1,2]/sqrt(V[2,2]) but # easier to see the logic without the simplification return(expectation) } ↩︎ I should really have an na.option in the call to \\(\\texttt{MCMCglmm}\\) that simply ignores missing values in the wide-format.↩︎ Obtaining comparable regression coefficients between the single and multi-response models is fairly involved. The reason for this is that the standard single-response probit model assumes a \\(\\texttt{units}\\) variance of one after conditioning on the predictors, yet the multiple-response model assumes a \\(\\texttt{units}\\) variance of one before conditioning on the predictors. Nevertheless, we can easily move between the two parameterisations. As detailed at the start of this Chapter and in the footnote[^schur], the $ variance for death after conditioning on the \\(\\texttt{id}\\) effects for \\(log(\\texttt{bili})\\) is \\(V_{2|1}=V_{2,2}-V_{1,2}^2/V_{1,1}\\) where \\({\\bf V}\\) is the covariance matrix. As covered in Section 3.6.3 and in footnote[^3.5] we can then rescale the regression coefficient from the multi-response model by \\(\\sqrt{2/(1+V_{2|1})}\\) to obtain the value we get if we could have set \\(V_{2|1}=1\\) as in the single-response model.↩︎ An identity matrix has ones along the diagonal and zero’s on the off-diagonals. A unit matrix is a matrix of all ones.↩︎ It is not immediately obvious what this is doing. The model formula specifies five parameters across the four traits. The term \\(\\texttt{trait}\\) defines four effects, one effect for each trait since the intercept is suppressed for formula inside variance functions. The term at.set(trait, 1:4 sets up a single binary indicator variable with a one when the trait level is 1, 2, 3, or 4. Since this is all the four traits, this is just setting up a column of ones. The associated parameter is common to all traits and this will be our \\(\\texttt{Other}\\) effect. If we interact the variance structure with year we end up with a set of five year effects for year \\(j\\) with design matrix: \\[{\\bf Z}_j=\\left[\\begin{array}{ccccc} 1&amp;0&amp;0&amp;0&amp;1\\\\ 0&amp;1&amp;0&amp;0&amp;1\\\\ 0&amp;0&amp;1&amp;0&amp;1\\\\ 0&amp;0&amp;0&amp;1&amp;1\\\\ \\end{array}\\right] \\] The \\(\\texttt{idv}\\) variance structure assumes the five effects are uncorrelated with a common variance, such that \\(\\sigma^2_u{\\bf Z}_j{\\bf Z}^{\\top}_j = \\sigma^2_u({\\bf I}+{\\bf J})\\): the specification is equivalent to just fitting four year effects with the desired covariance structure.↩︎ "],["pedigree.html", "8 Pedigrees and Phylogenies 8.1 Pedigree and phylogeny formats 8.2 The animal model and the phylogenetic mixed model", " 8 Pedigrees and Phylogenies Pedigrees and phylogenies are similar things: they are both ways of representing shared ancestry. Under a quantitative genetic model of inheritance, or a Brownian motion model of evolution, GLMM’s can be readily extended to model the similarities that exist between the phenotypes of related individuals or taxa. In the context of quantitative genetics these models are known as ‘animal’ models (Henderson 1976), and in the context of the comparative method these models are known as phylogenetic mixed models (Lynch 1991). The two models are almost identical, and are relatively minor modifications to the basic mixed model (Hadfield and Nakagawa 2010). 8.1 Pedigree and phylogeny formats 8.1.1 Pedigrees \\(\\texttt{MCMCglmm}\\) handles pedigrees stored in 3-column tabular form, with each row representing a single individual. The first column should contain the unique identifier of the individual, and columns 2 and 3 should be the unique identifiers of the individual’s parents. Parents must appear before their offspring in the table. I usually have the dam (mother) in the first column and the sire (father) in the third. I prefer the words dam and sire because if I subscript things with \\(m\\) and \\(f\\) I can never remember whether I mean male and female, or mother and father. In hermaphrodite systems the same individual may appear in both columns, even within the same row if an individual was produced through selfing. This is not a problem, but \\(\\texttt{MCMCglmm}\\) will issue a warning in case hermaphrodites are not present and a data entry mistake has been made. Impossible pedigrees (for example individual’s that give birth to their own mother) are a problem and \\(\\texttt{MCMCglmm}\\) will issue an error, hopefully with an appropriate message, when impossibilities are detected. If the parent(s) of an individual are unknown then a missing value (NA) should be assigned in the relevant column. All individuals appearing as dams or sires need to have their own record, even if both of their parents are unknown. Often the number of individuals in a pedigree will be greater than the number of individuals for which phenotypic data exist. \\(\\texttt{MCMCglmm}\\) can handle this, as long as all the individuals appearing in the data frame passed to data also appear in the pedigree. To illustrate, we can load a pedigree for a population of blue tits and display the pedigree for the nuclear family that has individuals \\(\\texttt{R187920}\\) and \\(\\texttt{R187921}\\) as parents: data(BTped) Nped &lt;- BTped[which(apply(BTped, 1, function(x) { any(x == &quot;R187920&quot; | x == &quot;R187921&quot;) })), ] Nped ## animal dam sire ## 66 R187920 &lt;NA&gt; &lt;NA&gt; ## 172 R187921 &lt;NA&gt; &lt;NA&gt; ## 325 R187726 R187920 R187921 ## 411 R187724 R187920 R187921 ## 503 R187723 R187920 R187921 ## 838 R187613 R187920 R187921 ## 932 R187612 R187920 R187921 ## 1030 R187609 R187920 R187921 Both parents form part of what is known as the base population - they are outbred and unrelated to anybody else in the pedigree. \\(\\texttt{MCMCglmm}\\) and MasterBayes have several pedigree manipulation functions. (MasterBayes::orderPed) orders a pedigree so parents appear before their offspring, (MasterBayes::insertPed) inserts records for individuals that only appear as parents (or a vector of specified individuals). When the number of individuals with phenotypic data is less than the number of individuals in the pedigree it is sometimes possible to remove uninformative individuals from the pedigree and thus reduce the computation time. This is known as pruning the pedigree and is implemented in the \\(\\texttt{MCMCglmm}\\) function prunePed. A vector of measured individuals is specified in the argument keep and specifying make.base=TRUE implements the most complete pruning. Note, make.base=FALSE is the default argument so you’ll need to explicitly specify TRUE in the call to prunePed. Michael Morrissey’s pedantics package, and the kinship2 package also have many other useful pedigree orientated functions. In fact, the orderPed function in MasterBayes is built around functions provided by kinship2. 8.1.2 Phylogenies Phylogenies can be expressed in tabular form, although only two columns are required because each species only has a single parent. In general however, phylogenies are not expressed in this form presumably because it is hard to traverse phylogenies (and pedigrees) backwards in time when they are stored this way. For phylogenetic mixed models we generally only need to traverse phylogenies forward in time (if at all) but I have stuck with convention and used the phylo class from the ape package to store phylogenies. As with pedigrees, all species appearing in the data frame passed to data need to appear in the phylogeny. Typically, this will only include species at the tips of the phylogeny and so the measured species should appear in the tip.label element of the phylo object. An error message will be issued if this is not the case. Data may also exist for ancestral species, or even for species present at the tips but measured many generations before. It is possible to include these data as long as the phylogeny has labelled internal nodes. If nodes are unlabelled then \\(\\texttt{MCMCglmm}\\) names them internally using the default arguments of makeNodeLabel from ape. To illustrate, lets take the phylogeny of bird families included in the ape package, and extract the phylogeny in tabular form for the Paridae (Tits), Certhiidae (Treecreepers), Gruidae (Cranes) and the Struthionidae (Ostriches): data(&quot;bird.families&quot;) bird.families &lt;- makeNodeLabel(bird.families) some.families &lt;- c(&quot;Certhiidae&quot;, &quot;Paridae&quot;, &quot;Gruidae&quot;, &quot;Struthionidae&quot;) Nphylo &lt;- drop.tip(bird.families, setdiff(bird.families$tip.label, some.families)) INphylo &lt;- inverseA(Nphylo) INphylo$pedigree ## node.names ## [1,] &quot;Node58&quot; NA NA ## [2,] &quot;Node122&quot; &quot;Node58&quot; NA ## [3,] &quot;Struthionidae&quot; NA NA ## [4,] &quot;Gruidae&quot; &quot;Node58&quot; NA ## [5,] &quot;Certhiidae&quot; &quot;Node122&quot; NA ## [6,] &quot;Paridae&quot; &quot;Node122&quot; NA (#fig:bird.families)A phylogeny of bird families from The families in red are the Tits (Paridae), Treecreepers (Certhiidae), Cranes (Gruidae) and the Ostriches (Struthionidae) from top to bottom. Blue tits are in the Paridae, and the word pedigree comes from the french for crane’s foot. The full phylogeny, with these families and their connecting notes displayed, is shown in Figure @ref(fig:bird.families). You will notice that \\(\\texttt{Node1}\\) - the root - does not appear in the phylogeny in tabular form. This is because the root is equivalent to the base population in a pedigree analysis, an issue which we will come back to later. Another piece of information that seems to be lacking in the tabular form is the branch length information. Branch lengths are equivalent to inbreeding coefficients in a pedigree. As with pedigrees the inbreeding coefficients are calculated by inverseA: INphylo$inbreeding ## [1] 0.2285714 0.3857143 1.0000000 0.7714286 0.3857143 0.3857143 You will notice that the Struthionidae have an inbreeding coefficient of 1 because we used the default scale=TRUE in the call to inverseA. Only ultrametric trees can be scaled in \\(\\texttt{MCMCglmm}\\) and in this case the sum of the inbreeding coefficients connecting the root to a terminal node is one. To take the Paridae as an example: sum(INphylo$inbreeding[which(INphylo$pedigree[, 1] %in% c(&quot;Paridae&quot;, &quot;Node122&quot;, &quot;Node58&quot;))]) ## [1] 1 The inbreeding coefficients for the members of the blue tit nuclear family are of course all zero: inverseA(Nped)$inbreeding ## [1] 0 0 0 0 0 0 0 0 8.2 The animal model and the phylogenetic mixed model The structure of pedigrees and phylogenies can be expressed in terms of the relatedness matrix \\({\\bf A}\\). This matrix is symmetric, square, and has dimensions equal to the number of individuals in the pedigree (or the number of taxa in the phylogeny). For pedigrees, element \\(A_{i,j}\\) is twice the probability that an allele drawn from individual \\(i\\) is identical by descent to an allele in individual \\(j\\). For phylogenies, element \\(A_{i,j}\\) is the amount of time that elapsed (since the common ancestor of all sampled taxa) before the speciation event that resulted in taxa \\(i\\) and \\(j\\). Simple, but perhaps slow, recursive methods exist for calculating \\({\\bf A}\\) in both cases: Aped &lt;- 2 * kinship2::kinship(Nped[, 1], Nped[, 2], Nped[, 3]) Aped ## R187920 R187921 R187726 R187724 R187723 R187613 R187612 R187609 ## R187920 1.0 0.0 0.5 0.5 0.5 0.5 0.5 0.5 ## R187921 0.0 1.0 0.5 0.5 0.5 0.5 0.5 0.5 ## R187726 0.5 0.5 1.0 0.5 0.5 0.5 0.5 0.5 ## R187724 0.5 0.5 0.5 1.0 0.5 0.5 0.5 0.5 ## R187723 0.5 0.5 0.5 0.5 1.0 0.5 0.5 0.5 ## R187613 0.5 0.5 0.5 0.5 0.5 1.0 0.5 0.5 ## R187612 0.5 0.5 0.5 0.5 0.5 0.5 1.0 0.5 ## R187609 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.0 Aphylo &lt;- vcv.phylo(Nphylo, cor = T) Aphylo ## Struthionidae Gruidae Certhiidae Paridae ## Struthionidae 1 0.0000000 0.0000000 0.0000000 ## Gruidae 0 1.0000000 0.2285714 0.2285714 ## Certhiidae 0 0.2285714 1.0000000 0.6142857 ## Paridae 0 0.2285714 0.6142857 1.0000000 Note that specifying cor=T is equivalent to scaling the tree as we did in the argument to inverseA. In fact, all of the mixed models we fitted in earlier sections also used an \\({\\bf A}\\) matrix, but in those cases the matrix was an identity matrix (i.e. \\({\\bf A}={\\bf I}\\)) and we didn’t have to worry about it. Let’s reconsider the Blue tit model m3a.1 from Section 5 where we were interested in estimating sex effects for tarsus length together with the amount of variance explained by genetic mother (dam) and foster mother (fosternest): data(BTdata) m3a.1 &lt;- MCMCglmm(tarsus ~ sex, random = ~dam + fosternest, data = BTdata, verbose = FALSE) All individuals that contributed to that analysis are from a single generation and appear in BTped together with their parents. However, individuals in the parental generation do not have tarsus length measurements so they do not have their own records in BTdata. The model can be expressed as: \\[{\\bf y} = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}_{1}{\\bf u}_{1}+{\\bf Z}_{2}{\\bf u}_{2}+{\\bf e}\\] where the design matrices contain information relating each individual to a sex (\\({\\bf X}\\)) a dam (\\({\\bf Z}_{1}\\)) and a fosternest(\\({\\bf Z}_{2}\\)). The associated parameter vectors (\\({\\boldsymbol{\\mathbf{\\beta}}}\\), \\({\\bf u}_{1}\\) and \\({\\bf u}_{2}\\)) are the effects of each sex, mother and fosternest on tarsus length, and \\({\\bf e}\\) is the vector of residuals. In the model, the \\(u\\)’s are treated as random so we estimate their variance instead of fixing it in the prior at some (large) value, as we did with the \\(\\beta\\)’s. We can be a little more explicit about what this means: \\[{\\bf u}_{1} \\sim N({\\bf 0},\\ {\\bf I}\\sigma^{2}_{1})\\] where \\(\\sim\\) stands for ‘is distributed as’ and \\(N\\) a (multivariate) normal distribution. The distribution has two sets of parameters; a vector of means and a covariance matrix. We assume the random effects are deviations around the fixed effect part of the model and so they have a prior expectation of zero. The (co)variance matrix of the random effects is \\({\\bf I}\\sigma^{2}_{1}\\) where \\(\\sigma^{2}_{1}\\) is the variance component to be estimated. The use of the identity matrix makes two things explicit. First, because all off-diagonal elements of an identity matrix are zero we are assuming that all dam effects are independent (no covariance exists between any two dam effects). Second, all diagonal elements of an identity matrix are 1 implying that the range of possible values the dam effect could take is equivalent for every dam, this range being governed by the magnitude of the variance component. Since dam’s have very little interaction with the subset of offspring that were moved to a fosternest, we may be willing to assume that any similarity that exists between the tarsus lengths of this susbset and the subset that remained at home must be due to genetic effects. Although not strictly true we can assume that individuals that shared the same dam also shared the same sire, and so share around 50% of their genes. References "],["measurement.html", "9 Measurement Error, Meta-analysis an Missing Values 9.1 Error in the Response 9.2 Error in the Predictors 9.3 Missing Values", " 9 Measurement Error, Meta-analysis an Missing Values 9.1 Error in the Response Random intercept-slope models implicitly assume that the variance changes as a quadratic function of the predictor. This can be used to our advantage because it allows us to fit meta-analytic models. In meta-analysis the data are usually some standardised statistic which has been estimated with different levels of measurement error. If we wanted to know the expected value of these statistics we would want to weight our answer to those measurements made with the smallest amount of error. If we assume that measurement error around the true value is normally distributed then we could assume the model: \\[y_{i} = \\beta_{1} + m_{i} +e _{i}\\] where \\(\\beta_{1}\\) is the expected value, \\(m_{i}\\) is some deviation due to measurement error, and \\(e_{i}\\) is the deviation of the statistic from the global intercept not due to measurement error. Some types of meta-analysis presume \\(e_{i}\\) does not exist and that the only variation between studies is due to measurement error. This is not realistic, I think. Often, standard errors are reported in the literature, and these can be viewed as an approximation to the expected standard deviation of the measurement error. If we put the standard errors for each statistic as a column in the data frame (and call it SE) then the random term idh(SE):units defines a diagonal matrix with the standard errors on the diagonal. Using results from Equation (??) \\[\\begin{array}{rl} \\textrm{VAR}[{\\bf m}] =&amp; {\\bf Z}{\\bf V}{\\bf Z}^{&#39;}\\\\ =&amp; {\\bf Z}\\sigma^{2}_{m}{\\bf I}{\\bf Z}^{&#39;}\\\\ =&amp; \\sigma^{2}_{m}{\\bf Z}{\\bf Z}^{&#39;}\\\\ \\end{array}\\] fixing \\(\\sigma^{2}_{m}=1\\) in the prior, the expected variance in the measurement errors are therefore the standard errors squared (the sampling variance) and all measurement errors are assumed to be independent of each other. The random regression therefore fits a random effect meta-analysis. 9.1.1 Meta-analysis 9.1.2 Interval Estimation 9.2 Error in the Predictors 9.3 Missing Values "],["path.html", "10 Path Analysis &amp; Antedependence Structures 10.1 Path Anlaysis 10.2 Antedependence 10.3 Scaling", " 10 Path Analysis &amp; Antedependence Structures There are many situations where it would seem reasonable to put some aspect of a response variable in as a predictor, and the only thing that stops us is some (often vague) notion that this is a bad thing to do from a statistical point of view. The approach appears to have a long history in economics but I came across the idea in a paper written by Gianola and Sorensen (2004). The notation of this section, and indeed the sampling strategy employed in MCMCglmm is derived from this paper. 10.1 Path Anlaysis \\(\\boldsymbol{\\Psi}^{(l)}\\) is a square matrix of dimension \\(N \\times N\\) where \\(\\Psi_{i,j}^{(l)}=k\\) sets up the equation \\(y_{i} = \\lambda_{l}ky_{j} \\dots\\). In matrix terms: \\[\\boldsymbol{\\Lambda}{\\bf y} = {\\bf y} - \\sum_{l}\\boldsymbol{\\Psi}^{(l)}{\\bf y}{\\lambda_{l}}\\] Having \\(\\boldsymbol{\\Psi} = \\left[\\boldsymbol{\\Psi}^{(1)}\\ \\boldsymbol{\\Psi}^{(2)}\\ \\dots \\boldsymbol{\\Psi}^{(L-1)}\\ \\boldsymbol{\\Psi}^{(L)}\\right]\\) we have \\[\\begin{array}{rl} \\boldsymbol{\\Lambda}{\\bf y} =&amp; {\\bf y}-\\boldsymbol{\\Psi}\\left({\\bf I}_{L} \\otimes {\\bf y} \\right)\\boldsymbol{\\Lambda}\\\\ =&amp; {\\bf y}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{N}\\right) {\\bf y}\\\\ \\end{array} \\label{rs-Eq} \\tag{10.1}\\] where \\(\\boldsymbol{\\Lambda} = \\left[\\lambda_{1}\\ \\lambda_{2} \\dots \\lambda_{L-1}\\ \\lambda_{L} \\right]^{\\top}\\), and \\(\\boldsymbol{\\Lambda}={\\bf I}_{N}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{N}\\right)\\) Each \\(\\boldsymbol{\\Psi}^{(l)}\\) can be formed using the function sir which takes two formulae. \\(\\boldsymbol{\\Psi} = {\\bf X}_{1}{\\bf X}_{2}^{\\top}\\) where \\({\\bf X}_{1}\\) and \\({\\bf X}_{2}\\) are the model matrices defined by the formulae (with intercept removed). \\({\\bf X}_{1}\\) and \\({\\bf X}_{2}^{\\top}\\) have to be conformable, and although this could be achieved in many ways, one way to ensure this is to have categorical predictors in each which have common factor levels. To give a concrete example, lets take a sample of individuals measured a variable number of times for 2 traits: id &lt;- sample(1:100, 100, replace = T) y1 &lt;- rnorm(100) y2 &lt;- rnorm(100) y &lt;- c(y1, y2) trait &lt;- gl(2, 100) Lets then imagine that each of these individuals interacts with another randomly chosen individual - indexed in the vector id1 id1 &lt;- sample(id, 100, replace = T) id &lt;- as.factor(c(id, id)) id1 &lt;- factor(c(id1, id1), levels = levels(id)) we will adopt a recursive model where by the phenotypes of individuals in the id1 vector affect those in the id vector: Psi &lt;- sir(~id1, ~id) we can see that the first record for individual id[1]=83 is directly affected by individual id1[1]=61’s traits: Psi[1, which(id == id1[1])] ## 38 83 138 183 ## 1 1 1 1 i.e individual id1[1]=61 has 4 records. We can build on this simple model by stating that only trait 2 affects trait 1: Psi &lt;- sir(~id1:at.level(trait, 1), ~id:at.level(trait, 2)) Psi[c(1, 101), which(id == id1[1])] ## 38 83 138 183 ## 1 0 0 1 1 ## 101 0 0 0 0 or that trait 2 affect both trait 2 and trait 1: Psi &lt;- sir(~id1, ~id:at.level(trait, 2)) Psi[c(1, 101), which(id == id1[1])] ## 38 83 138 183 ## 1 0 0 1 1 ## 101 0 0 1 1 my.data &lt;- data.frame(y1 = y1, y2 = y2, id = id[1:100], id1 = id1[1:100], x = rnorm(100)) m1 &lt;- MCMCglmm(y1 ~ x + sir(~id1, ~id) + y2, data = my.data) One problem is that \\({\\bf e}^{\\star}\\) the residual vector that appears in the likelihood for the latent variable does not have a simple (block) diagonal structure when (as in the case above) the elements of the response vector that are regressed on each other are not grouped in the R-structure: \\[{\\bf e}^{\\star} \\sim N\\left({\\boldsymbol{\\mathbf{0}}}, \\boldsymbol{\\Lambda}^{-1}{\\bf R}\\boldsymbol{\\Lambda}^{-\\top}\\right)\\] Consequently, analyses that involve latent variables (i.e. non-Gaussian data, or analyses that have incomplete records for determining the R-structure) are currently not implemented in MCMCglmm. The path function is a way of specifying path models that are less general than those formed by sir but are simple enough to allow updating of the latent variables associated with non-Gaussian data. Imagine a residual structure is fitted where the \\(N\\) observations are grouped into \\(n\\) blocks of \\(k\\). For instance this might be \\(k\\) different characteristics measured in \\(n\\) individuals. A path model may be entertained whereby an individual’s characteristics only affect their own characteristics rather than anyone else’s. In this case, \\(\\boldsymbol{\\Psi}^{(l)}=\\boldsymbol{\\Psi}^{(l)}\\otimes{\\bf I}_{n}\\) is block diagonal, and \\(\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}\\otimes{\\bf I}_{n}\\) where \\(\\boldsymbol{\\Psi} = \\left[\\boldsymbol{\\Psi}^{(1)}, \\boldsymbol{\\Psi}^{(2)} \\dots \\boldsymbol{\\Psi}^{(L)}\\right]\\). Consequently, \\[\\begin{array}{rl} \\boldsymbol{\\Lambda}=&amp;{\\bf I}_{N}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{N}\\right)\\\\ =&amp;{\\bf I}_{N}-\\left(\\boldsymbol{\\Psi}\\otimes{\\bf I}_{n}\\right)\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{N}\\right)\\\\ =&amp;\\left({\\bf I}_{k}\\otimes{\\bf I}_{n}\\right)-\\left(\\boldsymbol{\\Psi}\\otimes{\\bf I}_{n}\\right)\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{k}\\otimes{\\bf I}_{n}\\right)\\\\ =&amp;\\left({\\bf I}_{k}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{k}\\right)\\right)\\otimes{\\bf I}_{n}\\\\ \\end{array}\\] and so \\(\\boldsymbol{\\Lambda}^{-1}=\\left({\\bf I}_{k}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{k}\\right)\\right)^{-1}\\otimes{\\bf I}_{n}\\) and \\(|\\boldsymbol{\\Lambda}| = |{\\bf I}_{k}-\\boldsymbol{\\Psi}\\left(\\boldsymbol{\\Lambda}\\otimes {\\bf I}_{k}\\right)|^{n}\\) Mean-centring responses can help mixing, because \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\lambda}\\) are not sampled in a block. (Jarrod - I guess when \\(|\\boldsymbol{\\Lambda}|=1\\) this could be detected and updating occur in a block?) 10.2 Antedependence An \\(n\\times n\\) unstructured covariance matrix can be reparameterised in terms of regression coefficients and residual variances from a set of \\(n\\) nested multiple regressions. For example, for \\(n=3\\) the following 3 multiple regressions can be defined: \\[\\begin{array}{rl} u_{3} =&amp; u_{2}\\beta_{3|2}+u_{1}\\beta_{3|1}+e_{u_{3}}\\\\ u_{2} =&amp; u_{1}\\beta_{2|1}+e_{u_{2}}\\\\ u_{1} =&amp; e_{u_{1}}\\\\ \\end{array}\\] Arranging the regression coefficients and residual (‘innovation’) variances into a lower triangular matrix and diagonal matrix respectively: \\[{\\bf L}_{u}= \\left[ \\begin{array}{ccc} 1&amp;0&amp;0\\\\ -\\beta_{2|1}&amp;1&amp;0\\\\ -\\beta_{3|1}&amp;-\\beta_{3|2}&amp;1\\\\ \\end{array} \\right]\\] and \\[{\\bf D}_{u}= \\left[ \\begin{array}{ccc} \\sigma^{2}_{e_{u_{1}}}&amp;0&amp;0\\\\ 0&amp;\\sigma^{2}_{e_{u_{2}}}&amp;0\\\\ 0&amp;0&amp;\\sigma^{2}_{e_{u_{3}}}\\\\ \\end{array} \\right]\\] gives \\[{\\bf V}_{u} = {\\bf L}_{u}{\\bf D}_{u}{\\bf L}_{u}^{\\top}\\] Rather than fit the saturated model (in this case all 3 regression coefficients) \\(k^{th}\\) order antedependence models seek to model \\({\\bf V}_{u}\\) whilst constraining the regression coefficients in \\({\\bf L}_{u}\\) to be zero if they are on sub-diagonals. For example, a first order antedependence model would set the regression coefficients in the second off-diagonal (i.e. \\(\\beta_{3|1}\\)) to zero, but estimate those in the first sub-diagonal (i.e. \\(\\beta_{2|1}\\) and \\(\\beta_{3|2}\\)). For a \\(3\\times3\\) matrix, a second order antedependence model would fit a fully unstructured covariance matrix. In terms of Gibbs sampling this parameterisation is less efficient because \\({\\bf V}_{u}\\) is sampled in two blocks (the regression coefficients followed by the innovation variances) rather than in a single block from the inverse Wishart. However, more flexible conjugate prior specifications are possible by placing multivariate normal priors on the regression coefficients and independent inverse Wishart priors on the innovation variances. By constraining arbitrary regression coefficients to be zero in a fully unstructured model allows any fully recursive path model to be constructed for a set of random effects. 10.3 Scaling The path analyses described above essentially allow elements of the response vector to be regressed on each other. Regressing an observation on itself would seem like a peculiar thing to do, although with a little work we can show that by doing this we can allow two sets of observations to conform to the same model except for a difference in scale. References "],["technical-details.html", "11 Technical Details 11.1 Model Form 11.2 MCMC Sampling Schemes 11.3 Parameter Expansion 11.4 Priors for corg and corgh structures", " 11 Technical Details 11.1 Model Form The probability of the \\(i^{th}\\) data point is represented by: \\[f_{i}(y_{i} | l_{i}) \\label{pyl-Eq} \\tag{11.1}\\] where \\(f_{i}\\) is the probability density function associated with \\(y_{i}\\). For example, if \\(y_{i}\\) was assumed to be Poisson distributed and we used the canonical log link function, then Equation (11.1) would have the form: \\[f_{P}\\left(y_{i} | \\lambda = \\textrm{exp}(l_{i})\\right) \\label{pyl2-Eq} \\tag{11.2}\\] where \\(\\lambda\\) is the canonical parameter of the Poisson density function \\(f_{P}\\). Table 11.1 has a full list of supported distributions and link functions. The vector of latent variables follow the linear model \\[{\\bf l} = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}{\\bf u}+{\\bf e} \\label{l-Eq} \\tag{11.3}\\] where \\({\\bf X}\\) is a design matrix relating fixed predictors to the data, and \\({\\bf Z}\\) is a design matrix relating random predictors to the data. These predictors have associated parameter vectors \\({\\boldsymbol{\\mathbf{\\beta}}}\\) and \\({\\bf u}\\), and \\({\\bf e}\\) is a vector of residuals. In the Poisson case these residuals deal with any overdispersion in the data after accounting for fixed and random sources of variation. The location effects (\\({\\boldsymbol{\\mathbf{\\beta}}}\\) and \\({\\bf u}\\)), and the residuals (\\({\\bf e}\\)) are assumed to come from a multivariate normal distribution: \\[\\left[ \\begin{array}{c} {\\boldsymbol{\\mathbf{\\beta}}}\\\\ {\\bf u}\\\\ {\\bf e} \\end{array} \\right] \\sim N\\left( \\left[ \\begin{array}{c} {\\boldsymbol{\\mathbf{\\beta}}}_{0}\\\\ {\\bf 0}\\\\ {\\bf 0}\\\\ \\end{array} \\right] , \\left[ \\begin{array}{ccc} {\\bf B}&amp;{\\bf 0}&amp;{\\bf 0}\\\\ {\\bf 0}&amp;{\\bf G}&amp;{\\bf 0}\\\\ {\\bf 0}&amp;{\\bf 0}&amp;{\\bf R}\\\\ \\end{array} \\right] \\right) \\label{V-Eq} \\tag{11.4}\\] where \\({\\boldsymbol{\\mathbf{\\beta}}}_{0}\\) is a vector of prior means for the fixed effects with prior (co)variance \\({\\bf B}\\), and \\({\\bf G}\\) and \\({\\bf R}\\) are the expected (co)variances of the random effects and residuals respectively. The zero off-diagonal matrices imply a priori independence between fixed effects, random effects, and residuals. Generally, \\({\\bf G}\\) and \\({\\bf R}\\) are large square matrices with dimensions equal to the number of random effects or residuals. Typically they are unknown, and must be estimated from the data, usually by assuming they are structured in a way that they can be parameterised by few parameters. Below we will focus on the structure of \\({\\bf G}\\), but the same logic can be applied to \\({\\bf R}\\). At its most general, \\(\\texttt{MCMCglmm}\\) allows variance structures of the form: \\[{\\bf G}= \\left({\\bf V}_{1}\\otimes{\\bf A}_{1}\\right) \\oplus \\left({\\bf V}_{2}\\otimes{\\bf A}_{2}\\right) \\oplus \\ldots \\label{G3-Eq} \\tag{11.5}\\] where the parameter (co)variance matrices (\\({\\bf V}\\)) are usually low-dimensional and are to be estimated, and the structured matrices (\\({\\bf A}\\)) are usually high dimensional and treated as known. In the case of ordinal probit models with \\(&gt;2\\) categories (i.e. \"threshold\" or \"ordinal\" models), \\(f_{T}/f_{O}\\) depends on an extra set of parameters in addition to the latent variable: the \\(\\textrm{max}(y)+1\\) cutpoints \\({\\boldsymbol{\\mathbf{\\gamma}}}\\). The probability of \\(y_{i}\\) is then: \\[f_{T}(y_{i} | l_{i}, {\\boldsymbol{\\mathbf{\\gamma}}}) = 1\\ \\textrm{if}\\ \\gamma_{y_{i}+1} &lt; l_{i} &lt; \\gamma_{y_{i}}\\] and \\[f_{O}(y_{i} | l_{i}, {\\boldsymbol{\\mathbf{\\gamma}}}) = F_{N}(\\gamma_{y_{i}} | l_{i}, 1)-F_{N}(\\gamma_{y_{i}+1} | l_{i},1)\\] where \\(F_{N}\\) is the cumulative density function for the normal. Note that the two models can be made equivalent. 11.2 MCMC Sampling Schemes 11.2.1 Updating the latent variables \\({\\bf l}\\) The conditional density of \\(l\\) is given by: \\[Pr(l_{i}| {\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}) \\propto f_{i}(y_{i} | l_{i})f_{N}(e_{i}|{\\bf r}_{i}{\\bf R}_{/i}^{-1}{\\bf e}_{/i}, r_{i}-{\\bf r}_{i}{\\bf R}_{/i}^{-1}{\\bf r}^{&#39;}_{i}) \\label{pcl-Eq} \\tag{11.6}\\] where \\(f_{N}\\) indicates a Multivariate normal density with specified mean vector and covariance matrix. Equation (11.6) is the probability of the data point \\(y_{i}\\) from distribution \\(f_{i}\\) with latent varaible \\(l_{i}\\), multiplied by the probability of the linear predictor residual. The linear predictor residual follows a conditional normal distribution where the conditioning is on the residuals associated with data points other than \\(i\\). Vectors and matrices with the row and/or column associated with \\(i\\) removed are denoted \\(/i\\). Three special cases exist for which we sample directly from Equation (11.6): i) When \\(y_{i}\\) is normal \\(f_{i}(y_{i} | l_{i})=1\\) if \\(y_{i}=l_{i}\\) and zero otherwise so \\(l_{i}=y_{i}\\) with out the need for updating, ii) when \\(y_{i}\\) is discrete and modelled using family=\"threshold\" then Equation defines a truncated normal distribution and can be slice sampled (Robert 1995) and iii) when \\(y_{i}\\) is missing \\(f_{i}(y_{i} | l_{i})\\) is not defined and samples can drawn directly from the normal. In practice, the conditional distribution in Equation (11.6) only involves other residuals which are expected to show some form of residual covariation, as defined by the \\({\\bf R}\\) structure. Because of this we actually update latent variables in blocks, where the block is defined as groups of residuals which are expected to be correlated: \\[Pr({\\bf l}_{j}|{\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}) \\propto \\prod_{i \\in j}{p}_{i}({y}_{i} | l_{i})f_{N}({\\bf e}_{j}|{\\bf 0}, {\\bf R}_{j}) \\label{pcl2-Eq} \\tag{11.7}\\] where \\(j\\) indexes blocks of latent variables that have non-zero residual covariances. For response variables that are neither Gaussian nor threshold, the density in equation (11.7) is in non-standard form and so Metropolis-Hastings updates are employed. We use adaptive methods during the burn-in phase to determine an efficient multivariate normal proposal distribution centered at the previous value of \\({\\bf l}_{j}\\) with covariance matrix \\(m{\\bf M}\\). For computational efficiency we use the same \\({\\bf M}\\) for each block \\(j\\), where \\({\\bf M}\\) is the average posterior (co)variance of \\({\\bf l}_{j}\\) within blocks and is updated each iteration of the burn-in period Haario, Saksman, and Tamminen (2001). The scalar \\(m\\) is chosen using the method of Ovaskainen et al. (2008) so that the proportion of successful jumps is optimal, with a rate of 0.44 when \\({\\bf l}_{j}\\) is a scalar declining to 0.23 when \\({\\bf l}_{j}\\) is high dimensional (Gelman et al. 2004). A special case arises for multi-parameter distributions in which each parameter is associated with a linear predictor. For example, in the zero-inflated Poisson two linear predictors are used to model the same data point, one to predict zero-inflation, and one to predict the Poisson variable. In this case the two linear predictors are updated in a single block even when the residual covariance between them is set to zero, because the first probability in Equation (11.7) cannot be factored: \\[Pr({\\bf l}_{j}|{\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}) \\propto {p}_{i}({y}_{i} | {\\bf l}_{j})({\\bf e}_{j}|{\\bf 0}, {\\bf R}_{j}) \\label{pcl3-Eq} \\tag{11.8}\\] When the block size is one (i.e. a univariate analysis) then the latent variables can be slice sampled for two-category ordinal and categorical models if slice=TRUE is passed to \\(\\texttt{MCMCglmm}\\). 11.2.2 Updating the location vector \\(\\boldsymbol{\\theta} = \\left[{\\boldsymbol{\\mathbf{\\beta}}}^{&#39;}\\; {\\bf u}^{&#39;}\\right]^{&#39;}\\) Garcia-Cortes and Sorensen (2001) provide a method for sampling \\(\\boldsymbol{\\theta}\\) as a complete block that involves solving the sparse linear system: \\[\\tilde{\\boldsymbol{\\theta}} = {\\bf C}^{-1}{\\bf W}^{&#39;}{\\bf R}^{-1}({\\bf l} - {\\bf W}\\boldsymbol{\\theta}_{\\star}-{\\bf e}_{\\star}) \\label{sMME-Eq} \\tag{11.9}\\] where \\({\\bf C}\\) is the mixed model coefficient matrix: \\[{\\bf C} = {\\bf W}^{&#39;}{\\bf R}^{-1}{\\bf W}+ \\left[ \\begin{array}{c c} {\\bf B}^{-1}&amp;{\\bf 0}\\\\ {\\bf 0}&amp;{\\bf G}^{-1}\\\\ \\end{array} \\right]\\] and \\({\\bf W} = \\left[{\\bf X}\\; {\\bf Z}\\right]\\), and \\({\\bf B}\\) is the prior (co)variance matrix for the fixed effects. \\(\\boldsymbol{\\theta}_{\\star}\\) and \\({\\bf e}_{\\star}\\) are random draws from the multivariate normal distributions: \\[\\boldsymbol{\\theta}_{\\star} \\sim N\\left( \\left[ \\begin{array}{c} {\\boldsymbol{\\mathbf{\\beta}}_{0}}\\\\ {\\bf 0}\\\\ \\end{array} \\right] , \\left[ \\begin{array}{c c} {\\bf B}&amp;{\\bf 0}\\\\ {\\bf 0}&amp;{\\bf G}\\\\ \\end{array} \\right] \\right)\\] and \\[{\\bf e}_{\\star} \\sim N\\left({\\bf 0},{\\bf R}\\right)\\] \\(\\tilde{\\boldsymbol{\\theta}} + \\boldsymbol{\\theta}_{\\star}\\) gives a realisation from the required probability distribution: \\[Pr(\\boldsymbol{\\theta} | {\\bf l}, {\\bf W}, {\\bf R}, {\\bf G})\\] Equation (11.9) is solved using Cholesky factorisation. Because \\({\\bf C}\\) is sparse and the pattern of non-zero elements fixed, an initial symbolic Cholesky factorisation of \\({\\bf P}{\\bf C}{\\bf P}^{&#39;}\\) is preformed where \\({\\bf P}\\) is a fill-reducing permutation matrix (Davis 2006). Numerical factorisation must be performed each iteration but the fill-reducing permutation (found via a minimum degree ordering of \\({\\bf C}+{\\bf C}^{&#39;}\\)) reduces the computational burden dramatically compared to a direct factorisation of \\({\\bf C}\\) (Davis 2006). Forming the inverse of the variance structures is usually simpler because they can be expressed as a series of direct sums and Kronecker products: \\[{\\bf G}= \\left({\\bf V}_{1}\\otimes{\\bf A}_{1}\\right) \\oplus \\left({\\bf V}_{2}\\otimes{\\bf A}_{2}\\right) \\oplus \\ldots\\] and the inverse of such a structure has the form \\[{\\bf G}^{-1} = \\left({\\bf V}^{-1}_{1}\\otimes{\\bf A}^{-1}_{1}\\right) \\oplus \\left({\\bf V}^{-1}_{2}\\otimes{\\bf A}^{-1}_{2}\\right) \\oplus \\ldots\\\\\\] which involves inverting the parameter (co)variance matrices (\\({\\bf V}\\)), which are usually of low dimension, and inverting \\({\\bf A}\\). For many problems \\({\\bf A}\\) is actually an identity matrix and so inversion is not required. When \\({\\bf A}\\) is a relationship matrix associated with a pedigree, Henderson (1976; Meuwissen and Luo 1992) give efficient recursive algorithms for obtaining the inverse, and Hadfield and Nakagawa (2010) derive a similar procedure for phylogenies. 11.2.3 Updating the variance structures \\({\\bf G}\\) and \\({\\bf R}\\) Components of the direct sum used to construct the desired variance structures are conditionally independent. The sum of squares matrix associated with each component term has the form: \\[{\\bf S} = {\\bf U}^{&#39;}{\\bf A}^{-1}{\\bf U}\\] where \\({\\bf U}\\) is a matrix of random effects where each column is associated with the relevant row/column of \\({\\bf V}\\) and each row associated with the relevant row/column of \\({\\bf A}\\). The parameter (co)variance matrix can then be sampled from the inverse Wishart distribution: \\[{\\bf V} \\sim IW(({\\bf S}_{p}+{\\bf S})^{-1},\\ n_{p}+n) \\label{pIW-Eq} \\tag{11.10}\\] where \\(n\\) is the number of rows in \\({\\bf U}\\), and \\({\\bf S}_{p}\\) and \\(n_{p}\\) are the prior sum of squares and prior degree’s of freedom, respectively. In some models, some elements of a parameter (co)variance matrix cannot be estimated from the data and all the information comes from the prior. In these cases it can be advantageous to fix these elements at some value and Korsgaard, Andersen, and Sorensen (1999) provide a strategy for sampling from a conditional inverse-Wishart distribution which is appropriate when the rows/columns of the parameter matrix can be permuted so that the conditioning occurs on some diagonal sub-matrix. When this is not possible Metropolis-Hastings updates can be made. 11.2.4 Ordinal Models For ordinal models it is necessary to update the cutpoints which define the bin boundaries for latent variables associated with each category of the outcome. To achieve good mixing we used the method developed by (Cowles 1996) that allows the latent variables and cutpoints to be updated simultaneously using a Hastings-with-Gibbs update. 11.2.5 Path Analyses Elements of the response vector can be regressed on each other using the sir and path functions. Using the matrix notation of Gianola and Sorensen (2004), Equation (11.3) can be rewritten as: \\[\\boldsymbol{\\Lambda}{\\bf l} = {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}}+{\\bf Z}{\\bf u}+{\\bf e} \\label{rs-Eq1} \\tag{11.11}\\] where \\(\\boldsymbol{\\Lambda}\\) is a square matrix of the form: \\[\\begin{array}{rl} \\boldsymbol{\\Lambda} =&amp; {\\bf I}-\\sum_{l}\\boldsymbol{\\Psi}^{(l)}\\lambda_{l}\\\\ \\end{array} \\label{rs-Eq2} \\tag{11.12}\\] This sets up a regression where the \\(i^{th}\\) element of the response vector acts as a weighted (by \\(\\Psi^{(l)}_{i,j}\\)) predictor for the \\(j^{th}\\) element of the response vector with associated regression parameter \\(\\lambda_{l}\\). Often \\(\\boldsymbol{\\Psi}^{(l)}\\) is an incidence matrix with the patterns of ones determining which elements of the response are regressed on each other. Conditional on the vector of regression coefficients \\(\\boldsymbol{\\Lambda}\\), the location effects and variance structures can be updated as before by simply substituting \\({\\bf l}\\) for \\(\\boldsymbol{\\Lambda}{\\bf l}\\) in the necessary equations. Gianola and Sorensen (2004) provide a simple scheme for updating \\(\\boldsymbol{\\Lambda}\\). Note that Equation (11.11) can be rewritten as: \\[\\begin{array}{rl} {\\bf l} - {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}} - {\\bf Z}{\\bf u} =&amp; {\\bf e}+\\sum_{l}\\boldsymbol{\\Psi}^{(l)}{\\bf l}\\lambda_{l}\\\\ =&amp; {\\bf e}+{\\bf L}\\boldsymbol{\\Lambda}\\\\ \\end{array}\\] where \\({\\bf L}\\) is the design matrix \\(\\left[\\boldsymbol{\\Psi}^{(1)}{\\bf l}, \\boldsymbol{\\Psi}^{(2)}{\\bf l} \\dots \\boldsymbol{\\Psi}^{(L)}{\\bf l}\\right]\\) for the \\(L\\) path coefficients. Conditional on \\({\\boldsymbol{\\mathbf{\\beta}}}\\) and \\({\\boldsymbol{\\mathbf{u}}}\\), \\(\\boldsymbol{\\Lambda}\\) can then be sampled using the method of Garcia-Cortes and Sorensen (2001) with \\({\\bf l} - {\\bf X}{\\boldsymbol{\\mathbf{\\beta}}} - {\\bf Z}{\\bf u}\\) as response and \\({\\bf L}\\) as predictor. However, only in a fully recursive system (there exists a row/column permutation by which all \\(\\boldsymbol{\\Psi}\\)’s are triangular) are the resulting draws from the appropriate conditional distribution, which requires multiplication by the Jacobian of the transform: \\(|\\boldsymbol{\\Lambda}|\\). An extra Metropolis Hastings step is used to accept/reject the proposed draw when \\(|\\boldsymbol{\\Lambda}|\\neq 1\\). When the response vector is Gaussian and fully observed, the latent variable does not need updating. For non-Gaussian data, or with missing responses, updating the latent variable is difficult because Equation (11.6) becomes: \\[Pr(l_{i}| {\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}, \\boldsymbol{\\Lambda}) \\propto f_{i}(y_{i} | l_{i})f_{N}((\\boldsymbol{\\Lambda}^{-1}{\\bf e})_{i}|{\\bf q}_{i}{\\bf Q}_{/i}^{-1}{\\bf e}_{/i}, q_{i}-{\\bf q}_{i}{\\bf Q}_{/i}^{-1}{\\bf q}^{&#39;}_{i})\\] where \\({\\bf Q} = \\boldsymbol{\\Lambda}^{-1}{\\bf R}\\boldsymbol{\\Lambda}^{-\\top}\\). In the general case \\({\\bf Q}\\) will not have block diagonal structure like \\({\\bf R}\\) and so the scheme for updating latent variables within residual blocks (i.e. Equation (11.7)) is not possible. However, in some cases \\(\\boldsymbol{\\Lambda}\\) may have the form where all non-zero elements correspond to elements of the response vector that are in the same residual block. In such cases updating the latent variables remains relatively simple: \\[Pr({\\bf l}_{j}|{\\bf y}, \\boldsymbol{\\theta}, {\\bf R}, {\\bf G}) \\propto {p}_{i}({y}_{i} | {\\bf l}_{j})(\\boldsymbol{\\Lambda}^{-1}_{j}{\\bf e}_{j}|{\\bf 0}, \\boldsymbol{\\Lambda}^{-1}_{j}{\\bf R}_{j}\\boldsymbol{\\Lambda}^{-\\top}_{j})\\] 11.2.6 Deviance and DIC The deviance \\(D\\) is defined as: \\[D = -2\\textrm{log}(\\Pr({\\bf y} | {\\boldsymbol{\\mathbf{\\Omega}}}))\\] where \\({\\boldsymbol{\\mathbf{\\Omega}}}\\) is some parameter set of the model. The deviance can be calculated in different ways depending on what is in ‘focus’, and \\(\\texttt{MCMCglmm}\\) calculates this probability for the lowest level of the hierarchy (Spiegelhalter et al. 2002). For fully-observed Gaussian response variables in the likelihood is the density: \\[f_{N}({\\bf y} | {\\bf W}\\boldsymbol{\\theta},\\ {\\bf R})\\] where \\({\\boldsymbol{\\mathbf{\\Omega}}} = \\left\\{\\boldsymbol{\\theta},\\ {\\bf R}\\right\\}\\). For discrete response variables in univariate analyses modeled using family=\"threshold\" the density is \\[\\prod_{i} F_{N}(\\gamma_{y_{i}} | {\\bf w}_{i}\\boldsymbol{\\theta}, \\ r_{ii})-F_{N}(\\gamma_{y_{i}+1} | {\\bf w}_{i}\\boldsymbol{\\theta}, \\ r_{ii})\\] where \\({\\boldsymbol{\\mathbf{\\Omega}}} = \\left\\{{\\boldsymbol{\\mathbf{\\gamma}}},\\ \\boldsymbol{\\theta},\\ {\\bf R}\\right\\}\\). For other response variables variables (including discrete response variables modeled using family=\"ordinal\") it is the product: \\[\\prod_{i}f_{i}(y_{i} | l_{i}) \\label{LLikL} \\tag{11.13}\\] with \\({\\boldsymbol{\\mathbf{\\Omega}}} = {\\bf l}\\). For multivariate models with mixtures of Gaussian (g), threshold (t) and other non-Gaussian (n) data (including missing data) we can define the deviance in terms of three conditional densities: \\[\\begin{array}{rl} Pr({\\bf y} | {\\boldsymbol{\\mathbf{\\Omega}}}) =&amp; \\Pr({\\bf y}_{g}, {\\bf y}_{t}, {\\bf y}_{n} | {\\boldsymbol{\\mathbf{\\gamma}}}, \\boldsymbol{\\theta}_{g}, \\boldsymbol{\\theta}_{t}, {\\boldsymbol{\\mathbf{l}}}_{n}, {\\boldsymbol{\\mathbf{R}}})\\\\ =&amp; \\Pr({\\bf y}_{t} | {\\boldsymbol{\\mathbf{\\gamma}}}, \\boldsymbol{\\theta}_{t}, {\\bf y}_{g}, {\\boldsymbol{\\mathbf{l}}}_{n}, {\\boldsymbol{\\mathbf{R}}})\\Pr({\\bf y}_{g} | \\boldsymbol{\\theta}_{g}, {\\boldsymbol{\\mathbf{l}}}_{n}, {\\boldsymbol{\\mathbf{R}}})\\Pr({\\bf y}_{n} | {\\boldsymbol{\\mathbf{l}}}_{n})\\\\ \\label{Eq-MVdeviance} \\end{array} \\tag{11.14}\\] with \\({\\boldsymbol{\\mathbf{\\Omega}}} = \\left\\{{\\boldsymbol{\\mathbf{\\gamma}}},\\ \\boldsymbol{\\theta}_{/n},\\ {\\boldsymbol{\\mathbf{l}}}_{n}\\ {\\bf R}\\right\\}\\). Have \\(({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{a|b}={\\boldsymbol{\\mathbf{W}}}_{a}\\boldsymbol{\\theta}_{a}+{\\bf R}_{a,b}{\\bf R}^{-1}_{b,b}({\\bf l}_{b}-{\\boldsymbol{\\mathbf{W}}}_{b}\\boldsymbol{\\theta}_{b})\\) and \\({\\bf R}_{a |b} = {\\bf R}_{a,a}-{\\bf R}_{a,b}{\\bf R}^{-1}_{b,b}{\\bf R}_{a,b}\\) where the subscripts denote rows of the data vector/design matrices or rows/columns of the \\({\\bf R}\\)-structure. Then, the conditional density of \\({\\bf y}_{g}\\) in Equation (11.14) is: \\[f_{N}\\left({\\bf y}_{g} | ({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{g|n},\\ {\\bf R}_{g|n}\\right)\\] The conditional density of \\({\\bf y}_{n}\\) in Equation (11.14) is identical to that given in Equation (11.13), and for a single \"threshold\" trait \\[\\prod_{i} F_{N}(\\gamma_{y_{i}} | ({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{ti|g,n}, \\ r_{ti|g, n})-F_{N}(\\gamma_{y_{i}+1} | ({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{ti|g,n}, \\ r_{ti|g, n}) \\label{Eq-cpmvnorm} \\tag{11.15}\\] is the conditional density for \\({\\bf y}_{t}\\) in Equation (11.14), where \\(({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{ti|g,n}\\) is the \\(i^{\\textrm{th}}\\) element of \\(({\\boldsymbol{\\mathbf{W}}}\\boldsymbol{\\theta})_{t|g,n}\\). Currently the deviance (and hence the DIC) will not be returned if there is more than one threshold trait. The deviance is calculated at each iteration if DIC=TRUE and stored each thin\\(^{th}\\) iteration after burn-in. However, for computational reasons the deviance is calculated mid-iteration such that the deviance returned at iteration \\(i\\) uses \\({\\boldsymbol{\\mathbf{\\Omega}}}_{i} = \\left\\{{\\boldsymbol{\\mathbf{\\gamma}}}_{i},\\ \\boldsymbol{\\theta}_{/n, i},\\ {\\boldsymbol{\\mathbf{l}}}_{n, i-1}\\ {\\bf R}_{i}\\right\\}\\). The mean deviance (\\(\\bar{D}\\)) is calculated over all iterations, as is the mean of the latent variables (\\({\\bf l}\\)) the \\({\\bf R}\\)-structure and the vector of predictors (\\({\\bf W}\\boldsymbol{\\theta}\\)). The deviance is calculated at the mean estimate of the parameters (\\(D(\\bar{\\boldsymbol{\\mathbf{\\Omega}}})\\)) and the deviance information criterion calculated as: \\[\\textrm{DIC} = 2\\bar{D}-D(\\bar{\\boldsymbol{\\mathbf{\\Omega}}})\\] Table 11.1: Distribution types that can fitted using \\(\\texttt{MCMCglmm}\\). The prefixes \"zi\", \"zt\", \"hu\" and \"za\" stand for zero-inflated, zero-truncated, hurdle and zero-altered respectively. The prefix \"cen\" standards for censored where \\(y_{1}\\) and \\(y_{2}\\) are the upper and lower bounds for the unobserved datum \\(y\\). \\(J\\) stands for the number of categories in the multinomial, categorical and zero-truncated multivariate Bernoulli distributions and this must be specified in the family argument for the multinomial and zero-truncated multivariate Bernoulli distributions. The density function is for a single datum in a univariate model with \\({\\bf w}^{\\top}\\) being a row vector of \\({\\bf W}\\). \\(f\\) and \\(F\\) are the density and distribution functions for the subscripted distribution (\\(N\\)=Normal, \\(P\\)=Poisson, \\(E\\)=Exponential, \\(G\\)=Geometric, \\(B\\)=Binomial, \\(NCt\\)=non-central \\(t\\) and \\(t\\)=Student’s \\(t\\)). The \\(J-1\\) \\(\\gamma\\)’s in the ordinal/threshold models are the cut-points, with \\(\\gamma_{1}\\) set to zero. The normalising probability \\(P_k(k)\\) in the denominator of the zero-truncated multinomial is not given explicitly but is the probability of no zero’s in the non-zero categories under a standard multinomial. Distribution NoDataColumns NoTraits Density Function \"gaussian\" 1 1 \\(Pr(y)\\) \\(=f_{N}({\\bf w}^{\\top}\\boldsymbol{\\theta},\\sigma^{2}_{e})\\) \"poisson\" 1 1 \\(Pr(y)\\) \\(=f_{P}(\\textrm{exp}(l))\\) \"categorical\" 1 \\(J-1\\) \\(Pr(y = k| k\\neq k_1)\\) \\(Pr(y=k | k=k_1)\\) \\(=\\frac{\\textrm{exp}(l_{k})}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\) \\(=\\frac{1}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\) \"multinomialJ\" \\(J\\) \\(J-1\\) \\(Pr(y_k | k\\neq J)\\) \\(Pr(y_k | k=J)\\) \\(={\\sum^{J}_{j=1}y_j \\choose y_k}\\left(\\frac{\\textrm{exp}(l_{k})}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\right)^{y_k}\\left(1-\\frac{\\textrm{exp}(l_{k})}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\right)^{\\sum^{J}_{j\\neq k} y_j}\\) \\(={\\sum^{J}_{j=1}y_j \\choose y_k}\\left(\\frac{1}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\right)^{y_k}\\left(1-\\frac{1}{1+\\sum^{J-1}_{j=1}\\textrm{exp}(l_{j})}\\right)^{\\sum^{J}_{j\\neq k} y_j}\\) \"ordinal\" 1 1 \\(Pr(y=k)\\) \\(=F_{N}(\\gamma_{k} | l,1)-F_{N}(\\gamma_{k+1} | l,1)\\) \"threshold\" 1 1 \\(Pr(y=k)\\) \\(=F_{N}(\\gamma_{k} | {\\bf w}^{\\top}\\boldsymbol{\\theta}, \\sigma^{2}_{e})-F_{N}(\\gamma_{k+1} | {\\bf w}^{\\top}\\boldsymbol{\\theta}, \\sigma^{2}_{e})\\) \"exponential\" 1 1 \\(Pr(y)\\) \\(=f_{E}(\\textrm{exp}(-l))\\) \"geometric\" 1 1 \\(Pr(y)\\) \\(=f_{G}(\\frac{\\textrm{exp}(l)}{1+\\textrm{exp}(l)})\\) \"cengaussian\" 2 1 \\(Pr(y_{1}&gt;y&gt;y_{2})\\) \\(=F_{N}(y_{2} | {\\bf w}^{\\top}\\boldsymbol{\\theta},\\sigma^{2}_{e})-F_{N}(y_{1} | {\\bf w}^{\\top}\\boldsymbol{\\theta},\\sigma^{2}_{e})\\) \"cenpoisson\" 2 1 \\(Pr(y_{1}&gt;y&gt;y_{2})\\) \\(=F_{P}(y_{2} | l)-F_{P}(y_{1} | l)\\) \"cenexponential\" 2 1 \\(Pr(y_{1}&gt;y&gt;y_{2})\\) \\(=F_{E}(y_{2} | l)-F_{E}(y_{1} | l)\\) \"zipoisson\" 1 2 \\(Pr(y=0)\\) \\(Pr(y | y&gt;0)\\) \\(=\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}+\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)f_{P}(y|\\textrm{exp}(l_{1}))\\) \\(=\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)f_{P}(y |\\textrm{exp}(l_{1}))\\) \"ztpoisson\" 1 1 \\(Pr(y)\\) \\(=\\frac{f_{P}(y |\\textrm{exp}(l))}{1-f_{P}(0 |\\textrm{exp}(l))}\\) \"hupoisson\" 1 2 \\(Pr(y=0)\\) \\(Pr(y | y&gt;0)\\) \\(=\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\) \\(=\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)\\frac{f_{P}(y |\\textrm{exp}(l_{1}))}{1-f_{P}(0 |\\textrm{exp}(l_{1}))}\\) \"zapoisson\" 1 2 \\(Pr(y=0)\\) \\(Pr(y | y&gt;0)\\) \\(=1-\\textrm{exp}(\\textrm{exp}(l_{2}))\\) \\(=\\textrm{exp}(\\textrm{exp}(l_{2}))\\frac{f_{P}(y |\\textrm{exp}(l_{1}))}{1-f_{P}(0 |\\textrm{exp}(l_{1}))}\\) \"hubinomial\" 2 2 \\(Pr(y_{1}=0)\\) \\(Pr(y_{1} | y_{1}&gt;0)\\) \\(=\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}+\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)f_{B}(0 | n=y_{1}+y_{2}, \\frac{\\textrm{exp}(l_{1})}{1+\\textrm{exp}(l_{1})})\\) \\(=\\left(1-\\frac{\\textrm{exp}(l_{2})}{1+\\textrm{exp}(l_{2})}\\right)f_{B}(y_{1} | n=y_{1}+y_{2} \\frac{\\textrm{exp}(l_{1})}{1+\\textrm{exp}(l_{1})})\\) \"nzbinom\" 2 1 \\(Pr(y=1)\\) \\(Pr(y=0)\\) \\(=1-\\left(1-\\frac{\\textrm{exp}(l)}{1+\\textrm{exp}(l)}\\right)^{y_2}\\) \\(=\\left(1-\\frac{\\textrm{exp}(l)}{1+\\textrm{exp}(l)}\\right)^{y_2}\\) \"ncst\" 3 1 \\(Pr(y)\\) \\(=f_{NCt}(y_1 | \\nu=y_3, \\mu=l/y_2)/y_2\\) \"msst\" 3 1 \\(Pr(y)\\) \\(=f_{t}(y_1-l/y_2 | \\nu=y_3)/y_2\\) \"ztmbJ\" \\(J\\) \\(J\\) \\(Pr(y_k=1)\\) \\(Pr(y_k=0)\\) \\(=\\left(1-\\frac{\\textrm{exp}(l_k)}{1+\\textrm{exp}(l_k)}\\right)\\left(1-\\prod^{J}_{j=1}\\frac{\\textrm{exp}(l_k)}{1+\\textrm{exp}(l_k)}\\right)^{-1}\\) \\(=\\left(\\frac{\\textrm{exp}(l_k)}{1+\\textrm{exp}(l_k)}\\right)\\left(1-\\prod^{J}_{j=1}\\frac{\\textrm{exp}(l_k)}{1+\\textrm{exp}(l_k)}\\right)^{-1}\\) \"ztmultinomialJ\" \\(J\\) \\(J-1\\) \\(Pr(y_k | k\\neq J, y_k&gt;0)\\) \\(Pr(y_k | k=J, y_k&gt;0)\\) \\(Pr(y_k=0 | k=J)\\) \\(={\\sum^{J}_{j=1}y_j \\choose y_k}\\left(\\frac{\\textrm{exp}(l_{k})}{P_k(k)}\\right)^{y_k}\\left(1-\\frac{\\textrm{exp}(l_{k})}{P_k(k)}\\right)^{\\sum^{J}_{j\\neq k} y_j}\\) \\(={\\sum^{J}_{j=1}y_j \\choose y_k}\\left(\\frac{1}{P_k(k)}\\right)^{y_k}\\left(1-\\frac{1}{P_k(k)}\\right)^{\\sum^{J}_{j\\neq k} y_j}\\)      ignored 11.3 Parameter Expansion As the covariance matrix approaches a singularity the mixing of the chain becomes notoriously slow. This problem is often encountered in single-response models when a variance component is small and the chain becomes stuck at values close to zero. Similar problems occur for the EM algorithm and C. H. Liu, Rubin, and Wu (1998) introduced parameter expansion to speed up the rate of convergence. The idea was quickly applied to Gibbs sampling problems (J. S. Liu and Wu 1999) and has now been extensively used to develop more efficient mixed-model samplers (e.g. Dyk and Meng 2001; Gelman, Dyk, et al. 2008; Browne et al. 2009). The columns of the design matrix (\\({\\bf W}\\)) can be multiplied by the non-identified working parameters \\({\\boldsymbol{\\mathbf{\\alpha}}} = \\left[1,\\ \\alpha_{1},\\ \\alpha_{2},\\ \\dots \\alpha_{k}\\right]^{&#39;}\\): \\[{\\bf W}_{\\alpha} = \\left[{\\bf X}\\ {\\bf Z}_{1}\\alpha_{1}\\ {\\bf Z}_{2}\\alpha_{2}\\ \\dots\\ {\\bf Z}_{k}\\alpha_{k}\\right] \\label{wstar} \\tag{11.16}\\] where the indices denote submatrices of \\({\\bf Z}\\) which pertain to effects associated with the same variance component. Replacing \\({\\bf W}\\) with \\({\\bf W}_{\\alpha}\\) we can sample the new location effects \\(\\boldsymbol{\\theta}_{\\alpha}\\) as described above, and rescale them to obtain \\(\\boldsymbol{\\theta}\\): \\[\\boldsymbol{\\theta} = ({\\bf I}_{\\beta}\\oplus_{i=1}^{k}{\\bf I}_{u_{i}}\\ \\alpha_{i})\\boldsymbol{\\theta}_{\\alpha}\\] where the identity matrices are of dimension equal to the length of the subscripted parameter vectors. Likewise, the (co)variance matrices can be rescaled by the set of \\(\\alpha\\)’s associated with the variances of a particular variance structure component (\\({\\boldsymbol{\\mathbf{\\alpha}}}_{\\mathcal{V}}\\)): \\[{\\bf V} = Diag({\\boldsymbol{\\mathbf{\\alpha}}}_{\\mathcal{V}}){\\bf V}_{\\alpha}Diag({\\boldsymbol{\\mathbf{\\alpha}}}_{\\mathcal{V}})\\] The working parameters are not identifiable in the likelihood, but do have a proper conditional distribution. Defining the \\(n\\times(k+1)\\) design matrix \\({\\bf X}_{\\alpha}\\) with each column equal to the submatrices in Equation (11.16) postmultiplied by the relevant subvectors of \\(\\boldsymbol{\\theta}_{\\alpha}\\), we can see that \\({\\boldsymbol{\\mathbf{\\alpha}}}\\) is a vector of regression coefficients: \\[\\begin{array}{rl} \\bf{l} =&amp; {\\bf X}_{\\alpha}{\\boldsymbol{\\mathbf{\\alpha}}}+\\bf{e}\\\\ \\end{array}\\] and so the methods described above can be used to update them. 11.4 Priors for corg and corgh structures For corg and corgh structures40 the diagonals of V define the fixed variances (corgh) or are ignored and the variances set to one (corg). I use the prior specification in Barnard, McCulloch, and Meng (2000) where nu controls how much the correlation matrix approaches an identity matrix. The marginal distribution of individual correlations (\\(r\\)) is given by Barnard, McCulloch, and Meng (2000; and Box and Tiao 1973): \\[\\begin{array}{lr} Pr(r) \\propto (1-r^{2})^\\frac{\\texttt{nu-dim(V)-1}}{\\texttt{2}}, &amp; |r|&lt;1\\\\ \\end{array}\\] and as shown above setting nu =dim(V)+1 results in marginal correlations that are uniform on the interval \\[-1,1\\]. In most cases correlation matrices do not have known form and so cannot be directly Gibbs sampled. \\(\\texttt{MCMCglmm}\\) uses a method proposed by X. F. Liu and Daniels (2006) with the target prior as in Barnard, McCulloch, and Meng (2000). Generally this algorithm is very efficient as the Metropolis-Hastings acceptance probability only depends on the degree to which the candidate prior and the target prior (the prior you specify) conflict. The candidate prior is equivalent to the prior in Barnard, McCulloch, and Meng (2000) with nu=0 so as long as a diffuse prior is set, mixing is generally not a problem. If nu=0 is set (the default) then the Metropolis-Hastings steps are always accepted resulting in Gibbs sampling. However, a prior of this form puts high density on extreme correlations which can cause problems if the data give support to correlations in this region. References "],["references.html", "12 References", " 12 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
