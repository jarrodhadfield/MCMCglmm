<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Technical Details | MCMCglmm Course Notes</title>
  <meta name="description" content="Extended documentation and course notes for the MCMCglmm R package." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Technical Details | MCMCglmm Course Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Technical Details | MCMCglmm Course Notes" />
  
  <meta name="twitter:description" content="Extended documentation and course notes for the MCMCglmm R package." />
  

<meta name="author" content="Jarrod Hadfield" />


<meta name="date" content="2026-01-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="path.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script>
<link href="libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-1.3.31/rglClass.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/utils.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/buffer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/subscenes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shaders.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/shadersrc.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/textures.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/projection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/mouse.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/init.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pieces.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/draw.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/controls.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/selection.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/rglTimer.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/pretty.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/axes.src.js"></script>
<script src="libs/rglwidgetClass-1.3.31/animation.src.js"></script>
<script src="libs/CanvasMatrix4-1.3.31/CanvasMatrix.src.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html#outline"><i class="fa fa-check"></i><b>1.1</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian Analysis and MCMC</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian.html"><a href="bayesian.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>2.2</b> Likelihood</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-ml"><i class="fa fa-check"></i><b>2.2.1</b> Maximum Likelihood (ML)</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian.html"><a href="bayesian.html#restricted-maximum-likelihood-reml"><i class="fa fa-check"></i><b>2.2.2</b> Restricted Maximum Likelihood (REML)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian.html"><a href="bayesian.html#prior-distribution"><i class="fa fa-check"></i><b>2.3</b> Prior Distribution</a></li>
<li class="chapter" data-level="2.4" data-path="bayesian.html"><a href="bayesian.html#posterior-distribution"><i class="fa fa-check"></i><b>2.4</b> Posterior Distribution</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian.html"><a href="bayesian.html#marginal-posterior-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Marginal Posterior Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="bayesian.html"><a href="bayesian.html#intervals-sec"><i class="fa fa-check"></i><b>2.4.2</b> Credible Intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian.html"><a href="bayesian.html#MCMC"><i class="fa fa-check"></i><b>2.5</b> MCMC</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian.html"><a href="bayesian.html#starting-values"><i class="fa fa-check"></i><b>2.5.1</b> Starting values</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian.html"><a href="bayesian.html#metropolis-hastings-updates"><i class="fa fa-check"></i><b>2.5.2</b> Metropolis-Hastings updates</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian.html"><a href="bayesian.html#gibbs-sampling"><i class="fa fa-check"></i><b>2.5.3</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian.html"><a href="bayesian.html#diagnostics-sec"><i class="fa fa-check"></i><b>2.5.4</b> MCMC Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian.html"><a href="bayesian.html#Vprior-sec"><i class="fa fa-check"></i><b>2.6</b> Priors for Residual Variances</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian.html"><a href="bayesian.html#IP-sec"><i class="fa fa-check"></i><b>2.6.1</b> Improper Priors</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian.html"><a href="bayesian.html#transform-sec"><i class="fa fa-check"></i><b>2.7</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>3</b> Linear and Generalised Linear Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="glm.html"><a href="glm.html#linear-model-lm"><i class="fa fa-check"></i><b>3.1</b> Linear Model (LM)</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="glm.html"><a href="glm.html#lm-sec"><i class="fa fa-check"></i><b>3.1.1</b> Linear Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="glm.html"><a href="glm.html#generalised-linear-model-glm"><i class="fa fa-check"></i><b>3.2</b> Generalised Linear Model (GLM)</a></li>
<li class="chapter" data-level="3.3" data-path="glm.html"><a href="glm.html#poisglm-sec"><i class="fa fa-check"></i><b>3.3</b> Poisson GLM</a></li>
<li class="chapter" data-level="3.4" data-path="glm.html"><a href="glm.html#od-sec"><i class="fa fa-check"></i><b>3.4</b> Overdispersion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="glm.html"><a href="glm.html#multiplicative-overdispersion"><i class="fa fa-check"></i><b>3.4.1</b> Multiplicative Overdispersion</a></li>
<li class="chapter" data-level="3.4.2" data-path="glm.html"><a href="glm.html#addod-sec"><i class="fa fa-check"></i><b>3.4.2</b> Additive Overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="glm.html"><a href="glm.html#prediction-in-glm"><i class="fa fa-check"></i><b>3.5</b> Prediction in GLM</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="glm.html"><a href="glm.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="glm.html"><a href="glm.html#binom-sec"><i class="fa fa-check"></i><b>3.6</b> Binomial and Bernoulli GLM</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="glm.html"><a href="glm.html#od-binom"><i class="fa fa-check"></i><b>3.6.1</b> Overdispersion</a></li>
<li class="chapter" data-level="3.6.2" data-path="glm.html"><a href="glm.html#binom-pred-sec"><i class="fa fa-check"></i><b>3.6.2</b> Prediction</a></li>
<li class="chapter" data-level="3.6.3" data-path="glm.html"><a href="glm.html#bernoulli-sec"><i class="fa fa-check"></i><b>3.6.3</b> Bernoulli GLM</a></li>
<li class="chapter" data-level="3.6.4" data-path="glm.html"><a href="glm.html#probit-link"><i class="fa fa-check"></i><b>3.6.4</b> Probit link</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="glm.html"><a href="glm.html#ordinal-data"><i class="fa fa-check"></i><b>3.7</b> Ordinal Data</a></li>
<li class="chapter" data-level="3.8" data-path="glm.html"><a href="glm.html#non-zero-binomial-data"><i class="fa fa-check"></i><b>3.8</b> Non-zero Binomial Data</a></li>
<li class="chapter" data-level="3.9" data-path="glm.html"><a href="glm.html#complete-separation"><i class="fa fa-check"></i><b>3.9</b> Complete Separation</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="glm.html"><a href="glm.html#gelman-prior-sec"><i class="fa fa-check"></i><b>3.9.1</b> The <span class="citation">Gelman, Jakulin, et al. (2008)</span> prior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ranef.html"><a href="ranef.html"><i class="fa fa-check"></i><b>4</b> Random effects</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ranef.html"><a href="ranef.html#GLMM"><i class="fa fa-check"></i><b>4.1</b> Generalised Linear Mixed Model (GLMM)</a></li>
<li class="chapter" data-level="4.2" data-path="ranef.html"><a href="ranef.html#ranpred-sec"><i class="fa fa-check"></i><b>4.2</b> Prediction with Random Effects</a></li>
<li class="chapter" data-level="4.3" data-path="ranef.html"><a href="ranef.html#overdispersed-binomial-as-a-bernoulli-glmm"><i class="fa fa-check"></i><b>4.3</b> Overdispersed Binomial as a Bernoulli GLMM</a></li>
<li class="chapter" data-level="4.4" data-path="ranef.html"><a href="ranef.html#ICC"><i class="fa fa-check"></i><b>4.4</b> Intra-class Correlations</a></li>
<li class="chapter" data-level="4.5" data-path="ranef.html"><a href="ranef.html#sec-underdispersion"><i class="fa fa-check"></i><b>4.5</b> Underdispersion</a></li>
<li class="chapter" data-level="4.6" data-path="ranef.html"><a href="ranef.html#PXprior-sec"><i class="fa fa-check"></i><b>4.6</b> Priors for Random Effect Variances</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="ranef.html"><a href="ranef.html#f-and-folded-t-priors"><i class="fa fa-check"></i><b>4.6.1</b> <span class="math inline">\(F\)</span> and folded-<span class="math inline">\(t\)</span> priors</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="ranef.html"><a href="ranef.html#Vprior-gen-sec"><i class="fa fa-check"></i><b>4.7</b> Prior Generators</a></li>
<li class="chapter" data-level="4.8" data-path="ranef.html"><a href="ranef.html#priors-on-functions-of-variances"><i class="fa fa-check"></i><b>4.8</b> Priors on Functions of Variances</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="ranef.html"><a href="ranef.html#intra-class-correlation"><i class="fa fa-check"></i><b>4.8.1</b> Intra-class Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="ranef.html"><a href="ranef.html#fix-or-rand"><i class="fa fa-check"></i><b>4.9</b> Fixed or Random?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cat-int.html"><a href="cat-int.html"><i class="fa fa-check"></i><b>5</b> Categorical Random Interactions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="cat-int.html"><a href="cat-int.html#vstruct-sec"><i class="fa fa-check"></i><b>5.1</b> Variance Structures</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="cat-int.html"><a href="cat-int.html#idh-sec"><i class="fa fa-check"></i><b>5.1.1</b> <span class="math inline">\(\texttt{idh}\)</span> Variance Structure</a></li>
<li class="chapter" data-level="5.1.2" data-path="cat-int.html"><a href="cat-int.html#us-sec"><i class="fa fa-check"></i><b>5.1.2</b> <span class="math inline">\(\texttt{us}\)</span> Variance Structure</a></li>
<li class="chapter" data-level="5.1.3" data-path="cat-int.html"><a href="cat-int.html#other-variance-structures"><i class="fa fa-check"></i><b>5.1.3</b> Other Variance Structures</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="cat-int.html"><a href="cat-int.html#linking-functions"><i class="fa fa-check"></i><b>5.2</b> Linking Functions</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="cat-int.html"><a href="cat-int.html#textttstr-covariances-between-random-terms"><i class="fa fa-check"></i><b>5.2.1</b> <span class="math inline">\(\texttt{str}\)</span>: covariances between random terms</a></li>
<li class="chapter" data-level="5.2.2" data-path="cat-int.html"><a href="cat-int.html#multim-sec"><i class="fa fa-check"></i><b>5.2.2</b> <span class="math inline">\(\texttt{mm}\)</span>: multi-membership models</a></li>
<li class="chapter" data-level="5.2.3" data-path="cat-int.html"><a href="cat-int.html#textttcovu-covariances-between-random-and-residual-terms"><i class="fa fa-check"></i><b>5.2.3</b> <span class="math inline">\(\texttt{covu}\)</span>: covariances between random and residual terms</a></li>
<li class="chapter" data-level="5.2.4" data-path="cat-int.html"><a href="cat-int.html#texttttheta_scale-scaled-linear-predictor"><i class="fa fa-check"></i><b>5.2.4</b> <span class="math inline">\(\texttt{theta_scale}\)</span>: scaled linear predictor</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="cat-int.html"><a href="cat-int.html#VCVprior-sec"><i class="fa fa-check"></i><b>5.3</b> Priors for Covariance Matrices</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="cat-int.html"><a href="cat-int.html#marginal-priors-for-variances"><i class="fa fa-check"></i><b>5.3.1</b> Marginal Priors for Variances</a></li>
<li class="chapter" data-level="5.3.2" data-path="cat-int.html"><a href="cat-int.html#VCVprior-r-sec"><i class="fa fa-check"></i><b>5.3.2</b> Marginal Priors for Covariances and Correlations</a></li>
<li class="chapter" data-level="5.3.3" data-path="cat-int.html"><a href="cat-int.html#full-joint-prior"><i class="fa fa-check"></i><b>5.3.3</b> Full joint prior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cont-int.html"><a href="cont-int.html"><i class="fa fa-check"></i><b>6</b> Continuous Random Interactions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="cont-int.html"><a href="cont-int.html#random-regression"><i class="fa fa-check"></i><b>6.1</b> Random Regression</a></li>
<li class="chapter" data-level="6.2" data-path="cont-int.html"><a href="cont-int.html#het-res"><i class="fa fa-check"></i><b>6.2</b> Heterogeneous (Residual) Variances</a></li>
<li class="chapter" data-level="6.3" data-path="cont-int.html"><a href="cont-int.html#autoc-sec"><i class="fa fa-check"></i><b>6.3</b> Autocorrelation</a></li>
<li class="chapter" data-level="6.4" data-path="cont-int.html"><a href="cont-int.html#vstab-sec"><i class="fa fa-check"></i><b>6.4</b> Variance stabilisation</a></li>
<li class="chapter" data-level="6.5" data-path="cont-int.html"><a href="cont-int.html#ante-sec"><i class="fa fa-check"></i><b>6.5</b> Antedependence and Autoregressive Models</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="cont-int.html"><a href="cont-int.html#autoregressive-models"><i class="fa fa-check"></i><b>6.5.1</b> Autoregressive Models</a></li>
<li class="chapter" data-level="6.5.2" data-path="cont-int.html"><a href="cont-int.html#prior-ante-sec"><i class="fa fa-check"></i><b>6.5.2</b> Priors in Antedependence models</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="cont-int.html"><a href="cont-int.html#user-defined-sec"><i class="fa fa-check"></i><b>6.6</b> User-defined Design Matrices</a></li>
<li class="chapter" data-level="6.7" data-path="cont-int.html"><a href="cont-int.html#splines"><i class="fa fa-check"></i><b>6.7</b> Splines</a></li>
<li class="chapter" data-level="6.8" data-path="cont-int.html"><a href="cont-int.html#penalised-signal-regression"><i class="fa fa-check"></i><b>6.8</b> Penalised Signal Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multi-response Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="multi.html"><a href="multi.html#gaussian-multi-response-models"><i class="fa fa-check"></i><b>7.1</b> Gaussian Multi-response Models</a></li>
<li class="chapter" data-level="7.2" data-path="multi.html"><a href="multi.html#non-gaussian-multi-response-models"><i class="fa fa-check"></i><b>7.2</b> Non-Gaussian Multi-response Models</a></li>
<li class="chapter" data-level="7.3" data-path="multi.html"><a href="multi.html#covariances-between-random-and-residual-terms-textttcovu"><i class="fa fa-check"></i><b>7.3</b> Covariances between random and residual terms (<span class="math inline">\(\texttt{covu}\)</span>)</a></li>
<li class="chapter" data-level="7.4" data-path="multi.html"><a href="multi.html#texttttheta_scale-scaled-linear-predictor-1"><i class="fa fa-check"></i><b>7.4</b> <span class="math inline">\(\texttt{theta_scale}\)</span>: scaled linear predictor</a></li>
<li class="chapter" data-level="7.5" data-path="multi.html"><a href="multi.html#multinomial-models"><i class="fa fa-check"></i><b>7.5</b> Multinomial Models</a></li>
<li class="chapter" data-level="7.6" data-path="multi.html"><a href="multi.html#zero-inflated-models"><i class="fa fa-check"></i><b>7.6</b> Zero-inflated Models</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="multi.html"><a href="multi.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>7.6.1</b> Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="multi.html"><a href="multi.html#Hurdle"><i class="fa fa-check"></i><b>7.7</b> Hurdle Models</a></li>
<li class="chapter" data-level="7.8" data-path="multi.html"><a href="multi.html#ZAP"><i class="fa fa-check"></i><b>7.8</b> Zero-altered Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pedigree.html"><a href="pedigree.html"><i class="fa fa-check"></i><b>8</b> Pedigrees and Phylogenies</a>
<ul>
<li class="chapter" data-level="8.1" data-path="pedigree.html"><a href="pedigree.html#pedigree-and-phylogeny-formats"><i class="fa fa-check"></i><b>8.1</b> Pedigree and phylogeny formats</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="pedigree.html"><a href="pedigree.html#pedigrees"><i class="fa fa-check"></i><b>8.1.1</b> Pedigrees</a></li>
<li class="chapter" data-level="8.1.2" data-path="pedigree.html"><a href="pedigree.html#phylogenies"><i class="fa fa-check"></i><b>8.1.2</b> Phylogenies</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="pedigree.html"><a href="pedigree.html#the-animal-model-and-the-phylogenetic-mixed-model"><i class="fa fa-check"></i><b>8.2</b> The animal model and the phylogenetic mixed model</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="measurement.html"><a href="measurement.html"><i class="fa fa-check"></i><b>9</b> Measurement Error, Meta-analysis an Missing Values</a>
<ul>
<li class="chapter" data-level="9.1" data-path="measurement.html"><a href="measurement.html#error-in-the-response"><i class="fa fa-check"></i><b>9.1</b> Error in the Response</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="measurement.html"><a href="measurement.html#meta-sec"><i class="fa fa-check"></i><b>9.1.1</b> Meta-analysis</a></li>
<li class="chapter" data-level="9.1.2" data-path="measurement.html"><a href="measurement.html#interval-estimation"><i class="fa fa-check"></i><b>9.1.2</b> Interval Estimation</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="measurement.html"><a href="measurement.html#error-in-the-predictors"><i class="fa fa-check"></i><b>9.2</b> Error in the Predictors</a></li>
<li class="chapter" data-level="9.3" data-path="measurement.html"><a href="measurement.html#missing-values"><i class="fa fa-check"></i><b>9.3</b> Missing Values</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="path.html"><a href="path.html"><i class="fa fa-check"></i><b>10</b> Path Analysis &amp; Antedependence Structures</a>
<ul>
<li class="chapter" data-level="10.1" data-path="path.html"><a href="path.html#path-anlaysis"><i class="fa fa-check"></i><b>10.1</b> Path Anlaysis</a></li>
<li class="chapter" data-level="10.2" data-path="cont-int.html"><a href="cont-int.html#ante-sec"><i class="fa fa-check"></i><b>10.2</b> Antedependence</a></li>
<li class="chapter" data-level="10.3" data-path="path.html"><a href="path.html#scaling"><i class="fa fa-check"></i><b>10.3</b> Scaling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="technical-details.html"><a href="technical-details.html"><i class="fa fa-check"></i><b>11</b> Technical Details</a>
<ul>
<li class="chapter" data-level="11.1" data-path="technical-details.html"><a href="technical-details.html#model-form"><i class="fa fa-check"></i><b>11.1</b> Model Form</a></li>
<li class="chapter" data-level="11.2" data-path="technical-details.html"><a href="technical-details.html#MCMC-app"><i class="fa fa-check"></i><b>11.2</b> MCMC Sampling Schemes</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="technical-details.html"><a href="technical-details.html#updating-the-latent-variables-bf-l"><i class="fa fa-check"></i><b>11.2.1</b> Updating the latent variables <span class="math inline">\({\bf l}\)</span></a></li>
<li class="chapter" data-level="11.2.2" data-path="technical-details.html"><a href="technical-details.html#updating-the-location-vector-boldsymboltheta-leftboldsymbolmathbfbeta-bf-uright"><i class="fa fa-check"></i><b>11.2.2</b> Updating the location vector <span class="math inline">\(\boldsymbol{\theta} = \left[{\boldsymbol{\mathbf{\beta}}}^{&#39;}\; {\bf u}^{&#39;}\right]^{&#39;}\)</span></a></li>
<li class="chapter" data-level="11.2.3" data-path="technical-details.html"><a href="technical-details.html#updating-the-variance-structures-bf-g-and-bf-r"><i class="fa fa-check"></i><b>11.2.3</b> Updating the variance structures <span class="math inline">\({\bf G}\)</span> and <span class="math inline">\({\bf R}\)</span></a></li>
<li class="chapter" data-level="11.2.4" data-path="technical-details.html"><a href="technical-details.html#ordinal-models"><i class="fa fa-check"></i><b>11.2.4</b> Ordinal Models</a></li>
<li class="chapter" data-level="11.2.5" data-path="technical-details.html"><a href="technical-details.html#path-analyses"><i class="fa fa-check"></i><b>11.2.5</b> Path Analyses</a></li>
<li class="chapter" data-level="11.2.6" data-path="technical-details.html"><a href="technical-details.html#deviance-and-dic"><i class="fa fa-check"></i><b>11.2.6</b> Deviance and DIC</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="technical-details.html"><a href="technical-details.html#parameter-expansion"><i class="fa fa-check"></i><b>11.3</b> Parameter Expansion</a></li>
<li class="chapter" data-level="11.4" data-path="technical-details.html"><a href="technical-details.html#priors-for-corg-and-corgh-structures"><i class="fa fa-check"></i><b>11.4</b> Priors for <code>corg</code> and <code>corgh</code> structures</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MCMCglmm Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="technical-details" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">11</span> Technical Details<a href="technical-details.html#technical-details" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="model-form" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Model Form<a href="technical-details.html#model-form" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The probability of the <span class="math inline">\(i^{th}\)</span> data point is represented by:</p>
<p><span class="math display" id="eq:pyl-Eq">\[f_{i}(y_{i} | l_{i})
\label{pyl-Eq}   \tag{11.1}\]</span></p>
<p>where <span class="math inline">\(f_{i}\)</span> is the probability density function associated with <span class="math inline">\(y_{i}\)</span>. For example, if <span class="math inline">\(y_{i}\)</span> was assumed to be Poisson distributed and we used the canonical log link function, then Equation <a href="technical-details.html#eq:pyl-Eq">(11.1)</a> would have the form:</p>
<p><span class="math display" id="eq:pyl2-Eq">\[f_{P}\left(y_{i} | \lambda = \textrm{exp}(l_{i})\right)
\label{pyl2-Eq}   \tag{11.2}\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the canonical parameter of the Poisson density function <span class="math inline">\(f_{P}\)</span>. Table <a href="technical-details.html#tab:dist">11.1</a> has a full list of supported distributions and link functions.</p>
<p>The vector of latent variables follow the linear model</p>
<p><span class="math display" id="eq:l-Eq">\[{\bf l}  = {\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf Z}{\bf u}+{\bf e}
\label{l-Eq}   \tag{11.3}\]</span></p>
<p>where <span class="math inline">\({\bf X}\)</span> is a design matrix relating fixed predictors to the data, and <span class="math inline">\({\bf Z}\)</span> is a design matrix relating random predictors to the data. These predictors have associated parameter vectors <span class="math inline">\({\boldsymbol{\mathbf{\beta}}}\)</span> and <span class="math inline">\({\bf u}\)</span>, and <span class="math inline">\({\bf e}\)</span> is a vector of residuals. In the Poisson case these residuals deal with any overdispersion in the data after accounting for fixed and random sources of variation.</p>
<p>The location effects (<span class="math inline">\({\boldsymbol{\mathbf{\beta}}}\)</span> and <span class="math inline">\({\bf u}\)</span>), and the residuals (<span class="math inline">\({\bf e}\)</span>) are assumed to come from a multivariate normal distribution:</p>
<p><span class="math display" id="eq:V-Eq">\[\left[
\begin{array}{c}
{\boldsymbol{\mathbf{\beta}}}\\
{\bf u}\\
{\bf e}
\end{array}
\right]
\sim N\left(
\left[
\begin{array}{c}
{\boldsymbol{\mathbf{\beta}}}_{0}\\
{\bf 0}\\
{\bf 0}\\
\end{array}
\right]
,
\left[
\begin{array}{ccc}
{\bf B}&amp;{\bf 0}&amp;{\bf 0}\\
{\bf 0}&amp;{\bf G}&amp;{\bf 0}\\
{\bf 0}&amp;{\bf 0}&amp;{\bf R}\\
\end{array}
\right]
\right)
\label{V-Eq}   \tag{11.4}\]</span></p>
<p>where <span class="math inline">\({\boldsymbol{\mathbf{\beta}}}_{0}\)</span> is a vector of prior means for the fixed effects with prior (co)variance <span class="math inline">\({\bf B}\)</span>, and <span class="math inline">\({\bf G}\)</span> and <span class="math inline">\({\bf R}\)</span> are the expected (co)variances of the random effects and residuals respectively. The zero off-diagonal matrices imply <em>a priori</em> independence between fixed effects, random effects, and residuals. Generally, <span class="math inline">\({\bf G}\)</span> and <span class="math inline">\({\bf R}\)</span> are large square matrices with dimensions equal to the number of random effects or residuals. Typically they are unknown, and must be estimated from the data, usually by assuming they are structured in a way that they can be parameterised by few parameters. Below we will focus on the structure of <span class="math inline">\({\bf G}\)</span>, but the same logic can be applied to <span class="math inline">\({\bf R}\)</span>.</p>
<p>At its most general, <span class="math inline">\(\texttt{MCMCglmm}\)</span> allows variance structures of the form:</p>
<p><span class="math display" id="eq:G3-Eq">\[{\bf G}= \left({\bf V}_{1}\otimes{\bf A}_{1}\right) \oplus \left({\bf V}_{2}\otimes{\bf A}_{2}\right) \oplus \ldots
\label{G3-Eq}   \tag{11.5}\]</span></p>
<p>where the parameter (co)variance matrices (<span class="math inline">\({\bf V}\)</span>) are usually low-dimensional and are to be estimated, and the structured matrices (<span class="math inline">\({\bf A}\)</span>) are usually high dimensional and treated as known.</p>
<p>In the case of ordinal probit models with <span class="math inline">\(&gt;2\)</span> categories (i.e. <code>"threshold"</code> or <code>"ordinal"</code> models), <span class="math inline">\(f_{T}/f_{O}\)</span> depends on an extra set of parameters in addition to the latent variable: the <span class="math inline">\(\textrm{max}(y)+1\)</span> cutpoints <span class="math inline">\({\boldsymbol{\mathbf{\gamma}}}\)</span>. The probability of <span class="math inline">\(y_{i}\)</span> is then:</p>
<p><span class="math display">\[f_{T}(y_{i} | l_{i}, {\boldsymbol{\mathbf{\gamma}}}) = 1\ \textrm{if}\ \gamma_{y_{i}+1} &lt;  l_{i} &lt; \gamma_{y_{i}}\]</span></p>
<p>and</p>
<p><span class="math display">\[f_{O}(y_{i} | l_{i}, {\boldsymbol{\mathbf{\gamma}}}) = F_{N}(\gamma_{y_{i}} | l_{i}, 1)-F_{N}(\gamma_{y_{i}+1} | l_{i},1)\]</span></p>
<p>where <span class="math inline">\(F_{N}\)</span> is the cumulative density function for the normal. Note that the two models can be made equivalent.</p>
</div>
<div id="MCMC-app" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> MCMC Sampling Schemes<a href="technical-details.html#MCMC-app" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="updating-the-latent-variables-bf-l" class="section level3 hasAnchor" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> Updating the latent variables <span class="math inline">\({\bf l}\)</span><a href="technical-details.html#updating-the-latent-variables-bf-l" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The conditional density of <span class="math inline">\(l\)</span> is given by:</p>
<p><span class="math display" id="eq:pcl-Eq">\[Pr(l_{i}| {\bf y}, \boldsymbol{\theta}, {\bf R}, {\bf G}) \propto  f_{i}(y_{i} | l_{i})f_{N}(e_{i}|{\bf r}_{i}{\bf R}_{/i}^{-1}{\bf e}_{/i}, r_{i}-{\bf r}_{i}{\bf R}_{/i}^{-1}{\bf r}^{&#39;}_{i})
\label{pcl-Eq}   \tag{11.6}\]</span></p>
<p>where <span class="math inline">\(f_{N}\)</span> indicates a Multivariate normal density with specified mean vector and covariance matrix. Equation <a href="technical-details.html#eq:pcl-Eq">(11.6)</a> is the probability of the data point <span class="math inline">\(y_{i}\)</span> from distribution <span class="math inline">\(f_{i}\)</span> with latent varaible <span class="math inline">\(l_{i}\)</span>, multiplied by the probability of the linear predictor residual. The linear predictor residual follows a conditional normal distribution where the conditioning is on the residuals associated with data points other than <span class="math inline">\(i\)</span>. Vectors and matrices with the row and/or column associated with <span class="math inline">\(i\)</span> removed are denoted <span class="math inline">\(/i\)</span>.</p>
<p>Three special cases exist for which we sample directly from Equation <a href="technical-details.html#eq:pcl-Eq">(11.6)</a>: i) When <span class="math inline">\(y_{i}\)</span> is normal <span class="math inline">\(f_{i}(y_{i} | l_{i})=1\)</span> if <span class="math inline">\(y_{i}=l_{i}\)</span> and zero otherwise so <span class="math inline">\(l_{i}=y_{i}\)</span> with out the need for updating, ii) when <span class="math inline">\(y_{i}\)</span> is discrete and modelled using <code>family="threshold"</code> then Equation <span id="pcl-Eq" label="pcl-Eq"></span> defines a truncated normal distribution and can be slice sampled <span class="citation">(<a href="#ref-Robert.1995">Robert 1995</a>)</span> and iii) when <span class="math inline">\(y_{i}\)</span> is missing <span class="math inline">\(f_{i}(y_{i} | l_{i})\)</span> is not defined and samples can drawn directly from the normal.</p>
<p>In practice, the conditional distribution in Equation <a href="technical-details.html#eq:pcl-Eq">(11.6)</a> only involves other residuals which are expected to show some form of residual covariation, as defined by the <span class="math inline">\({\bf R}\)</span> structure. Because of this we actually update latent variables in blocks, where the block is defined as groups of residuals which are expected to be correlated:</p>
<p><span class="math display" id="eq:pcl2-Eq">\[Pr({\bf l}_{j}|{\bf y}, \boldsymbol{\theta}, {\bf R}, {\bf G}) \propto   \prod_{i \in j}{p}_{i}({y}_{i} | l_{i})f_{N}({\bf e}_{j}|{\bf 0}, {\bf R}_{j})
\label{pcl2-Eq}   \tag{11.7}\]</span></p>
<p>where <span class="math inline">\(j\)</span> indexes blocks of latent variables that have non-zero residual covariances. For response variables that are neither Gaussian nor threshold, the density in equation <a href="technical-details.html#eq:pcl2-Eq">(11.7)</a> is in non-standard form and so Metropolis-Hastings updates are employed. We use adaptive methods during the burn-in phase to determine an efficient multivariate normal proposal distribution centered at the previous value of <span class="math inline">\({\bf l}_{j}\)</span> with covariance matrix <span class="math inline">\(m{\bf M}\)</span>. For computational efficiency we use the same <span class="math inline">\({\bf M}\)</span> for each block <span class="math inline">\(j\)</span>, where <span class="math inline">\({\bf M}\)</span> is the average posterior (co)variance of <span class="math inline">\({\bf l}_{j}\)</span> within blocks and is updated each iteration of the burn-in period <span class="citation">Haario, Saksman, and Tamminen (<a href="#ref-Haario.2001">2001</a>)</span>. The scalar <span class="math inline">\(m\)</span> is chosen using the method of <span class="citation">Ovaskainen et al. (<a href="#ref-Ovaskainen.2008">2008</a>)</span> so that the proportion of successful jumps is optimal, with a rate of 0.44 when <span class="math inline">\({\bf l}_{j}\)</span> is a scalar declining to 0.23 when <span class="math inline">\({\bf l}_{j}\)</span> is high dimensional <span class="citation">(<a href="#ref-Gelman.2004">Gelman et al. 2004</a>)</span>.</p>
<p>A special case arises for multi-parameter distributions in which each parameter is associated with a linear predictor. For example, in the zero-inflated Poisson two linear predictors are used to model the same data point, one to predict zero-inflation, and one to predict the Poisson variable. In this case the two linear predictors are updated in a single block even when the residual covariance between them is set to zero, because the first probability in Equation <a href="technical-details.html#eq:pcl2-Eq">(11.7)</a> cannot
be factored:</p>
<p><span class="math display" id="eq:pcl3-Eq">\[Pr({\bf l}_{j}|{\bf y}, \boldsymbol{\theta}, {\bf R}, {\bf G}) \propto    {p}_{i}({y}_{i} | {\bf l}_{j})({\bf e}_{j}|{\bf 0}, {\bf R}_{j})
\label{pcl3-Eq}   \tag{11.8}\]</span></p>
<p>When the block size is one (i.e. a univariate analysis) then the latent variables can be slice sampled for two-category <code>ordinal</code> and <code>categorical</code> models if <code>slice=TRUE</code> is passed to <span class="math inline">\(\texttt{MCMCglmm}\)</span>.<br />
</p>
</div>
<div id="updating-the-location-vector-boldsymboltheta-leftboldsymbolmathbfbeta-bf-uright" class="section level3 hasAnchor" number="11.2.2">
<h3><span class="header-section-number">11.2.2</span> Updating the location vector <span class="math inline">\(\boldsymbol{\theta} = \left[{\boldsymbol{\mathbf{\beta}}}^{&#39;}\; {\bf u}^{&#39;}\right]^{&#39;}\)</span><a href="technical-details.html#updating-the-location-vector-boldsymboltheta-leftboldsymbolmathbfbeta-bf-uright" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="citation">Garcia-Cortes and Sorensen (<a href="#ref-Garcia-Cortes.2001">2001</a>)</span> provide a method for sampling <span class="math inline">\(\boldsymbol{\theta}\)</span> as a complete block that involves solving the sparse linear system:</p>
<p><span class="math display" id="eq:sMME-Eq">\[\tilde{\boldsymbol{\theta}} = {\bf C}^{-1}{\bf W}^{&#39;}{\bf R}^{-1}({\bf l} - {\bf W}\boldsymbol{\theta}_{\star}-{\bf e}_{\star})
\label{sMME-Eq}   \tag{11.9}\]</span></p>
<p>where <span class="math inline">\({\bf C}\)</span> is the mixed model coefficient matrix:</p>
<p><span class="math display">\[{\bf C} = {\bf W}^{&#39;}{\bf R}^{-1}{\bf W}+
\left[
\begin{array}{c c}
{\bf B}^{-1}&amp;{\bf 0}\\
{\bf 0}&amp;{\bf G}^{-1}\\
\end{array}
\right]\]</span></p>
<p>and <span class="math inline">\({\bf W} = \left[{\bf X}\; {\bf Z}\right]\)</span>, and <span class="math inline">\({\bf B}\)</span> is the prior (co)variance matrix for the fixed effects.</p>
<p><span class="math inline">\(\boldsymbol{\theta}_{\star}\)</span> and <span class="math inline">\({\bf e}_{\star}\)</span> are random draws from the multivariate normal distributions:</p>
<p><span class="math display">\[\boldsymbol{\theta}_{\star} \sim N\left(
\left[
\begin{array}{c}
{\boldsymbol{\mathbf{\beta}}_{0}}\\
{\bf 0}\\
\end{array}
\right]
,
\left[
\begin{array}{c c}
{\bf B}&amp;{\bf 0}\\
{\bf 0}&amp;{\bf G}\\
\end{array}
\right]
\right)\]</span></p>
<p>and</p>
<p><span class="math display">\[{\bf e}_{\star} \sim N\left({\bf 0},{\bf R}\right)\]</span></p>
<p><span class="math inline">\(\tilde{\boldsymbol{\theta}} + \boldsymbol{\theta}_{\star}\)</span> gives a realisation from the required probability distribution:</p>
<p><span class="math display">\[Pr(\boldsymbol{\theta} | {\bf l}, {\bf W}, {\bf R}, {\bf G})\]</span></p>
<p>Equation <a href="technical-details.html#eq:sMME-Eq">(11.9)</a> is solved using Cholesky factorisation. Because <span class="math inline">\({\bf C}\)</span> is sparse and the pattern of non-zero elements fixed, an initial symbolic Cholesky factorisation of <span class="math inline">\({\bf P}{\bf C}{\bf P}^{&#39;}\)</span> is preformed where <span class="math inline">\({\bf P}\)</span> is a fill-reducing permutation matrix <span class="citation">(<a href="#ref-Davis.2006">Davis 2006</a>)</span>. Numerical factorisation must be performed each iteration but the fill-reducing permutation (found via a minimum degree ordering of <span class="math inline">\({\bf C}+{\bf C}^{&#39;}\)</span>) reduces the computational burden dramatically compared to a direct factorisation of <span class="math inline">\({\bf C}\)</span> <span class="citation">(<a href="#ref-Davis.2006">Davis 2006</a>)</span>.</p>
<p>Forming the inverse of the variance structures is usually simpler because they can be expressed as a series of direct sums and Kronecker products:</p>
<p><span class="math display">\[{\bf G}= \left({\bf V}_{1}\otimes{\bf A}_{1}\right) \oplus \left({\bf V}_{2}\otimes{\bf A}_{2}\right) \oplus \ldots\]</span></p>
<p>and the inverse of such a structure has the form</p>
<p><span class="math display">\[{\bf G}^{-1} = \left({\bf V}^{-1}_{1}\otimes{\bf A}^{-1}_{1}\right) \oplus \left({\bf V}^{-1}_{2}\otimes{\bf A}^{-1}_{2}\right) \oplus \ldots\\\]</span></p>
<p>which involves inverting the parameter (co)variance matrices (<span class="math inline">\({\bf V}\)</span>), which are usually of low dimension, and inverting <span class="math inline">\({\bf A}\)</span>. For many problems <span class="math inline">\({\bf A}\)</span> is actually an identity matrix and so inversion is not required. When <span class="math inline">\({\bf A}\)</span> is a relationship matrix associated with a pedigree, <span class="citation">Henderson (<a href="#ref-Henderson.1976">1976</a>; <a href="#ref-Meuwissen.1992">Meuwissen and Luo 1992</a>)</span> give efficient recursive algorithms for obtaining the inverse, and <span class="citation">Hadfield and Nakagawa (<a href="#ref-Hadfield.2010b">2010</a>)</span> derive a similar procedure for phylogenies.</p>
</div>
<div id="updating-the-variance-structures-bf-g-and-bf-r" class="section level3 hasAnchor" number="11.2.3">
<h3><span class="header-section-number">11.2.3</span> Updating the variance structures <span class="math inline">\({\bf G}\)</span> and <span class="math inline">\({\bf R}\)</span><a href="technical-details.html#updating-the-variance-structures-bf-g-and-bf-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Components of the direct sum used to construct the desired variance structures are conditionally independent. The sum of squares matrix associated with each component term has the form:</p>
<p><span class="math display">\[{\bf S} = {\bf U}^{&#39;}{\bf A}^{-1}{\bf U}\]</span></p>
<p>where <span class="math inline">\({\bf U}\)</span> is a matrix of random effects where each column is associated with the relevant row/column of <span class="math inline">\({\bf V}\)</span> and each row associated with the relevant row/column of <span class="math inline">\({\bf A}\)</span>. The parameter (co)variance matrix can then be sampled from the inverse Wishart distribution:</p>
<p><span class="math display" id="eq:pIW-Eq">\[{\bf V} \sim IW(({\bf S}_{p}+{\bf S})^{-1},\ n_{p}+n)
\label{pIW-Eq}   \tag{11.10}\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of rows in <span class="math inline">\({\bf U}\)</span>, and <span class="math inline">\({\bf S}_{p}\)</span> and <span class="math inline">\(n_{p}\)</span> are the prior sum of squares and prior degree’s of freedom, respectively.</p>
<p>In some models, some elements of a parameter (co)variance matrix cannot be estimated from the data and all the information comes from the prior. In these cases it can be advantageous to fix these elements at some value and <span class="citation">Korsgaard, Andersen, and Sorensen (<a href="#ref-Korsgaard.1999">1999</a>)</span> provide a strategy for sampling from a conditional inverse-Wishart distribution which is appropriate when the rows/columns of the parameter matrix can be permuted so that the conditioning occurs on some diagonal sub-matrix. When this is not possible Metropolis-Hastings updates can be made.</p>
</div>
<div id="ordinal-models" class="section level3 hasAnchor" number="11.2.4">
<h3><span class="header-section-number">11.2.4</span> Ordinal Models<a href="technical-details.html#ordinal-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For ordinal models it is necessary to update the cutpoints which define the bin boundaries for latent variables associated with each category of the outcome. To achieve good mixing we used the method developed by <span class="citation">(<a href="#ref-Cowles.1996">Cowles 1996</a>)</span> that allows the latent variables and cutpoints to be updated simultaneously using a Hastings-with-Gibbs update.</p>
</div>
<div id="path-analyses" class="section level3 hasAnchor" number="11.2.5">
<h3><span class="header-section-number">11.2.5</span> Path Analyses<a href="technical-details.html#path-analyses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Elements of the response vector can be regressed on each other using the <code>sir</code> and <code>path</code> functions. Using the matrix notation of <span class="citation">Gianola and Sorensen (<a href="#ref-Gianola.2004">2004</a>)</span>, Equation <a href="technical-details.html#eq:l-Eq">(11.3)</a> can be rewritten as:</p>
<p><span class="math display" id="eq:rs-Eq1">\[\boldsymbol{\Lambda}{\bf l}  = {\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf Z}{\bf u}+{\bf e}
\label{rs-Eq1}   \tag{11.11}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Lambda}\)</span> is a square matrix of the form:</p>
<p><span class="math display" id="eq:rs-Eq2">\[\begin{array}{rl}
\boldsymbol{\Lambda} =&amp; {\bf I}-\sum_{l}\boldsymbol{\Psi}^{(l)}\lambda_{l}\\
\end{array}
\label{rs-Eq2}   \tag{11.12}\]</span></p>
<p>This sets up a regression where the <span class="math inline">\(i^{th}\)</span> element of the response vector acts as a weighted (by <span class="math inline">\(\Psi^{(l)}_{i,j}\)</span>) predictor for the <span class="math inline">\(j^{th}\)</span> element of the response vector with associated regression parameter <span class="math inline">\(\lambda_{l}\)</span>. Often <span class="math inline">\(\boldsymbol{\Psi}^{(l)}\)</span> is an incidence matrix with the patterns of ones determining which elements of the response are regressed on each other.</p>
<p>Conditional on the vector of regression coefficients <span class="math inline">\(\boldsymbol{\Lambda}\)</span>, the location effects and variance structures can be updated as before by simply substituting <span class="math inline">\({\bf l}\)</span> for <span class="math inline">\(\boldsymbol{\Lambda}{\bf l}\)</span> in the necessary equations. <span class="citation">Gianola and Sorensen (<a href="#ref-Gianola.2004">2004</a>)</span> provide a simple scheme for updating <span class="math inline">\(\boldsymbol{\Lambda}\)</span>. Note that Equation <a href="technical-details.html#eq:rs-Eq1">(11.11)</a> can be rewritten as:</p>
<p><span class="math display">\[\begin{array}{rl}
{\bf l} - {\bf X}{\boldsymbol{\mathbf{\beta}}} - {\bf Z}{\bf u} =&amp; {\bf e}+\sum_{l}\boldsymbol{\Psi}^{(l)}{\bf l}\lambda_{l}\\
                                              =&amp; {\bf e}+{\bf L}\boldsymbol{\Lambda}\\
\end{array}\]</span></p>
<p>where <span class="math inline">\({\bf L}\)</span> is the design matrix <span class="math inline">\(\left[\boldsymbol{\Psi}^{(1)}{\bf l}, \boldsymbol{\Psi}^{(2)}{\bf l} \dots \boldsymbol{\Psi}^{(L)}{\bf l}\right]\)</span> for the <span class="math inline">\(L\)</span> path coefficients. Conditional on <span class="math inline">\({\boldsymbol{\mathbf{\beta}}}\)</span> and <span class="math inline">\({\boldsymbol{\mathbf{u}}}\)</span>, <span class="math inline">\(\boldsymbol{\Lambda}\)</span> can then be sampled using the method of <span class="citation">Garcia-Cortes and Sorensen (<a href="#ref-Garcia-Cortes.2001">2001</a>)</span> with <span class="math inline">\({\bf l} - {\bf X}{\boldsymbol{\mathbf{\beta}}} - {\bf Z}{\bf u}\)</span> as response and <span class="math inline">\({\bf L}\)</span> as predictor. However, only in a fully recursive system (there exists a row/column permutation by which all <span class="math inline">\(\boldsymbol{\Psi}\)</span>’s are triangular) are the resulting draws from the appropriate conditional distribution, which requires multiplication by the Jacobian of the transform: <span class="math inline">\(|\boldsymbol{\Lambda}|\)</span>. An extra Metropolis Hastings step is used to accept/reject the proposed draw when <span class="math inline">\(|\boldsymbol{\Lambda}|\neq 1\)</span>.</p>
<p>When the response vector is Gaussian and fully observed, the latent variable does not need updating. For non-Gaussian data, or with missing responses, updating the latent variable is difficult because Equation <a href="technical-details.html#eq:pcl-Eq">(11.6)</a> becomes:</p>
<p><span class="math display">\[Pr(l_{i}| {\bf y}, \boldsymbol{\theta}, {\bf R}, {\bf G}, \boldsymbol{\Lambda}) \propto  f_{i}(y_{i} | l_{i})f_{N}((\boldsymbol{\Lambda}^{-1}{\bf e})_{i}|{\bf q}_{i}{\bf Q}_{/i}^{-1}{\bf e}_{/i}, q_{i}-{\bf q}_{i}{\bf Q}_{/i}^{-1}{\bf q}^{&#39;}_{i})\]</span></p>
<p>where <span class="math inline">\({\bf Q} = \boldsymbol{\Lambda}^{-1}{\bf R}\boldsymbol{\Lambda}^{-\top}\)</span>. In the general case <span class="math inline">\({\bf Q}\)</span> will not have block diagonal structure like <span class="math inline">\({\bf R}\)</span> and so the scheme for updating latent variables within residual blocks (i.e. Equation <a href="technical-details.html#eq:pcl2-Eq">(11.7)</a>) is not possible. However, in some cases <span class="math inline">\(\boldsymbol{\Lambda}\)</span> may have the form where all non-zero elements correspond to elements of the response vector that are in the same residual block. In such cases updating the latent variables remains relatively simple:</p>
<p><span class="math display">\[Pr({\bf l}_{j}|{\bf y}, \boldsymbol{\theta}, {\bf R}, {\bf G}) \propto    {p}_{i}({y}_{i} | {\bf l}_{j})(\boldsymbol{\Lambda}^{-1}_{j}{\bf e}_{j}|{\bf 0}, \boldsymbol{\Lambda}^{-1}_{j}{\bf R}_{j}\boldsymbol{\Lambda}^{-\top}_{j})\]</span></p>
</div>
<div id="deviance-and-dic" class="section level3 hasAnchor" number="11.2.6">
<h3><span class="header-section-number">11.2.6</span> Deviance and DIC<a href="technical-details.html#deviance-and-dic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The deviance <span class="math inline">\(D\)</span> is defined as:</p>
<p><span class="math display">\[D = -2\textrm{log}(\Pr({\bf y} | {\boldsymbol{\mathbf{\Omega}}}))\]</span></p>
<p>where <span class="math inline">\({\boldsymbol{\mathbf{\Omega}}}\)</span> is some parameter set of the model. The deviance can be calculated in different ways depending on what is in ‘focus’, and <span class="math inline">\(\texttt{MCMCglmm}\)</span> calculates this probability for the lowest level of the hierarchy <span class="citation">(<a href="#ref-Spiegelhalter.2002">Spiegelhalter et al. 2002</a>)</span>. For fully-observed Gaussian response variables in the likelihood is the density:</p>
<p><span class="math display">\[f_{N}({\bf y} | {\bf W}\boldsymbol{\theta},\ {\bf R})\]</span></p>
<p>where <span class="math inline">\({\boldsymbol{\mathbf{\Omega}}} = \left\{\boldsymbol{\theta},\ {\bf R}\right\}\)</span>. For discrete response variables in univariate analyses modeled using <code>family="threshold"</code> the density is</p>
<p><span class="math display">\[\prod_{i} F_{N}(\gamma_{y_{i}} | {\bf w}_{i}\boldsymbol{\theta}, \ r_{ii})-F_{N}(\gamma_{y_{i}+1} | {\bf w}_{i}\boldsymbol{\theta}, \ r_{ii})\]</span></p>
<p>where <span class="math inline">\({\boldsymbol{\mathbf{\Omega}}} = \left\{{\boldsymbol{\mathbf{\gamma}}},\ \boldsymbol{\theta},\ {\bf R}\right\}\)</span>. For other response variables variables (including discrete response
variables modeled using <code>family="ordinal"</code>) it is the product:</p>
<p><span class="math display" id="eq:LLikL">\[\prod_{i}f_{i}(y_{i} | l_{i})
\label{LLikL}   \tag{11.13}\]</span></p>
<p>with <span class="math inline">\({\boldsymbol{\mathbf{\Omega}}} = {\bf l}\)</span>.</p>
<p>For multivariate models with mixtures of Gaussian (g), threshold (t) and other non-Gaussian (n) data (including missing data) we can define the deviance in terms of three conditional densities:</p>
<p><span class="math display" id="eq:Eq-MVdeviance">\[\begin{array}{rl}
Pr({\bf y} | {\boldsymbol{\mathbf{\Omega}}}) =&amp; \Pr({\bf y}_{g}, {\bf y}_{t}, {\bf y}_{n} | {\boldsymbol{\mathbf{\gamma}}}, \boldsymbol{\theta}_{g}, \boldsymbol{\theta}_{t}, {\boldsymbol{\mathbf{l}}}_{n}, {\boldsymbol{\mathbf{R}}})\\
                           =&amp; \Pr({\bf y}_{t} | {\boldsymbol{\mathbf{\gamma}}}, \boldsymbol{\theta}_{t}, {\bf y}_{g}, {\boldsymbol{\mathbf{l}}}_{n}, {\boldsymbol{\mathbf{R}}})\Pr({\bf y}_{g} | \boldsymbol{\theta}_{g}, {\boldsymbol{\mathbf{l}}}_{n}, {\boldsymbol{\mathbf{R}}})\Pr({\bf y}_{n} | {\boldsymbol{\mathbf{l}}}_{n})\\
\label{Eq-MVdeviance}
\end{array}   \tag{11.14}\]</span></p>
<p>with <span class="math inline">\({\boldsymbol{\mathbf{\Omega}}} = \left\{{\boldsymbol{\mathbf{\gamma}}},\ \boldsymbol{\theta}_{/n},\ {\boldsymbol{\mathbf{l}}}_{n}\ {\bf R}\right\}\)</span>. Have <span class="math inline">\(({\boldsymbol{\mathbf{W}}}\boldsymbol{\theta})_{a|b}={\boldsymbol{\mathbf{W}}}_{a}\boldsymbol{\theta}_{a}+{\bf R}_{a,b}{\bf R}^{-1}_{b,b}({\bf l}_{b}-{\boldsymbol{\mathbf{W}}}_{b}\boldsymbol{\theta}_{b})\)</span> and <span class="math inline">\({\bf R}_{a |b} = {\bf R}_{a,a}-{\bf R}_{a,b}{\bf R}^{-1}_{b,b}{\bf R}_{a,b}\)</span> where the subscripts denote rows of the data vector/design matrices or rows/columns of the <span class="math inline">\({\bf R}\)</span>-structure. Then, the conditional density of <span class="math inline">\({\bf y}_{g}\)</span> in Equation <a href="technical-details.html#eq:Eq-MVdeviance">(11.14)</a> is:</p>
<p><span class="math display">\[f_{N}\left({\bf y}_{g} | ({\boldsymbol{\mathbf{W}}}\boldsymbol{\theta})_{g|n},\ {\bf R}_{g|n}\right)\]</span></p>
<p>The conditional density of <span class="math inline">\({\bf y}_{n}\)</span> in Equation <a href="technical-details.html#eq:Eq-MVdeviance">(11.14)</a> is identical to that given in Equation <a href="technical-details.html#eq:LLikL">(11.13)</a>, and for a single <code>"threshold"</code> trait</p>
<p><span class="math display" id="eq:Eq-cpmvnorm">\[\prod_{i} F_{N}(\gamma_{y_{i}} | ({\boldsymbol{\mathbf{W}}}\boldsymbol{\theta})_{ti|g,n}, \ r_{ti|g, n})-F_{N}(\gamma_{y_{i}+1} | ({\boldsymbol{\mathbf{W}}}\boldsymbol{\theta})_{ti|g,n}, \ r_{ti|g, n})
\label{Eq-cpmvnorm}   \tag{11.15}\]</span></p>
<p>is the conditional density for <span class="math inline">\({\bf y}_{t}\)</span> in Equation <a href="technical-details.html#eq:Eq-MVdeviance">(11.14)</a>, where <span class="math inline">\(({\boldsymbol{\mathbf{W}}}\boldsymbol{\theta})_{ti|g,n}\)</span> is the <span class="math inline">\(i^{\textrm{th}}\)</span> element of <span class="math inline">\(({\boldsymbol{\mathbf{W}}}\boldsymbol{\theta})_{t|g,n}\)</span>. Currently the deviance (and hence the DIC) will not be returned if there
is more than one threshold trait.</p>
<p>The deviance is calculated at each iteration if <code>DIC=TRUE</code> and stored each <code>thin</code><span class="math inline">\(^{th}\)</span> iteration after burn-in. However, for computational reasons the deviance is calculated mid-iteration such that the deviance returned at iteration <span class="math inline">\(i\)</span> uses <span class="math inline">\({\boldsymbol{\mathbf{\Omega}}}_{i} = \left\{{\boldsymbol{\mathbf{\gamma}}}_{i},\ \boldsymbol{\theta}_{/n, i},\ {\boldsymbol{\mathbf{l}}}_{n, i-1}\ {\bf R}_{i}\right\}\)</span>. The mean deviance (<span class="math inline">\(\bar{D}\)</span>) is calculated over all iterations, as is the mean of the latent variables (<span class="math inline">\({\bf l}\)</span>) the <span class="math inline">\({\bf R}\)</span>-structure and the vector of predictors (<span class="math inline">\({\bf W}\boldsymbol{\theta}\)</span>). The deviance is calculated at the mean estimate of the parameters (<span class="math inline">\(D(\bar{\boldsymbol{\mathbf{\Omega}}})\)</span>) and the deviance information criterion calculated as:</p>
<p><span class="math display">\[\textrm{DIC} = 2\bar{D}-D(\bar{\boldsymbol{\mathbf{\Omega}}})\]</span></p>
<div style="border: 0px;overflow-x: scroll; width:100%; ">
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:dist">Table 11.1: </span>Distribution types that can fitted using <span class="math inline">\(\texttt{MCMCglmm}\)</span>. The prefixes <code>"zi"</code>, <code>"zt"</code>, <code>"hu"</code> and <code>"za"</code> stand for zero-inflated, zero-truncated, hurdle and zero-altered respectively. The prefix <code>"cen"</code> standards for censored where <span class="math inline">\(y_{1}\)</span> and <span class="math inline">\(y_{2}\)</span> are the upper and lower bounds for the unobserved datum <span class="math inline">\(y\)</span>. <span class="math inline">\(J\)</span> stands for the number of categories in the multinomial, categorical and zero-truncated multivariate Bernoulli distributions and this must be specified in the family argument for the multinomial and zero-truncated multivariate Bernoulli distributions. The density function is for a single datum in a univariate model with <span class="math inline">\({\bf w}^{\top}\)</span> being a row vector of <span class="math inline">\({\bf W}\)</span>. <span class="math inline">\(f\)</span> and <span class="math inline">\(F\)</span> are the density and distribution functions for the subscripted distribution (<span class="math inline">\(N\)</span>=Normal, <span class="math inline">\(P\)</span>=Poisson, <span class="math inline">\(E\)</span>=Exponential, <span class="math inline">\(G\)</span>=Geometric, <span class="math inline">\(B\)</span>=Binomial, <span class="math inline">\(NCt\)</span>=non-central <span class="math inline">\(t\)</span> and <span class="math inline">\(t\)</span>=Student’s <span class="math inline">\(t\)</span>). The <span class="math inline">\(J-1\)</span> <span class="math inline">\(\gamma\)</span>’s in the ordinal/threshold models are the cut-points, with <span class="math inline">\(\gamma_{1}\)</span> set to zero. The normalising probability <span class="math inline">\(P_k(k)\)</span> in the denominator of the zero-truncated multinomial is not given explicitly but is the probability of no zero’s in the non-zero categories under a standard multinomial.
</caption>
<thead>
<tr>
<th style="text-align:center;">
Distribution
</th>
<th style="text-align:center;">
NoDataColumns
</th>
<th style="text-align:center;">
NoTraits
</th>
<th style="text-align:right;">
Density
</th>
<th style="text-align:left;">
Function
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
<code>"gaussian"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=f_{N}({\bf w}^{\top}\boldsymbol{\theta},\sigma^{2}_{e})\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"poisson"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=f_{P}(\textrm{exp}(l))\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"categorical"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
<span class="math inline">\(J-1\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y = k| k\neq k_1)\)</span><br />
<br />
<span class="math inline">\(Pr(y=k | k=k_1)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=\frac{\textrm{exp}(l_{k})}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}\)</span><br />
<span class="math inline">\(=\frac{1}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"multinomialJ"</code>
</td>
<td style="text-align:center;">
<span class="math inline">\(J\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(J-1\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y_k | k\neq J)\)</span><br />
<br />
<span class="math inline">\(Pr(y_k | k=J)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(={\sum^{J}_{j=1}y_j \choose y_k}\left(\frac{\textrm{exp}(l_{k})}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}\right)^{y_k}\left(1-\frac{\textrm{exp}(l_{k})}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}\right)^{\sum^{J}_{j\neq k} y_j}\)</span><br />
<span class="math inline">\(={\sum^{J}_{j=1}y_j \choose y_k}\left(\frac{1}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}\right)^{y_k}\left(1-\frac{1}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}\right)^{\sum^{J}_{j\neq k} y_j}\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"ordinal"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y=k)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=F_{N}(\gamma_{k} | l,1)-F_{N}(\gamma_{k+1} | l,1)\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"threshold"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y=k)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=F_{N}(\gamma_{k} | {\bf w}^{\top}\boldsymbol{\theta}, \sigma^{2}_{e})-F_{N}(\gamma_{k+1} | {\bf w}^{\top}\boldsymbol{\theta}, \sigma^{2}_{e})\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"exponential"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=f_{E}(\textrm{exp}(-l))\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"geometric"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=f_{G}(\frac{\textrm{exp}(l)}{1+\textrm{exp}(l)})\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"cengaussian"</code>
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y_{1}&gt;y&gt;y_{2})\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=F_{N}(y_{2} | {\bf w}^{\top}\boldsymbol{\theta},\sigma^{2}_{e})-F_{N}(y_{1} | {\bf w}^{\top}\boldsymbol{\theta},\sigma^{2}_{e})\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"cenpoisson"</code>
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y_{1}&gt;y&gt;y_{2})\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=F_{P}(y_{2} | l)-F_{P}(y_{1} | l)\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"cenexponential"</code>
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y_{1}&gt;y&gt;y_{2})\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=F_{E}(y_{2} | l)-F_{E}(y_{1} | l)\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"zipoisson"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y=0)\)</span><br />
<br />
<span class="math inline">\(Pr(y | y&gt;0)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}+\left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)f_{P}(y|\textrm{exp}(l_{1}))\)</span><br />
<span class="math inline">\(=\left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)f_{P}(y |\textrm{exp}(l_{1}))\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"ztpoisson"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=\frac{f_{P}(y |\textrm{exp}(l))}{1-f_{P}(0 |\textrm{exp}(l))}\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"hupoisson"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y=0)\)</span><br />
<br />
<span class="math inline">\(Pr(y | y&gt;0)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\)</span><br />
<span class="math inline">\(=\left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)\frac{f_{P}(y |\textrm{exp}(l_{1}))}{1-f_{P}(0 |\textrm{exp}(l_{1}))}\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"zapoisson"</code>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y=0)\)</span><br />
<span class="math inline">\(Pr(y | y&gt;0)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=1-\textrm{exp}(\textrm{exp}(l_{2}))\)</span><br />
<span class="math inline">\(=\textrm{exp}(\textrm{exp}(l_{2}))\frac{f_{P}(y |\textrm{exp}(l_{1}))}{1-f_{P}(0 |\textrm{exp}(l_{1}))}\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"hubinomial"</code>
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y_{1}=0)\)</span><br />
<br />
<span class="math inline">\(Pr(y_{1} | y_{1}&gt;0)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}+\left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)f_{B}(0 | n=y_{1}+y_{2}, \frac{\textrm{exp}(l_{1})}{1+\textrm{exp}(l_{1})})\)</span><br />
<span class="math inline">\(=\left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)f_{B}(y_{1} | n=y_{1}+y_{2} \frac{\textrm{exp}(l_{1})}{1+\textrm{exp}(l_{1})})\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"nzbinom"</code>
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y=1)\)</span><br />
<br />
<span class="math inline">\(Pr(y=0)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=1-\left(1-\frac{\textrm{exp}(l)}{1+\textrm{exp}(l)}\right)^{y_2}\)</span><br />
<span class="math inline">\(=\left(1-\frac{\textrm{exp}(l)}{1+\textrm{exp}(l)}\right)^{y_2}\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"ncst"</code>
</td>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=f_{NCt}(y_1 | \nu=y_3, \mu=l/y_2)/y_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"msst"</code>
</td>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=f_{t}(y_1-l/y_2 | \nu=y_3)/y_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"ztmbJ"</code>
</td>
<td style="text-align:center;">
<span class="math inline">\(J\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(J\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y_k=1)\)</span><br />
<br />
<span class="math inline">\(Pr(y_k=0)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(=\left(1-\frac{\textrm{exp}(l_k)}{1+\textrm{exp}(l_k)}\right)\left(1-\prod^{J}_{j=1}\frac{\textrm{exp}(l_k)}{1+\textrm{exp}(l_k)}\right)^{-1}\)</span><br />
<span class="math inline">\(=\left(\frac{\textrm{exp}(l_k)}{1+\textrm{exp}(l_k)}\right)\left(1-\prod^{J}_{j=1}\frac{\textrm{exp}(l_k)}{1+\textrm{exp}(l_k)}\right)^{-1}\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<code>"ztmultinomialJ"</code>
</td>
<td style="text-align:center;">
<span class="math inline">\(J\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(J-1\)</span>
</td>
<td style="text-align:right;">
<span class="math inline">\(Pr(y_k | k\neq J, y_k&gt;0)\)</span><br />
<br />
<span class="math inline">\(Pr(y_k | k=J, y_k&gt;0)\)</span><br />
<br />
<span class="math inline">\(Pr(y_k=0 | k=J)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(={\sum^{J}_{j=1}y_j \choose y_k}\left(\frac{\textrm{exp}(l_{k})}{P_k(k)}\right)^{y_k}\left(1-\frac{\textrm{exp}(l_{k})}{P_k(k)}\right)^{\sum^{J}_{j\neq k} y_j}\)</span><br />
<span class="math inline">\(={\sum^{J}_{j=1}y_j \choose y_k}\left(\frac{1}{P_k(k)}\right)^{y_k}\left(1-\frac{1}{P_k(k)}\right)^{\sum^{J}_{j\neq k} y_j}\)</span><br />
     ignored
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="parameter-expansion" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> Parameter Expansion<a href="technical-details.html#parameter-expansion" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As the covariance matrix approaches a singularity the mixing of the chain becomes notoriously slow. This problem is often encountered in single-response models when a variance component is small and the chain becomes stuck at values close to zero. Similar problems occur for the EM algorithm and <span class="citation">C. H. Liu, Rubin, and Wu (<a href="#ref-Liu.1998">1998</a>)</span> introduced parameter expansion to speed up the rate of convergence. The idea was quickly applied to Gibbs sampling problems <span class="citation">(<a href="#ref-Liu.1999">J. S. Liu and Wu 1999</a>)</span> and has now been extensively used to develop more efficient mixed-model samplers <span class="citation">(e.g. <a href="#ref-vanDyk.2001">Dyk and Meng 2001</a>; <a href="#ref-Gelman.2008b">Gelman, Dyk, et al. 2008</a>; <a href="#ref-Browne.2009">Browne et al. 2009</a>)</span>.</p>
<p>The columns of the design matrix (<span class="math inline">\({\bf W}\)</span>) can be multiplied by the non-identified working parameters <span class="math inline">\({\boldsymbol{\mathbf{\alpha}}} = \left[1,\ \alpha_{1},\ \alpha_{2},\ \dots \alpha_{k}\right]^{&#39;}\)</span>:</p>
<p><span class="math display" id="eq:wstar">\[{\bf W}_{\alpha} = \left[{\bf X}\ {\bf Z}_{1}\alpha_{1}\ {\bf Z}_{2}\alpha_{2}\ \dots\ {\bf Z}_{k}\alpha_{k}\right]
\label{wstar}   \tag{11.16}\]</span></p>
<p>where the indices denote submatrices of <span class="math inline">\({\bf Z}\)</span> which pertain to effects associated with the same variance component. Replacing <span class="math inline">\({\bf W}\)</span> with <span class="math inline">\({\bf W}_{\alpha}\)</span> we can sample the new location effects <span class="math inline">\(\boldsymbol{\theta}_{\alpha}\)</span> as described above, and rescale them to obtain <span class="math inline">\(\boldsymbol{\theta}\)</span>:</p>
<p><span class="math display">\[\boldsymbol{\theta} = ({\bf I}_{\beta}\oplus_{i=1}^{k}{\bf I}_{u_{i}}\ \alpha_{i})\boldsymbol{\theta}_{\alpha}\]</span></p>
<p>where the identity matrices are of dimension equal to the length of the subscripted parameter vectors.</p>
<p>Likewise, the (co)variance matrices can be rescaled by the set of <span class="math inline">\(\alpha\)</span>’s associated with the variances of a particular variance structure component (<span class="math inline">\({\boldsymbol{\mathbf{\alpha}}}_{\mathcal{V}}\)</span>):</p>
<p><span class="math display">\[{\bf V} = Diag({\boldsymbol{\mathbf{\alpha}}}_{\mathcal{V}}){\bf V}_{\alpha}Diag({\boldsymbol{\mathbf{\alpha}}}_{\mathcal{V}})\]</span></p>
<p>The working parameters are not identifiable in the likelihood, but do have a proper conditional distribution. Defining the <span class="math inline">\(n\times(k+1)\)</span> design matrix <span class="math inline">\({\bf X}_{\alpha}\)</span> with each column equal to the submatrices in Equation <a href="technical-details.html#eq:wstar">(11.16)</a> postmultiplied by the relevant subvectors of <span class="math inline">\(\boldsymbol{\theta}_{\alpha}\)</span>, we can see that <span class="math inline">\({\boldsymbol{\mathbf{\alpha}}}\)</span> is a vector of regression coefficients:</p>
<p><span class="math display">\[\begin{array}{rl}
\bf{l} =&amp; {\bf X}_{\alpha}{\boldsymbol{\mathbf{\alpha}}}+\bf{e}\\
\end{array}\]</span></p>
<p>and so the methods described above can be used to update them.</p>
</div>
<div id="priors-for-corg-and-corgh-structures" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Priors for <code>corg</code> and <code>corgh</code> structures<a href="technical-details.html#priors-for-corg-and-corgh-structures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For <code>corg</code> and <code>corgh</code> structures<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a> the diagonals of <code>V</code> define the fixed variances (<code>corgh</code>) or are ignored and the variances set to one (<code>corg</code>). I use the prior specification in <span class="citation">Barnard, McCulloch, and Meng (<a href="#ref-Barnard.2000">2000</a>)</span> where <code>nu</code> controls how much the correlation matrix approaches an identity matrix. The marginal distribution of individual correlations (<span class="math inline">\(r\)</span>) is given by
<span class="citation">Barnard, McCulloch, and Meng (<a href="#ref-Barnard.2000">2000</a>; and <a href="#ref-Box.1973">Box and Tiao 1973</a>)</span>:</p>
<p><span class="math display">\[\begin{array}{lr}
Pr(r) \propto (1-r^{2})^\frac{\texttt{nu-dim(V)-1}}{\texttt{2}}, &amp; |r|&lt;1\\
\end{array}\]</span></p>
<p>and as shown above setting <code>nu</code> =<code>dim(V)+1</code> results in marginal correlations that are uniform on the interval <span class="math display">\[-1,1\]</span>.</p>
<p>In most cases correlation matrices do not have known form and so cannot be directly Gibbs sampled. <span class="math inline">\(\texttt{MCMCglmm}\)</span> uses a method proposed by <span class="citation">X. F. Liu and Daniels (<a href="#ref-Liu.2006">2006</a>)</span> with the target prior as in <span class="citation">Barnard, McCulloch, and Meng (<a href="#ref-Barnard.2000">2000</a>)</span>. Generally this algorithm is very efficient as the Metropolis-Hastings acceptance probability only depends on the degree to which the candidate prior and the target prior (the prior you specify) conflict. The candidate prior is equivalent to the prior in <span class="citation">Barnard, McCulloch, and Meng (<a href="#ref-Barnard.2000">2000</a>)</span> with <code>nu=0</code> so as long as a diffuse prior is set, mixing is generally not a problem. If <code>nu=0</code> is set (the default) then the Metropolis-Hastings steps are always accepted resulting in Gibbs sampling. However, a prior of this form puts high density on extreme correlations which can cause problems if the data give support to correlations in this region.</p>

</div>
</div>
<h3> References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Barnard.2000" class="csl-entry">
Barnard, J., R. McCulloch, and X. L. Meng. 2000. <span>“Modeling Covariance Matrices in Terms of Standard Deviations and Correlations, with Application to Shrinkage.”</span> <em>Statistica Sinica</em> 10 (4): 1281–311.
</div>
<div id="ref-Box.1973" class="csl-entry">
Box, G. E. P., and G. C. Tiao. 1973. <em>Bayesian Inference in Statistical Analysis</em>. New York: John Wiley &amp; Sons.
</div>
<div id="ref-Browne.2009" class="csl-entry">
Browne, W. J., F. Steele, M. Golalizadeh, and M. J. Green. 2009. <span>“The Use of Simple Reparameterizations to Improve the Efficiency of Markov Chain Monte Carlo Estimation for Multilevel Models with Applications to Discrete Time Survival Models.”</span> <em>Journal of the Royal Statistical Society Series <span>A</span> - Statistics in Society</em> 172: 579–98.
</div>
<div id="ref-Cowles.1996" class="csl-entry">
Cowles, M. K. 1996. <span>“Accelerating Monte Carlo Markov Chain Convergence for Cumulative–Link Generalized Linear Models.”</span> <em>Statistics and Computing</em> 6 (2): 101–11.
</div>
<div id="ref-Davis.2006" class="csl-entry">
Davis, T. A. 2006. <em>Direct Methods for Sparse Linear Systems</em>. Fundamentals of Algorithms. Philadelphia: SIAM.
</div>
<div id="ref-vanDyk.2001" class="csl-entry">
Dyk, D. A. van, and X. L. Meng. 2001. <span>“The Art of Data Augmentation.”</span> <em>Journal of Computational and Graphical Statistics</em> 10 (1): 1–50.
</div>
<div id="ref-Garcia-Cortes.2001" class="csl-entry">
Garcia-Cortes, L. A., and D. Sorensen. 2001. <span>“Alternative Implementations of <span>M</span>onte <span>C</span>arlo <span>EM</span> Algorithms for Likelihood Inferences.”</span> <em>Genetics Selection Evolution</em> 33 (4): 443–52.
</div>
<div id="ref-Gelman.2004" class="csl-entry">
Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin. 2004. <em>Bayesian Data Analysis</em>. 2nd ed. Texts in Statistical Science. Chapman <span>&amp;</span> Hall.
</div>
<div id="ref-Gelman.2008b" class="csl-entry">
Gelman, A., D. A. van Dyk, Z. Y. Huang, and W. J. Boscardin. 2008. <span>“Using Redundant Parameterizations to Fit Hierarchical Models.”</span> <em>Journal of Computational and Graphical Statistics</em> 17 (1): 95–122.
</div>
<div id="ref-Gianola.2004" class="csl-entry">
Gianola, D., and D. Sorensen. 2004. <span>“Quantitative Genetic Models for Describing Simultaneous and Recursive Relationships Between Phenotypes.”</span> <em>Genetics</em> 167 (3): 1407–24.
</div>
<div id="ref-Haario.2001" class="csl-entry">
Haario, H., E. Saksman, and J. Tamminen. 2001. <span>“An Adaptive Metropolis Algorithm.”</span> <em>Bernoulli</em> 7 (2): 223–42.
</div>
<div id="ref-Hadfield.2010b" class="csl-entry">
Hadfield, J. D., and S. Nakagawa. 2010. <span>“General Quantitative Genetic Methods for Comparative Biology: Phylogenies, Taxonomies, Meta-Analysis and Multi-Trait Models for Continuous and Categorical Characters.”</span> <em>Journal of Evolutionary Biology</em> 23 (3): 494–508.
</div>
<div id="ref-Henderson.1976" class="csl-entry">
Henderson, C. R. 1976. <span>“Simple Method for Computing Inverse of a Numerator Relationship Matrix Used in Prediction of Breeding Values.”</span> <em>Biometrics</em> 32 (1): 69–83.
</div>
<div id="ref-Korsgaard.1999" class="csl-entry">
Korsgaard, I. R., A. H. Andersen, and D. Sorensen. 1999. <span>“A Useful Reparameterisation to Obtain Samples from Conditional Inverse Wishart Distributions.”</span> <em>Genetics Selection Evolution</em> 31 (2): 177–81.
</div>
<div id="ref-Liu.1998" class="csl-entry">
Liu, C. H., D. B. Rubin, and Y. N. Wu. 1998. <span>“Parameter Expansion to Accelerate EM: The PX–EM Algorithm.”</span> <em>Biometrika</em> 85 (4): 755–70.
</div>
<div id="ref-Liu.1999" class="csl-entry">
Liu, J. S., and Y. N. Wu. 1999. <span>“Parameter Expansion for Data Augmentation.”</span> <em>Journal of the American Statistical Association</em> 94 (448): 1264–74.
</div>
<div id="ref-Liu.2006" class="csl-entry">
Liu, X. F., and M. J. Daniels. 2006. <span>“A New Algorithm for Simulating a Correlation Matrix Based on Parameter Expansion and Reparameterization.”</span> <em>Journal of Computational and Graphical Statistics</em> 15 (4): 897–914.
</div>
<div id="ref-Meuwissen.1992" class="csl-entry">
Meuwissen, T. H. E., and Z. Luo. 1992. <span>“Computing Inbreeding Coefficients in Large Populations.”</span> <em>Genetics Selection Evolution</em> 24 (4): 305–13.
</div>
<div id="ref-Ovaskainen.2008" class="csl-entry">
Ovaskainen, O., H. Rekola, E. Meyke, and E. Arjas. 2008. <span>“Bayesian Methods for Analyzing Movements in Heterogeneous Landscapes from Mark–Recapture Data.”</span> <em>Ecology</em> 89 (2): 542–54.
</div>
<div id="ref-Robert.1995" class="csl-entry">
Robert, C. P. 1995. <span>“Simulation of Truncated Normal Variables.”</span> <em>Statistics and Computing</em> 5 (2): 121–25.
</div>
<div id="ref-Spiegelhalter.2002" class="csl-entry">
Spiegelhalter, D. J., N. G. Best, B. R. Carlin, and A. van der Linde. 2002. <span>“Bayesian Measures of Model Complexity and Fit.”</span> <em>Proceedings of the Royal Society of London Series <span>B</span> - Biological Sciences</em> 64 (4): 583–639.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="33">
<li id="fn33"><p>In versions <span class="math inline">\(&lt;2.18\)</span> <code>cor</code> fitted what is now a <code>corg</code> structure. The reason for the change is to keep the <code>asreml</code> and <span class="math inline">\(\texttt{MCMCglmm}\)</span> syntax equivalent. However, the <code>corgh</code> structure in asreml is a reparameterised <code>us</code> structure whereas in <span class="math inline">\(\texttt{MCMCglmm}\)</span> the variances are fixed in the prior.<a href="technical-details.html#fnref33" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="path.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["MCMCglmm-course-notes.pdf", "MCMCglmm-course-notes.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
