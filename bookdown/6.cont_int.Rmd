# Continuous Random Interactions {#cont-int}

In Chapter \@ref(cat-int) we saw how we could define a linear model within a variance function and then interact these terms with a random effect. In the example, we did this in order to fit a $\texttt{virus}$ by $\texttt{line}$ interaction: `~us(virus):line`. Since the variance function model involved a categorical predictor and the intercept is omitted by default, it is not really necessary to engage with the fact that the variance function is taking a model formula. Most users familiar with covariance matrices will be happy that `~us(virus):line` specifies the covariance matrix: 

$${\bf V}_{{\color{red}{\texttt{line}}}}=
\left[
\begin{array}{ccc}
\sigma^{2}_{\color{blue}{\texttt{France}}}&\sigma_{\color{blue}{\texttt{France}, \texttt{Spain}}}&\sigma_{\color{blue}{\texttt{France}, \texttt{Greece}}}\\
\sigma_{\color{blue}{\texttt{France}, \texttt{Spain}}}&\sigma^{2}_{\color{blue}{\texttt{Spain}}}&\sigma_{\color{blue}{\texttt{Spain}, \texttt{Greece}}}\\
\sigma_{\color{blue}{\texttt{France}, \texttt{Greece}}}&\sigma_{\color{blue}{\texttt{Spain}, \texttt{Greece}}}&\sigma^{2}_{\color{blue}{\texttt{Greece}}}\\
\end{array}
\right]$$

However, we are also free to define the variance function model with continuous covariates, or even a mixture of continuous and categorical factors. Although the resulting covariance matrix is interpreted in the same way it can be less intuitive.

## Random Regression

As an example, we'll use a longitudinal data set on chicken growth (See Figure \@ref(fig:ChickWeight)):

``` {r echo=TRUE}
data(ChickWeight)
head(ChickWeight)
```

The data consist of body weights (`weight`) for 50 chicks (`Chick`) measured up to 12 times over a 3 week period. The variable `Time` is the number of days since hatching, and `Diet` is a four level factor indicating the type of protein diet the chicks received. The data are plotted in Figure \@ref(fig:ChickWeight).

``` {r label=ChickWeight, echo=FALSE, include=TRUE, fig.cap="Weight data of 50 chicks from hatching until three weeks old.", fig.width=7, fig.height=8}
xyplot(weight~Time|Chick, data=ChickWeight)
```

Growth curves tend to be sigmoidal and so one of the non-linear growth curves such as the Gompertz or logistic may fit the data well. However, these can be tricky to use and an alternative is to try and capture the form of the curve using polynomials. We'll start with a quadratic function at the population level and and fit chick as a random term. 

``` {r echo=TRUE}
prior.weight<-list(R=IW(1,0.002), G=F(2, 10000))

mweight.1<-MCMCglmm(weight~Diet+Time+I(Time^2), random=~Chick, data=ChickWeight, pr=TRUE, prior=prior.weight)

summary(mweight.1)
```

Note that I have specified the prior using a prior generator function (Sections \@ref(Vprior-sec) and \@ref(VCVprior-sec)) but I've upped the scale to 10,000 from the usual 1,000 because of the scale of the data (in fact it makes almost no difference). 

``` {r echo=TRUE}
summary(mweight.1)
```

Since we saved the random chick effects (`pr=TRUE`) we can obtain the predictions for each bird by specifying `marginal=NULL` (Section \@ref(ranpred-sec)):

``` {r echo=TRUE}
pweight<-predict(mweight.1, marginal=NULL)
```

and we can add them to the $\texttt{xyplot}$ plot (Figure \@ref(fig:CWpred-1)).


``` {r label=CWpred-1, echo=TRUE, fig=TRUE, include=TRUE, fig.cap="Weights of each chick as a function of age in blue, with the predicted weights in orange. A quadratic population growth curve was fitted with random chick intercepts.", fig.width=7, fig.height=8}
xyplot(weight+pweight~Time|Chick, data=ChickWeight, distribute.type = TRUE, type   = c("p", "l"), lwd=c(1, 2))
```

The predictions don't look that bad, but you will notice that for some chicks (e.g. 13,19,35) the slope of the predicted growth seems either to shallow, or too steep. Let's think about the model more carefully. Ignoring diet for now the model for $\texttt{Chick}$ $i$ at $\texttt{Time}$  $j$ is


$\texttt{Weight}_{ij} = \beta_{0} + \beta_{1}\texttt{Time}_{j} +  \beta_{2}\texttt{Time}^{2}_{j} + u^{(0)}_{i} + e_{ij}$


The population growth curve is slightly convex because the quadratic term for time is positive, and the predictions for each chick are parallel to this curve. By fitting chick as a random effect we have allowed variation in the intercept only, and often this is not enough. To account for this we can start by fitting `us(1+time):Chick`. The linear model inside the variance function has two parameters, an intercept (`1`) and a regression slope associated with `Time` which define the set of interactions:

$$\begin{array}{c|rrrrrc}
&{\color{red}{\texttt{Chick1}}}&{\color{red}{\texttt{Chick2}}}&{\color{red}{\texttt{Chick3}}}&\dots\\
\hline\\
{\color{blue}{\texttt{(Intercept)}}}&{\color{blue}{\texttt{(Intercept)}}}.{\color{red}{\texttt{Chick1}}}&{\color{blue}{\texttt{(Intercept)}}}.{\color{red}{\texttt{Chick2}}}&{\color{blue}{\texttt{(Intercept)}}}.{\color{red}{\texttt{Chick3}}}&\dots\\
{\color{blue}{\texttt{Time}}}&{\color{blue}{\texttt{Time}}}.{\color{red}{\texttt{Chick1}}}&{\color{blue}{\texttt{Time}}}.{\color{red}{\texttt{Chick2}}}&{\color{blue}{\texttt{Time}}}.{\color{red}{\texttt{Chick3}}}&\dots\\
\end{array}$$

Each chick now has an intercept and a slope, and because we have used the `us` function we are estimating the $2\times2$ matrix:

$${\bf V}_{{\color{red}{\texttt{Chick}}}}=
\left[
\begin{array}{cc}
\sigma^{2}_{\color{blue}{\texttt{(Intercept)}}}&\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}}}\\
\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}}}&\sigma^{2}_{\color{blue}{\texttt{Time}}}\\
\end{array}
\right]$$

$\sigma^{2}_{\color{blue}{\texttt{(Intercept)}}}$ is the amount of variation in intercepts between chicks, and $\sigma^{2}_{\color{blue}{\texttt{Time}}}$ is the amount of variation in the regression slopes between chicks. If the `idh` function had been used the covariance would have been set to zero and we could have interpreted variation in intercepts as variation in overall size, and variation in slopes as variation in growth rate. However, there is often covariance between intercepts and slopes and it is usually a good idea to use the `us` function and estimate them. We shall do so:

``` {r echo=TRUE, cache=TRUE}
mweight.2<-MCMCglmm(weight~Diet+Time+I(Time^2), random=~us(1+Time):Chick, data=ChickWeight, pr=TRUE, prior=prior.weight)
summary(mweight.2)
```

Again we can get the predicted weights from this model and we can see that the fit is much better (See Figure \@ref(fig:CWpred-2)). 

``` {r label=CWpred-2, fig=TRUE, include=TRUE, fig.cap="Weights of each chick as a function of age in blue, with the predicted weights in orange. A quadratic population growth curve was fitted with a first order random regression for chicks (i.e. a random intercept-slope model).", fig.width=7, fig.height=8}
xyplot(weight+predict(mweight.2, marginal=NULL)~Time|Chick, data=ChickWeight, distribute.type = TRUE, type   = c("p", "l"), lwd=c(1, 2))
```

If we write down the model for this data we have

$\texttt{Weight}_{ij} = \beta_{0} + \beta_{1}\texttt{Time}_{j} +  \beta_{2}\texttt{Time}^{2}_{j} + u^{(0)}_{i} + u^{(1)}_{i}\texttt{Time}_{j}+e_{ij}$


In theory we could fit higher order random regressions (data and prior permitting). For example, fitting `random=~us(1+Time+I(Time^2)):Chick` would allow the quadratic term for each Chick to deviate from the population mean and we would be estimating the $3\times3$ covariance matrix:

$${\bf V}_{{\color{red}{\texttt{Chick}}}}=
\left[
\begin{array}{ccc}
\sigma^{2}_{\color{blue}{\texttt{(Intercept)}}}&\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}}}&\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time^{2}}}}\\
\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}}}&\sigma^{2}_{\color{blue}{\texttt{Time}}}&\sigma_{\color{blue}{\texttt{Time}, \texttt{Time^{2}}}}\\
\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time^2}}}&\sigma_{\color{blue}{\texttt{Time}, \texttt{Time^{2}}}}&\sigma^{2}_{\color{blue}{\texttt{Time^2}}}\\
\end{array}
\right]$$


``` {r label=mweight-2, echo=TRUE, include=TRUE, fig.cap="MCMC trace for the chick covariance components from model \\texttt{mweight.2}. The lower and upper panels are the intercept and slope variance components respectively, and the middle panel the intercept-slope covariance.", fig.width=7, fig.height=8}
plot(mweight.2$VCV[,c(1,2,4)], density=FALSE)
```

The traces look OKish for the chick (co)variance matrices (Figure \@ref(fig:RR2VCV) but if we look at the trace for the correlation


``` {r label=mweight-2-r, echo=FALSE, include=TRUE, fig.cap="MCMC summary plots for the chick covariance components from model \\texttt{mweight.2}. The lower and upper plots are the intercept and slope variance components respectively, and the middle two plots are the intercept-slope covariance.", fig.width=7, fig.height=8}
plot(posterior.cor(mweight.2$VCV[,1:4])[,2], density=FALSE)
```

we see it is a lot worse and we should run it for longer in order to sample the posterior adequately.  Ill-conditoned.... 

### Mean centring {#RRcentering}


``` {r echo=TRUE, cache=TRUE}
ChickWeight$TimeC<-ChickWeight$Time-10
mweight.3<-MCMCglmm(weight~Diet+TimeC, random=~us(1+TimeC):Chick, data=ChickWeight, pr=TRUE, prior=prior.weight)
```


### Expected Variances and Covariances

One under-appreciated facet of random regression models is that when a polynomial of order $n$ is fitted, the variance is predicted to change as a polynomial of order $n+1$. Consequently, part of the information for estimating the random effects (e.g. slopes) may come from a mismatch between the (conditional) distribution assumed for the response and it's actual distribution.  

```{r }
Grumpy$x<-as.numeric(Grumpy$type=="grumpy")
m<-MCMCglmm(y~type, random=~us(x):photo, data=Grumpy, prior=list(R=IW(1,0.002), G=IW(1,0.002)))
```


Conversely, 

## User-defined Design Matrices {#user-defined-sec}

$\texttt{MCMCglmm}$ generally uses $\texttt{model.matrix}$, or a native equivalent, when evaluating a model formula to obtain the design matrices. Sometimes R formula syntax is not flexible enough to generate the appropriate design matrix. However, if a random-effect design matrix can be generated, it is possible to fit the model by exploiting the $\texttt{idv}$ variance structure. However, the parameters defined by the formula in the $\texttt{idv}$ structure should not be interacted with another factor. Consider the term `~idv(z1+z2+z3+z4):1` and let's set up the table of random effects as we have before:


$$\begin{array}{c|c}
&{\color{red}{\texttt{(Intercept)}}}\\
\hline\\
{\color{blue}{\texttt{z1}}}&{\color{blue}{\texttt{z1}}}.{\color{red}{\texttt{(Intercept)}}}\\
{\color{blue}{\texttt{z2}}}&{\color{blue}{\texttt{z2}}}.{\color{red}{\texttt{(Intercept)}}}\\
{\color{blue}{\texttt{z3}}}&{\color{blue}{\texttt{z3}}}.{\color{red}{\texttt{(Intercept)}}}\\
{\color{blue}{\texttt{z4}}}&{\color{blue}{\texttt{z4}}}.{\color{red}{\texttt{(Intercept)}}}\\
\end{array}$$

An $\texttt{idv}$ structure, like an $\texttt{idh}$ structure, assumes no correlation between effects on different rows: the $\texttt{id}$ in $\texttt{idh}$ stands for identity matrix. The $\texttt{h}$ in $\texttt{idh}$ stands for heterogeneous and so each row is allowed a different variance. Clearly, with only one effect per row estimating a variance is not feasible. However, with $\texttt{idv}$ the variances are assumed homogenous and so only a single variance (the $\texttt{v}$ in $\texttt{idv}$) is estimated that is common to all effects. The (four) regression coefficient are then treated exactly like simple random effects: identically and independently distributed with zero mean and a variance to be estimated. When there are many random effects, writing `~idv(z1+z2+z3+z4+...z999+z1000):1` is going to be painful to write and read. However, you can simply including the design matrix as a matrix in the data-frame and fit it inside the $\texttt{idv}$ structure. 

The multi-membership model fitted in Section \@ref(multim-sec) is a case in point. In that example, each child had five friends and we would like to fit $\texttt{friend}$ as a random effect that predicts the child's well-being. We can take a look at the random effect design matrix for that model:

```{r echo=FALSE}
data.friends[2,5]<-"S"
Z<-model.matrix(~friend.1-1, data=data.friends)
Z<-Z+model.matrix(~friend.2-1, data=data.friends)
Z<-Z+model.matrix(~friend.3-1, data=data.friends)
Z<-Z+model.matrix(~friend.4-1, data=data.friends)
Z<-Z+model.matrix(~friend.5-1, data=data.friends)
colnames(Z)<-LETTERS
``` 

```{r echo=FALSE}
head(Z)
```

The row for each child has five ones indicating their friends. This design matrix cannot be created using a standard R formula and the multi-membership linking function `~mm(freind.1+freind.2+freind.3+freind.4+freind.5)` was used to create the design matrix. However, if we had the incidence matrix, we could place it in the data-frame and fit an identical model:

```{r user-defined}
data.friends$Z<-Z
m.mm2<-MCMCglmm(y~1, random=~idv(Z), data=data.friends, prior=prior.mm)
```

Note that there is no need to explicitly interact the $\texttt{idv}$ structure with an intercept (in fact $\texttt{MCMCglmm}$ will complain if you try). In this example, the predictor variables ($\texttt{freind}$) are categorical, but there is nothing to stop design matrices from being formed using continuous predictors, as we'll see in the next section on splines.  

## Splines

The function $\texttt{smspline}$ in $\texttt{lmeSplines}$ can be used to generate design matrices for penalised splines that are transformed such that the random effects are expected to be identically and independently distributed with unknown variance. Using the ideas discussed above for user-specified design matrices (Section \@ref(user-defined-sec)) we can use an $\texttt{idv}$ structure to fit the spline. However, rather than generating the design matrix outside of $\texttt{MCMCglmm}$ and adding it to the data-frame, we can simply use the $\texttt{smspline}$ function inside $\texttt{idv}$. For example, we can fit a penalised cubic spline to one of the test data sets in $\texttt{lmeSplines}$:

```{r }
spline.prior=list(R=IW(1, 0.002), G=F(1,1000))
m.spline<-MCMCglmm(y~time, random= ~idv(smspline(time)), data=smSplineEx1, pr=TRUE, prior=spline.prior)
```

The random effects need to be saved (`pr=TRUE`) because these are the coefficients of the spline. Similarly, when making a prediction we want to include the spline coefficients in the prediction, rather than marginalising them (Section \@ref(ranpred-sec)).

```{r }
p.slpine<-predict(m.spline, marginal=NULL)
```

The model wiggles its way through the data, as splines do ...

```{r spline, echo=TRUE, include=TRUE, fig.cap="Data and prediction from a penalised cubic spline fitted to the example data $\\texttt{smSplineEx1}$ from $\\texttt{lmeSplines}$", fig.width=7, fig.height=5}
plot(y~time, data=smSplineEx1)
lines(p.slpine~smSplineEx1$time)
```
