# Continuous Random Interactions {#cont-int}

In Chapter \@ref(cat-int) we saw how we could define a set of random effects by specifying a linear model within a variance function and then interacting this term with a categorical predictor. In that Chapter, we used the random term `~us(virus):line` in order to model $\texttt{virus}$ by $\texttt{line}$ interactions. Since the variance function model involved a categorical predictor (`virus`), and the intercept is omitted by default, it was not really necessary to fully engage with the idea that the variance function is taking a model formula - most users familiar with covariance matrices will be happy that `~us(virus):line` specifies the covariance matrix:

$${\bf V}_{{\color{red}{\texttt{line}}}}=
\left[
\begin{array}{ccc}
\sigma^{2}_{\color{blue}{\texttt{France}}}&\sigma_{\color{blue}{\texttt{France}, \texttt{Spain}}}&\sigma_{\color{blue}{\texttt{France}, \texttt{Greece}}}\\
\sigma_{\color{blue}{\texttt{France}, \texttt{Spain}}}&\sigma^{2}_{\color{blue}{\texttt{Spain}}}&\sigma_{\color{blue}{\texttt{Spain}, \texttt{Greece}}}\\
\sigma_{\color{blue}{\texttt{France}, \texttt{Greece}}}&\sigma_{\color{blue}{\texttt{Spain}, \texttt{Greece}}}&\sigma^{2}_{\color{blue}{\texttt{Greece}}}\\
\end{array}
\right]$$

However, we are also free to use continuous covariates in the variance function model, or even a mixture of continuous and categorical predictors. Although the resulting covariance matrix is interpreted in the same way, it can be less intuitive.

## Random Regression

As an example, we'll use a longitudinal data set on chicken growth (See Figure \@ref(fig:ChickWeight)):

``` {r echo=TRUE}
data(ChickWeight)
head(ChickWeight)
```

The data consist of body weights (`weight`) for 50 chicks (`Chick`) measured up to 12 times over a 3 week period. The variable `Time` is the number of days since hatching, and `Diet` is a four level factor indicating the type of protein diet the chicks received. The data are plotted in Figure \@ref(fig:ChickWeight).

``` {r label=ChickWeight, echo=FALSE, include=TRUE, fig.cap="Weight data of 50 chicks from hatching until three weeks old.", fig.width=7, fig.height=8}
xyplot(weight~Time|Chick, data=ChickWeight)
```

Growth curves tend to be sigmoidal and so one of the non-linear growth curves such as the Gompertz or logistic may fit the data well. However, these can be tricky to use and an alternative is to try and capture the form of the curve using polynomials. We'll start with a quadratic function at the population level and and fit chick effects as random[^cont-1]. 

``` {r echo=TRUE}
prior.weight<-list(R=IW(1,0.002), G=F(2, 10000))

mweight.1<-MCMCglmm(weight~Time+I(Time^2)+Diet:Time, random=~Chick, data=ChickWeight, pr=TRUE, prior=prior.weight)
```

I can hear the rule-of-thumbers squeal - I've fitted an interaction with $\texttt{Diet}$, but no 'main' effect. Let's think about the fixed effect part of the model. For the reference $\texttt{Diet}$ ($\texttt{Diet 1}$) the linear model is


$$E[\texttt{weight} | \texttt{Diet}=1] = \beta_0+\beta_1\texttt{Time} +  \beta_{2}\texttt{Time}^{2}$$

At hatching ($\texttt{Time}$=0) the expected weight of a chick is $\beta_0$. The slope, $\beta_1$, is a rate because it is in units of grams per day - it's a growth rate. The quadratic term, $\beta_2$, is in units of grams per day$^{2}$ and represents acceleration, or more intuitively, the rate of change in the growth rate in units [grams per day] per day. At hatching, the growth rate is $\beta_1$ but at other values of $\texttt{Time}$ the growth rate is $\beta_1+\beta_{2}\texttt{Time}$[^rate]. For a non-reference $\texttt{Diet}$ (for example, $\texttt{Diet 2}$) the model is

$$E[\texttt{weight} | \texttt{Diet}=\texttt{2}] = \beta_0+\beta_1\texttt{Time} +  \beta_{2}\texttt{Time}^{2}+\beta_{3}\texttt{Time}$$

where $\beta_{3}$ is the interaction effect, which will be labelled as $\texttt{Diet2:Time}$ in the model output. The equation can be more easily understood with a little rearrangement:

$$E[\texttt{weight} | \texttt{Diet}=\texttt{2}] = \beta_0+\left(\beta_1+\beta_3\right)\texttt{Time} +  \beta_{2}\texttt{Time}^{2}$$

At hatching the model predicts chicks on different diets to have the same expected weight ($\beta_0$). This is why I intentionally dropped the 'main' effect: at hatching the diet is yet to have an effect on the chick's weight and since chicks were randomly assigned to a treatment they should be equivalent at $\texttt{Time}$ 0. We could test this if we wished, but I'm prepared to trust the experimenters. After hatching, however, we may expect the diet to cause the weights of the chicks to diverge because it alters their growth rate. The difference in growth rate between $\texttt{Diet 2}$ and $\texttt{Diet 1}$ is $\beta_{3}$. We've assumed that the effect of $\texttt{Time}^{2}$ is constant over diets. This means that if the growth rates differ between the treatment groups, that difference remains constant: the growth rate on $\texttt{Diet 2}$ at some value of $\texttt{Time}$ is $\beta_1+\beta_3+\beta_{2}\texttt{Time}$, and consistently differs from  $\texttt{Diet 1}$ by an amount $\beta_\texttt{3}$ at all values of $\texttt{Time}$.


Let's have a look at the model summary:

``` {r echo=TRUE}
summary(mweight.1)
```

Hopefully the output is now familiar. At hatching the chicks weigh approximately `r round(mean(mweight.1$Sol[,"(Intercept)"]))` grams. On the reference diet the chicks are growing at a rate of `r round(mean(mweight.1$Sol[,"Time"]),1)` grams per day at the beginning ($\texttt{Time}=0$) but as they get older they start to grow faster (the coefficient associated with $\texttt{Time}^2$ is positive). All diets seem to increase growth rate compared to $\texttt{Diet 1}$.  

While the explanation of how to interpret the fixed effects was perhaps laboured, it should be remembered that fixed and random effects are completely equivalent in how they enter the linear predictor (Chapter \@ref(ranef)). If the explanation of the fixed effect part of the model is understood, then nothing new needs to be learned when we try to understand the chick effects. If we consider $\texttt{Chick}$ $i$ fed on the reference diet, its expected weight is:

$$E[\texttt{weight} | \texttt{Diet}=1, \texttt{Chick}=i] = \beta_0+\beta_1\texttt{Time} +  \beta_{2}\texttt{Time}^{2}+u^{(0)}_i$$

where $u^{(0)}_i$ is the random chick effect. The model assumes that chicks deviate consistently in their weight across time, but chicks do not deviate in their growth rate.  Since we saved the random chick effects (`pr=TRUE`) we can obtain the predictions for each chick by specifying `marginal=NULL` (Section \@ref(ranpred-sec)):

``` {r echo=TRUE}
pweight<-predict(mweight.1, marginal=NULL)
```

and we can add them to the $\texttt{xyplot}$ plot as orange lines (Figure \@ref(fig:CWpred-1)).

``` {r label=CWpred-1, echo=FALSE, fig=TRUE, include=TRUE, fig.cap="Weights of each chick as a function of age in blue, A quadratic population growth curve was fitted to the data with random chick intercepts. The predicted weights are in orange.", fig.width=7, fig.height=8}
xyplot(weight+pweight~Time|Chick, data=ChickWeight, distribute.type = TRUE, type   = c("p", "l"), lwd=c(1, 2))
```

The predictions don't look too bad (but see Section \@ref(covf-sec)), although you will notice that for some chicks (e.g. 7, 13 and 35) the slope of the predicted growth seems either too shallow, or too steep. As we did with the $\texttt{Diet}$ by $\texttt{Time}$ interaction, we could also allow our chicks to deviate in their growth rates and fit the model: 

$$E[\texttt{weight} | \texttt{Diet}=1, \texttt{Chick}=i] = \beta_0+(\beta_1+u^{(1)}_i)\texttt{Time} + \beta_{2}\texttt{Time}^{2}+u^{(0)}_i
\label{rr-eq}   (\#eq:rr)$$


where $u^{(1)}_i$ is how much $\texttt{Chick}$ $i$'s growth rate differs from the average ($\beta_1$ for $\texttt{Diet 1}$). To do this we can fit the term `us(1+Time):Chick`. The linear model inside the variance function has two parameters, an intercept (`1` - which he have to specify explicitly[^4.1]) and a regression slope associated with `Time`. Consequently, the interaction with $\texttt{Chick}$ defines the set of coefficients:

$$\begin{array}{c|rrrrrc}
&{\color{red}{\texttt{Chick1}}}&{\color{red}{\texttt{Chick2}}}&{\color{red}{\texttt{Chick3}}}&\dots\\
\hline
{\color{blue}{\texttt{(Intercept)}}}&{\color{blue}{\texttt{(Intercept)}}}.{\color{red}{\texttt{Chick1}}}&{\color{blue}{\texttt{(Intercept)}}}.{\color{red}{\texttt{Chick2}}}&{\color{blue}{\texttt{(Intercept)}}}.{\color{red}{\texttt{Chick3}}}&\dots\\
{\color{blue}{\texttt{Time}}}&{\color{blue}{\texttt{Time}}}.{\color{red}{\texttt{Chick1}}}&{\color{blue}{\texttt{Time}}}.{\color{red}{\texttt{Chick2}}}&{\color{blue}{\texttt{Time}}}.{\color{red}{\texttt{Chick3}}}&\dots\\
\end{array}$$

Each chick now has an intercept and a slope, as in Equation \@ref(eq:rr), and because we have used the `us` variance structure  we are estimating the $2\times2$ matrix:

$${\bf V}_{{\color{red}{\texttt{Chick}}}}=
\left[
\begin{array}{cc}
\sigma^{2}_{\color{blue}{\texttt{(Intercept)}}}&\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}}}\\
\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}}}&\sigma^{2}_{\color{blue}{\texttt{Time}}}\\
\end{array}
\right]$$

$\sigma^{2}_{\color{blue}{\texttt{(Intercept)}}}$ is the amount of variation in intercepts between chicks, and $\sigma^{2}_{\color{blue}{\texttt{Time}}}$ is the amount of variation in the regression slopes between chicks (their growth rate). If the `idh` function had been used, the covariance between intercepts and slopes would have been set to zero, but they are often correlated and it is usually a good idea to use the `us` function and estimate the covariance. We shall do so:

``` {r echo=TRUE, cache=TRUE}
mweight.2<-MCMCglmm(weight~Time+I(Time^2)+Diet:Time, random=~us(1+Time):Chick, data=ChickWeight, pr=TRUE, prior=prior.weight)
summary(mweight.2)
```

There seems to be substantial variation in growth rate between chicks after controlling for the diet treatment. The posterior mean of the slope variance is `r round(mean(mweight.2$VCV[,"Time:Time.Chick"]), 2)` which in terms of a standard deviation is `r round(sqrt(mean(mweight.2$VCV[,"Time:Time.Chick"])), 2)` - larger than the effects of $\texttt{Diet}$. Again, we can get the predicted weights from this model and we can see that the fit is better (See Figure \@ref(fig:CWpred-2)). 

``` {r label=CWpred-2, echo=FALSE, fig=TRUE, include=TRUE, fig.cap="Weights of each chick as a function of age in blue, with the predicted weights in orange. A quadratic population growth curve was fitted with a first order random regression for chicks (i.e. a random intercept-slope model).", fig.width=7, fig.height=8}
xyplot(weight+predict(mweight.2, marginal=NULL)~Time|Chick, data=ChickWeight, distribute.type = TRUE, type   = c("p", "l"), lwd=c(1, 2))
```

In theory we could fit higher degree random regressions (data and prior permitting). For example, fitting `random=~us(1+Time+I(Time^2)):Chick` would allow the quadratic term for each Chick to deviate from the population mean and we would be estimating the $3\times3$ covariance matrix:

$${\bf V}_{{\color{red}{\texttt{Chick}}}}=
\left[
\begin{array}{ccc}
\sigma^{2}_{\color{blue}{\texttt{(Intercept)}}}&\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}}}&\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}^{2}}}\\
\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}}}&\sigma^{2}_{\color{blue}{\texttt{Time}}}&\sigma_{\color{blue}{\texttt{Time}, \texttt{Time}^{2}}}\\
\sigma_{\color{blue}{\texttt{(Intercept)}, \texttt{Time}^2}}&\sigma_{\color{blue}{\texttt{Time}, \texttt{Time}^{2}}}&\sigma^{2}_{\color{blue}{\texttt{Time}^2}}\\
\end{array}
\right]$$


### Covariance Functions {#covf-sec}

One under-appreciated facet of random regression models is that when a polynomial of degree $n$ is fitted, the variance in the response is predicted to change as a polynomial of order $n+1$. For example, the covariance in $\texttt{weight}$ between ages $\texttt{Time}_1$ and $\texttt{Time}_2$ within a diet is[^cov]

[^cov]: In this formula $\texttt{Time}$ is fixed and the random effects are random variables: the covariance is over chicks. The general rules for obtaining a covariance are:<br>a) The covariance between sums is the sum of the covariances between their summands. For example $\textrm{Cov}(a+b, c+d) = \textrm{Cov}(a, b)+\textrm{Cov}(a, c)+\textrm{Cov}(b, c)+\textrm{Cov}(b, d)$.<br>b) The covariance between products, for example $\textrm{Cov}(ab, cd)$, is often a nightmare unless only one of the variables on each side is random and the remainder are constants. For example, if $a$ and $c$ are constants and $b$ and $d$ are random $\textrm{Cov}(ab, cd)=ac\textrm{Cov}(b, d)$.<br>c) The covariance between a constant and a random variable is zero.<br>d) And finally, $\textrm{Var}(b)=\textrm{Cov}(b, b)$. 

$$
\begin{array}{rl}
\textrm{Cov}(\texttt{weight}_1, \texttt{weight}_2)=&Cov(u^{(0)}+u^{(1)}\texttt{Time}_1, u^{(0)}+u^{(1)}\texttt{Time}_2)\\
										 =&\sigma^2_{\texttt{(Intercept)}}+(\texttt{Time}_1+\texttt{Time}_2)\sigma_{\texttt{(Intercept)}, \texttt{Time}}+\texttt{Time}_1\texttt{Time}_2\sigma^2_{\texttt{Time}}\\
\end{array}
\label{Crr-eq}   (\#eq:Crr)
$$

Consequently, the *variance* in $\texttt{weight}$ due to the random terms is 

$$
\begin{array}{rl}
\textrm{Var}(\texttt{weight}) =&\sigma^2_{\texttt{(Intercept)}}+2\texttt{Time}\sigma_{\texttt{(Intercept)}, \texttt{Time}}+\texttt{Time}^2\sigma^2_{\texttt{Time}}\\
\end{array}
\label{Vrr-eq}   (\#eq:Vrr)
$$

Since we specified a 1st degree polynomial at the level of $\texttt{Chick}$ (an intercept and slope) the variance is a 1st degree polynomial of $\texttt{Time}$ (it has a quadratic term, with coefficient $\sigma^2_{\texttt{Time}}$).  


Because we can conceptualise the random regression in terms of the covariance between observations, rather than in terms of intercepts, slopes, and so on, random regression models are sometimes known as covariance function models. The function `buildV` returns the predicted variance of observations (and the covariances if `diag=FALSE`) after conditioning on the fixed effect model. As with the predict function, there is also the option to condition on some of the random effects (i.e. treat them as constants) by excluding them from the argument passed to marginal (see Section \@ref(ranpred-sec)). However, the default is not to condition on them and so we can get the total predicted variance for each observation due to chick effects and the residuals. The residual variance in our model ($\sigma^2_{\textrm{units}}$) is constant and so the total variance is obtained by simply adding $\sigma^2_{\textrm{units}}$ to Equation \@ref(eq:Vrr).

```{r }
predv.weight.2<-buildV(mweight.2)
```

The predicted variance goes up dramatically with $\texttt{Time}$ (red points in Figure \@ref(fig:V-rr)). In contrast, model (`mweight.1`) where chicks do not vary in their growth rate assumes that the variance is constant with $\texttt{Time}$ (black points in Figure \@ref(fig:V-rr)).

``` {r V-rr, echo=FALSE, fig=TRUE, include=TRUE, fig.cap="Predicted variance in weight due to random effects from model `mweight.2`.", fig.width=7, fig.height=5}
plot(t(predv.weight.2)~ChickWeight$Time, ylab="Predicted Variance", xlab="Time", col="red")
points(rep(mean(rowSums(mweight.1$VCV)), 12)~unique(ChickWeight$Time))
```

The two models are dramatically different in how they predict the variance changes with $\texttt{Time}$. This important difference isn't really apparent from comparing the panel plots (Figures \@ref(fig:CWpred-1) and Figures \@ref(fig:CWpred-2). However, if we obtain the 95% prediction intervals for the two models (the interval within which we expect to see 95% of observations)


```{r }
pred.weight.1<-predict(mweight.1, interval="prediction")
pred.weight.2<-predict(mweight.2, interval="prediction")
```

and overlay these on the data for all chicks plotted simultaneously, we see that our first model - that assumes constant variance - has serious problems (black lines in Figure \@ref(fig:pred-int1)). Initially, it predicts far more variability than observed, but after around $\texttt{Time}$ 12 it predicts too little variability. The random slope model (red lines) does much better, although it still predicts too much variability initially, particularly at hatching.  

``` {r pred-int1, echo=FALSE, fig=TRUE, include=TRUE, fig.cap="Chick weights plotted against time with predicted mean weight (solid lines) and 95% predictions intervals (dashed line). The black line is for a random intercept model (`mweight.1`) that implicitly assumes the variance is constant over $\\texttt{Time}$.  The red line is for a random intercept-slope model (`mweight.2`) that allows the variance to change as a quadratic function of $\\texttt{Time}$.", fig.width=7, fig.height=5}

mean.1<-tapply(pred.weight.1[,1], ChickWeight$Time, mean)
lower.1<-tapply(pred.weight.1[,2], ChickWeight$Time, mean)
upper.1<-tapply(pred.weight.1[,3], ChickWeight$Time, mean)
mean.2<-tapply(pred.weight.2[,1], ChickWeight$Time, mean)
lower.2<-tapply(pred.weight.2[,2], ChickWeight$Time, mean)
upper.2<-tapply(pred.weight.2[,3], ChickWeight$Time, mean)
plot(ChickWeight$weight~ChickWeight$Time, ylim=range(c(ChickWeight$weight, lower.1, upper.1, lower.2, upper.2)), ylab="Weight (g)", xlab = "Time (days)")

lines(mean.1~unique(ChickWeight$Time))
lines(lower.1~unique(ChickWeight$Time), lty=2)
lines(upper.1~unique(ChickWeight$Time), lty=2)

lines(mean.2~unique(ChickWeight$Time), col="red")
lines(lower.2~unique(ChickWeight$Time), col="red", lty=2)
lines(upper.2~unique(ChickWeight$Time), col="red", lty=2)

```

At this point we may be fairly chuffed with the random intercept-slope model - not only does it seem to predict the expected response well (Figure \@ref(fig:CWpred-2)), but in terms of the variance in the response, it's a massive improvement. However, it does raise an issue. The only way the model can accommodate changes in the variance over $\texttt{Time}$ is through the (co)variance matrix of chick effects (Equation \@ref(eq:Vrr)). Consequently, evidence that $\sigma^2_\texttt{Time}$ is non-zero may indicate that chick's vary in their growth rate but it may also just be capturing increases in variance due to some other mechanism. Perhaps the evidence for a non-zero $\sigma^2_\texttt{Time}$ is coming solely from how the variance in weight changes as a function of $\texttt{Time}$ (i.e from Equation \@ref(eq:Vrr)) not from assessing whether weights from the same chick are correlated (i.e from Equation \@ref(eq:Crr))? 

To see this, let's take the $\texttt{ChickWeight}$ data frame and permute $\texttt{Chick}$ labels within each $\texttt{Time}$ point. 

``` {r echo=TRUE}
ChickWeightPerm<-ChickWeight
ChickWeightPerm$Chick<-unlist(with(ChickWeight, tapply(Chick, Time, sample)))
```

The $\texttt{Chick}$ labels in this new data frame do *not* link multiple observations from the same chick, but let's fit the same random regression model that we have fitted previously:

```{r echo=TRUE}
mweight.perm<-MCMCglmm(weight~Time+I(Time^2)+Diet:Time, random=~us(1+Time):Chick, data=ChickWeightPerm, pr=TRUE, prior=prior.weight)
```

Given the $\texttt{Chick}$ labels are essentially meaningless, we might expect a model fitted to these permuted data would find no evidence for variation in chick intercepts or slopes.  However, if we look at the posterior distributions for the variance in intercepts and slopes we see that there is very strong evidence that chicks vary in the growth rate (Figure \@ref(fig:perm)).

```{r perm, echo=FALSE, fig=TRUE, include=TRUE, fig.cap="Posterior distributions for the variances in chick intercepts and slopes. The model was fitted to a data set where chick labels at each time-point were randomised.", fig.width=7, fig.height=5}
par(mfrow=c(1,2))
hist(mweight.perm$VCV[,1], breaks=50, main="", freq=FALSE, xlab=expression(sigma[(Intercept)]^2))
hist(mweight.perm$VCV[,4], breaks=50, main="", freq=FALSE, xlab=expression(sigma[Time]^2))
```

This is quite disconcerting - our model tells us that chicks systematically differ in their growth rate, yet we've jumbled the chick labels up in a way that weights from the same chick must be completely uncorrelated.  How do we know that something similar isn't happening with our original model? One option would be to allow the residual variance to change as a function of $\texttt{Time}$ and so we can be sure that the information for estimating the random effect part of the model is really coming from the repeated nature of the observations.

## Heterogeneous Residual Variances {#het-res}

In Section \@ref(covf-sec) we saw that we can think of random regression models in terms of the covariance between observations at pairs of values for the continuous predictor variable (Equation \@ref(eq:Vrr)).  When the pair of values are the same we obtain the variance of observations at a given value of the predictor (Equation \@ref(eq:Vrr)). We can use this idea to allow the residual variance to increase with the square of some covariate. Although I won't discuss it here, $\texttt{covu}$ models (Section \@ref(covu-sec)) could be used to allow the residual variance to change as a polynomial function of the predictor with arbitrary degree. 

To our model of chick weights lets add the term `us(Time):units` to the random part of the model. 

``` {r mweight-3}
mweight.3<-MCMCglmm(weight~Time+I(Time^2)+Diet:Time, random=~us(1+Time):Chick+us(Time):units, data=ChickWeight, pr=TRUE, prior=prior.weight, longer=10)
summary(mweight.3)
```

The model mixes quite poorly, so I've ran it for 10 times longer than the default using the argument `longer`[^longer]. For a real analysis I would increase it even further, but the qualitative features of the posterior can be reliably assessed. The variance associated with `us(Time):units` lies roughly between one and two, and is certainly not zero. The (co)variances of the $\texttt{Chick}$ effects have dramatically decreased compared to the model where `us(Time):units` was not fitted (model `mweight.2`).

[^longer]: `longer` takes a positive integer and multiplies the arguments `nitt`, `thin` and `burnin` (which in this example were at there defaults, 13,000, 10 and 3,000)

What is the term `us(Time):units` fitting? The term `units` is factor with a unique level for each row of the data-frame. When we fit `~units` (the default argument to `rcov`) we are simply fitting residuals. The term `us(Time):units` is fitting a slope effect for each level of `units` (note we have not included the intercept in the variance function). Let's call the residual for observation $i$, as specified by `~units`, as $e^{(0)}_i$ (rather than the usual $e_i$). Let's call the random effect $i$ specified by the term `us(Time):units` as $e^{(1)}_i$ since it is the slope for observation $i$ with respect to $\texttt{Time}$. Then, 

$$\sigma^2_e = Var(e^{(0)}+e^{(1)}\texttt{Time})=\sigma^2_{\texttt{units}}+\texttt{Time}^{2}\sigma^2_{\texttt{Time:units}}$$

This has the same form as Equation \@ref(eq:Vrr) although note that that it misses a term $\sigma_{\texttt{Time:units}, \texttt{units}}$ and so the residual variance, $\sigma^2_e$ is forced to be a positive function of $\texttt{Time}$[^covu2] 

[^covu2]: `covu` models can overcome this constraint, or we could fit the reverse of $\texttt{Time}$ if we wanted a negative relationship (i.e. a covariate which is zero at day 21, one at day 20, two at day 19 and so on). 

The reason that the (co)variances of the $\texttt{Chick}$ effects has dramatically decreased is because the variability in weights is increasing over time, but only part of this is a consequence of variation in growth rate across chicks. We can see this by predicting the total change in the variance,  


```{r }
pred.weight.3<-predict(mweight.3, interval="prediction")
pred.weight.3b<-predict(mweight.3, interval="confidence", marginal=~us(Time):units)
```

``` {r pred-int2, echo=FALSE, fig=TRUE, include=TRUE, fig.cap="Chick weights plotted against time with predicted mean weight (solid lines) and 95% predictions intervals (dashed line). The red line is for a random intercept-slope model (`mweight.2`) that allows the variance to change as a quadratic function of $\\texttt{Time}$. The blue line is also for a random intercept-slope model (`mweight.3`) but .........", fig.width=7, fig.height=5}


mean.3<-tapply(pred.weight.3[,1], ChickWeight$Time, mean)
lower.3<-tapply(pred.weight.3[,2], ChickWeight$Time, mean)
upper.3<-tapply(pred.weight.3[,3], ChickWeight$Time, mean)


mean.3b<-tapply(pred.weight.3b[,1], ChickWeight$Time, mean)
lower.3b<-tapply(pred.weight.3b[,2], ChickWeight$Time, mean)
upper.3b<-tapply(pred.weight.3b[,3], ChickWeight$Time, mean)


plot(ChickWeight$weight~ChickWeight$Time, ylim=range(c(ChickWeight$weight, lower.1, upper.1, lower.2, upper.2)), ylab="Weight (g)", xlab = "Time (days)")

lines(mean.2~unique(ChickWeight$Time), col="red")
lines(lower.2~unique(ChickWeight$Time), col="red", lty=2)
lines(upper.2~unique(ChickWeight$Time), col="red", lty=2)
lines(mean.3~unique(ChickWeight$Time), col="blue")
lines(lower.3~unique(ChickWeight$Time), lty=2, col="blue")
lines(upper.3~unique(ChickWeight$Time), lty=2, col="blue")
lines(mean.3b~unique(ChickWeight$Time), col="green")
lines(lower.3b~unique(ChickWeight$Time), lty=2, col="green")
lines(upper.3b~unique(ChickWeight$Time), lty=2, col="green")


```


### Variance stabilisation

``` {r echo=TRUE, cache=TRUE}
mweight.4<-MCMCglmm(log(weight)~Time+I(Time^2)+Diet:Time, random=~Chick, data=ChickWeight, pr=TRUE, prior=prior.weight)
mweight.5<-MCMCglmm(log(weight)~Time+I(Time^2)+Diet:Time, random=~us(1+Time):Chick, data=ChickWeight, pr=TRUE, prior=prior.weight)
mweight.6<-MCMCglmm(log(weight)~Time+I(Time^2)+Diet:Time, random=~us(1+Time):Chick+us(Time):units, data=ChickWeight, pr=TRUE, prior=prior.weight)
```


``` {r pred-int3, echo=FALSE, fig=TRUE, include=TRUE, fig.cap="Log chick weights plotted against time with predicted mean weight (solid lines) and 95% predictions intervals (dashed line). The black line is for a random intercept model (`mweight.4`) that implicitly assumes the variance is constant over $\\texttt{Time}$.  The red line is for a random intercept-slope model (`mweight.5`) that allows the variance to change as a quadratic function of $\\texttt{Time}$. The blue line is also for a random intercept-slope model (`mweight.6`) but .........", fig.width=7, fig.height=5}

pred.weight.4<-predict(mweight.4, interval="prediction")
pred.weight.5<-predict(mweight.5, interval="prediction")
pred.weight.6<-predict(mweight.6, interval="prediction")
mean.4<-tapply(pred.weight.4[,1], ChickWeight$Time, mean)
lower.4<-tapply(pred.weight.4[,2], ChickWeight$Time, mean)
upper.4<-tapply(pred.weight.4[,3], ChickWeight$Time, mean)
mean.5<-tapply(pred.weight.5[,1], ChickWeight$Time, mean)
lower.5<-tapply(pred.weight.5[,2], ChickWeight$Time, mean)
upper.5<-tapply(pred.weight.5[,3], ChickWeight$Time, mean)
mean.6<-tapply(pred.weight.6[,1], ChickWeight$Time, mean)
lower.6<-tapply(pred.weight.6[,2], ChickWeight$Time, mean)
upper.6<-tapply(pred.weight.6[,3], ChickWeight$Time, mean)


plot(log(ChickWeight$weight)~ChickWeight$Time, ylim=range(c(log(ChickWeight$weight), lower.4, upper.4, lower.5, upper.5, lower.6, upper.6)), ylab="Log Weight", xlab = "Time (days)")


lines(mean.4~unique(ChickWeight$Time))
lines(lower.4~unique(ChickWeight$Time), lty=2)
lines(upper.4~unique(ChickWeight$Time), lty=2)
lines(mean.5~unique(ChickWeight$Time), col="red")
lines(lower.5~unique(ChickWeight$Time), col="red", lty=2)
lines(upper.5~unique(ChickWeight$Time), col="red", lty=2)
lines(mean.6~unique(ChickWeight$Time), col="blue")
lines(lower.6~unique(ChickWeight$Time), lty=2, col="blue")
lines(upper.6~unique(ChickWeight$Time), lty=2, col="blue")
```



## Antedependence Models


``` {r echo=TRUE}
prior.7<-list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=1, alpha.mu=rep(0,12), alpha.V=diag(12)*10000, beta.mu=0, beta.V=diag(1)*10000)))
mweight.7<-MCMCglmm(weight~Time+I(Time^2)+Diet:Time, random=~antec1v(as.factor(Time)):Chick, data=ChickWeight, prior=prior.7)

prior.8<-list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=1, alpha.mu=rep(0,12), alpha.V=diag(12)*10000, beta.mu=0, beta.V=diag(1)*10000), G2=list(V=1, nu=1, alpha.mu=0, alpha.V=1)))
mweight.8<-MCMCglmm(weight~Time+I(Time^2)+Diet:Time, random=~antec1v(as.factor(Time)):Chick+us(Time):units, data=ChickWeight, prior=prior.8)
```

``` {r pred-int4, echo=FALSE, fig=TRUE, include=TRUE, fig.cap="Chick weights plotted against time with predicted mean weight (solid lines) and 95% predictions intervals (dashed line). The black line is for a 1st order antedependence model (`mweight.7`). The red line for a 1st order antedependence model (`mweight.8`) but .........", fig.width=7, fig.height=5}

pred.weight.7<-predict(mweight.7, interval="prediction")
pred.weight.8<-predict(mweight.8, interval="prediction")
mean.7<-tapply(pred.weight.7[,1], ChickWeight$Time, mean)
lower.7<-tapply(pred.weight.7[,2], ChickWeight$Time, mean)
upper.7<-tapply(pred.weight.7[,3], ChickWeight$Time, mean)
mean.8<-tapply(pred.weight.8[,1], ChickWeight$Time, mean)
lower.8<-tapply(pred.weight.8[,2], ChickWeight$Time, mean)
upper.8<-tapply(pred.weight.8[,3], ChickWeight$Time, mean)


plot(ChickWeight$weight~ChickWeight$Time, ylim=range(ChickWeight$weight, lower.1, upper.1, lower.2, upper.2), ylab="Weight (g)", xlab = "Time (days)")

lines(mean.7~unique(ChickWeight$Time), col="red")
lines(lower.7~unique(ChickWeight$Time), col="red", lty=2)
lines(upper.7~unique(ChickWeight$Time),col="red", lty=2)
lines(mean.8~unique(ChickWeight$Time), col="blue")
lines(lower.8~unique(ChickWeight$Time), lty=2, col="blue")
lines(upper.8~unique(ChickWeight$Time), lty=2, col="blue")
```
Under-predicts. Probably OK. Correcting for Selection bias.


## User-defined Design Matrices {#user-defined-sec}

$\texttt{MCMCglmm}$ generally uses $\texttt{model.matrix}$, or a native equivalent, when evaluating a model formula to obtain the design matrices. Sometimes R formula syntax is not flexible enough to generate the appropriate design matrix. However, if a random-effect design matrix can be generated, it is possible to fit the model by exploiting the $\texttt{idv}$ variance structure. However, the parameters defined by the formula in the $\texttt{idv}$ structure should not be interacted with another factor. Consider the term `~idv(z1+z2+z3+z4):1` and let's set up the table of random effects as we have before:


$$\begin{array}{c|c}
&{\color{red}{\texttt{(Intercept)}}}\\
\hline\\
{\color{blue}{\texttt{z1}}}&{\color{blue}{\texttt{z1}}}.{\color{red}{\texttt{(Intercept)}}}\\
{\color{blue}{\texttt{z2}}}&{\color{blue}{\texttt{z2}}}.{\color{red}{\texttt{(Intercept)}}}\\
{\color{blue}{\texttt{z3}}}&{\color{blue}{\texttt{z3}}}.{\color{red}{\texttt{(Intercept)}}}\\
{\color{blue}{\texttt{z4}}}&{\color{blue}{\texttt{z4}}}.{\color{red}{\texttt{(Intercept)}}}\\
\end{array}$$

An $\texttt{idv}$ structure, like an $\texttt{idh}$ structure, assumes no correlation between effects on different rows: the $\texttt{id}$ in $\texttt{idh}$ stands for identity matrix. The $\texttt{h}$ in $\texttt{idh}$ stands for heterogeneous and so each row is allowed a different variance. Clearly, with only one effect per row estimating a variance is not feasible. However, with $\texttt{idv}$ the variances are assumed homogenous and so only a single variance (the $\texttt{v}$ in $\texttt{idv}$) is estimated that is common to all effects. The (four) regression coefficient are then treated exactly like simple random effects: identically and independently distributed with zero mean and a variance to be estimated. When there are many random effects, writing `~idv(z1+z2+z3+z4+...z999+z1000):1` is going to be painful to write and read. However, you can simply including the design matrix as a matrix in the data-frame and fit it inside the $\texttt{idv}$ structure. 

The multi-membership model fitted in Section \@ref(multim-sec) is a case in point. In that example, each child had five friends and we would like to fit $\texttt{friend}$ as a random effect that predicts the child's well-being. We can take a look at the random effect design matrix for that model:

```{r echo=FALSE}
data.friends[2,5]<-"S"
Z<-model.matrix(~friend.1-1, data=data.friends)
Z<-Z+model.matrix(~friend.2-1, data=data.friends)
Z<-Z+model.matrix(~friend.3-1, data=data.friends)
Z<-Z+model.matrix(~friend.4-1, data=data.friends)
Z<-Z+model.matrix(~friend.5-1, data=data.friends)
colnames(Z)<-LETTERS
``` 

```{r echo=FALSE}
head(Z)
```

The row for each child has five ones indicating their friends. This design matrix cannot be created using a standard R formula and the multi-membership linking function `~mm(freind.1+freind.2+freind.3+freind.4+freind.5)` was used to create the design matrix. However, if we had the incidence matrix, we could place it in the data-frame and fit an identical model:

```{r user-defined}
data.friends$Z<-Z
m.mm2<-MCMCglmm(y~1, random=~idv(Z), data=data.friends, prior=prior.mm)
```

Note that there is no need to explicitly interact the $\texttt{idv}$ structure with an intercept (in fact $\texttt{MCMCglmm}$ will complain if you try). In this example, the predictor variables ($\texttt{freind}$) are categorical, but there is nothing to stop design matrices from being formed using continuous predictors, as we'll see in the next section on splines.  

## Splines

The function $\texttt{smspline}$ in $\texttt{lmeSplines}$ can be used to generate design matrices for penalised splines that are transformed such that the random effects are expected to be identically and independently distributed with unknown variance. Using the ideas discussed above for user-specified design matrices (Section \@ref(user-defined-sec)) we can use an $\texttt{idv}$ structure to fit the spline. However, rather than generating the design matrix outside of $\texttt{MCMCglmm}$ and adding it to the data-frame, we can simply use the $\texttt{smspline}$ function inside $\texttt{idv}$. For example, we can fit a penalised cubic spline to one of the test data sets in $\texttt{lmeSplines}$:

```{r }
spline.prior=list(R=IW(1, 0.002), G=F(1,1000))
m.spline<-MCMCglmm(y~time, random= ~idv(smspline(time)), data=smSplineEx1, pr=TRUE, prior=spline.prior)
```

The random effects need to be saved (`pr=TRUE`) because these are the coefficients of the spline. Similarly, when making a prediction we want to include the spline coefficients in the prediction, rather than marginalising them (Section \@ref(ranpred-sec)).

```{r }
p.slpine<-predict(m.spline, marginal=NULL)
```

The model wiggles its way through the data, as splines do ...

```{r spline, echo=TRUE, include=TRUE, fig.cap="Data and prediction from a penalised cubic spline fitted to the example data $\\texttt{smSplineEx1}$ from $\\texttt{lmeSplines}$", fig.width=7, fig.height=5}
plot(y~time, data=smSplineEx1)
lines(p.slpine~smSplineEx1$time)
```


[^cont-1]: Note that I specified the prior using a prior generator function (Sections \@ref(Vprior-sec) and \@ref(VCVprior-sec)) but I upped the scale to 10,000 from the usual 1,000 because of the scale of the data. However, using a scale of 1,000 generates an almost identical posterior in this instance.

[^rate]: Taking the derivative of $E[y | \texttt{Diet}=1]$ with respect to $\texttt{Time}$ gives us the rate. 







