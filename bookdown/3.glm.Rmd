# Linear and Generalised Linear Models {#glm}

## Linear Model (LM)

A linear model is one in which unknown parameters are multiplied by (functions of) observed variables and then added together to give a prediction for the response variable. As an example, lets take the results from a Swedish experiment from the sixties:

``` {r label=traffic, echo=TRUE, include=FALSE}
data(Traffic, package="MASS")
Traffic$year<-as.factor(Traffic$year)
Traffic[c(1,2,184),]
```

The experiment involved enforcing speed limits on Swedish roads on some days, but on other days letting everyone drive as fast as they liked. The response variable (`y`) is the number of accidents recorded. The experiment was conducted in 1961 and 1962 for 92 days in each year. As a first attempt we could specify the linear model:

``` r
y ~ limit + year + day
```

but what does this mean?

### Linear Predictors {#lm-sec}

The model formula defines a set of simultaneous (linear) equations

$$\begin{array}{cl}
E[y\texttt{[1]}] &=\beta_{1}+\beta_{2}(\texttt{limit[1]=="yes"})+\beta_{3}(\texttt{year[1]=="1962"})+\beta_{4}\texttt{day[1]}\\
E[y\texttt{[2]}] &= \beta_{1}+\beta_{2}(\texttt{limit[2]=="yes"})+\beta_{3}(\texttt{year[2]=="1962"})+\beta_{4}\texttt{day[2]}\\
\vdots&=\vdots\\
E[y\texttt{[184]}] &= \beta_{1}+\beta_{2}(\texttt{limit[184]=="yes"})+\beta_{3}(\texttt{year[184]=="1962"})+\beta_{4}\texttt{day[184]}\\
\end{array}
\label{SE-eq}   (\#eq:SE-eq)$$

where the $\beta$'s are the unknown coefficients to be estimated, and the variables in $\texttt{this font}$ are observed predictors. Continuous predictors such as `day` remain unchanged, but categorical predictors are expanded into a series of binary variables of the form '*do the data come from 1961, yes or no?*', '*do the data come from 1962, yes or no?*', and so on for as many years for which there are data.

It is cumbersome to write out the equation for each data point in this way, and a more compact way of representing the system of equations is

$$
E[{\bf y}] = {\bf X}{\boldsymbol{\mathbf{\beta}}}
(\#eq:lm)
$$

where ${\bf X}$ is called a design matrix and contains the predictor information for all observations, and ${\boldsymbol{\mathbf{\beta}}} = [\beta_{1}\ \beta_{2}\ \beta_{3}\ \beta_{4}]^{'}$ is the vector of parameters.  Here,  $E[{\bf y}]$ is a vector of the 184 expected values.

``` {r echo=TRUE}
X<-model.matrix(y~limit+year+day, data=Traffic)
X[c(1,2,184),]
```

The binary predictors *do the data come from 1961, yes or no?* and *there was no speed limit, yes or no?* do not appear. These are the first factor levels of `year` and `limit` respectively, and are absorbed into the global intercept ($\beta_{1}$) which is fitted by default in R. Hence the expected number of accidents for the four combinations (on day zero) are $\beta_{1}$ for 1961 with no speed limit, $\beta_{1}+\beta_{2}$ for 1961 with a speed limit, $\beta_{1}+\beta_{3}$ for 1962 with no speed limit and $\beta_{1}+\beta_{2}+\beta_{3}$ for 1962 with a speed limit.

The simultaneous equations defined by Equation \@ref(eq:lm) cannot be solved directly because we do not know the left-hand side - expected values of $y$. We only know the observed value, which we assume is distributed around the expected value with some error. In a normal linear model we assume that these errors (residuals) are normally distributed:

$${\bf y}-{\bf X}{\boldsymbol{\mathbf{\beta}}} = {\bf e} \sim N(0, \sigma^{2}_{e}{\bf I})$$

${\bf I}$ is a $184\times 184$ identity matrix. It has ones along the diagonal, and zeros in the off-diagonals. The zero off-diagonals imply that the residuals are uncorrelated, and the ones along the diagonal imply that they have the same variance ($\sigma^{2}_{e}$). Thinking about the distribution of residuals is less helpful when we move on to GLM's and so I prefer to think about the model in the form:


$${\bf y}\sim N({\bf X}{\boldsymbol{\mathbf{\beta}}}, \sigma^{2}_{e}{\bf I})$$

and say the response is *conditionally* normal, with the conditioning on the model (${\bf X}{\boldsymbol{\mathbf{\beta}}}$). It is important to note that this is different from saying the response is normal. If having a speed limit had a very strong effect the (marginal) distribution of the response may be bimodal and far from normal, and yet by including speed-limit as a predictor, conditional normality may be achieved.

We could use `MCMCglmm` to fit this model, but to connect better with what comes next, let's use `glm` to estimate ${\bf \beta}$ and $\sigma^{2}_{e}$ assuming that the number of accidents follow a conditional normal distribution (the `MCMCglmm` syntax is identical):

``` {r echo=TRUE}
m2a.1<-glm(y ~ limit + year + day, data=Traffic)
summary(m2a.1)
```

On day zero in 1961 in the absence of a speed limit we expect `r round(coef(m2a.1)["(Intercept)"],1)` accidents (the intercept). With a speed limit we expect `r round(abs(coef(m2a.1)["limityes"]),1)` fewer accidents and we can quite confidently reject the null-hypothesis of no effect - particularly if we were willing to use a one-tailed test, which seems reasonable. There are `r round(abs(coef(m2a.1)["year1962"]),1)` fewer accidents in 1962, although this could just be due to chance, and for every unit increase in $\texttt{day}$ the number of accidents is predicted to go up by `r round(coef(m2a.1)["day"],2)`. The $\texttt{day}$ variable is encoded as integers from 1 to 92 with the same $\texttt{day}$ in different years being comparable (for example, the same day of the week and roughly the same date). If $\texttt{day}$'s are evenly spaced throughout the year the $\texttt{day}$ effect is roughly the effect of increasing calender date by four (365/92) days. The estimate of the residual variance, $\sigma^2_e$, is the dispersion parameter (`r round(sigma(m2a.1)^2,1)`). 

Because the number of accidents are count data we might worry about the assumption of conditional normality, and indeed the residuals show the typical right skew:

``` {r label=hist-traffic, echo=TRUE, include=TRUE, fig.cap="Histogram of residuals from model `m2a.1` which assumed they followed a Normal distribution."}
hist(resid(m2a.1))
```

It's not extreme, and the conclusions probably won't change, but we could assume that the data follow some other distribution.

## Generalised Linear Model (GLM)

Generalised linear models extend the linear model to non-normal data. They are essentially the same as the linear model described above, except they differ in two aspects. First, it is not necessarily the mean response that is predicted, but some function of the mean response. This function is called the link function. For example, with a log link we are trying to predict the logged expectation:

$$\textrm{log}(E[{\bf y}]) = {\bf X}{\boldsymbol{\mathbf{\beta}}}$$

or alternatively

$$E[{\bf y}] = \textrm{exp}({\bf X}{\boldsymbol{\mathbf{\beta}}})$$

where $\textrm{exp}$ is the inverse of the log link function- exponentiating. The second difference is that many distributions are single parameter distributions for which a variance does not need to be estimated because it can be inferred from the mean. For example, we could assume that the number of accidents are Poisson distributed, in which case we also make the assumption that the variance is equal to the expected value. Technically, GLM's only apply to a restricted set of distributions (those in the exponential family) but $\texttt{MCMCglmm}$ can accommodate a range of GLM-like models for other distributions (see Table \@ref(tab:dist)). 

## Poisson GLM

For now we will concentrate on a Poisson GLM with log link (the default link function for the Poisson distribution):

``` {r echo=TRUE}
m2a.2<-glm(y ~ limit + year + day, family=poisson, data=Traffic)
summary(m2a.2)
```

While the sign of the effects are comparable to that seen in the linear model, their numerical values are completely different and the significance of all effects has increased dramatically. Should we worry? The model is defined on the log scale and so to get back to the data scale we need to exponentiate. Exponentiating the intercept gives us the predicted number of accidents on day zero in 1961 without a speed limit:

``` {r echo=TRUE}
exp(m2a.2$coef["(Intercept)"])
```

which is very close to the intercept in the linear model (`r round(m2a.1$coef["(Intercept)"], 3)`), which is reassuring. 

To get the prediction for the same day with a speed limit we need to add the $\texttt{limityes}$ coefficient

``` {r echo=TRUE}
exp(m2a.2$coef["(Intercept)"]+m2a.2$coef["limityes"])
```

With a speed limit there are expected to be `r formatC(exp(m2a.2$coef["(Intercept)"]+m2a.2$coef["limityes"])/exp(m2a.2$coef["(Intercept)"]), format="f", 3)` as many accidents than if there was no speed limit. This value can be more directly obtained:

``` {r echo=TRUE}
exp(m2a.2$coef["limityes"])
```

and holds true for any given day in either year. The proportional change is identical because the model is *linear* on the log scale and $exp(\beta+\dots)=exp(\beta)exp(\dots)$. There is not always a direct relationship with the corresponding coefficients from the linear model but we can reassure ourselves that the parameters have the same qualitative meaning. For example, for $\texttt{day}$ 0 in 1961 the linear model predicts a drop from `r round(m2a.1$coef["(Intercept)"],1)` to `r round(m2a.1$coef["(Intercept)"]+m2a.1$coef["limityes"],1)` accidents when a speed limit is in place - around  `r round(1+m2a.1$coef["limityes"]/m2a.1$coef["(Intercept)"],2)` as many accidents, comparable to that predicted in the log-linear model.    

So in terms of the reported coefficients, the linear model and the Poisson log-linear model are roughly consistent with each other. However, in terms of accurately quantifying the uncertainty in those coefficients the Poisson model has a serious problem - it is very over confident.

## Overdispersion

Most count data do not conform to a Poisson distribution because the variance in the response exceeds the expectation.  In the summary to `m2a.2` the ratio of the residual deviance to the residual degrees of freedom is `r formatC(m2a.2$deviance/m2a.2$df.residual, format="f",3)` which means, roughly speaking, there is `r formatC(m2a.2$deviance/m2a.2$df.residual, format="f",1)` times more variation in our response (after conditioning on the model) than what we expect. This is known as overdispersion and it is easy to see how it arises, and why it is so common. 

If the predictor data had not been available to us then the only model we could have fitted was one with just an intercept:

``` {r echo=TRUE}
m2a.3<-glm(y ~ 1, data=Traffic, family="poisson")
summary(m2a.3)
```

for which the residual variance exceeds that expected by a factor of `r formatC(m2a.3$deviance/m2a.2$df.residual, format="f",1)`. Of course, the variability in the residuals must go up if there are factors that influence the number of accidents, but which we hadn't measured. It's likely that in most studies there are things that influence the response that haven't been measured, and even if each thing has a small effect individually, in aggregate they can cause substantial overdispersion.

### Multiplicative Overdispersion

There are two ways of dealing with overdispersion. With `glm` the distribution name can be prefixed with `quasi` and a dispersion parameter estimated:

``` {r echo=TRUE}
m2a.4<-glm(y ~ limit + year + day, family=quasipoisson, data=Traffic)
summary(m2a.4)
```

`glm` uses a multiplicative model of overdispersion and so the estimate of the dispersion parameter is roughly equivalent to how many times greater the variance is than expected, after taking into account the predictor variables. You will notice that although the parameter estimates have changed very little, the standard errors have gone up and the significance gone down. Overdispersion, if not dealt with, can result in extreme anti-conservatism. For example, the second lowest number of accidents (8) occurred on $\texttt{day}$ 91 of 1961 without a speed limit. Our model predicts this should have been the second worst day for accidents over the whole two years, and the probability of observing 8 or less accidents on this day is predicted to be approximately 3 in a 100,000:

``` {r echo=TRUE}
ppois(8, exp(m2a.2$coef["(Intercept)"]+91*m2a.2$coef["day"]))
```

If we did not accommodate the overdispersion, anything additional we put in in the model that could potentially explain such an improbable occurrence would come out as significant even if in reality it wasn't important. This is because there simply isn't any flexibility in the null model to accommodate such occurrences.  For example, if the extreme value happened to be associated with a particular level of a categorical predictor or happened to be associated with an extreme value of some continuous predictor, then the coefficients associated with these predictors may well come out as significant. However, under a more plausible null model the extreme observations may not be too surprising and there may be little support for the predictors having an effect on the response. A more plausible model, and one that we've alluded to, would be to allow the \emph{expected} number of accidents to vary across sampling points due to unmeasured variables. This would allow the variation in the number of \emph{observed} accidents to exceed the predicted mean based on the measured variables (the assumption of the standard Poisson). 

### Additive Overdispersion {#addod-sec}

I believe that a model assuming all relevant variables have been measured or controlled for, should **not** be the default model, and so when you specify `family=poisson` in $\texttt{MCMCglmm}$, overdispersion is always dealt with[^3.1]. However, $\texttt{MCMCglmm}$ does not use a multiplicative model, but an additive model.


``` {r m2a.5}
prior<-list(R=list(V=1, nu=0.002))
m2a.5<-MCMCglmm(y ~ limit + year + day, family="poisson", data=Traffic, prior=prior, pl=TRUE)
```

The element `Sol` contains the posterior distribution of the coefficients of the linear model, and we can plot their marginal distributions:

``` {r label=mcmc-traffic, echo=FALSE, include=TRUE, fig.width=7, fig.height=10,  fig.cap="MCMC summary plot for the coefficients from a Poisson glm (model `m2a.5`)."}
plot(m2a.5$Sol)
```

Note that the posterior distribution for the `year1962` spans zero, in agreement with the quasipoisson `glm` model, and that in general the estimates for the two models (and their uncertainty - see Section \@ref(intervals-sec)) are broadly similar:

``` {r }
summary(m2a.4)
```

With additive overdispersion the linear predictor includes a 'residual', for which a residual variance is estimated (hence our prior specification).

$$E[{\bf y}] = \textrm{exp}({\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf e})$$

At this point it will be handy to represent the linear model in a new way:

$${\bf l} = {\boldsymbol{\mathbf{\eta}}}+{\bf e}$$

where ${\bf l}$ is a vector of latent variables ($\textrm{log}(E[{\bf y}])$ in this case) and ${\boldsymbol{\mathbf{\eta}}}$ is the usual symbol for the linear predictor (${\bf X}{\boldsymbol{\mathbf{\beta}}}$). The data we observe are assumed to be Poisson variables with expectation equal to the exponentiated latent variables:

$${\bf y} \sim Pois(\textrm{exp}({\bf l}))$$

Note that the latent variable does not exactly predict $y$, as it would if the data were Normal, because there is additional variability in the Poisson process[^3.2]. In the call to $\texttt{MCMCglmm}$ I specified `pl=TRUE` to indicate that I wanted to store the posterior distributions of the latent variables (also known as the liabilities). This is not usually necessary and can require a lot of memory (we have 1000 posterior samples for each of the 182 data points). However, as an example we can obtain the posterior mean residual for data point 92 which is the data from $\texttt{day}$ 92 in 1961 when there was no speed limit:

``` {r echo=TRUE}
lat92<-m2a.5$Liab[,92]
# posterior distribution of the 92nd latent variable (liability)

eta92<-m2a.5$Sol[,"(Intercept)"]+m2a.5$Sol[,"day"]*Traffic$day[92]
# posterior distribution of X\beta for the 92nd observation

resid92<-lat92-eta92
# posterior distribution of e for the 92nd observation

mean(resid92)
# posterior mean of e for the 92nd observation
```

This particular observation has a negative expected residual indicating that the probability of getting injured was less than expected for this *particular* realisation of that $\texttt{day}$ in that year without a speed limit. If that combination of predictors ($\texttt{day}$=92, $\texttt{year}$=1961 and $\texttt{limit}$=$\texttt{no}$) could be repeated it does not necessarily mean that the actual number of accidents would always be less than expected, because it would follow a Poisson distribution with a mean equal to `exp(lat92)` (`r formatC(exp(mean(lat92)), format="f", 3)`). 

Like residuals in a standard linear model, the residuals are assumed to be independently and normally distributed with an expectation of zero and an estimated variance. If the residual variance was zero then ${\bf e}$ would be a vector of zeros and the model would conform to the standard Poisson GLM. However, the posterior distribution of the residual variance is located well away form zero:

```{r label=vcv-traffic, echo=TRUE, include=TRUE, fig.cap="MCMC summary plot for the residual (`units`) variance from a Poisson glm (model `m2a.5`). The residual variance models any overdispersion, and a residual variance of zero would imply that the response conforms to a standard Poisson."}
plot(m2a.5$VCV)
```

## Prediction in GLM

To get the expected number of accidents for the 92nd observation we simply exponentiated the latent variable: exp(`lat92`). However, it is important to realise that this is the expected number had the residual been exactly equal to the observed residual for that observation (`resid92`): we are calculating the expected number conditional on the set of unmeasured variables that affected that *particular* realisation of $\texttt{day}$ 92 in 1961 without a speed limit.  When calculating a prediction we usually aim to average over these residuals (or random effects - see \@ref(ranpred-sec)) since we would like to know what the average response would be for observations made on a $\texttt{day}$ of *type* 92 in 1961 without a speed limit. On the log-scale the expectation is simply the linear predictor ($\eta$):

$$
log(E_e[y]) = E_e[l] = E_e[\eta+e] =  \eta+E_e[e]=\eta
$$

since the residuals have zero expectation (here I have subscripted the expectation with the variable we are averaging over). The `predict` function can be applied to `MCMCglmm` objects and if we specify `type="terms"` we get the prediction of the link scale - the log scale in this case:

```{r }
predict(m2a.5, type="terms")[92]
```

which is equal to the posterior mean of `eta92` obtained earlier. We can see this visually in Figure \@ref(fig:prediction2) where I have plotted the distribution of the latent variable on a $\texttt{day}$ of type 92 in 1961 without a speed limit.

```{r pred-var, echo=FALSE}
resid.var<-mean(m2a.5$VCV)
intercept.coef<-mean(m2a.5$Sol[,"(Intercept)"])
day.coef<-mean(m2a.5$Sol[,"day"])
pos.ly<-seq(intercept.coef+day.coef*92-sqrt(resid.var), intercept.coef+day.coef*92+sqrt(resid.var), length=100)
d.ly<-dnorm(pos.ly,intercept.coef+day.coef*92, sqrt(resid.var))
d.y<-dlnorm(exp(pos.ly),intercept.coef+day.coef*92, sqrt(resid.var))
```

```{r label=prediction2, echo=FALSE, include=TRUE, fig.width=7, fig.height=5, fig.cap="The predicted distribution for the average number of accidents on the log scale for a $\\texttt{day}$ of type 92 in 1961 without a speed limit (in red). On the log scale the distribution is assumed to be normal around the linear predictor ($\\eta=$) with a variance of $\\sigma^{2}_e$. As a consequence the mean, median and mode of the distribution are equal to the linear predictor on the log scale."}

plot(d.ly~pos.ly,type="l", col="red",, xlab="log(E[y])", ylab="Density", ylim=c(min(d.ly), max(d.ly)*1.1))
abline(v=intercept.coef+day.coef*92, col="black", lwd=2)
text(intercept.coef+day.coef*92, max(d.ly)*1.05,expression(paste("log(", E[e], paste("[y])=",eta))), pos=4)
```

To get the prediction on the data scale (i.e. in terms of the actual expected number of accidents) it is tempting to think we could just calculate `exp(eta92)`. However, this is the expected number of accidents had the residual been exactly zero. If we wish to average over the residuals we require:

$$
E_e[y] = E_e[\textrm{exp}(l)] = E_e[\textrm{exp}(\eta+e)]
$$


and because exponentiation is a non-linear function this average will deviate from $\textrm{exp}(\eta)$ (Figure \@ref(fig:prediction3). 

```{r label=prediction3, echo=FALSE, include=TRUE, fig.width=7, fig.height=5, fig.cap="The predicted distribution for the average number of accidents on the data scale for a $\\texttt{day}$ of type 92 in 1961 without a speed limit (in red).  On the log scale the distribution is assumed to be normal around the linear predictor ($\\eta$) with a variance of $\\sigma^{2}_e$ (see \\@ref(fig:prediction2)). However when transforming to the data scale (by exponentiating) the symmetry is lost and the different measures of central tendency do not coincide. Since the residuals are normal on the log scale, the distribution on the data scale is log-normal and so analytical solutions exist for the mean, mode and median."}

plot(d.y~exp(pos.ly),type="l", col="red", xlab="E[y]", ylab="Density", ylim=c(min(d.y), max(d.y)*1.1), xlim=c(min(exp(pos.ly))*0.95, max(exp(pos.ly))))
med<-exp(intercept.coef+day.coef*92)
mu<-exp(intercept.coef+day.coef*92+0.5*resid.var)
mod<-exp(intercept.coef+day.coef*92-resid.var)
abline(v=med, col="black", lwd=2)
text(med+2.23, max(d.y)*0.8,pos=2, expression(paste(MED[e], paste("[y] =  ",exp(eta)))))
abline(v=mu, lwd=1, lty=1)
text(mu, max(d.y)*0.6,expression(paste(E[e], paste("[y]=",exp(eta+scriptstyle(frac(1,2))*sigma[e]^2)))), pos=4)
abline(v=mod, lwd=1, lty=1)
text(mod, max(d.y)*1.05,expression(paste(MODE[e], paste("[y]=",exp(eta-sigma[e]^2)))), pos=2)
```

To obtain predictions on the data scale we can specify `type="response"` (the default) when using `predict`:

```{r }
predict(m2a.5)[92]
```

which is slightly greater than `exp(eta92)` (`r round(mean(exp(eta92)),3)`). For all link-functions, the median value on the data scale can be easily calculated by taking the inverse-link transform of the linear predictor. However, obtaining the mean and mode is often more challenging than it is for log-link, and numerical integration or approximations are required. 

The `predict` function is returning a single number for observation 92 yet the model object contains 1,000 samples from the posterior distribution of all model parameters. This is because the `predict` function returns the posterior mean of the predicted value. Since we have the complete posterior distribution we can also place a 95\% credible interval on the prediction (see Section \@ref(intervals-sec)):

```{r }
predict(m2a.5, interval="confidence")[92,]
```

### Posterior Predictive Distribution

In some cases we would like to visualise or summarise aspects of the predictive distribution other than the mean. The complete predictive distribution is hard to work with, but the `simulate` function allows you to draw samples from the predictive distribution. The default is to generate a sample using a random draw from the posterior distribution, resulting in a draw from what is known as the posterior predictive distribution:  

```{r }
ypred<-simulate(m2a.5)
```

We can use these simulated values to characterise any aspect of the predictive distribution we want. For example, we can obtain quantiles and compare them to the quantiles of the actual data to see how well the model captures aspects of the observed marginal distribution: 

```{r marginal-qq, fig.width=7, fig.height=5, fig.cap="qq-plot of the posterior predictive distribution and the data distribution"}
qqplot(ypred, Traffic$y)
abline(0,1)
```

Not too bad, although the predictive distribution perhaps has greater support for extreme values than are observed. If we merely wish to know the interval in which some specified percentage of the data are predicted to lie, we can also use the predict function but with `interval="prediction"`. By default the 95\% (highest posterior density) interval is calculated using as many simulated samples as there are saved posterior samples. For the 92nd observation the prediction interval is

```{r }
predict(m2a.5, interval="prediction")[92,]
```

Note that the reported mean (`fit`) differs from that returned by `interval="confidence"` due to Monte Carol error only.

## Binomial and Bernoulli GLM

The general concepts introduced for the Poisson GLM extend naturally to Binomial data, albeit with a different link function. However, it is worth spending a little time exploring a Binomial GLM as I think overdispersion, and how we deal with it, is easier to understand with binomial data. However, we'll also see how the 'residuals' defined earlier for capturing overdispersion complicate the analysis of Bernoulli data and result in MCMcglmm using a non-standard parameterisation.

The Binomial distribution has two parameters - the number of trials $n$ and the probability of success, $p$. In a Binomial GLM the number of trials is assumed known leaving only $p$ to be estimated from the number of trials that are 'successes' or 'failures'. When `family="binomial"` is specified (or equivalently `family="multinomial2"`) MCMCglmm uses the standard link function for the Binomial - the logit link - and the logit probability of success is modelled as

$$log\left(\frac{p}{1-p}\right)  = l = \eta+e$$

The logit transform takes a probability and turns it into a log odds ratio. If we want to get back to the probability we use the 
inverse of the logit transform:

$$p = \frac{exp(l)}{1+exp(l)}$$

The logit link is actually the quantile function for the logistic distribution and so is available as the function `qlogis`. The inverse of a quantile function is a cumulative distribution function and so the inverse-logit transform  is `plogis`. 


To introduce the Binomial GLM we will analyse some data I collected on how grumpy my colleagues look. I took two photos ($\texttt{photo}$) of 22 people ($\texttt{person}$) working in the Institute of Evolution and Ecology, Edinburgh. In one photo the person was happy and in the other they were grumpy ($\texttt{type}$). 122 respondents gave a score between 1 and 10 indicating how grumpy they thought each person looked in each photo (with 10 being the most grumpy). 

```{r }
data(Grumpy)
Grumpy[c(1:3, 44),]
```     

$\texttt{y}$ gives the average score given by the 122 respondents.  The number of respondents giving a photo a score of five or less ($\texttt{l5}$) or more than five ($\texttt{g5}$) is also recorded in addition to the person's age ($\texttt{age}$) and a proxy for how long they had been academia - the number of years since they published their first academic paper ($\texttt{ypub}$). Here, we will model the probability of getting a grumpy score greater than five as a function of whether the person was happy or grumpy and how long they had been in academia. As with `glm`, successes should be in the first column of the response, and failures in the second:


``` {r mbinom}
mbinom.1<-MCMCglmm(cbind(g5, l5)~type+ypub, data=Grumpy, family="multinomial2", pl=TRUE)
summary(mbinom.1)
```

The model coefficients are most easily interpreted after exponentiating as they then give the proportional change in the odds ratio.  The odds of having a score greater than five is `r paste0("exp(", round(mean(mbinom.1$Sol[,"typehappy"]),3), ")=", round(exp(mean(mbinom.1$Sol[,"typehappy"])),3))` times lower when happy, as expected. The odds increases by a factor `r paste0("exp(", round(mean(mbinom.1$Sol[,"ypub"]),3), ")=", round(exp(mean(mbinom.1$Sol[,"ypub"])),3))` for each year in academia. When the coefficient is small in magnitude like this, you can get a rough estimate by looking directly at the coefficient: `r round(mean(mbinom.1$Sol[,"ypub"]),3)` roughly translates into a `r round(mean(mbinom.1$Sol[,"ypub"])*100,1)`% increase and `r -round(mean(mbinom.1$Sol[,"ypub"]),3)` would translate into a `r round(mean(mbinom.1$Sol[,"ypub"])*100,1)`% decrease[^3.3]. The $\texttt{units}$ (residual) variance is also large with credible intervals that are far from zero. 

### Overdispersion

As with overdispersion in the Poisson model, this excess variation can be attributed to predictors that are not included in the model but cause the probability of success to vary over observations (photos in this case). For example, the 3rd and 25th observation have the same values for every predictor

```{r }
Grumpy[c(3,25),]
```

and so in the absence of overdispersion these parameters would result in a predicted probability of success of

```{r echo=FALSE}
p<-plogis(predict(mbinom.1, posterior="mean", type="terms")[3])
```

$$p = \textrm{plogis}(\beta_{\texttt{(Intercept)}}+\beta_{\texttt{typehappy}}\times0+\beta_{\texttt{ypub}}\times16)=`r round(p,3)`
\label{plogis-eq}   (\#eq:plogis)$$

Given there were 122 respondents (trials) we can calculate the two values between which the number of successes is expected to fall 95 % of the time.


```{r }
qbinom(c(0.025, 0.975), size=122, prob=p)
```

While photo $\texttt{4527}$ is well within the range with `r Grumpy[25,"g5"]` successes, the number of respondents giving photo $\texttt{4521}$ is substantially higher (`r Grumpy[3,"g5"]`) - it's an outlier. In Figure \@ref(fig:DandC) the two photos are shown and it is clear why their underlying probabilities may deviate from that predicted. Most obviously the two photos are of different people and people vary in how grumpy they look. Since we have two photos per person we could (and should) estimate $\texttt{person}$ effects, however this is best done by treating these effects as random, which we will cover later, in Chapter \@ref(ranef). Even if $\texttt{person}$ effects were fitted there's also likely to be a whole host of observation-level (photo-specific) effects that are not captured in the model such as whether the person had their eyes closed or was wearing a dreary grey fleece.

```{r DandC, echo=FALSE,out.width="40%", fig.show='hold',fig.align='center', out.extra = 'style="display:inline-block; vertical-align:middle; margin-right:2%;"', fig.cap="Photo 4521 (left) and photo 4527 (right). For the predictors fitted in model `mbinom.1`, these photos have the same values ($\\texttt{type}=\\texttt{grumpy}$ and $\\texttt{ypub}=16$ years)"}
knitr::include_graphics(c("Figures/IMG_4521.JPG","Figures/IMG_4527.JPG"))
```

If we include the 'residuals' when calculating the predicted probability for these two photos we can see that indeed their probabilities are quite different:

```{r }
mean(plogis(mbinom.1$Liab[,3]))
# predicted probability for photo 4521
mean(plogis(mbinom.1$Liab[,25]))
# predicted probability for photo 4527
``` 

### Prediction


When calculating the predicted probability in the absence of the residuals in Equation \@ref(eq:plogis) I was careful to say that the prediction assumed an absence of overdispersion. However, when overdispersion is present we need to average over the distribution of the residuals in order to get an average. As we saw in the log-linear Poisson model, because the inverse-link function (`plogis`) is non-linear the average of $E_e[\texttt{plogis}(\eta+e)]$ is different from $\texttt{plogis}(E_e[\eta+e])=\texttt{plogis}(\eta)$. Unlike the Poisson log-linear model this expectation cannot be calculated analytically and the predict function by default numerically evaluates the integral:

$$\int_l \texttt{plogis}(l)f_N(l | \eta, \sigma^2_e)dl$$

where $f_N$ is the probability density function of the normal. For each of the 44 observations this is done 1,000 times (the number of saved posterior samples) to get the posterior mean prediction and can be very slow (it is actually faster to fit the model). The number reported by `predict` is $np$ rather than $p$ and so to get the predicted probability for a photo of someone who has grumpy and had been publishing for 16 years we can get the prediction for the 3rd observation and divide by the number of trials (122):


```{r }
predict(mbinom.1)[3]/122
```

A little different from $\texttt{plogis}(\eta)=$ `r round(p,3)`. For the binomial with logit link, analytical approximations have been developed in @Diggle.2004 and @McCulloch.2001 which are considerably faster and reasonably accurate:

```{r }
predict(mbinom.1, approx="diggle")[3]/122
predict(mbinom.1, approx="mcculloch")[3]/122
```


### Bernoulli GLM {#bernoulli-sec}

Bernoulli data are a special case of the Binomial in which the number of trials is equal to one and so either a success or a failure is observed. To explore a Brenoulli model we can take our *average* Grumpy scores and simply dichotomise them into whether the average score was five or less or more than five:

```{r }
Grumpy$mean5<-Grumpy$y>5
``` 

If we fit a binomial model to these data in `MCMCglmm` it has exactly the same form as before (although a single column of outcomes can be passed). But importantly, for Bernoulli data there is no information to estimate the residual variance. This does not necessarily mean that variation in the probability of success across observations is absent, only that we can't estimate it. For example, imagine we took 100 people who had been publishing for 16 years and took a photograph of them when they were grumpy. Let's say the probability that the mean score for such photos exceeded 5 was 0.5. If the probability for all photos was exactly 0.5 (i.e. the probability of success did not vary over observations) then we expect 50 success and 50 failures across our observations. However, imagine the case where the probability of success was 100% for 50 photos and 0% for 50 photos (i.e. the probability of success varies considerably over observations). We would also expect 50 success and 50 failures, and so the distribution of successes with and without variation in the underlying probability would be identical. In the absence of information most software sets the 'residual' variance to zero (i.e. the probability of success dose not vary over observations), but it is important to understand that this is a convenient but arbitrary choice. Given this, it is desirable that any conclusions drawn from the model do not depend on this arbitrary choice. Worryingly, both the location effects (fixed and random) and variance components are completely dependent on the magnitude of the residual variance. `MCMCglmm` allows the user to fix the residual variance at a value of their choice, but unfortunately a value of zero results in a chain that will not mix and so I usually fix the residual variance to one[^3.4]:

```{r mbinom.2}
prior.mbinom.2=list(R=list(V=1, fix=1))
mbinom.2<-MCMCglmm(mean5~type+ypub, family="categorical", data=Grumpy, prior=prior.mbinom.2)
```

However, it would have been equally valid to fixed the residual variance at three:

```{r mbinom.3}
prior.mbinom.3=list(R=list(V=3, fix=1))
mbinom.3<-MCMCglmm(mean5~type+ypub, family="categorical", data=Grumpy, prior=prior.mbinom.3)
```

and if we compare the MCMC traces for the coefficients we can see that we are sampling different posterior distributions (Figure \@ref(fig:bernoulli)).


```{r bernoulli, fig.width=7, fig.height=8, fig.cap="MCMC trace for coefficients of a Bernoulli GLM from two models (`mbinom.2` in black and `mbinom.3` in red). The data and model structure are identical but in `mbinom.2` the residual variance was set to one and in `mbinom.3` the residual variance was set to three. The data provide no information about the residual variance."}
plot(mcmc.list(mbinom.2$Sol, mbinom.3$Sol), density=FALSE)
```

Should we worry? Not really. The two models give almost identical predictions (Figure \@ref(fig:bernoulli-pred)).

```{r bernoulli-pred, fig.width=7, fig.height=5, fig.cap="Predicted probabilities from a Bernoulli GLM from two models. The data and model structure are identical but in `mbinom.2` the residual variance was set to one and in `mbinom.3` the residual variance was set to three. The data provide no information about the residual variance."}
plot(predict(mbinom.2), predict(mbinom.3))
abline(0,1)
```

We just have to be careful about how we express the results. Stating that the `typehappy` coefficient is `r round(mean(mbinom.2$Sol[,"typehappy"]),3)` (the posterior mean estimate from `mbinom.2`) is meaningless without putting it in the context of the assumed residual variance (one). Although the @Diggle.2004 approximation is less accurate than that in @McCulloch.2001 we can use it rescale the estimates by the assumed residual variance (we'll call it $\sigma^{2}_{\texttt{units}}$) in order to obtain the posterior distributions of the parameters under the assumption that the actual residual variance (we'll call it $\sigma^{2}_{e}$) is equal to some other value. For location effects the posterior distribution needs to be multiplied by $\sqrt{\frac{1+c^{2}\sigma^{2}_{e}}{1+c^{2}\sigma^{2}_{\texttt{units}}}}$ where $c=16\sqrt{3}/15\pi$. If obtain estimates under the assumption that $\sigma^{2}_{e}=0$ and we see the posterior distributions of the coefficients are very similar from the two models (Figure \@ref(fig:bernoulli-rescale)).

```{r bernoulli-rescale, echo=TRUE, include=TRUE, fig.cap="MCMC trace for rescaled coefficients of a Bernoulli GLM from two models (`mbinom.2` in black and `mbinom.3` in red). The data and model structure are identical but in `mbinom.2` the residual variance was set to one and in `mbinom.3` the residual variance was set to three. However, the coefficients have been rescaled using the @Diggle.2004 approximation such that they represent what the coefficients would be if the residual variance was zero. The data provide no information about the residual variance.", fig.width=7, fig.height=8}
c2 <- ((16*sqrt(3))/(15*pi))^2
rescale.2 <- mbinom.2$Sol*sqrt(1/(1+c2*1))
rescale.3 <- mbinom.3$Sol*sqrt(1/(1+c2*3))
plot(mcmc.list(as.mcmc(rescale.2), as.mcmc(rescale.3)), density=FALSE)
```

In addition, the posterior distributions are centred on the ML estimates obtained by `glm` which implicitly assumes $\sigma^2_e=0$ (Figure \@ref(fig:bernoulli-glm)).

```{r bernoulli-glm, echo=FALSE, include=TRUE, fig.width=7, fig.height=8, fig.cap="Posterior distributions for rescaled coefficients of a Bernoulli GLM (`mbinom.3`). The rescaling gives approximate (but accurate) posterior distributions had the residual variance been set to zero, rather than three. The red lines indicate estimates from `glm` that implicitly assumes the residual variance is zero and the blue lines indicate the unscaled posterior means."}
par(mfrow=c(3,1))
m.binom.2.glm<-glm(mean5~type+ypub, family="binomial", data=Grumpy)
mbinom.3.plot<-MCMCglmm(mean5~type+ypub, family="categorical", data=Grumpy, prior=prior.mbinom.3, nitt=13000*10)
rescale.3.plot<-mbinom.3.plot$Sol*sqrt(1/(1+c2*3))

hist(rescale.3.plot[,1], breaks=50, main=NULL, xlab="(Intercept)")
abline(v=coef(m.binom.2.glm)[1], col="red", lwd=2)
abline(v=mean(mbinom.3.plot$Sol[,1]), col="blue", lwd=2)
hist(rescale.3.plot[,2], breaks=50, main=NULL, xlab="typehappy")
abline(v=coef(m.binom.2.glm)[2], col="red", lwd=2)
abline(v=mean(mbinom.3.plot$Sol[,2]), col="blue", lwd=2)
hist(rescale.3.plot[,3], breaks=50, main=NULL, xlab="ypub")
abline(v=coef(m.binom.2.glm)[3], col="red", lwd=2)
abline(v=mean(mbinom.3.plot$Sol[,3]), col="blue", lwd=2)
```

### Probit link

The inverse-logit function is the cumulative distribution function for the logistic distribution. It makes sense that a cumulative distribution function for a continuous distribution that can take any value would serve as a good inverse link function for a probability: it takes any value between plus and minus infinity and squeezes it to be between zero and one. While the logit is the most common link function for binomial data, the probit link and complementary log-log (cloglog) link are also widely used. They are the cumulative distribution functions for the unit normal and standard Gumbel distributions respectively. In Figure \@ref(fig:link) we can see how the probability changes as a function of a covariate ($x$) for the different links (with intercepts and slopes chosen so that the functions are matched at the origin). 


```{r link, echo=FALSE, include=TRUE, fig.width=7, fig.height=5, fig.cap="Predicted probabilities as a function of covariate $x$ using the inverse logit (black), inverse probit (red) and inverse complementary log-log link functions. For each link the intercept and slope were chosen such that when $x= 0$ the function equals 0.5 and has a derivative of one."}
pos.l<-seq(-3, 3, length=100)
plot(plogis(pos.l/dlogis(0))~pos.l, type="l", xlab="x", ylab="Probability", lwd=1.5)
lines(pnorm(pos.l/dnorm(0))~pos.l, col="red", lwd=1.5)
lines(1-exp(-exp(pos.l/exp(-1)+log(-log(1/2))))~pos.l, col="blue", lwd=1.5)
legend(-2.5,0.9, legend=c("logit", "probit", "cloglog"), fill=c("black", "red", "blue"))
```

We can see that the link functions generate rather similar predicted probabilities, particularly the logit and probit links. For Bernoulli data, another way to conceptualise these link functions is in terms of threshold models. Imagine the case where we have managed to set the non-identifiable residual variance to zero and all residuals $e$ are zero and can be omitted. The probability of success is then given by the inverse-link of $\eta$. For probit link this would be the probability of getting a value less than $\eta$ from the unit normal (the grey area in the left panel of Figure \@ref(fig:thresh)). Equivalently, it is the probability of getting a value greater than 0 from a normal with a mean equal to $\eta$ and a standard deviation of one (the grey area in the right panel of Figure \@ref(fig:thresh)). 

```{r thresh, echo=FALSE, include=TRUE, fig.width=7, fig.height=5, fig.cap="Probability density function for $\\epsilon$ (left) or  $\\eta+\\epsilon$ (right) where $\\epsilon$ is normal with mean zero and a standard deviation of one. Applying `pnorm` to $\\eta$ gives the shaded area on the left and is the probability of success using probit link. The shaded area on the right gives the same probability and is the chance of getting a value greater than zero from a normal with mean $\\eta$ and a standard deviation of one."}

par(mfrow=c(1,2))
eta<-1
pos.l<-seq(-4, 4, length=1000)

plot(dnorm(pos.l)~pos.l, lwd=1.5, type="l", ylim=c(0.0165, 0.45), ylab="Probability", xlab=expression(epsilon))

xshade <- pos.l[pos.l >= -4 & pos.l <= eta]   
yshade <- dnorm(xshade)                  

polygon( c(min(xshade), xshade,max(xshade)),   # x: down left, along curve, down right
         c(0,yshade,0),             # y: x-axis up, along curve, back to axis
         col = "grey")

abline(v=eta, lwd=1.5)
abline(v=0, lwd=1.5, lty=2)

text(eta+0.5, 0.42, expression(eta), cex=1.2)

plot(dnorm(pos.l, eta)~pos.l, lwd=1.5, type="l", ylim=c(0.0165, 0.45), ylab="Probability", xlab=expression(eta+epsilon))

xshade <- pos.l[pos.l >= 0 & pos.l <= 4]   
yshade <- dnorm(xshade, eta)                  

polygon( c(min(xshade), xshade,max(xshade)),   # x: down left, along curve, down right
         c(0,yshade,0),             # y: x-axis up, along curve, back to axis
         col = "grey")

abline(v=eta, lwd=1.5)
abline(v=0, lwd=1.5, lty=2)

text(eta+0.5, 0.42, expression(eta), cex=1.2)
```


If we think explicitly about the normal deviates that underpin these probability calculations (we'll call them $\epsilon$), then in the left panel $\epsilon$ falls below the 'threshold' $\eta$ with the required probability or in the right panel, $\eta+\epsilon$ falls above the 'threshold' zero with the required probability. Since $\epsilon$ is unit-normal we can equate  $\eta$ with $e$ and $\eta+\epsilon$ with $l$ and apply the inverse link function $\mathbf{1}_{\{l>0\}}$. This function outputs 1 (a success) if $l>0$ and a failure otherwise (as in the right panel of Figure \@ref(fig:thresh)). This is implemented as `family=threshold` in MCMCglmm, and if the residual variance (i.e. the variance of $\epsilon$ or $e$) is fixed at one corresponds exactly to standard probit regression[^3.5]. 


```{r mbinom.4}
prior.mbinom.4=list(R=list(V=1, fix=1))
mbinom.4<-MCMCglmm(mean5~type+ypub, family="threshold", data=Grumpy, prior=prior.mbinom.4)
```


Unfortunately the same trick can't be used for other link functions because the $\epsilon$'s' cannot be equated with $e$'s because they come from different distributions. In some ways the probit link is a natural link function for models that contain random effects, which are also usually assumed to be normal. However, the downside is that the coefficients in a probit model do not have a direct interpretation like they do in a logit model. Nevertheless, both models give very similar predictions and are unlikely to be statistically distinguishable in the vast mean5 of cases (Figure \@ref(fig:thresh-logit)).


```{r thresh-logit, echo=FALSE, include=TRUE, fig.width=7, fig.height=5, fig.cap="Predicted probabilities from a Bernoulli GLM from two models with the same model structure. However, `mbinom.2` uses a logit link with a residual variance set to one and in `mbinom.4` uses a standard probit link."}
 plot(predict(mbinom.4), predict(mbinom.2))
 abline(0,1)
```

## Ordinal Data  

Thinking about Bernoulli GLM's in terms of thresholds provides a natural way of thinking about how we could model categorical data that falls into a natural ordering. For example, rather than dichotomising our outcome into those that had a mean score greater than, or less than, five, lets place the observations into three categories: (1, 4], (4, 6], (6, 10]. 

```{r}
Grumpy$categories<-as.numeric(cut(Grumpy$y, c(1,4,6,10)))
```

Rather than just having a single threshold at zero, we can imagine adding another threshold that chops the distribution of $\eta+e=l$ into three regions, and hence probabilities. To make things simple we will just fit an intercept only model (so all observations have the same value of $\eta$) as this will be easier to visualise:

```{r mordinal}
prior.mordinal=list(R=list(V=1, fix=1))
mordinal<-MCMCglmm(categories~1, data=Grumpy, family="threshold", prior=prior.mordinal)
```

The output of `mordinal` gives the intercept ($\eta$ in this case) as before but also the additional threshold (cutpoint - stored as `CP` in the model object):

```{r }
summary(mordinal)
```

As we did in the right panel of Figure \@ref(fig:thresh)) we can draw this model with the estimated threshold ($\gamma$) included (Figure \@ref(fig:ordinal)). 


```{r ordinal, echo=FALSE, include=TRUE, fig.width=7, fig.height=5, fig.cap=paste0("Probability density function for  $l=\\eta+e$ where $e$ is normal with mean zero and a standard deviation of one. $\\eta=$ ", round(colMeans(mordinal$Sol), 3), " and was obtained from the model `mordinal`. The distribution is 'cut' into three regions by the fixed threshold at zero and the estimated threshold ($\\gamma$) at ", round(colMeans(mordinal$CP), 3), ". The shaded areas correspond to the probabilities of observing the three (ordered) outcomes.")}


eta<-colMeans(mordinal$Sol)
thresh<-colMeans(mordinal$CP)

pos.l<-seq(-4, 4, length=1000)

plot(dnorm(pos.l, eta)~pos.l, lwd=1.5, type="l", ylim=c(0.0165, 0.45), ylab="Probability", xlab=expression(eta+epsilon))

xshade <- pos.l[pos.l >= 0 & pos.l <= thresh]   
yshade <- dnorm(xshade, eta)                  

polygon( c(min(xshade), xshade,max(xshade)),   # x: down left, along curve, down right
         c(0,yshade,0),             # y: x-axis up, along curve, back to axis
         col = "grey")


xshade <- pos.l[pos.l >= thresh & pos.l <= 4]   
yshade <- dnorm(xshade, eta)                  

polygon( c(min(xshade), xshade,max(xshade)),   # x: down left, along curve, down right
         c(0,yshade,0),             # y: x-axis up, along curve, back to axis
         col = "skyblue")

abline(v=eta, lwd=1.5)
abline(v=0, lwd=1.5, lty=2)
abline(v=thresh, lwd=1.5, lty=2)
text(eta+0.25, 0.42, expression(eta), cex=1.2)
text(thresh+0.25, 0.42, expression(gamma), cex=1.2)
```

The shaded areas give the probability for each category and the posterior mean probabilities can be easily calculated:

```{r }
mean(pnorm(0, mordinal$Sol))
mean(pnorm(mordinal$CP, mordinal$Sol)-pnorm(0, mordinal$Sol))
mean(1-pnorm(mordinal$CP, mordinal$Sol))
```

These correspond closely to the observed frequencies in the data:

```{r }
table(Grumpy$categories)/44
```

## Non-zero Binomial Data

The final distribution that we will cover in this Chapter is something I have called the Non-zero Binomial which can be fitted using `family="nzbinom"`. It was implemented for a specific application in which $n$ ($\texttt{number}$) bumblebees were pooled and assayed for the presence of the Acute Bee Paralysis virus [@Pascall.2018]. If the assay came back positive then at least one bee in the pool was infected and the outcome was recorded as a 'success' - otherwise non of the bees in the pool were infected and we have a 'failure' ($\texttt{infected}$). 

```{r }
data(ABPvirus)
ABPvirus[c(1:2,98:99),]
```

The observations are essentially censored, and while we cover censoring more generally in Chapter \@ref(measurement), the Non-zero Binomial is perhaps best covered here. The probability of a success for the Non-zero Binomial (which we will designate as $P$) is  $1-(1-p)^n$ where $p$ is the probability that an *individual* bumblebee was infected. If the number of bumblebees in a pool varies over observations, `family="nzbinom"` is a useful tool and the linear model is defined for the logit transform of $p$, as in the standard binomial.  In the standard Bernoulli model the residual variance could not be estimated and was fixed at some value. Perhaps surprisingly, there is some information to estimate the residual variance (overdispersion) in a Non-zero Binomial model as long as $n$ varies over pools. However, the amount of information is so small that it is probably safest to fix the residual variance at some non-zero value (I use one as a convention) rather than risk sampling very high values of the residual variance that can cause numerical issues. We will ignore which species of bumblebee the observations were made on, as these are best treated as random effects \@ref(ranef)

```{r mnzbinom}
mnzbinom<-MCMCglmm(cbind(infected, number)~1, family="nzbinom", data=ABPvirus, prior=list(R=list(V=1, fix=1)))
summary(mnzbinom)
```

```{r echo=FALSE}
newdata<-data.frame(infected=c(0, rep(1, 10)), number=1:11)
pnzbinom<-predict(mnzbinom, newdata=newdata, interval="confidence")
```

The median probability of an individual bumblebee being infected is low: $\texttt{plogis}(eta)=$ `r round(mean(plogis(mnzbinom$Sol)))` such that the expected probability of at least one infection is small for the lowest pool-size ($\texttt{number}=$ `r min(ABPvirus$number)`) and is still well below one in the largest pool size ($\texttt{number}=$ `r max(ABPvirus$number)`) - see Figure \@ref(fig:nzbinom). 

```{r nzbinom, echo=FALSE, include=TRUE, fig.width=7, fig.height=5, fig.cap=paste0("Probability of seeing at least one infection ($P$) in pools of varying size when the probability of an individual bumblebee being infected ($p$) is constant. The solid black line is the posterior mean and the shaded area is the 95\\% credible interval. The estimated median probability of an individual bumblebee being infected is", round(mean(plogis(mnzbinom$Sol)), 3),  "and the mean probability is", round(pnzbinom[1], 3), ".")}

plot(pnzbinom[,"fit"]~I(1:11), type="l", lwd=1.5, ylab="Probability of Success (at least one infection)", xlab="Pool size (Number)", xlim=c(2+0.33,11-0.33), ylim=c(0, 0.5))

polygon(x = c(1:11, 11:1), y = c(pnzbinom[,"lwr"],rev(pnzbinom[,"upr"])), col =  adjustcolor("dodgerblue", alpha.f = 0.10), lty=2)
```

If the data set was larger and/or variation in the pool size was greater, there may be sufficient information to justify trying to estimate the residual variance, rather than fixing it to one.  To see where the information comes from first imagine that you have information on samples all with a pool-size of 1. You could jut average you the resulting zeros and ones to get an estimate of $P_1$ where the subscript designates the size of the pool for which the probability of at least one infection is calculated. Even if the probability of infection varied over individuals, the average probability $E[p]$ is equal to $P_1$, as we saw when we determined why variation in the probability cannot be estimated with standard Bernoulli data. Let's then imagine we move to pool sizes of 2. $P_2$ (the probability of at least one success in a pool of 2) is then $E[1-(1-p)^2]$. If there is no variation in $p$ then $P_2=1-(1-P_1)^2$ since $p$ is a constant and equal to $P_1$. However, when there is variation in $p$ the non-linearity in the function (i.e. $(1-p)^2$) means that $E[1-(1-p)^2]$ will deviate from $1-(1-E[p])^2=1-(1-P_1)^2$: we expect the the number of successful pools of size 2 to be less than that predicted from estimating $p$ from pools of size 1[^3.6]. Consequently, the rate at which $P$ asymptotes with increasing pool-size (as seen in Figure \@ref(fig:nzbinom)) provides information about how much variation in $p$ exists.

## Complete Separation

One potential issue that can occur in GLM is something known as complete separation or the extreme category problem. While it can occur for any distribution which is discrete, it is most commonly seen when the response is Bernoulli. It occurs when the predictors perfectly predict the outcome. For example,  in Section \@ref(bernoulli-sec) we generated Bernoulli data for each photo by assessing whether the mean score was greater than or less than 5. Photo $\texttt{type}$ is a very good predictor of the outcome, but not perfect:


```{r }
table(mean5=Grumpy$mean5, type=Grumpy$type)
```

However, if we removed the three observations for which the mean grumpy score was greater than five despite the person being happy, then $\texttt{type}$ perfectly predicts the outcome. If we fit a probit model to these data, but using `glm` a very strange thing happens:

```{r }
data.cs<-subset(Grumpy, !(mean5==TRUE & type=="happy"))

mcs<-glm(mean5~type, family=binomial(link=probit), data=data.cs)
summary(mcs)
```

The coefficient for $\texttt{type}$ now has a huge standard error and the p-value is close to one, despite being a very good predictor of the outcome. If we assess significance using Fisher's exact test we get:

```{r }
fisher.test(table(data.cs$mean5, data.cs$type))
```

we get a very significant result, as expected. 

The model fitted using $\texttt{MCMCglmm}$ with default priors also behaves oddly (see Figure \@ref(fig:separation1)):

``` {r label=separation1, include=TRUE, fig.cap="MCMC summary plots for the intercept and $\\texttt{type}$ effect in a binary GLM (`mcs.2`) with probit link. For $\\texttt{happy}$ photos all 19 observations are failures (the mean grumpy score is less than 5) and we have complete separation. A normal prior with large variance was used for the model coefficients."}
prior.mcs.2=list(R=list(V=1, fix=1))
mcs.2<-MCMCglmm(mean5~type, family="threshold", data=data.cs, prior=prior.mcs.2)
plot(mcs.2$Sol)
```

This strange behaviour occurs because the ML estimate for the probability of success is zero for $\texttt{happy}$ photos. On the probit scale this translates into $-\infty$ and so the ML estimate of the difference between $\texttt{happy}$ and $\texttt{grumpy}$ photos (the $\texttt{typehappy}$ effect) will also be $-\infty$. Consequently, with a flat prior on the $\texttt{typehappy}$ effect the posterior distribution is improper. The default prior for model coefficients is not flat - they are normal with a large variance ($10^8$), but even so this diffuse prior can cause problems if there is complete (or near complete) separation. To see this, think about an intercept-only model. A diffuse prior on the probit scale puts a lot of density on very large positive or negative values and so puts a lot of density close to zero and one on the probability scale (see Figure \@ref(fig:link)). If the likelihood prevents the posterior from reaching these extremes then the prior has little influence because its largely flat outside of the extremes (it's U shaped). However, with complete separation the likelihood is indeed placing a lot of density at these extreme values and the prior holds the posterior at these values. If, on the other hand we had specified the prior on the intercept to be normal with a mean of zero and a variance of one then the prior on the probability scale would be flat (since the inverse-link function is the cumulative density function for the unit normal).  If a logit-link had been used then there is no normal prior that would result in flatness on the probability scale, and having a mean of zero and a variance of $\pi^2/3$ (the variance of a logistic distribution) is a close as you can get.        


``` {r label=gelman-prior, include=TRUE, fig.cap="Prior density on the probability of success in an intercept-only Bernoulli GLM. In black a probit link was used and the prior on the intercept is normal with zero mean and a variance of one.  In red a logit link was used and the prior on the intercept is normal with zero mean and a variance of $\\pi^2/3$."}
x<-seq(-7, 7, length=100)
jacob.plogis<-eval(D(expression(exp(x)/(1+exp(x))), "x"), list(x=x))
plot(I(dnorm(x, 0, sqrt(pi^2/3))/jacob.plogis)~plogis(x), type="l", xlim=c(0+0.05,1-0.05), ylab="Prior Density", xlab="Probability of Success")
abline(h=1)
```

Fitting a flat prior on the probability results in a better mixing chain.

``` {r label=separation2, include=TRUE, fig.cap="MCMC summary plots for the intercept and $\\texttt{type}$ effect in a binary GLM (`mcs.2`) with probit link. For $\\texttt{happy}$ photos all 19 observations are failures (the mean grumpy score is less than 5) and we have complete separation. A normal prior with a variance of one was used for the model coefficients."}

prior.mcs.3=list(B=list(mu=rep(0,2), V=gelman.prior(~type, data=data.cs)), R=list(V=1, fix=1))
mcs.3<-MCMCglmm(mean5~type, family="threshold", data=data.cs, prior=prior.mcs.3)
plot(mcs.3$Sol)
```









To demonstrate we will use some data from a pilot study on the Indian meal moth (*Plodia interpunctella*) and its granulosis virus (PiGV) collected by Hannah Tidbury & Mike Boots at the University of Sheffield.

``` {r echo=TRUE}
data(PlodiaRB)
```

The data are taken from 874 moth pupae for which the `Pupated` variable is zero if they failed to pupate (because they were infected with the virus) or one if they successfully pupated. The 874 individuals are spread across 49 full-sib families, with family sizes ranging from 6 to 38.

To start we will fix the residual variance at 1:

``` {r echo=TRUE, cache=TRUE}
prior.m2b.1=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=0.002)))
m2b.1<-MCMCglmm(Pupated~1, random=~FSfamily, family="categorical", data=PlodiaRB, prior=prior.m2b.1)
```

and then fit a second model where the residual variance is fixed at 2:

``` {r echo=TRUE, cache=TRUE}
prior.m2b.2=list(R=list(V=2, fix=1), G=list(G1=list(V=1, nu=0.002)))
m2b.2<-MCMCglmm(Pupated~1, random=~FSfamily, family="categorical", data=PlodiaRB, prior=prior.m2b.2)
```

The posterior distribution for the intercept differs between the two models (see Figure \@ref(fig:Bin1)):

``` {r label=Bin1, echo=TRUE, include=TRUE, fig.cap="MCMC summary plots for the intercept of a binary GLMM where the residual variance was fixed at one (black) and two (red)."}
plot(mcmc.list(m2b.1$Sol,m2b.2$Sol))
```

as do the variance components (see Figure \@ref(fig:Bin2)):

``` {r label=Bin2, echo=TRUE, include=TRUE, fig.width=7, fig.height=5, fig.cap="MCMC summary plots for the between family variance component of a binary GLMM where the residual variance was fixed at one (black) and two (red)."}
plot(mcmc.list(m2b.1$VCV,m2b.2$VCV))
```

Should we worry? Not really. We just have to be careful about how we express the results. Stating that the family variance is `r formatC(posterior.mode(m2b.1$VCV[,1]), 3, format="f")` is meaningless without putting it in the context of the assumed residual variance. It is therefore more appropriate to report the intraclass correlation which in this context is the expected correlation between the state Pupated/Not Pupated, for members of the same family. It can be
calculated as:

$$\texttt{IC} =  \frac{\sigma^{2}_{\texttt{FSfamily}}}{\sigma^{2}_{\texttt{FSfamily}}+\sigma^{2}_{\texttt{units}}+\pi^{2}/3}$$

for the logit link, which is used when `family=categorical`, or

$$\texttt{IC} =  \frac{\sigma^{2}_{\texttt{FSfamily}}}{\sigma^{2}_{\texttt{FSfamily}}+\sigma^{2}_{\texttt{units}}+1}$$

for the probit link, which is used if `family=ordinal` was specified.

Obtaining the posterior distribution of the intra-class correlation for each model shows that they are sampling very similar posterior distributions (see Figure \@ref(fig:IC))

``` {r label=IC, echo=TRUE, include=TRUE, fig.cap="MCMC summary plots for the intra-family correlation from  a binary GLMM where the residual variance was fixed at one (black) and two (red).", fig.width=7, fig.height=5}
IC.1<-m2b.1$VCV[,1]/(rowSums(m2b.1$VCV)+pi^2/3)
IC.2<-m2b.2$VCV[,1]/(rowSums(m2b.2$VCV)+pi^2/3)
plot(mcmc.list(IC.1,IC.2))
```

Using the approximation due to @Diggle.2004 described earlier we can also rescale the estimates by the estimated residual variance ($\sigma^{2}_{\texttt{units}}$) in order to obtain the posterior distributions of the parameters under the assumption that the actual residual variance ($\sigma^{2}_{e}$) is equal to some other value. For location effects the posterior distribution needs to be multiplied by $\sqrt{\frac{1+c^{2}\sigma^{2}_{e}}{1+c^{2}\sigma^{2}_{\texttt{units}}}}$ and for the variance components the posterior distribution needs to be multiplied by $\frac{1+c^{2}\sigma^{2}_{e}}{1+c^{2}\sigma^{2}_{\texttt{units}}}$ where $c$ is some constant that depends on the link function. For the probit $c=1$ and for the logit $c=16\sqrt{3}/15\pi$. We can obtain estimates under the assumption that $\sigma^{2}_{e}=0$:

``` {r label=ICI, echo=TRUE, include=TRUE, fig.cap="MCMC summary plots for the expected proportion of caterpillars pupating from  a binary GLMM where the residual variance was fixed at one (black) and two (red).", fig.width=7, fig.height=5}
c2 <- ((16 * sqrt(3))/(15 * pi))^2
Int.1 <- m2b.1$Sol/sqrt(1 + c2 * m2b.1$VCV[, 2])
Int.2 <- m2b.2$Sol/sqrt(1 + c2 * m2b.2$VCV[, 2])
plot(mcmc.list(as.mcmc(Int.1), as.mcmc(Int.2)))
```

The posteriors should be virtually identical under a flat prior (See Figure \@ref(fig:ICI)) although with different priors this is not always the case. Remarkably, @vanDyk.2001 show that leaving a diffuse prior on
$\sigma^{2}_{\texttt{units}}$ and rescaling the estimates each iteration, a Markov chain with superior mixing and convergence properties can be obtained (See section \@ref(parameter-expansion)).


For these types of problems, I usually remove the global intercept (`-1`) and use the prior $N(0, \sigma^{2}_{\texttt{units}}+\pi^2/3)$ because this is reasonably flat on the probability scale when a logit link is used. For example,

``` {r echo=TRUE, eval=FALSE}
prior.m2c.4=list(B=list(mu=c(0,0), V=diag(2)*(1+pi^2/3)), R=list(V=1, fix=1))
m2c.4<-MCMCglmm(y~treatment-1, data=data.bin, family="categorical", prior=prior.m2c.4)
plot(m2c.4$Sol)
```

looks a little better (see Figure \@ref(fig:separation1)), and the posterior distribution for the probability of success in treatment 2 is consistent with the exact binomial test for which the 95% CI were (`r formatC(m2c.2$conf.int[1], 3, format="f")` -`r formatC(m2c.2$conf.int[2], 3, format="f")`). With such a simple model, the prediction for observation 26 is equal to the treatment 2 effect and so we can get the the credible interval (on the data scale) for treatment 2 using the predict function:

``` {r echo=FALSE, cache=TRUE}
prior.m2c.4=list(B=list(mu=c(0,0), V=diag(2)*(pi^2/3+1)), R=list(V=1, fix=1))
m2c.4<-MCMCglmm(y~treatment-1, data=data.bin, family="categorical", prior=prior.m2c.4)
```

``` {r echo=TRUE}
predict(m2c.4, interval = "confidence")[26, ]
```

[^3.1]: This is a bit disingenuous. The MCMC algorithm implemented in \texttt{MCMcglmm} depends on a non-zero residual variance to ensure mixing  -  if the residual variance was set to zero (i.e. no overdispersion in GLM(M)) then the Markov chain would be reducible.

[^3.2]: Since the residuals are assumed normal the exponentiated residuals are log-normal, and so this model is often referred to as the Poisson log-normal [@Hinde.1982]. The Negative Binomial distribution is a commonly used alternative for overdispersed count data. The Negative Binomial is conceptually identical to the Poisson log-normal except the exponentiated residuals are assumed to be gamma distributed. The log-normal and gamma distributions are so similar that for most data sets it would be hard to distinguish between the Negative Binomial and the Poisson log-normal.  

[^3.3]: When $x$ is small $\textrm{exp}(x)\approx 1+x$.

[^3.4]: For more complicated models a (co)variance matrix may be estimated for a particular random component rather than just a single variance as here. In such cases, `V` is a matrix.  The value at which (part of) the (co)variance matrix is fixed at is determined by `V`. Any elements of the covariance matrix in rows and/or columns equal to or greater than `fix` are fixed. In the case of a single variance `fix=1` simply fixes the variance (element 1,1 of the (co)variance matrix) at whatever is specified in `V` (one in this example).

[^3.5]: `family="ordinal"` is also available but it is only retained to ensure back compatibility: it is equivalent to `family="threshold"` with the variance fixed at the specified value plus one. When `family="ordinal"` was implemented I had misunderstood the paper by @Albert.1993 and didn't realise the latent variable could be Gibbs sampled with a threshold link ($\mathbf{1}_{\{l>0\}}$).

[^3.6]: I can confidently say that $E[1-(1-p)^2]$ will be less than $1-(1-E[p])^2$ because of Jensen's inequality. Jensen's inequality tells us that $E[f(x)]$ will be less than $f(E[x])$ if $f$ is convex, or greater than than $f(E[x])$ if $f$ is concave. In the current example, $f(x) = (1-x)^2=1-2x+x^2$, which is a quadratic with a positive quadratic coefficient and so convex. 

