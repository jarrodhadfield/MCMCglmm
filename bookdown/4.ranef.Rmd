# Random effects {#ranef}

In some cases we may have measured variables whose effects we would like to treat as random. Often the distinction between fixed and random is given by example: things like city, species, individual and vial are random, but sex, treatment and age are not. Or the distinction is made using rules of thumb: if there are few factor levels and they are interesting to other people they are fixed. However, this doesn't really confer any understanding about what it means to treat something as fixed or random, and doesn't really allow judgements to be made for variables in which the rules of thumb seem to contradict each other. Similarly, these 'explanations' don't give any insight into the fact that all effects are technically random in a Bayesian analysis.

Random effect models are often expressed as an extension of Equation \@ref(eq:lm):

$$E[{\bf y}] = {\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf Z}{\bf u}
 \label{MM}   (\#eq:MM)$$

where ${\bf Z}$ is a design matrix like ${\bf X}$, and ${\bf u}$ is a vector of parameters like ${\boldsymbol{\mathbf{\beta}}}$. However, at this stage there is simply no distinction between fixed and random effects. We could combine the design matrices (${\bf W} = [{\bf X}, {\bf Z}]$) and combine the vectors of parameters ($\boldsymbol{\theta} = [{\boldsymbol{\mathbf{\beta}}}^{'}, {\bf u}^{'}]^{'}$) to get:

$$E[{\bf y}] = {\bf W}\boldsymbol{\theta}
 \label{MM2}   (\#eq:MM2)$$

which is **identical** to Equation \@ref(eq:MM).  So if we don't need to distinguish between fixed and random effects at this stage, when should we distinguish between them, and what distinguishes them?  

When we treat an effect as random we believe that the coefficients have some distribution around a mean of zero; often we assume they are normal[^ranef.1] and that they are independent (represented by an identity matrix) and identically distributed with variance $\sigma^{2}_{u}$:

$${\bf u} \sim N({\bf 0}, {\bf I}\sigma^{2}_{u})$$

$\sigma^{2}_{u}$ is a parameter of the model which we estimate, in addition to ${\bf u}$. In a Bayesian analysis we would also assign $\sigma^{2}_{u}$ a prior, and $\sigma^{2}_{u}$ is often called a hyper-parameter with an associated hyper-prior. 

Fixed effects in a frequentist analysis are not assigned a distribution, but we can understand this in terms of the limit to the normal distribution

$$\boldsymbol{\beta} \sim N({\bf 0}, {\bf I}\sigma^{2}_{\beta})$$

as $\sigma^{2}_{\beta}$ tends to infinity. In a Bayesian setting we would call this a flat improper prior. In practice, we often use diffuse proper priors in Bayesian analyses. For example, the default in $\texttt{MCMCglmm}$ is to set $\sigma^{2}_{\beta}=10^8$. Then, $\boldsymbol{\beta}$ are technically random - they are assigned a distribution - but I find it useful to retain the frequentist terminology 'fixed'. The only difference then is that the 'fixed' effects are assigned a prior distribution with a variance that is defined by the user-specified prior ($\sigma^{2}_{\beta}$ - which is often set to be large) and the 'random' effects are assigned a prior distribution with a variance that is estimated ($\sigma^{2}_{u}$ - which could be large, but also zero). 

That is the distinction between fixed and random effects. The difference really is that simple, but it takes a long time and a lot of practice to understand what this means in practical terms, and why working with random effects can be a very powerful way of modelling data.  To get a feel for why we might want to fit an effect as random or not, lets work through an example before moving on to model fitting. In Section \@ref(binom-sec) we analysed binomial data where 122 respondents had looked at 44 photographs of people and given them a 'grumpy score' of more than five (a success) or less than five (a failure). If, instead of 122 respondents, there had been a zillion respondents, we could use the average proportion of success for each photo as a nearly perfect estimates of their probabilities of success. The variance of these near-perfect estimates could serve as a reasonable estimate of the variance in photo effects. If the probabilities were all clustered tightly around 0.5: 0.505, 0.501, 0.499 and so on, then variance would be estimated to be small. Let's then imagine that we obtained a $45^\textrm{th}$ photograph but by this point the respondents were so bored I managed to only recruit a single person who gave the photo a score greater than five - a success. Since we only have one observation for this photo the average proportion of success would be one. Do you think the best estimate of the probability of success for the $45^\textrm{th}$ photograph is then 1.000? I think you wouldn't: you would use the knowledge that you have gained from the other photos and say that it is more likely that if you had managed to recruit more respondents you would have got a roughly even split of success and failures. You have used common sense, treated the photo effects as random, and *shrunk* photo 45's effect towards the average because the variance ($\sigma^2_u$) was small and we have a strong prior. If we had treated the photo effects as fixed, we believe that the only information regarding a photo's value comes from data associated with that particular photo, and the estimate of photo 45's probability would have been one. When we treat an effect as random, we also use the information that comes from data associated with that particular photo (obviously), but we weight that information by what the data associated with other photos tell us about the likely values that the effect could take - through the parameter $\sigma^2_u$. What if the probabilities weren't all clustered tightly around 0.5, but took on values 0.500, 0.998, 0.002, 0.327 ...?  The variance $\sigma^2_u$ would be larger and the prior information for our $45^\textrm{th}$ photo would be weaker: perhaps we got a success because the underlying probability was 0.998, but a single success would also not be very surprising if the underlying probability was 0.500, or even 0.327. We might then be happy that our best estimate of the probability of success for the $45^\textrm{th}$ photograph was close to one, although with such weak prior information (large $\sigma^2_u$) the uncertainty would remain large.   

When the motivation for treating an effect as random is explained this way, it is hard to come up with a reason why you wouldn't treat all effects as random. However, you have to consider how much information is in a given data set to estimate $\sigma^2_u$, which we will cover in Section \@ref(fixed-or-random-sec)

## GLMM {#GLMM}

In Section \@ref(binom-sec), the binomial model we fitted only contained fixed effects, as specified in the `fixed` argument to `MCMCglmm` (`fixed=cbind(g5,l5)~type+ypub`). No random effects were fitted, although 'residuals' were fitted as default to absorb any overdispersion. Residuals are random effects for which we estimate a variance - the hyperparameter, $\sigma^2_e$ - and when used with Binomial or Poisson responses are commonly referred to as observation-level random effects. Since there is a one-to-one correspondence between observation and photo in this data set, $\sigma^2_e$ is equivalent to the $\sigma^2_u$ discussed above (although $\sigma^2_e$ refers to the variance on the logit scale rather the probability scale used implicitly above). We saw that the probability of success varied greatly across photos (model `mbinom.1`) but we also noted that some of this variation may be due to the person being photographed and we could tease apart the effect of person from the specifics of the photo since each person was photographed twice - once when happy and once when grumpy. \texttt{person} has 22 levels and you are probably not interested in knowing the grumpy score of someone you didn't know - \texttt{person} effects seem to satisfy the rule of thumb often used to decide that they should be treated as random. The random effect model is specified through the argument `random` and has a simple form in this case:


``` {r mbinom2}
mbinom.2<-MCMCglmm(cbind(g5, l5)~type+ypub, random=~person, data=Grumpy, family="multinomial2", pr=TRUE)
summary(mbinom.2)
```   

We can see that the between-person variance is comparable to the residual (across-photo within-person) variance although the credible intervals on both variances is wide. $\texttt{MCMCglmm}$ does not store the posterior distribution of the random effects by default, as there may be a lot of them and they are often not of interest. However, since I specified `pr=TRUE`, the whole of $\boldsymbol{\theta}$ is stored rather than just ${\boldsymbol{\mathbf{\beta}}}$. In Section \@ref(binom-sec) we saw that photo 4521 and photo 4527, despite having the same fixed effect prediction ($\texttt{type}$ = $\texttt{grumpy}$, $\texttt{ypub}$ = 16 years),  had quite different probabilities of success, with the posterior mean probabilities being `r formatC(round(mean(plogis(mbinom.1$Liab[,3])),3), format="f", digits=3)` and `r formatC(round(mean(plogis(mbinom.1$Liab[,25])),3), format="f", digits=3)` respectively. What we didn't know is whether this divergence in probability was due to the person being photographed or some property of the photo. 

## Prediction with Random Effects

If we use the $\texttt{predict}$ method on our model the default is to not only marginalise the residuals, but also to marginalise any other random effects.  If we predict the probability of success for these two photos they are identical, because we are calculating the expectation based on ${\bf X}{\boldsymbol{\beta}}$ only: 

``` {r }
predict(mbinom.2)[c(3,25),]/122
```   

The $\texttt{predict}$ method (and $\texttt{simulate}$ method) for $\texttt{MCMCglmm}$ includes the argument $\texttt{marginal}$ which by default takes the $\texttt{random}$ argument used to fit the model. If we want to obtain a prediction that includes (some of) the random effects we can remove the corresponding term from the formula passed to  $\texttt{marginal}$. Since we only have one random term, which we like to include in the prediction, $\texttt{marginal}$ is empty:

``` {r }
predict(mbinom.2, marginal=NULL, interval="confidence")[c(3,25),]/122
```  

It seems that some of the divergence in probability is due to the person being photographed: our best estimate is that if we had taken many photos of $\texttt{darren_o}$ when grumpy  `r round(100*predict(mbinom.2, marginal=NULL)[3]/122,1)` \% of people would have scored him above five on the grumpy scale, but for $\texttt{craig_w}$ it would be lower (`r round(100*predict(mbinom.2, marginal=NULL)[3]/122,1)` \%). The 95\% credible (confidence) intervals on each are wide however, and a formal comparison (on the logit scale) gives a 95\% credible interval that overlaps zero:

``` {r }
HPDinterval(mbinom.2$Sol[,"person.darren_o"]-mbinom.2$Sol[,"person.craig_w"])
``` 

## Binomial versus Bernoulli

The $\texttt{Grumpy}$ data set aggregates the scores of the 122 respondents into a single binomial response for each photograph. However, we could imagine disaggregating the data such that each respondent for each photograph gets a Bernoulli response with a success if they gave a particular photo a score greater than five. The disaggregated data (`FullGrumpy`) have $122\times 44 = 5,368$ observations (although a few respondents did not assess all photos).  

```{r }
data(FullGrumpy)
head(FullGrumpy, 3)
```

$\texttt{y}$ is now the score each respondent $\texttt{respondent}$ gave each $\texttt{photo}$ (rather than the average score for each $\texttt{photo}$ in `Grumpy`). In addition, we have the respondent-level information $\texttt{student}$ which can be either $\texttt{YES}$ or $\texttt{NO}$. We will turn each persons score into the Bernoulli response

``` {r }
FullGrumpy$g5<-FullGrumpy$y>5
```   

and fit the model

``` {r mbinom3}
prior.mbinom.3=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=0.002), G2=list(V=1, nu=0.002)))

mbinom.3<-MCMCglmm(g5~type+ypub, random=~person+photo, data=FullGrumpy, family="categorical", prior=prior.mbinom.3)
summary(mbinom.3)
```   

Stating that the person variance is x is meaningless without putting it in the context of the assumed residual variance. It is therefore more appropriate to report the intraclass correlation which in this context is the expected correlation between the state Pupated/Not Pupated, for members of the same family. It can be
calculated as:

$$\texttt{IC} =  \frac{\sigma^{2}_{\texttt{FSfamily}}}{\sigma^{2}_{\texttt{FSfamily}}+\sigma^{2}_{\texttt{units}}+\pi^{2}/3}$$

for the logit link, which is used when `family=categorical`, or

$$\texttt{IC} =  \frac{\sigma^{2}_{\texttt{FSfamily}}}{\sigma^{2}_{\texttt{FSfamily}}+\sigma^{2}_{\texttt{units}}+1}$$

for the probit link, which is used if `family=ordinal` was specified.







It is common to hear things like 'year is a random effect' as if you just have to estimate *a* single effect for all years. It is also common to hear things like 'years are random' as if years were sampled at random. Better to say year effects are random and understand that it is the effects that are random not the years, and that we're trying testimate as many effects as there are years. In this sense they're the same as fixed effects, and we can easily treat the year effects as random to see what difference it makes.

Random effect models are often expressed as: \@ref(eq:lm)

$$E[{\bf y}] = \textrm{exp}({\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf Z}{\bf u}+{\bf e})$$

where ${\bf Z}$ is a design matrix like ${\bf X}$, and ${\bf u}$ is a vector of parameters like ${\boldsymbol{\mathbf{\beta}}}$.

In Chapter {#glm} we got introduced to the `Traffic` data set consisting of the the number of injuries on Sweedish roads in 1961 and 1962 when speed-limits were in place or not. We can specify simple random effect models in the same way that we specified the fixed effects:

``` r
random =  ~ year
```

although we don't need anything to the left of the $\sim$ because the response is known from the fixed effect specification. In addition, the global intercept is suppressed by default, so in fact this specification produces the design matrix:

``` {r echo=TRUE}
Z<-model.matrix(~year-1, data=Traffic)
Z[c(1,2,184),]
```

Earlier I said that there was no distinction between fixed and random effects in a Bayesian analysis - all effects are random - so lets not make the distinction and combine the design matrices (${\bf W} = [{\bf X}, {\bf Z}]$) and combine the vectors of parameters ($\boldsymbol{\theta} = [{\boldsymbol{\mathbf{\beta}}}^{'}, {\bf u}^{'}]^{'}$):

$$E[{\bf y}] = \textrm{exp}({\bf W}\boldsymbol{\theta}+{\bf e})
\label{MM-eq}   (\#eq:MM-eq)$$

If we drop year from the fixed terms, the new fixed effect design matrix looks like:

``` {r echo=TRUE}
X2<-model.matrix(y~limit+day, data=Traffic)
X2[c(1,2,184),]
```

and

``` {r echo=TRUE}
W<-cbind(X2,Z)
W[c(1,2,184),]
```

You will notice that this new design matrix is exactly equivalent to the original design matrix `X` except we have one additional variable `year1961`. In our first model this variable was absorbed in to the global intercept because it could no be uniquely estimated from the data. What has changed that could make this additional parameter estimable? As is usual in a Bayesian analysis, if there is no information in the data it has to come from the prior. In model `m2a.5` we used the default normal prior for the fixed effects with means of zero, large variances of $10^{8}$, and no covariances. Lets treat the year effects as random, but rather than estimate a variance component for them we'll fix the variance at $10^{8}$ in the prior:

``` {r echo=TRUE, eval=FALSE}
prior<-list(R=list(V=1, nu=0.002), G=list(G1=list(V=1e+8, fix=1)))
m2a.6<-MCMCglmm(y ~ limit + day, random=~year, family="poisson", data=Traffic, prior=prior, pr=TRUE)
plot(m2a.6$Sol)
```

``` {r echo=FALSE, cache=TRUE}
prior<-list(R=list(V=1, nu=0.002), G=list(G1=list(V=1e+8, fix=1)))
m2a.6<-MCMCglmm(y ~ limit + day, random=~year, family="poisson", data=Traffic, prior=prior, pr=TRUE)
```

``` {r label=yrandom, echo=FALSE, fig.width=7, fig.height=5, include=TRUE, fig.cap="MCMC summary plots for the intercept, speed limit and day coefficients from model `m2a.6` where year effects were treated as random. Note the high posterior variance for the intercept."}
plot(m2a.6$Sol[,1:3])
```

The estimates for the intercept, day and the effect of a speed limit now appear completely different (Figure
\@ref(fig:yrandom). However, in the original model (`m2a.5`) the prediction for each year is obtained by:

``` {r echo=TRUE}
y1961.m2a.5<-m2a.5$Sol[,"(Intercept)"]
y1962.m2a.5<-m2a.5$Sol[,"(Intercept)"]+m2a.5$Sol[,"year1962"]
```

However, for this model we have to add the intercept to both random effects to get the year predictions. $\texttt{MCMCglmm}$ does not store the posterior distribution of the random effects by default, but because we specified `pr=TRUE`, the whole of $\boldsymbol{\theta}$ is stored rather than just ${\boldsymbol{\mathbf{\beta}}}$:

``` {r echo=TRUE}
y1961.m2a.6<-m2a.6$Sol[,"(Intercept)"]+m2a.6$Sol[,"year.1961"]
y1962.m2a.6<-m2a.6$Sol[,"(Intercept)"]+m2a.6$Sol[,"year.1962"]
```

We can merge the two posterior distributions to see how they compare:

``` {r label=ypred, echo=TRUE, fig.width=7, fig.height=5, include=TRUE, fig.cap="MCMC summary plots for the year effects from a model where year effects were treated as fixed (black) and where they were treated as random (red) but with the variance component set at a large value rather than being estimated. The posterior distributions are virtually identical."}
y.m2a.5<-mcmc(cbind("y1961"=y1961.m2a.5,"y1962"=y1962.m2a.5))
y.m2a.6<-mcmc(cbind("y1961"=y1961.m2a.6,"y1962"=y1962.m2a.6))
plot(mcmc.list(y.m2a.5,y.m2a.6))
```

The posterior distributions are very similar (Figure \@ref(fig:ypred) but see Section
[7](#PriorContr-sec){reference-type="ref" reference="PriorContr-sec"} why they are not identical), highlighting the fact that effects that are fixed are those associated with a variance component which has been set *a priori* to something large ($10^8$ in this case), where effects that are random are associated with a variance component which is not set *a priori* but is estimated from the data. As the variance component tends to zero then no matter how many random effects there are, we are effectively only estimating a single parameter (the variance). This makes sense, if there were no differences between years we only need to estimate a global intercept and not separate effects for each year. Alternatively if the variance is infinite then we need to estimate separate effects for each year. In this case the intercept is confounded with the average value of the random effect, resulting in a wide marginal distribution for the intercept, and strong posterior correlations between the intercept and the mean of the random effects:

``` {r label=yfixed-int, echo=TRUE, fig.width=7, fig.height=5, include=TRUE, fig.cap="Joint posterior distribution of the intercept and the mean of the two random year effects. The variance component associated with year was fixed at a large value ($10^8$) and so the effects are almost completely confounded."}
plot(c(m2a.6$Sol[,"year.1961"]+m2a.6$Sol[,"year.1962"])/2, c(m2a.6$Sol[,"(Intercept)"]))
```

With only two levels, there is very little information to estimate the variance, and so we would often make the *a priori* decision to treat year effects as fixed, and fix the variance components to something
large (or infinity in a frequentist analysis).

At the moment we have day as a continuous covariate, but we could also have random day effects and ask whether the number of injuries on the same day but in different years are correlated. Rather than fixing the variance component at something large, we'll use the same weaker prior that we used for the residual variance:

``` {r echo=TRUE, cache=TRUE}
Traffic$day<-as.factor(Traffic$day)
prior<-list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=0.002)))
m2a.7<-MCMCglmm(y ~ year+limit+as.numeric(day), random=~day, family="poisson", data=Traffic, prior=prior)
```

`day` has also gone in the fixed formula, but as a numeric variable, in order to capture any time trends in the number of injuries. Most of the overdispersion seems to be captured by fitting day as a random term
(Figure \@ref(fig:GLMM-VCV)):

``` {r label=GLMM-VCV, echo=TRUE,  fig.width=7, fig.height=5, include=TRUE, fig.cap="MCMC summary plot of the variance component associated with day (top) and the residual variance component (below). The trace for the residual variance shows strong autocorrelation and needs to be ran for longer."}
plot(m2a.7$VCV)
```

In fact it explains so much that the residual variance is close to zero and mixing seems to be a problem. The chain would have to be run for longer, and the perhaps an alternative prior specification used.

## Prediction with Random effects {#pred-sec}



Stating that the family variance is x is meaningless without putting it in the context of the assumed residual variance. It is therefore more appropriate to report the intraclass correlation which in this context is the expected correlation between the state Pupated/Not Pupated, for members of the same family. It can be
calculated as:

$$\texttt{IC} =  \frac{\sigma^{2}_{\texttt{FSfamily}}}{\sigma^{2}_{\texttt{FSfamily}}+\sigma^{2}_{\texttt{units}}+\pi^{2}/3}$$

for the logit link, which is used when `family=categorical`, or

$$\texttt{IC} =  \frac{\sigma^{2}_{\texttt{FSfamily}}}{\sigma^{2}_{\texttt{FSfamily}}+\sigma^{2}_{\texttt{units}}+1}$$

for the probit link, which is used if `family=ordinal` was specified.



## A note on fixed effect priors and covariances {#PriorContr-sec}

Fixed and random effects are essentially the same thing. The only difference is that the variance component for the fixed effects is usually fixed at some large value, whereas the variance component for the random effects is estimated. In Section \@ref(ranef-sec) I demonstrated this by claiming that a model where year effects were fixed (`m2a.5`) was identical to one where they were treated as random, but with the variance component set to a large value (`m2a.6`). This was a white lie as I did not want to distract attention from the main point. The reason why they were not identical is as follows:

In the fixed effect model (`m2a.5`) we had the prior:

$$\begin{array}{rcl}
\left[
\begin{array}{c}
 \beta_{\texttt{(Intercept)}}\\
 \beta_{\texttt{year1962}}\\
\end{array}
\right]
\sim
&
\left[
\begin{array}{cc}
10^8&0\\
0&10^8\\
\end{array}
\right]\\
\end{array}$$

Where $\beta_{\texttt{(Intercept)}}$ and $\beta_{\texttt{year1962}}$ are the fixed effects to be estimated.

Remembering the identity $\sigma^{2}_{(a+b)} = \sigma^{2}_{a}+ \sigma^{2}_{b}+2\sigma_{a,b}$, this implies:

$$\begin{array}{rccl}
\left[
\begin{array}{c}
 \beta_{1961}\\
 \beta_{1962}\\
\end{array}
\right]
=
&
\left[
\begin{array}{c}
 \beta_{\texttt{(Intercept)}}\\
 \beta_{\texttt{(Intercept)}}+\beta_{\texttt{year1962}}\\
\end{array}
\right]
\sim
&
\left[
\begin{array}{cc}
10^8&10^8\\
10^8&10^8+10^8\\
\end{array}
\right]
&=
\left[
\begin{array}{cc}
10^8&10^8\\
10^8&20^8\\
\end{array}
\right]\\
\end{array}$$

where $\beta_{1961}$ and $\beta_{1962}$ are the actual year effects, rather than the global intercept and the contrast. In hindsight this is a bit odd, for one thing we expect the 1962 effect to be twice as variable as the 1961 effect. With such weak priors it makes little difference, but lets reparameterise the model anyway.

Rather than having a global intercept and a year contrast, we will have separate intercepts for each year:

``` {r echo=TRUE}
X3<-model.matrix(y ~ year-1, data=Traffic)
X3[c(1,2,184),]
```

and a prior that has a covariance between the two year effects:

``` {r echo=TRUE}
PBV.yfixed<-diag(2)*1e+8
PBV.yfixed[1,2]<-PBV.yfixed[2,1]<-1e+8/2
PBV.yfixed
prior.m2a.5.1<-list(B=list(mu=rep(0,2), V=PBV.yfixed), R=list(V=1, nu=0.002))
```

This new model:

``` {r echo=TRUE, cache=TRUE}
m2a.5.1<-MCMCglmm(y ~ year-1, family="poisson", data=Traffic, prior=prior.m2a.5.1)
```

has the same form as a mixed effect model with a prior variance of $\frac{10^{8}}{2}$ for the intercept, and the variance component associated with the random year effects also fixed at $\frac{10^{8}}{2}$:

``` {r echo=TRUE}
prior.m2a.6.1<-list(B=list(mu=0, V=1e+8/2), R=list(V=1, nu=0.002), G=list(G1=list(V=1e+8/2, fix=1)))
```

This arises because the two random effects have the joint prior distribution:

$$\begin{array}{rl}
\left[
\begin{array}{c}
 \beta_{\texttt{year.1961}}\\
 \beta_{\texttt{year.1962}}\\
\end{array}
\right]
\sim
&
\left[
\begin{array}{cc}
\frac{10^{8}}{2}&0\\
0&\frac{10^{8}}{2}\\
\end{array}
\right]\\
\end{array}$$

which when combined with the prior for the intercept, $N(0, \frac{10^{8}}{2})$, gives:

$$\begin{array}{rccl}
\left[
\begin{array}{c}
 \beta_{1961}\\
 \beta_{1962}\\
\end{array}
\right]
=
&
\left[
\begin{array}{c}
 \beta_{\texttt{(Intercept)}}+\beta_{\texttt{year.1961}}\\
 \beta_{\texttt{(Intercept)}}+\beta_{\texttt{year.1962}}\\
\end{array}
\right]
\sim
&
\left[
\begin{array}{cc}
\frac{10^{8}}{2}+\frac{10^{8}}{2}&\frac{10^{8}}{2}\\
\frac{10^{8}}{2}&\frac{10^{8}}{2}+\frac{10^{8}}{2}\\
\end{array}
\right]
&=
\left[
\begin{array}{cc}
10^8&\frac{10^{8}}{2}\\
\frac{10^{8}}{2}&10^8\\
\end{array}
\right]
\\
\end{array}$$

which is equivalent to the `PBV.yfixed` parameteristaion of for the two years.

The model:

``` {r echo=TRUE, cache=TRUE}
m2a.6.1<-MCMCglmm(y ~ 1, random=~year, family="poisson", data=Traffic, prior=prior.m2a.6.1, pr=TRUE)
```

is therefore sampling from the same posterior distribution as model `m2a.5.1`.

[^ranef.1]: If we assumed the distribution was Laplace (back to back exponentials) we have the LASSO. If we assumed the distribution was a mixture of normal and Laplace we have the elastic net. 
