# Random effects {#ranef}

In some cases we may have measured variables whose effects we would like to treat as random. Often the distinction between fixed and random is given by example: things like city, species, individual and vial are random, but sex, treatment and age are not. Or the distinction is made using rules of thumb: if there are few factor levels and they are interesting to other people they are fixed. However, this doesn't really confer any understanding about what it means to treat something as fixed or random, and doesn't really allow judgements to be made for variables in which the rules of thumb seem to contradict each other. Similarly, these 'explanations' don't give any insight into the fact that all effects are technically random in a Bayesian analysis.

Random effect models are often expressed as an extension of Equation \@ref(eq:lm):

$$E[{\bf y}] = {\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf Z}{\bf u}
 \label{MM}   (\#eq:MM)$$

where ${\bf Z}$ is a design matrix like ${\bf X}$, and ${\bf u}$ is a vector of parameters like ${\boldsymbol{\mathbf{\beta}}}$. However, at this stage there is simply no distinction between fixed and random effects. We could combine the design matrices (${\bf W} = [{\bf X}, {\bf Z}]$) and combine the vectors of parameters ($\boldsymbol{\theta} = [{\boldsymbol{\mathbf{\beta}}}^{'}, {\bf u}^{'}]^{'}$) to get:

$$E[{\bf y}] = {\bf W}\boldsymbol{\theta}
 \label{MM2}   (\#eq:MM2)$$

which is **identical** to Equation \@ref(eq:MM).  So if we don't need to distinguish between fixed and random effects at this stage, when should we distinguish between them, and what distinguishes them?  

When we treat an effect as random we believe that the coefficients have some distribution around a mean of zero; often we assume they are normal[^ranef.1] and that they are independent (represented by an identity matrix) and identically distributed with variance $\sigma^{2}_{u}$:

$${\bf u} \sim N({\bf 0}, {\bf I}\sigma^{2}_{u})$$

$\sigma^{2}_{u}$ is a parameter of the model which we estimate, in addition to ${\bf u}$. In a Bayesian analysis we would also assign $\sigma^{2}_{u}$ a prior, and $\sigma^{2}_{u}$ is often called a hyper-parameter with an associated hyper-prior. 

Fixed effects in a frequentist analysis are not assigned a distribution, but we can understand this in terms of the limit to the normal distribution

$$\boldsymbol{\beta} \sim N({\bf 0}, {\bf I}\sigma^{2}_{\beta})$$

as $\sigma^{2}_{\beta}$ tends to infinity. In a Bayesian setting we would call this a flat improper prior. In practice, we often use diffuse proper priors in Bayesian analyses. For example, the default in $\texttt{MCMCglmm}$ is to set $\sigma^{2}_{\beta}=10^8$. Then, $\boldsymbol{\beta}$ are technically random - they are assigned a distribution - but I find it useful to retain the frequentist terminology 'fixed'. The only difference then is that the 'fixed' effects are assigned a prior distribution with a variance that is defined by the user-specified prior ($\sigma^{2}_{\beta}$ - which is often set to be large) and the 'random' effects are assigned a prior distribution with a variance that is estimated ($\sigma^{2}_{u}$ - which could be large, but also zero). 

That is the distinction between fixed and random effects. The difference really is that simple, but it takes a long time and a lot of practice to understand what this means in practical terms, and why working with random effects can be a very powerful way of modelling data.  To get a feel for why we might want to fit an effect as random or not, lets work through an example before moving on to model fitting. In Section \@ref(binom-sec) we analysed binomial data where 122 respondents had looked at 44 photographs of people and given them a 'grumpy score' of more than five (a success) or less than five (a failure). If, instead of 122 respondents, there had been a zillion respondents, we could use the average proportion of success for each photo as a nearly perfect estimates of their probabilities of success. The variance of these near-perfect estimates could serve as a reasonable estimate of the variance in photo effects. If the probabilities were all clustered tightly around 0.5: 0.505, 0.501, 0.499 and so on, then variance would be estimated to be small. Let's then imagine that we obtained a $45^\textrm{th}$ photograph but by this point the respondents were so bored I managed to only recruit a single person who gave the photo a score greater than five - a success. Since we only have one observation for this photo the average proportion of success would be one. Do you think the best estimate of the probability of success for the $45^\textrm{th}$ photograph is then 1.000? I think you wouldn't: you would use the knowledge that you have gained from the other photos and say that it is more likely that if you had managed to recruit more respondents you would have got a roughly even split of success and failures. You have used common sense, treated the photo effects as random, and *shrunk* photo 45's effect towards the average because the variance ($\sigma^2_u$) was small and we have a strong prior. If we had treated the photo effects as fixed, we believe that the only information regarding a photo's value comes from data associated with that particular photo, and the estimate of photo 45's probability would have been one. When we treat an effect as random, we also use the information that comes from data associated with that particular photo (obviously), but we weight that information by what the data associated with other photos tell us about the likely values that the effect could take - through the parameter $\sigma^2_u$. What if the probabilities weren't all clustered tightly around 0.5, but took on values 0.500, 0.998, 0.002, 0.327 ...?  The variance $\sigma^2_u$ would be larger and the prior information for our $45^\textrm{th}$ photo would be weaker: perhaps we got a success because the underlying probability was 0.998, but a single success would also not be very surprising if the underlying probability was 0.500, or even 0.327. We might then be happy that our best estimate of the probability of success for the $45^\textrm{th}$ photograph was close to one, although with such weak prior information (large $\sigma^2_u$) the uncertainty would remain large.   

When the motivation for treating an effect as random is explained this way, it is hard to come up with a reason why you wouldn't treat all effects as random. However, you have to consider how much information is in a given data set to estimate $\sigma^2_u$, which we will cover in Section \@ref(fix-or-rand).

## Generalised Linear Mixed Model (GLMM) {#GLMM}

In Section \@ref(binom-sec), the binomial model we fitted only contained fixed effects, as specified in the `fixed` argument to `MCMCglmm` (`fixed=cbind(g5,l5)~type+ypub`). No random effects were fitted, although 'residuals' were fitted as default to absorb any overdispersion. Residuals are random effects for which we estimate a variance - the hyperparameter, $\sigma^2_e$ - and when used with Binomial or Poisson responses are commonly referred to as observation-level random effects. Since there is a one-to-one correspondence between observation and photo in this data set, $\sigma^2_e$ is equivalent to the $\sigma^2_u$ discussed above (although $\sigma^2_e$ refers to the variance on the logit scale rather the probability scale used implicitly above). We saw that the probability of success varied greatly across photos (model `mbinom.1`) but we also noted that some of this variation may be due to the person being photographed and we could tease apart the effect of person from the specifics of the photo since each person was photographed twice - once when happy and once when grumpy. $\texttt{person}$ has 22 levels and you are probably not interested in knowing the grumpy score of someone you didn't know - $\texttt{person}$ effects seem to satisfy the rule of thumb often used to decide that they should be treated as random. The random effect model is specified through the argument `random` and for simple effects as these we simply put the name of the corresponding column ($\texttt{person}$) in the model formula. We will also specify inverse-Wishart priors for both the residual variance and the variance of the $\texttt{person}$ effects (see Section \@ref(Vprior-sec)) although scaled non-central $F$-distribution priors are recommended for random-effect variances (see Section \@ref(PXprior-sec)):

```{r mbinom2}
prior.mbinom.2=list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=0.002)))

mbinom.2<-MCMCglmm(cbind(g5, l5)~type+ypub, random=~person, data=Grumpy, family="multinomial2", pr=TRUE, prior=prior.mbinom.2)
summary(mbinom.2)
```   

We can see that the between-person variance is comparable to the residual (across-photo within-person) variance although the credible intervals on both variances is wide. $\texttt{MCMCglmm}$ does not store the posterior distribution of the random effects by default, as there may be a lot of them and they are often not of interest. However, since I specified `pr=TRUE`, the whole of $\boldsymbol{\theta}$ is stored rather than just ${\boldsymbol{\mathbf{\beta}}}$. In Section \@ref(binom-sec) we saw that photo 4521 and photo 4527, despite having the same fixed effect prediction ($\texttt{type}$ = $\texttt{grumpy}$, $\texttt{ypub}$ = 16 years),  had quite different probabilities of success, with the posterior mean probabilities being `r formatC(round(mean(plogis(mbinom.1$Liab[,3])),3), format="f", digits=3)` and `r formatC(round(mean(plogis(mbinom.1$Liab[,25])),3), format="f", digits=3)` respectively. What we didn't know is whether this divergence in probability was due to the person being photographed or some property of the photo. 

## Prediction with Random Effects {#ranpred-sec}

If we use the $\texttt{predict}$ method on our model the default is to not only marginalise the residuals, but also to marginalise any other random effects.  If we predict the probability of success for these two photos they are identical, because we are calculating the expectation based on ${\bf X}{\boldsymbol{\beta}}$ only: 

```{r predict-mbinom2}
predict(mbinom.2)[c(3,25),]/122
```   

The $\texttt{predict}$ method (and $\texttt{simulate}$ method) for $\texttt{MCMCglmm}$ includes the argument $\texttt{marginal}$ which by default takes the $\texttt{random}$ argument used to fit the model. If we want to obtain a prediction that includes (some of) the random effects we can remove the corresponding term from the formula passed to  $\texttt{marginal}$. Since we only have one random term, which we like to include in the prediction, $\texttt{marginal}$ is empty:

```{r echo=FALSE}
p.mbinom.2<-predict(mbinom.2, marginal=NULL)
```


```{r predict-mbinom2-b}
predict(mbinom.2, marginal=NULL, interval="confidence")[c(3,25),]/122
```  

It seems that some of the divergence in probability is due to the person being photographed: our best estimate is that if we had taken many photos of $\texttt{darren_o}$ when grumpy  `r round(100*p.mbinom.2[3]/122,1)` \% of people would have scored him above five on the grumpy scale, but for $\texttt{craig_w}$ it would be lower (`r round(100*p.mbinom.2[25]/122,1)` \%). The 95\% credible (confidence) intervals on each are wide however, and a formal comparison (on the logit scale) gives a 95\% credible interval that overlaps zero:

```{r }
HPDinterval(mbinom.2$Sol[,"person.darren_o"]-mbinom.2$Sol[,"person.craig_w"])
``` 

## Overdispersed Binomial as a Bernoulli GLMM

The $\texttt{Grumpy}$ data set aggregates the scores of the 122 respondents into a single binomial response for each photograph. However, we could imagine disaggregating the data such that each respondent for each photograph gets a Bernoulli response with a success if they gave a particular photo a score greater than five. The disaggregated data (`FullGrumpy`) have $122\times 44 = 5,368$ observations (although a few respondents did not assess all photos).  

```{r }
data(FullGrumpy)
head(FullGrumpy, 3)
```

$\texttt{y}$ is now the score each respondent $\texttt{respondent}$ gave each $\texttt{photo}$ (rather than the average score for each $\texttt{photo}$ in `Grumpy`). In addition, we have the respondent-level information $\texttt{student}$ which can be either $\texttt{YES}$ or $\texttt{NO}$. We will turn each persons score into the Bernoulli response

```{r }
FullGrumpy$g5<-FullGrumpy$y>5
```   

and fit the model

```{r mbinom3}
prior.mbinom.3=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=0.002), G2=list(V=1, nu=0.002)))

mbinom.3<-MCMCglmm(g5~type+ypub, random=~person+photo, data=FullGrumpy, family="categorical", prior=prior.mbinom.3)

summary(mbinom.3)
```   

The \texttt{photo} random effects deal with any variation in the probability of success across photos and are exactly comparable to the residuals of the binomial model `mbinom.2`. It is therefore surprising that the posterior distributions for both the variance in $\texttt{person}$ effects and the variance in $\texttt{photo}$/residual effects appear to be different between the two models, as do the fixed effects. This is a peculiarity of logit-link models in $\texttt{MCMC}$ and wouldn't be seen in $\texttt{family="threshold"}$ models that implements the standard probit link. In the binomial model `mbinom.2` we implicitly assumed that the probability of success did not vary across respondents *within* photos - this was an assumption, and one that cannot be tested.  In the Bernoulli model `mbinom.3` we explicitly assumed that the probability of success varied across respondents within photos, and the variance on the logit-scale was one. Since Bernoulli data provide no information about observation-level variability either (or most likely, neither) assumption could be true but we have no way of knowing (Section \@ref(bernoulli-sec)). As we saw with fixed effect coefficients in Bernoulli GLM, stating that the variances in photo effects is `r round(mean(mbinom.3$VCV[,"photo"]), 3)` is meaningless in a Bernoulli GLMM without putting it in the context of the assumed residual variance. The standard approach - what I refer to as the standard logit model - is to assume the residual variance is zero. While I think this is a good standard, this is prohibited in $\texttt{MCMCglmm}$ because the chain will not mix. But as we saw with the fixed effects, we can rescale the variances by needs to be multiplied by $1/(1+c^{2}\sigma^{2}_{\texttt{units}})$ where $c=16\sqrt{3}/15\pi$ and $\sigma^{2}_{\texttt{units}}$ is our assumed residual variance, which is one (Figure \@ref(fig:bernoulli-rescale2)).

```{r bernoulli-rescale2, echo=TRUE, include=TRUE, fig.cap="MCMC traces for the estimated variances in $\\texttt{person}$ and $\\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`) of individual data (red) and a Binomial GLMM (model `mbinom.2`) where all data for a photo have been aggregated into a single Binomial response (black). The posterior distribution of the variances from the Bernoulli GLMM have been rescaled to what would be observed if the residual variance was zero (rather than one).", fig.width=7, fig.height=8}
c2 <- ((16*sqrt(3))/(15*pi))^2

rescale.VCV.2 <- mbinom.2$VCV
colnames(rescale.VCV.2)[2]<-"photo"

rescale.VCV.3 <- mbinom.3$VCV[,c("person", "photo")]/(1+c2)

plot(mcmc.list(as.mcmc(rescale.VCV.2), as.mcmc(rescale.VCV.3)), density=FALSE)
```

## Intra-class Correlations {#ICC}

A more common approach, however, is to express the variances as intra-class correlations where we take the variance of interest and express it as a proportion of the total. For example, for the $\texttt{person}$ effects, the intra-class correlation would be

$$ICC = \frac{\sigma^2_{\texttt{person}}}{\sigma^2_{\texttt{person}}+\sigma^2_{\texttt{photo}}+\sigma^2_{\texttt{units}}+\pi^2/3}$$

where the $\pi^2/3$ appears because we have used the logit link and this is the link variance (the variance of the unit logistic). 

```{r ICC, echo=TRUE, include=TRUE, fig.cap="MCMC traces for the estimated intra-class correlation for $\\texttt{person}$ and $\\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`) of individual data (red) and a Binomial GLMM (model `mbinom.2`) where all data for a photo have been aggregated into a single Binomial response (black).", fig.width=7, fig.height=8}

ICC.2 <- mbinom.2$VCV/(rowSums(mbinom.2$VCV)+pi^2/3)
colnames(ICC.2)[2]<-"photo"

ICC.3 <- mbinom.3$VCV[,c("person", "photo")]/(rowSums(mbinom.3$VCV)+pi^2/3)

plot(mcmc.list(as.mcmc(ICC.2), as.mcmc(ICC.3)), density=FALSE)
```

If we had used $\texttt{family="threshold"}$ this would be omitted because the link variance is zero as we are already working on the ... scale[^ranef.2]  Pierre de Villemereuil's [$\texttt{QGLMM}$](https://cran.r-project.org/web/packages/QGglmm/index.html) package. 


The eagle-eyed will have noticed that although the trace plots for the rescaled variance/intra-class correlation for the $\texttt{person}$ effects look identical between the two models, the trace plots for the $\texttt{photo}$ effects look slightly different with the posterior from model `mbinom.3` (the Bernoulli GLMM in red) appearing to have more density at higher values. However, this difference is due to Monte Carlo error, which is quite high for the variance estimates because the autocorrelation in the chain is moderate. The reported effective sample size in the model summary gives some indication of this - for example in the Bernoulli model the effective sample size for the variance in $\texttt{photo}$ effects is `r round(effectiveSize(mbinom.3$VCV[,"photo"]))`, quite a bit less than the 1,000 samples saved (Section \@ref(diagnostics-sec)). We can see this more clearly if we just plot the traces for Bernoulli model (Figure \@ref(fig:bernoulli-trace)).    


```{r bernoulli-trace, echo=FALSE, include=TRUE, fig.cap="MCMC trace for the variances in $\\texttt{person}$ and $\\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`).", fig.width=7, fig.height=5}

plot(mbinom.3$VCV[,c("person", "photo")], density=FALSE)
```

Two things are apparent from Figure \@ref(fig:bernoulli-trace). Autocorrelation is present - this is not surprising: for each iteration of the MCMC chain the random effects are Gibbs sampled conditional on their variance in the previous iteration, and then conditional on the updated random effects the variances are then Gibbs sampled (Section \@ref(MCMC)). This will invariably lead to autocorrelation. Second, the trace for the variance in $\texttt{person}$ effects appears to intermittently get 'stuck' at values close to zero. In part, this reflects the mechanics of the Gibbs sampling, but it is also a consequence of the inverse-Wishart prior used which has a sharp peak in density at small values (Section \@ref(Vprior-sec)).  When the variance in $\texttt{person}$ effects gets 'stuck' at zero, the variance in $\texttt{photo}$ effects appears to get 'stuck' at high values. This is because the data provide strong support for the combined effect of $\texttt{photo}$ and $\texttt{person}$ being large, but contain less information about their separate effects. Consequently, if the $\texttt{person}$ variance drops to zero, the $\texttt{photo}$ variance increases to compensate. In this example, the effects described above are quite subtle, and simply running the chain for longer would probably suffice. However, a better general strategy would be to employ parameter expansion and use scaled non-central $F$ priors.

## Priors for Random Effect Variances {#PXprior-sec}

Parameter expansion is an algorithmic trick for speeding up the mixing and convergence of the MCMC chain. An unintended but useful side-effect of parameter expansion is that it can allow a wider class of prior distributions while still permitting Gibbs sampling (Chapter \@ref(bayesian)). In order to explore parameter expansion, and the associated $F$ prior for random-effect variances, we will work with a model and data-set where the issues noted for model `mbinom.3` are much more obvious - the Schools example discussed in @Gelman.2006. 

``` {r }
schools<-data.frame(school= letters[1:8], 
                    estimate= c(28.39, 7.94, -2.75 , 6.82, -0.64, 0.63, 18.01, 12.16),
                    ve = c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6)^2)
head(schools)
```

The response variable `estimate` is the relative effect of Scholastic Aptitude Test coaching programs in 8 schools and @Gelman.2006 focusses on the variance in school effects. Since we only have a single estimate per school there will be a one-to-one mapping between $\texttt{school}$ effects and the residual. In most cases this would result in the variance in school effects being confounded with the residual variance. Here, however, we have been gifted the residual (within school) variance ($\texttt{ve}$) which varies from school to school. In reality, these residual variances are actually estimates and we might wish to factor in this additional complication, but for now we will ignore this complexity and come back to it in Chapter \@ref(measurement). First, lets fit the inverse-Wishart prior we have been using up to now with $\texttt{V}=1$ and $\texttt{nu}=0.002$. This prior is equivalent to an inverse-gamma prior with a shape and scale of 0.001 (Section \@ref(Vprior-sec)): 

``` {r }
prior.mschool.1<-list(R=list(V=diag(schools$ve), fix=1), 
					  G=list(G1=list(V=1, nu=0.002)))

mschool.1<-MCMCglmm(estimate~1, random=~school, rcov=~idh(school):units, data=schools, prior=prior.mschool.1)
```

The model contains an argument we haven't seen before: $\texttt{rcov}$. In all previous analyses we used the default `~units` which fits a set of independent and identically distributed residuals with a single variance ($\sigma^2_{\texttt{units}}$) to be estimated. $\texttt{MCMCglmm}$ allows this assumption to be relaxed, and Chapter \@ref(cat-int) is dedicated to this subject. Here, we will simply note that we have assigned each school a residual variance (the corresponding element of $\texttt{ve}$) and fixed it at this value, leaving only the intercept and the variance in school effects ($\sigma^2_{\texttt{school}}$) to be estimated.  The MCMC trace for $\sigma^2_{\texttt{school}}$ looks dreadful (Figure \@ref(fig:mschool-1)).

```{r mschool-1, echo=FALSE, include=TRUE, fig.cap="MCMC trace for the variance in $\\texttt{school}$ effects from model `mschool.1` in which an inverse-Wishart prior was used with $\\texttt{V=1}$ and $\\texttt{nu=0.002}.$", fig.width=7, fig.height=5}
plot(mschool.1$VCV[,"school", drop=FALSE], density=FALSE)
```

The autocorrelation in the chain is evident and there appears to be a lot of posterior density at zero. These are separate issues. Autocorrelation in the chain is due to algorithmic inefficiencies in sampling the posterior distribution, whereas a lot of posterior density near zero reflects the combined information coming from the data and coming from the prior (Chapter \@ref(bayesian)). Certainly, some posterior distributions are harder to sample from than others, and the efficiency of the MCMC algorithm may decrease when the posterior is close to zero. But if the chain can be run long enough that these inefficiencies are not consequential, situations where the posterior has 'too much' density near zero indicate potential problems with the prior, not algorithmic problems.

### $F$ and folded-$t$ priors

For the inverse-Wishart prior we specified the parameters $\texttt{V}$ and $\texttt{nu}$. The parameters $\texttt{alpha.mu}$ and $\texttt{alpha.V}$ can also be specified in the prior, and if $\texttt{alpha.V}$ is non-zero then parameter expansion is used. These additional parameters specify a prior for the variance, $\sigma^2_{\texttt{school}}$, that is a non-central scaled F-distribution with the numerator degrees-of-freedom set to one. We can also think about this prior in terms of the standard deviation, $\sigma_{\texttt{school}}$, which in some ways is more natural, since it is in the same units as the response.  The prior distribution for the standard deviation is a folded scaled non-central $t$. As the length of their names suggest, these two distributions are quite complicated. Regrettably, this complication is exacerbated in $\texttt{MCMCglmm}$ by specifying the prior through the distribution of the parameter expansion working parameters (See Section \@ref(px-sec)). I will leave the full relationship between the prior specification and the $F$ and $t$ distributions to the footnote[^ranef.3], and introduce two simplifications here. First, these distributions have three free parameters, yet the prior specification has four. Without loss of generality we will use $\texttt{V}=1$ throughout. Then,  $\texttt{alpha.V}$ specifies the scale of the $F$ prior for the variance and $\sqrt{\texttt{alpha.V}}$ specifies the scale of the $t$ prior for the standard deviation. Second, since the non-central forms of these distributions are rarely - if ever - used as priors, we will set the non-centrality parameter to zero via $\texttt{alpha.mu}=0$. For the standard deviation prior this results in the added simplification that the folded-$t$ becomes the half-$t$ (essentially a $t$ with the negative values missing). We are then left with two-parameter distributions with the scale set by $\texttt{alpha.V}$ and the (denominator) degrees-of-freedom, $\texttt{nu}$. 


Before discussing the properties of the $F$ and half-$t$ priors on the posterior distributions of variances and standard deviations, respectively, let's confirm that parameter expansion does indeed increase efficiency independent of the prior being used. In Section \@ref(Vprior-sec) we saw that an improper inverse-Wishart distribution with $\texttt{V}=0$ and $\texttt{nu}=-1$ is flat for the standard deviation. A half-$t$ with $\texttt{nu}=-1$ is also an improper flat prior on the standard deviation irrespective of what is specified for the other parameters. Let's fit these flat improper priors with and without parameter expansion:


``` {r }
prior.nopx<-list(R=list(V=diag(schools$ve), fix=1),    
					  G=list(G1=list(V=1e-16, nu=-1)))

m.nopx<-MCMCglmm(estimate~1, random=~school, rcov=~idh(school):units, data=schools, prior=prior.nopx)
# flat prior on the school standard deviation - no parameter expansion

prior.px<-list(R=list(V=diag(schools$ve), fix=1),   
					  G=list(G1=list(V=1e-16, nu=-1, alpha.mu=0, alpha.V=1000^2)))
# note I have set V=1e-16 rather than V=1: with nu=-1, V does not influence the prior 

m.px<-MCMCglmm(estimate~1, random=~school, rcov=~idh(school):units, data=schools, prior=prior.px)
# flat prior on the school standard deviation - with parameter expansion 
```

We can see that these two models are sampling from the same posterior (Figure \@ref(fig:mschool-2)) but the efficiency of the algorithm is greater when parameter expansion is used, with an effective posterior sample size of `r round(effectiveSize(m.px$VCV[,"school"]))` rather than `r round(effectiveSize(m.nopx$VCV[,"school"]))`. 


```{r mschool-2, echo=FALSE, include=TRUE, fig.cap="MCMC traces for the standard deviation in $\\texttt{school}$ effects. In black is the trace for model `m.nopx` in which an improper inverse-Wishart prior was used for the variance with $\\texttt{V=0}$ and $\\texttt{nu=-1}$. In red is the trace for model `m.px` in which an improper $F_{1,-1}$ prior was used with $\\texttt{nu=-1}$ and $$\\texttt{alpha.V}=1000^2$. Both priors are flat for the standard deviation but `m.px` employs parameter expansion.", fig.width=7, fig.height=5}
plot(mcmc.list(sqrt(m.nopx$VCV[,"school", drop=FALSE]), sqrt(m.px$VCV[,"school", drop=FALSE])), density=FALSE, main=expression(sigma[school]), cex.main=1.5)
```

While parameter expansion usually results in more efficient sampling of the posterior, to justify its use, it is important that it also allows us to specify priors with sensible properties. Those with Bayesian scruple may baulk at the use of an improper prior. However, if we specify $\texttt{nu=1}$ (a half-$t_1$ or half-Cauchy) we can specify a proper prior that becomes flat for the standard deviation as we increase the scale. Let's fit a proper half-Cauchy with a scale that is large (relative to the data) - $\sqrt{\texttt{alpha.V}}=1000$.


``` {r }
prior.mschool.2<-list(R=list(V=diag(schools$ve), fix=1),   
					  G=list(G1=list(V=1, nu=1, alpha.mu=0, alpha.V=1000^2)))
# half-Cauchy prior on the standard-deviation with scale 1000

mschool.2<-MCMCglmm(estimate~1, random=~school, rcov=~idh(school):units, data=schools, prior=prior.px)
# flatish prior on the school standard deviation 
```

Overlaying the trace on those obtained under flat improper priors shows that the posterior distributions for $\sigma_\texttt{school}$ are indistinguishable (Figure \@ref(fig:mschool-3)).

```{r mschool-3, echo=FALSE, include=TRUE, fig.cap="MCMC traces for the standard deviation in $\\texttt{school}$ effects. In black is the trace for model `m.nopx` in which an improper inverse-Wishart prior was used for the variance with $\\texttt{V=0}$ and $\\texttt{nu=-1}$. In red is the trace for model `m.px` in which an improper $t_{-1}$ prior was used for the standard deviation with $\\texttt{alpha.V}$ set to $1000^2$. Both priors are flat for the standard deviation but `m.px` employs parameter expansion. In green is the trace for model `mschool.2` in which a proper half-Cauchy ($t_{1}$) with a scale of $1000$ was used. This prior is almost flat for the standard deviation over the range of values that have reasonable support.", fig.width=7, fig.height=5}
plot(mcmc.list(sqrt(m.nopx$VCV[,"school", drop=FALSE]), sqrt(m.px$VCV[,"school", drop=FALSE]), sqrt(mschool.2$VCV[,"school", drop=FALSE])), density=FALSE, main=expression(sigma[school]), cex.main=1.5)
```

Is a flat(ish) prior on the standard deviation more sensible than the inverse-Wishart prior on the variance we have been using previously ($\texttt{V}=1$ and $\texttt{nu}$=0.002)? The short answer is yes. Setting $\texttt{nu}$ to be small in the inverse-Wishart is motivated by the idea that having few prior degrees of freedom provides a diffuse prior on the variance. However, as $\texttt{nu}$ becomes smaller the prior only becomes flat for the *log* of the variance, and there is a nasty spike at small values for both the variance (Section \@ref(Vprior-sec)) and the standard deviation. However, in some cases, as in the Schools example, the amount of information provided by the data may be so small that a flat prior on the standard deviation might be to vague. In such cases, reducing the scale has been advised, and @Gelman.2006 used a scale of 25 for this example:

``` {r }
prior.mschool.3<-list(R=list(V=diag(schools$ve), fix=1), 
					  G=list(G1=list(V=1, nu=1, alpha.mu=0, alpha.V=25^2)))

mschool.3<-MCMCglmm(estimate~1, random=~school, rcov=~idh(school):units, data=schools, prior=prior.mschool.3)
```

When reducing the scale, some care has to be taken that the scale is aligned with the scale of the data. For example, the common 'default' recommendation of setting the scale to 5 might prove problematic in this example where the standard deviation of the data exceeds one by some margin:

``` {r }
prior.mschool.4<-list(R=list(V=diag(schools$ve), fix=1), 
					  G=list(G1=list(V=1, nu=1, alpha.mu=0, alpha.V=5^2)))

mschool.4<-MCMCglmm(estimate~1, random=~school, rcov=~idh(school):units, data=schools, prior=prior.mschool.4)
```

The default in brms [@Burkner.2017] is to use a half-$t$ with 3 degrees of freedom and a scale of 2.5. The tails of this distribution are less 'heavy' than the Cauchy and in conjunction with the reduced scale penalise large values more strongly.

``` {r }
prior.mschool.5<-list(R=list(V=diag(schools$ve), fix=1), 
					  G=list(G1=list(V=1, nu=3, alpha.mu=0, alpha.V=2.5^2)))

mschool.5<-MCMCglmm(estimate~1, random=~school, rcov=~idh(school):units, data=schools, prior=prior.mschool.5)
```

When comparing these priors visually we see that they are quite different, despite all having some history as default and/or recommended priors (Figure \@ref(fig:prior-compare)).   

``` {r prior-compare, echo=FALSE, include=TRUE, fig.cap="Prior probability densities for the standard deviation of $\\texttt{school}$ effects used in the $\\texttt{mschool}$ models $\\texttt{1-5}$.  The scale of the half-Cauchy and half-t distributions are given after the colon. Note the inverse-gamma is the prior for the variance - not the standard deviation.", fig.width=7, fig.height=5}

sd.x<-seq(1e-18, max(sqrt(mschool.1$VCV[,"school"])), length=1000)

dsd.x<-as.list(1:5)

dsd.x[[1]]<-dprior(sd.x, prior.mschool.1$G$G1, sd=TRUE)
dsd.x[[2]]<-dprior(sd.x, prior.mschool.2$G$G1, sd=TRUE)
dsd.x[[3]]<-dprior(sd.x, prior.mschool.3$G$G1, sd=TRUE)
dsd.x[[4]]<-dprior(sd.x, prior.mschool.4$G$G1, sd=TRUE)
dsd.x[[5]]<-dprior(sd.x, prior.mschool.5$G$G1, sd=TRUE)


cols    <- palette()[1:5] 

plot(dsd.x[[1]]~sd.x,                     
     main = "",
     xlab = expression(sigma[school]), ylab = "Prior density",
     col  = cols[1], lwd = 2, type="l", ylim=c(0, max(unlist(dsd.x))))

for (i in 2:length(dsd.x))                  
  lines(dsd.x[[i]]~sd.x, col = cols[i], lwd = 2)

model.names<-c("inverse-gamma(0.001, 0.001)", "half-Cauchy: 1000", "half-Cauchy: 25", "half-Cauchy: 5", expression("half-"~t[3]~": 2.5"))

legend("topright", lwd = 2, col = cols,
       legend = model.names, bty = "n")
```

With so little information coming from the data, these priors also have considerable influence on the posterior (Figure \@ref(fig:posterior-compare)). When $\texttt{V}$=1 and $\texttt{nu}$=0.002, small values are strongly favoured, and this is also true to a lesser extent for the half-$t_3$ prior with low scale. As we up the scale and/or reduce the degree-of-freedom to one (the half-Cauchy) the prior becomes flatter and the resulting posteriors are more similar.  


``` {r posterior-compare, echo=FALSE, include=TRUE, fig.cap="Posterior probability densities for the standard deviation of $\\texttt{school}$ effects used in the $\\texttt{mschool}$ models $\\texttt{1-5}$.  The priors used in these models are given in the legend with the scale of the half-Cauchy and half-t distributions following the colon. Note the inverse-gamma is the prior for the variance - not the standard deviation.", fig.width=7, fig.height=5}

samples <- list(sqrt(mschool.1$VCV[,"school"]), sqrt(mschool.2$VCV[,"school"]), sqrt(mschool.3$VCV[,"school"]), sqrt(mschool.4$VCV[,"school"]), sqrt(mschool.5$VCV[,"school"]))

cols    <- palette()[1:5]  # nice colours

plot(density(samples[[1]]),                     
     main = "",
     xlab = expression(sigma[school]), ylab = "Posterior density",
     col  = cols[1], lwd = 2)

for (i in 2:length(samples))                  
  lines(density(samples[[i]]), col = cols[i], lwd = 2)

legend("topright", lwd = 2, col = cols,
       legend = model.names, bty = "n")
```

Figure \@ref(fig:posterior-compare) is anxiety-inducing - which prior to use? While I am reluctant to give general advise, in my own work I tend to use a half-Cauchy with a large scale: often $\sqrt{1000}$ (`alpha.V=1000`) depending on the scale of the data. However, I try to avoid collecting and analysing data that contains as little information as seen in this example (but see Section \@ref(fix-or-rand)). When this is the case, shifts in the posterior distribution under different priors are often subtle. However, for variances or standard deviations that have support close to zero the spike of the inverse-Wishart/inverse-gamma distribution can cause problems, and for this reason I usually avoid it. The added bonus is faster mixing under parameter expansion. While the prior for the residual variance is restricted to the the inverse-Wishart/inverse-gamma distribution in MCMCglmm (Section \@ref(Vprior-sec)) the data usually contains so much information for the residual variance that prior sensitivity is limited. For models where the random-effect specification defines a (co)variance matrix rather than a scalar variance, see Section \@ref(VCVprior-sec).

## Prior Generators {#Vprior-gen-sec}

Even when a user knows which prior they would like to use, it can be rather fiddly generating the prior list, especially when there are many random terms, some of which define (co)variance matrices. In order to simplify the process of prior specification a set of prior generator functions are available in versions >4.0.  These functions $\texttt{IW}$ (inverse-Wishart), $\texttt{IG}$ (inverse-gamma) and $\texttt{F}$ (central $F$ with 1 numerator degree of freedom) take two arguments specifying the parameters of the prior distribution to be used for the variances. The function $\texttt{tSD}$ can also be used which specifies a half-$t$-distribution for the standard deviation. Note that a set parameters can always be chosen such that $\texttt{IW}=\texttt{IG}$ and $\texttt{F}=\texttt{tSD}$. The difficult topic of prior specification for (co)variance matrices is covered in Section \@ref(VCVprior-sec) and I'll leave the documentation of the prior generator functions in this context to then.  



``` {r echo=FALSE, results='asis'}
prior.functions <- data.frame(
  Function = c("IW", "IG", "F", "tSD"),
  Distribution = c("Inverse-Wishart", "Inverse-gamma", "F with df1=1", "Half-t (for SD)"),
  Parameter1 = c("V=1", "shape=0.001", "df2=1", "df=1"),
  Parameter2 = c("nu=0.002", "scale=0.001", "scale=1000", "scale=$\\sqrt{1000}$"),
  stringsAsFactors = FALSE
)

kable(prior.functions, "html", escape = FALSE, caption = "Table of prior generator functions for specifying the (marginal) prior of a variance ($\\texttt{IW}$, $\\texttt{IG}$ and $\\texttt{F}$) or a standard deviation ($\\texttt{tSD}$). The two parameters for each distribution are given together with their defaults.", label="prior-functions") |>
  kable_styling(full_width = FALSE)%>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```



## Priors on Functions of Variances 

### Intra-class Correlation


## Fixed or Random? {#fix-or-rand}

At the start of this Chapter I highlighted the central decision that needs to be made when deciding if an effect should be random or fixed: should a vague prior for the effect (or flat prior in a frequentist analysis) be specified entirely by the analyst (fixed) or should the prior be updated using all data that has relevant information (random)?  When put this way, treating something as random would always seem preferable. However, sometimes there is so little relevant information that the benefit of treating an effect as random is outweighed by the cost of the increased complexity. In my experience, when there is a lack of relevant information - the information required to estimate the random-effect variance, $\sigma^2_u$ - it is almost always because there are only a few levels of the predictor, not because the replication per level is small. This is why people often use the rule-of-thumb that if there are few factor levels ($<5$) then we should treat the effects as fixed. In this scenario, even if we had infinite replication at each level, we would still only have fewer than five observations from which to estimate the variance. In such cases a second rule-of-thumb is often satisfied: if the levels are interesting to other people, they are fixed. However, when the two rules-of-thumb are in conflict, people often agonise about the choice.  If your data set is of an admirable size, I would argue that it often doesn't matter what you choose. If you have few levels of a predictor, that probably means you have a lot of replication per level.  In these situations, the amount of information coming from observations from that level overwhelm any prior information and so it doesn't matter whether you say the prior information is vague (fixed) or try and update the prior information from all observations (random). 

In my own field, year is often a good example. A field project has ran for less than five years but each year a lot data is collected. Should the year effects be treated as fixed or random? Well there are few levels, suggesting fixed, but the effect of year 2014 (for example) isn't particularly interesting, suggesting random. People sometimes claim fixed because years haven't been sampled at random, but this argument shows a deep misunderstanding of what a random effect is. 'random' isn't referring to *years* being *sampled at random*, but referring to the fact that we would like to treat year *effects* as *random variables* coming from a distribution. If I had observations from many years I would certainly fit year effects as random. However, with few years and a lot of data per year I would treat them as fixed. Let's consider a data-set I collected and that I use for teaching:

```{r }
load(url("https://github.com/jarrodhadfield/sda/raw/master/data/BTtarsus.rda"))
head(BTtarsus)
```

$\texttt{tarsus_mm}$ are the lengths of the tarsus bone in Blue Tit chicks and $\texttt{year}$ is the year (2011-2014) in which the chicks hatched and I measured them. The remaining variables will not be relevant here. The key point is that $\texttt{year}$ only has four levels but the number of birds with tarsus measurements is high in each year (between 593 and 854). Let's fit two models, one where year effects are fixed and one where they are random:

```{r }
BTtarsus$year<-as.factor(BTtarsus$year)

prior.myear.fixed=list(R=list(V=1, nu=0.002))
myear.fixed<-MCMCglmm(tarsus_mm~year, data=BTtarsus, prior=prior.myear.fixed)

prior.myear.random=list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=1, alpha.mu=0, alpha.V=1000)))
myear.random<-MCMCglmm(tarsus_mm~1, random=~year, data=BTtarsus, pr=TRUE, prior=prior.myear.random)
```

If we plot the posterior distribution for the estimated year means we can see that they are extremely similar (Figure \@ref(fig:year)). 


``` {r year, echo=FALSE, include=TRUE, fig.cap="MCMC traces for the estimated year means when year effects are treated as fixed (black) or random (red). Because the amount of replication per year is high the posterior distributions are very similar", fig.width=7, fig.height=8}

fixed.year<-cbind(myear.fixed$Sol[,"(Intercept)"], myear.fixed$Sol[,"(Intercept)"]+myear.fixed$Sol[,c("year2012", "year2013", "year2014")])

fixed.random<-myear.random$Sol[,"(Intercept)"]+myear.random$Sol[,c("year.2011", "year.2012", "year.2013", "year.2014")]
colnames(fixed.year)<-colnames(fixed.random)

plot(mcmc.list(mcmc(fixed.year), mcmc(fixed.random)))
```

With a flatish prior on the standard deviation of year effects (half-Cauchy with scale $\sqrt{1000}$) the posterior distribution is, for those familiar with Blue Tit tarsus lengths, hopeless uncertain (Figure \@ref(fig:year-sd)). A standard deviation of 0.5 has some support. This implies that some years we expect some seriously stumpy Blue Tits and in other years we expect some seriously leggy Blue Tits. But a standard deviation of 0.05 also has some support and this implies that tarsus length hardly varies from year-to-year.  In such cases we might as well admit that we cannot estimate the between year variability with sufficient precision even though we can infer with precision the effect of the handful of years we have measurements for. When this is the case, we may as well treat the effects as fixed.


```{r year-sd, echo=FALSE, include=TRUE, fig.cap="Posterior distribution for the standard deviation of year effects from model `prior.myear.random`."}

hist(sqrt(myear.random$VCV[,"year"]), prob=TRUE, breaks=50, main="", xlab=expression(sigma[year]))
```


[^ranef.1]: If we assumed the distribution was Laplace (back to back exponentials) we have the LASSO. If we assumed the distribution was a mixture of normal and Laplace we have the elastic net. 

[^ranef.2]: With the now defunct $\texttt{family="ordinal"}$ the $\pi^2/3$ in the denominator for the intra-class correlation would have be replaced by a one which is the link variance for the probit (the variance of the unit normal).  

[^ranef.3]: For the $F$ prior on the variance, the scale is set by $\texttt{alpha.V}*\texttt{V}$ and the normalised variance $\sigma^2_{\texttt{school}}/(\texttt{alpha.V}*\texttt{V})$ follows a non-central F distribution with one numerator degree-of-freedom ($\texttt{df1}$), $\texttt{nu}$ denominator degrees-of-freedom ($\texttt{df2}$) and a non-centrality parameter ($\texttt{ncp}$) equal to $\texttt{alpha.mu}^2/\texttt{alpha.V}$.  The density of $\sigma^2_{\texttt{school}}$ is therefore the density of $\sigma^2_{\texttt{school}}/(\texttt{alpha.V}*\texttt{V})$ in this $F$ distribution divided by the Jacobian, $\texttt{alpha.V}*\texttt{V}$ (Section \@ref(transform-sec)). For the $t$ prior on the standard deviation, the scale is set by $\sqrt{\texttt{alpha.V}*\texttt{V}}$ and the normalised standard deviation $\sigma_{\texttt{school}}/\sqrt{\texttt{alpha.V}*\texttt{V}}$ follows a folded non-central $t$ distribution with $\texttt{nu}$ degrees-of-freedom and non-centrality parameter $\texttt{alpha.mu}/\sqrt{\texttt{alpha.V}}$. The folding is because we are working with the absolute value of the $t$ variable and so the density for negative values is reflected onto positive values. The density of $\sigma_{\texttt{school}}$ is therefore the summed density of $\sigma_{\texttt{school}}/(\texttt{alpha.V}*\texttt{V})$ and $-\sigma_{\texttt{school}}/(\texttt{alpha.V}*\texttt{V})$ in this $t$ distribution divided by the Jacobian, $\sqrt{\texttt{alpha.V}*\texttt{V}}$. This is all very hard to remember! The density of these distributions can be obtained using the function `dprior` which evaluates $\texttt{x}$ according to the MCMCglmm specification passed as a list to the argument $\texttt{prior}$. The density of the standard deviation ($\texttt{sd=TRUE}$) or the variance (the default - $\texttt{sd=FALSE}$) can be obtained. For example, `dprior(1/2, prior=list(V=1, nu=1, alpha.mu=0, alpha.V=10^2), sd=TRUE)`, returns the density of $\sigma_{\texttt{school}}=1/2$ for a half-$t$ (since $\texttt{alpha.mu}=0$) with one degree-of-freedom and scale 10 (i.e a half-Cauchy with scale 10).


```{r echo=FALSE, eval=FALSE}
# Better check new dprior function!
V<-runif(1, 1, 10)
nu<-sample(10:100, 1)
alpha.mu<-runif(1, 1, 10)
alpha.V<-runif(1, 1, 10)
alpha.V<-runif(1, 1, 10)

scale<-alpha.V*V

pxdat<-data.frame(y=c(NA,NA), fac=1:2)
prior<-list(B=list(mu=0, V=1e-16), R=list(V=1, fix=1), G=list(G1=list(V=V, nu=nu, alpha.mu=alpha.mu, alpha.V=alpha.V)))
pxm<-MCMCglmm(y~1, random=~fac, dat=pxdat, pr=TRUE, singular=TRUE, prior=prior, nitt=13000*10, thin=5*10, burnin=3000*10)
# simulate from prior by having completely missing observations and fixed intercept/residual variance

par(mfrow=c(2,2))
vf<-rf(2000, df1=1, df2=nu, ncp=(alpha.mu^2)/alpha.V)
qqplot(vf, pxm$VCV[,"fac"]/scale, main="qq-plot of variances", ylab="MCMCglmm", xlab="rf")

abline(0,1)

h<-hist(pxm$VCV[,"fac"], breaks=100, prob=TRUE, main="", xlab="Variance")
dv<-dprior(h$mids, prior=prior$G$G1)
lines(dv~h$mids, col="red")

sdt<-abs(rt(2000, df=nu, ncp=alpha.mu/sqrt(alpha.V)))
qqplot(sdt, sqrt(pxm$VCV[,"fac"])/sqrt(scale), main="qq-plot of standard deviation", ylab="MCMCglmm", xlab="rt")

abline(0,1)

h<-hist(sqrt(pxm$VCV[,"fac"]), breaks=100, prob=TRUE, main="", xlab="Standard deviation")
dsd<-dprior(h$mids, prior=prior$G$G1, sd=TRUE)

lines(dsd~h$mids, col="red")

V<-as.matrix(runif(1, 1, 10))
nu<-sample(3:10, 1)

pxdat<-data.frame(y=c(NA,NA), fac=1:2)
prior<-list(B=list(mu=0, V=1e-16), R=list(V=1, fix=1), G=list(G1=list(V=V, nu=nu)))
iwm<-MCMCglmm(y~1, random=~fac, dat=pxdat, pr=TRUE, singular=TRUE, prior=prior, nitt=13000*10, thin=5*10, burnin=3000*10)
# simulate from prior by having completely missing observations and fixed intercept/residual variance

par(mfrow=c(2,2))
vIW<-rIW(V=V, nu=nu, n=2000)
qqplot(vIW, iwm$VCV[,"fac"], main="qq-plot of variances", ylab="MCMCglmm", xlab="rf")

abline(0,1)

h<-hist(iwm$VCV[,"fac"], breaks=100, prob=TRUE, main="", xlab="Variance")
dv<-dprior(h$mids, prior=prior$G$G1)
lines(dv~h$mids, col="red")

sdIW<-sqrt(rIW(V=V, nu=nu, n=2000))
qqplot(sdIW, sqrt(iwm$VCV[,"fac"]), main="qq-plot of standard deviation", ylab="MCMCglmm", xlab="rt")

abline(0,1)

h<-hist(sqrt(iwm$VCV[,"fac"]), breaks=100, prob=TRUE, main="", xlab="Standard deviation")
dsd<-dprior(h$mids, prior=prior$G$G1, sd=TRUE)

lines(dsd~h$mids, col="red")
```
