# Random effects {#ranef}

In some cases we may have measured variables whose effects we would like to treat as random. Often the distinction between fixed and random is given by example: things like city, species, individual and vial are random, but sex, treatment and age are not. Or the distinction is made using rules of thumb: if there are few factor levels and they are interesting to other people they are fixed. However, this doesn't really confer any understanding about what it means to treat something as fixed or random, and doesn't really allow judgements to be made for variables in which the rules of thumb seem to contradict each other. Similarly, these 'explanations' don't give any insight into the fact that all effects are technically random in a Bayesian analysis.

Random effect models are often expressed as an extension of Equation \@ref(eq:lm):

$$E[{\bf y}] = {\bf X}{\boldsymbol{\mathbf{\beta}}}+{\bf Z}{\bf u}
 \label{MM}   (\#eq:MM)$$

where ${\bf Z}$ is a design matrix like ${\bf X}$, and ${\bf u}$ is a vector of parameters like ${\boldsymbol{\mathbf{\beta}}}$. However, at this stage there is simply no distinction between fixed and random effects. We could combine the design matrices (${\bf W} = [{\bf X}, {\bf Z}]$) and combine the vectors of parameters ($\boldsymbol{\theta} = [{\boldsymbol{\mathbf{\beta}}}^{'}, {\bf u}^{'}]^{'}$) to get:

$$E[{\bf y}] = {\bf W}\boldsymbol{\theta}
 \label{MM2}   (\#eq:MM2)$$

which is **identical** to Equation \@ref(eq:MM).  So if we don't need to distinguish between fixed and random effects at this stage, when should we distinguish between them, and what distinguishes them?  

When we treat an effect as random we believe that the coefficients have some distribution around a mean of zero; often we assume they are normal[^ranef.1] and that they are independent (represented by an identity matrix) and identically distributed with variance $\sigma^{2}_{u}$:

$${\bf u} \sim N({\bf 0}, {\bf I}\sigma^{2}_{u})$$

$\sigma^{2}_{u}$ is a parameter of the model which we estimate, in addition to ${\bf u}$. In a Bayesian analysis we would also assign $\sigma^{2}_{u}$ a prior, and $\sigma^{2}_{u}$ is often called a hyper-parameter with an associated hyper-prior. 

Fixed effects in a frequentist analysis are not assigned a distribution, but we can understand this in terms of the limit to the normal distribution

$$\boldsymbol{\beta} \sim N({\bf 0}, {\bf I}\sigma^{2}_{\beta})$$

as $\sigma^{2}_{\beta}$ tends to infinity. In a Bayesian setting we would call this a flat improper prior. In practice, we often use diffuse proper priors in Bayesian analyses. For example, the default in $\texttt{MCMCglmm}$ is to set $\sigma^{2}_{\beta}=10^8$. Then, $\boldsymbol{\beta}$ are technically random - they are assigned a distribution - but I find it useful to retain the frequentist terminology 'fixed'. The only difference then is that the 'fixed' effects are assigned a prior distribution with a variance that is defined by the user-specified prior ($\sigma^{2}_{\beta}$ - which is often set to be large) and the 'random' effects are assigned a prior distribution with a variance that is estimated ($\sigma^{2}_{u}$ - which could be large, but also zero). 

That is the distinction between fixed and random effects. The difference really is that simple, but it takes a long time and a lot of practice to understand what this means in practical terms, and why working with random effects can be a very powerful way of modelling data.  To get a feel for why we might want to fit an effect as random or not, lets work through an example before moving on to model fitting. In Section \@ref(binom-sec) we analysed binomial data where 122 respondents had looked at 44 photographs of people and given them a 'grumpy score' of more than five (a success) or less than five (a failure). If, instead of 122 respondents, there had been a zillion respondents, we could use the average proportion of success for each photo as a nearly perfect estimates of their probabilities of success. The variance of these near-perfect estimates could serve as a reasonable estimate of the variance in photo effects. If the probabilities were all clustered tightly around 0.5: 0.505, 0.501, 0.499 and so on, then variance would be estimated to be small. Let's then imagine that we obtained a $45^\textrm{th}$ photograph but by this point the respondents were so bored I managed to only recruit a single person who gave the photo a score greater than five - a success. Since we only have one observation for this photo the average proportion of success would be one. Do you think the best estimate of the probability of success for the $45^\textrm{th}$ photograph is then 1.000? I think you wouldn't: you would use the knowledge that you have gained from the other photos and say that it is more likely that if you had managed to recruit more respondents you would have got a roughly even split of success and failures. You have used common sense, treated the photo effects as random, and *shrunk* photo 45's effect towards the average because the variance ($\sigma^2_u$) was small and we have a strong prior. If we had treated the photo effects as fixed, we believe that the only information regarding a photo's value comes from data associated with that particular photo, and the estimate of photo 45's probability would have been one. When we treat an effect as random, we also use the information that comes from data associated with that particular photo (obviously), but we weight that information by what the data associated with other photos tell us about the likely values that the effect could take - through the parameter $\sigma^2_u$. What if the probabilities weren't all clustered tightly around 0.5, but took on values 0.500, 0.998, 0.002, 0.327 ...?  The variance $\sigma^2_u$ would be larger and the prior information for our $45^\textrm{th}$ photo would be weaker: perhaps we got a success because the underlying probability was 0.998, but a single success would also not be very surprising if the underlying probability was 0.500, or even 0.327. We might then be happy that our best estimate of the probability of success for the $45^\textrm{th}$ photograph was close to one, although with such weak prior information (large $\sigma^2_u$) the uncertainty would remain large.   

When the motivation for treating an effect as random is explained this way, it is hard to come up with a reason why you wouldn't treat all effects as random. However, you have to consider how much information is in a given data set to estimate $\sigma^2_u$, which we will cover in Section \@ref(fixed-or-random-sec)

## Generalised Linear Mixed Model (GLMM) {#GLMM}

In Section \@ref(binom-sec), the binomial model we fitted only contained fixed effects, as specified in the `fixed` argument to `MCMCglmm` (`fixed=cbind(g5,l5)~type+ypub`). No random effects were fitted, although 'residuals' were fitted as default to absorb any overdispersion. Residuals are random effects for which we estimate a variance - the hyperparameter, $\sigma^2_e$ - and when used with Binomial or Poisson responses are commonly referred to as observation-level random effects. Since there is a one-to-one correspondence between observation and photo in this data set, $\sigma^2_e$ is equivalent to the $\sigma^2_u$ discussed above (although $\sigma^2_e$ refers to the variance on the logit scale rather the probability scale used implicitly above). We saw that the probability of success varied greatly across photos (model `mbinom.1`) but we also noted that some of this variation may be due to the person being photographed and we could tease apart the effect of person from the specifics of the photo since each person was photographed twice - once when happy and once when grumpy. $\texttt{person}$ has 22 levels and you are probably not interested in knowing the grumpy score of someone you didn't know - $\texttt{person}$ effects seem to satisfy the rule of thumb often used to decide that they should be treated as random. The random effect model is specified through the argument `random` and for simple effects as these we simply put the name of the corresponding column ($\texttt{person}$) in the model formula. We will also specify inverse-Wishart priors for both the residual variance and the variance of the $\texttt{person}$ effects (see Section \@ref(Vprior-sec)) although scaled non-central $F$-distribution priors are recommended for random-effect variances (see Section \@ref(PXprior-sec)):

``` {r mbinom2}
prior.mbinom.2=list(R=list(V=1, nu=0.002), G=list(G1=list(V=1, nu=0.002)))

mbinom.2<-MCMCglmm(cbind(g5, l5)~type+ypub, random=~person, data=Grumpy, family="multinomial2", pr=TRUE, prior=prior.mbinom.2)
summary(mbinom.2)
```   

We can see that the between-person variance is comparable to the residual (across-photo within-person) variance although the credible intervals on both variances is wide. $\texttt{MCMCglmm}$ does not store the posterior distribution of the random effects by default, as there may be a lot of them and they are often not of interest. However, since I specified `pr=TRUE`, the whole of $\boldsymbol{\theta}$ is stored rather than just ${\boldsymbol{\mathbf{\beta}}}$. In Section \@ref(binom-sec) we saw that photo 4521 and photo 4527, despite having the same fixed effect prediction ($\texttt{type}$ = $\texttt{grumpy}$, $\texttt{ypub}$ = 16 years),  had quite different probabilities of success, with the posterior mean probabilities being `r formatC(round(mean(plogis(mbinom.1$Liab[,3])),3), format="f", digits=3)` and `r formatC(round(mean(plogis(mbinom.1$Liab[,25])),3), format="f", digits=3)` respectively. What we didn't know is whether this divergence in probability was due to the person being photographed or some property of the photo. 

## Prediction with Random Effects {#ranpred-sec}

If we use the $\texttt{predict}$ method on our model the default is to not only marginalise the residuals, but also to marginalise any other random effects.  If we predict the probability of success for these two photos they are identical, because we are calculating the expectation based on ${\bf X}{\boldsymbol{\beta}}$ only: 

``` {r predict-mbinom2}
predict(mbinom.2)[c(3,25),]/122
```   

The $\texttt{predict}$ method (and $\texttt{simulate}$ method) for $\texttt{MCMCglmm}$ includes the argument $\texttt{marginal}$ which by default takes the $\texttt{random}$ argument used to fit the model. If we want to obtain a prediction that includes (some of) the random effects we can remove the corresponding term from the formula passed to  $\texttt{marginal}$. Since we only have one random term, which we like to include in the prediction, $\texttt{marginal}$ is empty:

```{r echo=FALSE}
p.mbinom.2<-predict(mbinom.2, marginal=NULL)
```


``` {r predict-mbinom2-b}
predict(mbinom.2, marginal=NULL, interval="confidence")[c(3,25),]/122
```  

It seems that some of the divergence in probability is due to the person being photographed: our best estimate is that if we had taken many photos of $\texttt{darren_o}$ when grumpy  `r round(100*p.mbinom.2[3]/122,1)` \% of people would have scored him above five on the grumpy scale, but for $\texttt{craig_w}$ it would be lower (`r round(100*p.mbinom.2[25]/122,1)` \%). The 95\% credible (confidence) intervals on each are wide however, and a formal comparison (on the logit scale) gives a 95\% credible interval that overlaps zero:

``` {r }
HPDinterval(mbinom.2$Sol[,"person.darren_o"]-mbinom.2$Sol[,"person.craig_w"])
``` 

## Overdispersed Binomial as a Bernoulli GLMM

The $\texttt{Grumpy}$ data set aggregates the scores of the 122 respondents into a single binomial response for each photograph. However, we could imagine disaggregating the data such that each respondent for each photograph gets a Bernoulli response with a success if they gave a particular photo a score greater than five. The disaggregated data (`FullGrumpy`) have $122\times 44 = 5,368$ observations (although a few respondents did not assess all photos).  

```{r }
data(FullGrumpy)
head(FullGrumpy, 3)
```

$\texttt{y}$ is now the score each respondent $\texttt{respondent}$ gave each $\texttt{photo}$ (rather than the average score for each $\texttt{photo}$ in `Grumpy`). In addition, we have the respondent-level information $\texttt{student}$ which can be either $\texttt{YES}$ or $\texttt{NO}$. We will turn each persons score into the Bernoulli response

``` {r }
FullGrumpy$g5<-FullGrumpy$y>5
```   

and fit the model

``` {r mbinom3}
prior.mbinom.3=list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=0.002), G2=list(V=1, nu=0.002)))

mbinom.3<-MCMCglmm(g5~type+ypub, random=~person+photo, data=FullGrumpy, family="categorical", prior=prior.mbinom.3)
summary(mbinom.3)
```   

The \texttt{photo} random effects deal with any variation in the probability of success across photos and are exactly comparable to the residuals of the binomial model `mbinom.2`. It is therefore surprising that the posterior distributions for both the variance in $\texttt{person}$ effects and the variance in $\texttt{photo}$/residual effects appear to be different between the two models, as do the fixed effects. This is a peculiarity of logit-link models in $\texttt{MCMC}$ and wouldn't be seen in $\texttt{family="threshold"}$ models that implements the standard probit link. In the binomial model `mbinom.2` we implicitly assumed that the probability of success did not vary across respondents *within* photos - this was an assumption, and one that cannot be tested.  In the Bernoulli model `mbinom.3` we explicitly assumed that the probability of success varied across respondents within photos, and the variance on the logit-scale was one. Since Bernoulli data provide no information about observation-level variability either (or most likely, neither) assumption could be true but we have no way of knowing (Section \@ref(bernoulli-sec)). As we saw with fixed effect coefficients in Bernoulli GLM, stating that the variances in photo effects is `r round(mean(mbinom.3$VCV[,"photo"]), 3)` is meaningless in a Bernoulli GLMM without putting it in the context of the assumed residual variance. The standard approach - what I refer to as the standard logit model - is to assume the residual variance is zero. While I think this is a good standard, this is prohibited in $\texttt{MCMCglmm}$ because the chain will not mix. But as we saw with the fixed effects, we can rescale the variances by needs to be multiplied by $1/(1+c^{2}\sigma^{2}_{\texttt{units}})$ where $c=16\sqrt{3}/15\pi$ and $\sigma^{2}_{\texttt{units}}$ is our assumed residual variance, which is one (Figure \@ref(fig:bernoulli-rescale2)).

```{r bernoulli-rescale2, echo=TRUE, include=TRUE, fig.cap="MCMC traces for the estimated variances in $\\texttt{person}$ and $\\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`) of individual data (red) and a Binomial GLMM (model `mbinom.2`) where all data for a photo have been aggregated into a single Binomial response (black). The posterior distribution of the variances from the Bernoulli GLMM have been rescaled to what would be observed if the residual variance was zero (rather than one).", fig.width=7, fig.height=8}
c2 <- ((16*sqrt(3))/(15*pi))^2

rescale.VCV.2 <- mbinom.2$VCV
colnames(rescale.VCV.2)[2]<-"photo"

rescale.VCV.3 <- mbinom.3$VCV[,c("person", "photo")]/(1+c2)

plot(mcmc.list(as.mcmc(rescale.VCV.2), as.mcmc(rescale.VCV.3)), density=FALSE)
```

## Intra-class Correlations

A more common approach, however, is to express the variances as intra-class correlations where we take the variance of interest and express it as a proportion of the total. For example, for the $\texttt{person}$ effects, the intra-class correlation would be

$$ICC = \frac{\sigma^2_{\texttt{person}}}{\sigma^2_{\texttt{person}}+\sigma^2_{\texttt{photo}}+\sigma^2_{\texttt{units}}+\pi^2/3}$$

where the $\pi^2/3$ appears because we have used the logit link and this is the link variance (the variance of the unit logistic). 

```{r ICC, echo=TRUE, include=TRUE, fig.cap="MCMC traces for the estimated intra-class correlation for $\\texttt{person}$ and $\\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`) of individual data (red) and a Binomial GLMM (model `mbinom.2`) where all data for a photo have been aggregated into a single Binomial response (black).", fig.width=7, fig.height=8}

ICC.2 <- mbinom.2$VCV/(rowSums(mbinom.2$VCV)+pi^2/3)
colnames(ICC.2)[2]<-"photo"

ICC.3 <- mbinom.3$VCV[,c("person", "photo")]/(rowSums(mbinom.3$VCV)+pi^2/3)

plot(mcmc.list(as.mcmc(ICC.2), as.mcmc(ICC.3)), density=FALSE)
```

If we had used $\texttt{family="threshold"}$ this would be omitted because the link variance is zero as we are already working on the ... scale[^ranef.2]  Pierre de Villemereuil's [$\texttt{QGLMM}$](https://cran.r-project.org/web/packages/QGglmm/index.html) package. 


The eagle-eyed will have noticed that although the trace plots for the rescaled variance/intra-class correlation for the $\texttt{person}$ effects look identical between the two models, the trace plots for the $\texttt{photo}$ effects look slightly different with the posterior from model `mbinom.3` (the Bernoulli GLMM in red) appearing to have more density at higher values. However, this difference is due to Monte Carlo error, which is quite high for the variance estimates because the autocorrelation in the chain is moderate. The reported effective sample size in the model summary gives some indication of this - for example in the Bernoulli model the effective sample size for the variance in $\texttt{photo}$ effects is `r round(effectiveSize(mbinom.3$VCV[,"photo"]))`, quite a bit less than the 1,000 samples saved (Section \@ref(diagnostics-sec)). We can see this more clearly if we just plot the traces for Bernoulli model (Figure \@ref(fig:bernoulli-trace)).    


```{r bernoulli-trace, echo=FALSE, include=TRUE, fig.cap="MCMC trace for the variances in $\\texttt{person}$ and $\\texttt{photo}$ effects from a Bernoulli GLMM (model `mbinom.3`).", fig.width=7, fig.height=5}

plot(mbinom.3$VCV[,c("person", "photo")], density=FALSE)
```

Two things are apparent from Figure \@ref(fig:bernoulli-trace). Autocorrelation is present - this is not surprising: for each iteration of the MCMC chain the random effects are Gibbs sampled conditional on their variance in the previous iteration, and then conditional on the updated random effects the variances are then Gibbs sampled (Section \@ref(MCMC)). This will invariably lead to autocorrelation. Second, the trace for the variance in $\texttt{person}$ effects appears to intermittently get 'stuck' at values close to zero. In part, this reflects the mechanics of the Gibbs sampling, but it is also a consequence of the inverse-Wishart prior used which has a sharp peak in density at small values (Section \@ref(Vprior-sec)).  When the variance in $\texttt{person}$ effects gets 'stuck' at zero, the variance in $\texttt{photo}$ effects appears to get 'stuck' at high values. This is because the data provide strong support for the combined effect of $\texttt{photo}$ and $\texttt{person}$ being large, but contain less information about their separate effects. Consequently, if the $\texttt{person}$ variance drops to zero, the $\texttt{photo}$ variance increases to compensate. In this example, the effects described above are quite subtle, and simply running the chain for longer would probably suffice. However, a better general strategy would be to employ parameter expansion and use scaled non-central $F$ priors.

## Parameter Expansion and $F$ priors {#PXprior-sec}


## Fixed or Random?


[^ranef.1]: If we assumed the distribution was Laplace (back to back exponentials) we have the LASSO. If we assumed the distribution was a mixture of normal and Laplace we have the elastic net. 

[^ranef.2]: With the now defunct $\texttt{family="ordinal"}$ the $\pi^2/3$ in the denominator for the intra-class correlation would have be replaced by a one which is the link variance for the probit (the variance of the unit normal).  

