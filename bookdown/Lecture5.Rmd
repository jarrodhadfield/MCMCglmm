# Multi-response models

So far we have only fitted models to a single response variable.
Multi-response models are not that widely used, except perhaps in
quantitative genetics, and deserve wider use. They allow some of the
assumptions of single response models to be relaxed and can be an
effective way of dealing with missing data problems.

## Relaxing the univariate assumptions of causality

Imagine we knew how much money 200 people had spent on their holiday and
on their car in each of four years, and we want to know whether a
relationship exists between the two. A simple correlation would be one
possibility, but then how do we control for the repeated measures? An
often used solution to this problem is to choose one variable as the
response (lets say the amount spent on a car) and have the other
variable as a fixed covariate (the amount spent on a holiday). The
choice is essentially arbitrary, highlighting the belief that any
relationship between the two types of spending maybe in part due to
unmeasured variables, rather than being completely causal.\
In practice does this matter? Lets imagine there was only one unmeasured
variable: disposable income. There are repeatable differences between
individuals in their disposable income, but also some variation within
individuals across the four years. Likewise, people vary in what
proportion of their disposable income they are willing to spend on a
holiday versus a car, but this also changes from year to year. We can
simulate some toy data to get a feel for the issues:

``` {r echo=TRUE, tidy=FALSE}
id<-gl(200,4)                  # 200 people recorded four times                      

av_wealth<-rlnorm(200, 0, 1)               
ac_wealth<-av_wealth[id]+rlnorm(800, 0, 1) 
# expected disposable incomes + some year to year variation

av_ratio<-rbeta(200,10,10)                 
ac_ratio<-rbeta(800, 2*(av_ratio[id]), 2*(1-av_ratio[id])) 
# expected proportion spent on car + some year to year variation

y.car<-(ac_wealth*ac_ratio)^0.25     # disposable income * proportion spent on car
y.hol<-(ac_wealth*(1-ac_ratio))^0.25 # disposable income * proportion spent on holiday                              

Spending<-data.frame(y.hol=y.hol, y.car=y.car, id=id)
```

A simple regression suggests the two types of spending are negatively
related but the association is weak with the $R^{2}=$
`r round(summary(lm(y.car~y.hol))$r.squared,3)`.

``` {r echo=TRUE}
summary(lm(y.car~y.hol, data=Spending))
```

With `id` added as a random term to deal with the the repeated measures,
a similar conclusion is reached although the estimate is more negative:

``` {r echo=TRUE, eval=TRUE, cache=TRUE}
m5a.1<-MCMCglmm(y.car~y.hol, random=~id, data=Spending, verbose=FALSE)
summary(m5a.1$Sol[,"y.hol"])
```

We may be inclined to stop there, but lets proceed with a multi-response
model of the problem. The two responses are passed as a matrix using
`cbind()`, and the rows of this matrix are indexed by the reserved
variable `units`, and the columns by the reserved variable ` ``trait`.\
It is useful to think of a new data frame where the response variables
have been stacked column-wise and the other predictors duplicated
accordingly. Below is the original data frame on the left (`Spending`)
and the stacked data frame on the right:\

$$\begin{array}{cc}
\begin{array}{cccc}
&{\color{blue} \texttt{y.hol}}&{\color{blue} \texttt{y.car}}&\texttt{id}\\
{\color{red} \texttt{1}}&\texttt{`r formatC(Spending[,1][1],format="f", 6)`}&\texttt{`r formatC(Spending[,2][1],format="f", 6)`}&\texttt{`r Spending[,3][1]`}\\
{\color{red} \texttt{2}}&\texttt{`r formatC(Spending[,1][2],format="f", 6)`}&\texttt{`r formatC(Spending[,2][2],format="f", 6)`}&\texttt{`r Spending[,3][2]`}\\
\vdots&\vdots&\vdots\\
{\color{red} \texttt{800}}&\texttt{`r formatC(Spending[,1][800],format="f", 6)`}&\texttt{`r formatC(Spending[,2][800],format="f", 6)`}&\texttt{`r Spending[,3][800]`}\\
\end{array}&
\Longrightarrow
\begin{array}{ccccc}
&\texttt{y}&{\color{blue} \texttt{trait}}&\texttt{id}&{\color{red} \texttt{units}}\\
1&\texttt{`r formatC(Spending[,1][1],format="f", 6)`}&{\color{blue} \texttt{y.hol}}&\texttt{`r Spending[,3][1]`}&{\color{red} \texttt{1}}\\
2&\texttt{`r formatC(Spending[,1][2],format="f", 6)`}&{\color{blue} \texttt{y.hol}}&\texttt{`r Spending[,3][2]`}&{\color{red} \texttt{2}}\\
\vdots&\vdots&\vdots&\vdots\\
800&\texttt{`r formatC(Spending[,1][800],format="f", 6)`}&{\color{blue} \texttt{y.hol}}&\texttt{`r Spending[,3][800]`}&{\color{red} \texttt{800}}\\
801&\texttt{`r formatC(Spending[,2][1],format="f", 6)`}&{\color{blue} \texttt{y.car}}&\texttt{`r Spending[,3][1]`}&{\color{red} \texttt{1}}\\
802&\texttt{`r formatC(Spending[,2][2],format="f", 6)`}&{\color{blue} \texttt{y.car}}&\texttt{`r Spending[,3][2]`}&{\color{red} \texttt{2}}\\
\vdots&\vdots&\vdots&\vdots\\
1600&\texttt{`r formatC(Spending[,2][800],format="f", 6)`}&{\color{blue} \texttt{y.car}}&\texttt{`r Spending[,3][800]`}&{\color{red} \texttt{800}}\\
\end{array}
\end{array}$$

From this we can see that fitting a multi-response model is a direct
extension to how we fitted models with categorical random interactions
(Chapter [\[chap3\]](#chap3){reference-type="ref" reference="chap3"}):

``` {r echo=TRUE, eval=FALSE}
m5a.2<-MCMCglmm(cbind(y.hol, y.car)~trait-1, random=~us(trait):id, rcov=~us(trait):units, data=Spending, family=c("gaussian", "gaussian"), verbose=FALSE)
```

We have fitted the fixed effect `trait` so that the two types of
spending can have different intercepts. I usually suppress the intercept
(`-1`) for these types of models so the second coefficient is not the
difference between the intercept for the first level of `trait`
(`y.hol`) and the second level (`y.car`) but the actual trait specific
intercepts. In other words the design matrix for the fixed effects has
the form:

$$\begin{array}{rl}
\left[
\begin{array}{cc}
\texttt{trait[1]=="y.hol"}&\texttt{trait[1]=="y.car"}\\
\texttt{trait[2]=="y.hol"}&\texttt{trait[2]=="y.car"}\\
\vdots&\vdots\\
\texttt{trait[800]=="y.hol"}&\texttt{trait[800]=="y.car"}\\
\texttt{trait[801]=="y.hol"}&\texttt{trait[801]=="y.car"}\\
\texttt{trait[802]=="y.hol"}&\texttt{trait[802]=="y.car"}\\
\vdots&\vdots\\
\texttt{trait[1600]=="y.hol"}&\texttt{trait[1600]=="y.car"}\\
\end{array}
\right]
=&
\left[
\begin{array}{cc}
1&0\\
1&0\\
\vdots&\vdots\\
1&0\\
0&1\\
0&1\\
\vdots&\vdots\\
0&1\\
\end{array}
\right]\\
\end{array}$$

A $2\times2$ covariance matrix is estimated for the random term where
the diagonal elements are the variance in consistent individual effects
for each type of spending. The off-diagonal is the covariance between
these effects which if positive suggests that people that consistently
spend more on their holidays consistently spend more on their cars. A
$2\times2$ residual covariance matrix is also fitted. In Section
[\[heter-sec\]](#heter-sec){reference-type="ref" reference="heter-sec"}
we fitted heterogeneous error models using `idh():units` which made
sense in this case because each level of `unit` was specific to a
particular datum and so any covariances could not be estimated. In
multi-response models this is not the case because both traits have
often been measured on the same observational unit and so the covariance
can be measured. In the context of this example a positive covariance
would indicate that in those years an individual spent a lot on their
car they also spent a lot on their holiday.\
A univariate regression is defined as the covariance between the
response and the predictor divided by the variance in the predictor. We
can therefore estimate a regression coefficient for these two levels of
random variation, and compare them with the regression coefficient we
obtained in the simpler model:

``` {r echo=TRUE, eval=FALSE}
id.regression<-m5a.2$VCV[,2]/m5a.2$VCV[,1]
units.regression<-m5a.2$VCV[,6]/m5a.2$VCV[,5]
plot(mcmc.list(m5a.1$Sol[,"y.hol"], id.regression,units.regression), density=FALSE)
```

The regression coefficients (see Figure
[\[asUV-fig\]](#asUV-fig){reference-type="ref" reference="asUV-fig"})
differ substantially at the within individual (green) and between
individual (red) levels, and neither is entirely consistent with the
regression coefficient from the univariate model (black). The process by
which we generated the data gives rise to this phenomenon - large
variation between individuals in their disposable income means that
people who are able to spend a lot on their holiday can also afford to
spend a lot on their holidays (hence positive covariation between `id`
effects). However, a person that spent a large proportion of their
disposable income in a particular year on a holiday, must have less to
spend that year on a car (hence negative residual (within year)
covariation).\
``` {r echo=FALSE, eval=TRUE, cache=TRUE}
m5a.2<-MCMCglmm(cbind(y.hol, y.car)~trait-1, random=~us(trait):id, rcov=~us(trait):units, data=Spending, family=c("gaussian", "gaussian"), verbose=FALSE)
id.regression<-m5a.2$VCV[,2]/m5a.2$VCV[,1]
units.regression<-m5a.2$VCV[,6]/m5a.2$VCV[,5]
```

``` {r label=asUV, echo=FALSE, include=TRUE, fig.cap="MCMC summary plot of the coefficient from a regression of car spending on holiday spending in black. The red and green traces are from a model where the regression coefficient is estimated at two levels: within an individual (green) and across individuals (red). The relationship between the two types of spending is in part mediating by a third unmeasured variable, disposable income.", fig.width=7, fig.height=5}
muv<-max(m5a.1$Sol[,"y.hol"])
mid<-max(id.regression)
mu<-min(units.regression)
plot(mcmc.list(m5a.1$Sol[,"y.hol"], id.regression,units.regression), density=FALSE, ylim=c(mu, mid))
text(13000, muv, "univariate regression", pos=2)
text(13000, mid,  "id regression", pos=2, col="red")
text(13000, mu, "units regression", pos=2, col="green")
```

When fitting the simpler univariate model we make the assumption that
the effect of spending money on a car directly effects how much you
spend on a holiday. If this relationship was purely causal then all
regression coefficients would have the same expectation, and the simpler
model would be justified.\
For example, we could set up a simpler model where two thirds of the
variation in holiday expenditure is due to between individual
differences, and holiday expenditure directly affects how much an
individual will spend on their car (using a regression coefficient of
-0.3). The variation in car expenditure not caused by holiday
expenditure is also due to individual differences, but in this case they
only explain a third of the variance.

``` {r echo=TRUE, eval=TRUE}
Spending$y.hol2<-rnorm(200,0,sqrt(2))[Spending$id]+rnorm(800,0,sqrt(1))
Spending$y.car2<-Spending$y.hol2*-0.3+rnorm(200,0,sqrt(1))[Spending$id]+rnorm(800,0,sqrt(2))
```

We can fit the univariate and multivariate models to these data, and
compare the regression coefficients as we did before. Figure
[\[MVvUV2-fig\]](#MVvUV2-fig){reference-type="ref"
reference="MVvUV2-fig"} shows that the regression coefficients are all
very similar and a value of -0.3 has a reasonably high posterior
probability. However, it should be noted that the posterior standard
deviation is smaller in the simpler model because the more strict
assumptions have allowed us to pool information across the two levels to
get a more precise answer.

``` {r echo=FALSE, eval=TRUE, cache=TRUE}
m5a.3<-MCMCglmm(y.car2~y.hol2, random=~id, data=Spending, verbose=FALSE)
m5a.4<-MCMCglmm(cbind(y.hol2, y.car2)~trait-1, random=~us(trait):id, rcov=~us(trait):units, data=Spending, family=c("gaussian", "gaussian"), verbose=FALSE)
```

``` {r label=MVvUV2, echo=FALSE, include=TRUE, fig.cap="MCMC summary plot of the coefficient from a regression of car spending on holiday spending in black. The red and green traces are from a model where the regression coefficient is estimated at two levels: within an individual (green) and across individuals (red). In this model the relationship between the two types of spending is causal and the regression coefficients have the same expectation. However, the posterior standard deviation from the simple regression is smaller because information from the two different levels is pooled.", fig.width=7, fig.height=5}
md<-max(c(m5a.3$Sol[,2], m5a.4$VCV[,2]/m5a.4$VCV[,1],m5a.4$VCV[,6]/m5a.4$VCV[,5]))+0.075
mind<-min(c(m5a.3$Sol[,2], m5a.4$VCV[,2]/m5a.4$VCV[,1],m5a.4$VCV[,6]/m5a.4$VCV[,5]))
plot(mcmc.list(m5a.3$Sol[,2], m5a.4$VCV[,2]/m5a.4$VCV[,1],m5a.4$VCV[,6]/m5a.4$VCV[,5]), density=FALSE, ylim=c(mind, md))
text(13000, md, "Posterior Standard Deviation", pos=2)
text(13000, md-0.030, paste("univariate regression =", formatC(sd(m5a.3$Sol[,2]),format="f", 3)), pos=2)
text(13000, md-0.055, paste("id regression =", formatC(sd(m5a.4$VCV[,2]/m5a.4$VCV[,1]),format="f",3)), pos=2, col="red")
text(13000, md-0.080, paste("units regression =", formatC(sd(m5a.4$VCV[,6]/m5a.4$VCV[,5]),format="f",3)), pos=2, col="green")
```

## Multinomial Models

Multinomial models are difficult - both to fit and interpret. This is
particularly true when each unit of observation only has a single
realisation from the multinomial. In these instances the data can be
expressed as a single vector of factors, and the family argument can be
specified as `categorical`. To illustrate, using a very simple example,
we'll use data collected on 666 Soay sheep from the island of Hirta in
the St. Kilda archipelago [@Clutton-Brock.2004 Table A2.5].

``` {r echo=FALSE, eval=FALSE}
counts<-c(4,2,1,45,27,28,0,7,0,25,25,3,0,2,2,22,2,49,3,13,27,147,3,3,35,62,1,15,1,112)
sex<-as.factor(rep(c(rep("female", 18), rep("male", 12)), counts))
horn<-as.factor(rep(c(rep(c("scurred", "polled", "normal"),6),rep(c("scurred", "normal"),6)), counts))
id<-as.factor(1:666)
SShorns<-data.frame(id=id, horn=horn, sex=sex)
write.table(SShorns, "~/Work/AManal/MCMCglmm_1.12/data/SShorns.tab", row.names=FALSE, quote=FALSE)
```

``` {r echo=TRUE}
data(SShorns)
head(SShorns)
```

The sex and horn morph were recorded for each individual, giving the
contingency table:

``` {r echo=TRUE}
Ctable<-table(SShorns$horn, SShorns$sex)
Ctable
```

and we'll see if the frequencies of the three `horn` types differ, and
if the trait is sex dependent. The usual way to do this would be to use
a Chi square test, and to address the first question we could add the
counts of the two sexes:

``` {r echo=TRUE}
chisq.test(rowSums(Ctable))
```

which strongly suggests the three morphs differ in frequency. We could
then ask whether the frequencies differ by sex:

``` {r echo=TRUE}
chisq.test(Ctable)
```

which again they do, which is not that surprising since the trait is
partly sex limited, with males not expressing the polled phenotype.\
If there were only two horn types, polled and normal for example, then
we could have considered transforming the data into the binary variable
*polled or not?* and analysing using a glm with sex as a predictor. In
doing this we have reduced the dimension of the data from $J=2$
categories to a single ($J-1=1$) contrast. The motivation for the
dimension reduction is obvious; if being a male increased the
probability of expressing normal horns by 10%, it must by necessity
reduce the probability of expressing polled horn type by 10%, because an
individual cannot express both horn types simultaneously. The dimension
reduction essentially constrains the probability of expressing either
horn type to unity:\

$$Pr(\texttt{horn[i]}=\textrm{normal})+Pr(\texttt{horn[i]}=\textrm{polled}) = 1$$

These concepts can be directly translated into situations with more than
two categories where the unit sum constraint has the general form:

$$\sum_{k=1}^{J}Pr(y_{i}=k)=1$$

For binary data we designated one category to be the success (polled)
and one category to be the failure (normal) which we will call the
baseline category. The latent variable in this case was the log-odds
ratio of succeeding versus failing:

$$l_{i} = \textrm{log}\left(\frac{Pr(\texttt{horn[i]}=\textrm{polled})}{Pr(\texttt{horn[i]}=\textrm{normal})}\right) = \textrm{logit}\left(Pr(\texttt{horn[i]}=\textrm{polled})\right)$$

With more than two categories we need to have $J-1$ latent variables,
which in the original horn type example are:

$$l_{i,\textrm{polled}} = \textrm{log}\left(\frac{Pr(\texttt{horn[i]}=\textrm{polled})}{Pr(\texttt{horn[i]}=\textrm{normal})}\right)$$

and

$$l_{i,\textrm{scurred}} = \textrm{log}\left(\frac{Pr(\texttt{horn[i]}=\textrm{scurred})}{Pr(\texttt{horn[i]}=\textrm{normal})}\right)$$

The two latent variables are indexed as `trait`, and the unit of
observation ($i$) as `units`, as in multi-response models. As with
binary models the residual variance is not identified, and can be set to
any arbitrary value. For reasons that will become clearer later I like
to work with the residual covariance matrix
$\frac{1}{J}({\bf I}+{\bf J})$ where ${\bf I}$ and ${\bf J}$ are $J-1$
dimensional identity and unit matrices, respectively.\
To start we will try a simple model with an intercept:\
``` {r echo=TRUE, eval=TRUE, cache=TRUE}
IJ<-(1/3)*(diag(2)+matrix(1,2,2))
prior=list(R=list(V=IJ, fix=1))
m5c.1<-MCMCglmm(horn~trait-1, rcov=~us(trait):units, prior=prior, data=SShorns, family="categorical", verbose=FALSE)
```

The posterior distribution for the intercepts is shown in Figure
[\[MN1\]](#MN1){reference-type="ref" reference="MN1"}, and the model
clearly needs to be run for longer (Figure
[\[MN1\]](#MN1){reference-type="ref" reference="MN1"}). However\...\
``` {r label=MN1, echo=FALSE, include=TRUE, fig.cap="Posterior distribution of fixed effects from model \\texttt{m5c.1}: a simple multinomial logit model with intercepts only}", fig.width=7, fig.height=5}
plot(m5c.1$Sol)
```

The problem can also be represented using the contrast matrix
${\bf \Delta}$ [@Bunch.1991]:

$${\boldsymbol{\mathbf{\Delta}}}=
\left[
\begin{array}{c c}
-1&-1\\
1&0\\
0&1\\
\end{array}
\right]$$

where the rows correspond to the factor levels (`normal`, `polled` and
`scurred`) and the columns to the two latent variables. For example
column one corresponds to $l_{i,\textrm{polled}}$ which on the log scale
is
$Pr(\texttt{horn[i]}=\textrm{polled}) - Pr(\texttt{horn[i]}=\textrm{normal})$.\

$$\textrm{exp}\left(({\boldsymbol{\mathbf{\Delta}}}{\boldsymbol{\mathbf{\Delta}}}^{'})^{-1}{\boldsymbol{\mathbf{\Delta}}}{\bf l}_{i}\right) \propto E\left[\begin{array}{c} Pr(\texttt{horn[i]}=\textrm{normal})\\ Pr(\texttt{horn[i]}=\textrm{polled})\\ Pr(\texttt{horn[i]}=\textrm{scurred}) \end{array} \right]$$

The residual and any random effect covariance matrices are for
estimability purposes estimated on the $J-1$ space with
${\boldsymbol{\mathbf{V}}}={\boldsymbol{\mathbf{\Delta}}}^{'}\tilde{\bf V}{\boldsymbol{\mathbf{\Delta}}}$
where $\tilde{\bf V}$ is the covariance matrix estimated on the $J-1$
space. To illustrate, we will rescale the intercepts as if the residual
covariance matrix was zero (see Sections and []{#pred-sec
label="pred-sec"} []{#cat-sec label="cat-sec"}) and predict the expected
probability for each horn type:

``` {r echo=TRUE}
Delta<-cbind(c(-1,1,0), c(-1,0,1))   
c2<-(16*sqrt(3)/(15*pi))^2   
D<-ginv(Delta%*%t(Delta))%*%Delta        
Int<-t(apply(m5c.1$Sol,1, function(x){D%*%(x/sqrt(1+c2*diag(IJ)))}))
summary(mcmc(exp(Int)/rowSums(exp(Int))))
```

which agrees well with those observed:

``` {r echo=TRUE}
prop.table(rowSums(Ctable))
```

To test for the effects of sex specific expression we can also fit a
model with a sex effect:

``` {r echo=TRUE, cache=TRUE}
m5c.2<-MCMCglmm(horn~trait+sex-1, rcov=~us(trait):units, data=SShorns, family="categorical", prior=prior, verbose=FALSE)
```

In this case we have not interacted sex with trait, and so we are
estimating the difference between the sexes in their expression of
normal and polled+scurred jointly. The posterior distribution is plotted
in Figure [\[MN2\]](#MN2){reference-type="ref" reference="MN2"} and
clearly shows that males are more likely to express the normal horn
phenotype than females.

``` {r label=MN2, echo=FALSE, include=TRUE, fig.cap="Posterior distribution of fixed effects from model \\texttt{m5c.2} in which a main effect of sex was included", fig.width=7, fig.height=5}
plot(m5c.2$Sol)
```

A more general model would be to estimate separate probabilities for
each cell, but the contingency table indicates that one cell (polled
males) has zero counts which will cause extreme separation problems. We
could choose to have a better prior for the fixed effects, that is close
to being flat for the two-way (i.e. polled vs scurred, normal vs.scurred
& polled vs. normal) marginal probabilities within each sex:

``` {r echo=TRUE, cache=TRUE}
prior$B=list(mu=rep(0,4), V=kronecker(IJ,diag(2))*(1.7+pi^2/3))
m5c.3<-MCMCglmm(horn~at.level(sex,1):trait+at.level(sex,2):trait-1, rcov=~us(trait):units, data=SShorns, family="categorical", prior=prior, verbose=FALSE)
```

The female specific probabilities appear reasonable:

``` {r echo=TRUE}
Int<-t(apply(m5c.3$Sol[,1:2],1, function(x){D%*%(x/sqrt(1+c2*diag(IJ)))}))
summary(mcmc(exp(Int)/rowSums(exp(Int))))
```

compared to the observed frequencies:

``` {r echo=TRUE}
prop.table(Ctable[,1])
```

as do the male probabilities:

``` {r echo=TRUE}
Int<-t(apply(cbind(m5c.3$Sol[,3:4]),1, function(x){D%*%(x/sqrt(1+c2*diag(IJ)))}))
summary(mcmc(exp(Int)/rowSums(exp(Int))))
```

compared to the observed frequencies:

``` {r echo=TRUE}
prop.table(Ctable[,2])
```

## Zero-inflated Models

Each datum in a zero-inflated model is associated with two latent
variables. The first latent variable is associated with the named
distribution and the second latent variable is associated with zero
inflation. I'll work through a zero-inflated Poisson (ZIP) model to make
things clearer. As the name suggests, a ZIP distribution is a Poisson
distribution with extra zero's. The observed zeros are modelled as a
mixture distribution of zero's originating form the Poisson process and
zero's arising through zero-inflation. It is the probability (on the
logit scale) that a zero is from the zero-inflation process that we aim
to model with the second latent variable. The likelihood has the form:

$$\begin{array}{rl}
Pr(y=0) =& \texttt{plogis}(l_{2})+\texttt{plogis}(-l_{2})\ast \texttt{dpois}(0, \texttt{exp}(l_{1}))\\
Pr(y | y>0) =& \texttt{plogis}(-l_{2})\ast \texttt{dpois}(y, \texttt{exp}(l_{1}))\\
\end{array}$$

**pscl** fits zero-inflated models very well through the `zeroinfl`
function, and I strongly recommend using it if you do not want to fit
random effects. To illustrate the syntax for fitting ZIP models in
MCMCglmm I will take one of their examples:

``` {r echo=TRUE}
data("bioChemists", package = "pscl")
head(bioChemists)
```

`art` is the response variable - the number of papers published by a
Ph.D student - and the remaining variables are to be fitted as fixed
effects. Naively, we may expect zero-inflation to be a problem given 30%
of the data are zeros, and based on the global mean we only expect
around 18%.

``` {r echo=TRUE}
table(bioChemists$art==0)
ppois(0,mean(bioChemists$art))
```

As with binary models we do not observe any residual variance for the
zero-inflated process, and in addition the residual covariance between
the zero-inflation and the Poisson process cannot be estimated because
both processes cannot be observed in a single data point. To deal with
this I've fixed the residual variance for the zero-inflation at 1, and
the covariance is set to zero using the idh structure. Setting
`V=diag(2)` and `nu=0.002`[^1] we have the inverse-gamma prior with
`shape=scale=0.001` for the residual component of the Poisson process
which captures over-dispersion:

``` {r echo=TRUE, cache=TRUE}
prior.m5d.1=list(R=list(V=diag(2), nu=0.002, fix=2))
m5d.1<-MCMCglmm(art~trait-1+at.level(trait,1):fem + at.level(trait,1):mar + at.level(trait,1):kid5 + at.level(trait,1):phd + at.level(trait,1):ment, rcov=~idh(trait):units, data=bioChemists, prior=prior.m5d.1, family="zipoisson", verbose=FALSE)
```

As is often the case the parameters of the zero-inflation model mixes
poorly (See Figure [\[ZIP\]](#ZIP){reference-type="ref"
reference="ZIP"}) especially when compared to equivalent hurdle models
(See Section [4](#Hurdle){reference-type="ref" reference="Hurdle"}).
Poor mixing is often associated with distributions that may *not* be
zero-inflated but instead over-dispersed.\
``` {r label=ZIP, echo=FALSE, include=TRUE, fig.cap="Posterior distribution of fixed effects from model \\texttt{m5d.1} in which trait 1 (\\texttt{art}) is the Poisson process and trait 2 (\\texttt{zi.art}) is the zero-inflation.", fig.width=7, fig.height=5}
plot(m5d.1$Sol[,1:4])
```

The model would have to be run for (much) longer to say something
concrete about the level of zero-inflation but my guess would be it's
not a big issue, given the probability is probably quite small:

``` {r echo=TRUE}
quantile(plogis(m5d.1$Sol[,2]/sqrt(1+c2)))
```

### Posterior predictive checks

Another useful check is to fit the standard Poisson model and use
posterior predictive checks to see how many zero's you would expect
under the simple model:

``` {r echo=TRUE, cache=TRUE}
prior.m5d.2=list(R=list(V=diag(1), nu=0.002))
m5d.2<-MCMCglmm(art~fem + mar + kid5 + phd + ment, data=bioChemists, prior=prior.m5d.2, family="poisson", saveX=TRUE, verbose=FALSE)

nz<-1:1000
oz<-sum(bioChemists$art==0)

for(i in 1:1000){
pred.l<-rnorm(915, (m5d.2$X%*%m5d.2$Sol[i,])@x, sqrt(m5d.2$VCV[i]))
nz[i]<-sum(rpois(915, exp(pred.l))==0)
}
```

Figure [\[PPZIP\]](#PPZIP){reference-type="ref" reference="PPZIP"} shows
a histogram of the posterior predictive distribution of zero's (`nz`)
from the model compared to the observed number of zeros (`oz`). The
simpler model seems to be consistent with the data, suggesting that a
ZIP model may not be required.

``` {r label=PPZIP, echo=FALSE, include=TRUE, fig.cap="Posterior predictive distribution of zeros from model \\texttt{m5d.2} with the observed number in red.", fig.width=7, fig.height=5}
hist(nz, breaks=30)
abline(v=oz, col="red", lwd=2)
```

## Hurdle Models {#Hurdle}

Hurdle models are very similar to zero-inflated models but they can be
used to model zero-deflation as well as zero-inflation and seem to have
much better mixing properties in `MCMCglmm`. As in ZIP models each datum
in the hurdle model is associated with two latent variables. However,
whereas in a ZIP model the first latent variable is the mean parameter
of a Poisson distribution the equivalent latent variable in the hurdle
model is the mean parameter of a zero-truncated Possion distribution
(i.e. a Poisson distribution without the zeros observed). In addition
the second latent variable in a ZIP model is the probability that an
observed zero is due to zero-inflation rather than the Poisson process.
In hurdle models the second latent variable is simply the probability
(on the logit scale) that the response variable is zero or not. The
likelihood is:

$$\begin{array}{rl}
Pr(y=0) =& \texttt{plogis}(l_{2})\\
Pr(y | y>0) =& \texttt{plogis}(-l_{2})\ast \texttt{dpois}(y, \texttt{exp}(l_{1}))/(1-\texttt{ppois}(0, \texttt{exp}(l_{1})))\\
\end{array}$$

To illustrate, we will refit the ZIP model (`m5d.1`) as a hurdle-Poisson
model.

``` {r echo=TRUE, cache=TRUE}
m5d.3<-MCMCglmm(art~trait-1+at.level(trait,1):fem + at.level(trait,1):mar + at.level(trait,1):kid5 + at.level(trait,1):phd + at.level(trait,1):ment, rcov=~idh(trait):units, data=bioChemists, prior=prior.m5d.1, family="hupoisson", verbose=FALSE)
```

Plotting the Markov chain for the equivalent parameters that were
plotted for the ZIP model shows that the mixing properties are much
better (compare Figure [\[ZIP\]](#ZIP){reference-type="ref"
reference="ZIP"} with Figure [\[HU\]](#HU){reference-type="ref"
reference="HU"}).\
``` {r label=HU, echo=FALSE, include=TRUE, fig.cap="Posterior distribution of fixed effects from model \\texttt{m5d.3} in which trait 1 (\\texttt{art}) is the zero-truncated Poisson process and trait 2 (\\texttt{hu.art}) is the binary trait zero or non-zero.", fig.width=7, fig.height=5}
plot(m5d.3$Sol[,1:4])
```

The interpretation of the model is slightly different. Fitting just an
intercept in the hurdle model implies that the proportion of zeros
observed across different combinations of those fixed effects fitted for
the Poisson process is constant. Our 95% credible intervals for this
proportion is (See section
[\[pred-sec\]](#pred-sec){reference-type="ref" reference="pred-sec"}):

``` {r echo=TRUE}
c2<-(16*sqrt(3)/(15*pi))^2
HPDinterval(plogis(m5d.3$Sol[,2]/sqrt(1+c2))) 
```

and we can compare this to the predicted number of zero's from the
Poisson process if it had not been zero-truncated:

``` {r echo=TRUE}
HPDinterval(ppois(0, exp(m5d.3$Sol[,1]+0.5*m5d.3$VCV[,1])))  
```

The credible intervals largely overlap, strongly suggesting a standard
Poisson model would be adequate. However, our prediction for the number
of zero's that would arise form a non-truncated Poisson process only
involved the intercept term. This prediction therefore pertains to the
number of articles published by single women with no young children who
obtained their Ph.D's from departments scoring zero for prestige (`phd`)
and whose mentors had published nothing in the previous 3 years. Our
equivalent prediction for men is a little lower

``` {r echo=TRUE}
HPDinterval(ppois(0, exp(m5d.3$Sol[,1]+m5d.3$Sol[,3]+0.5*m5d.3$VCV[,1])))  
```

suggesting that perhaps the number of zero's is greater than we expected
for this group. However, this may just be a consequence of us fixing the
proportion of zero's to be constant across these groups. We can relax
this assumption by fitting a separate term for the proportion of zeros
for men:

``` {r echo=TRUE, cache=TRUE}
m5d.4<-MCMCglmm(art~trait-1+at.level(trait,1:2):fem+at.level(trait,1):mar + at.level(trait,1):kid5 + at.level(trait,1):phd + at.level(trait,1):ment, rcov=~idh(trait):units, data=bioChemists, prior=prior.m5d.1, family="hupoisson", verbose=FALSE)
```

which reveals that although this proportion is expected to be (slightly)
smaller:

``` {r echo=TRUE}
HPDinterval(plogis((m5d.4$Sol[,2]+m5d.4$Sol[,4])/sqrt(1+c2))) 
```

the proportion of zeros expected for men is probably still less than
what we expect from a non-truncated Poisson process for which the
estimates have changed very little:

``` {r echo=TRUE}
HPDinterval(ppois(0, exp(m5d.4$Sol[,1]+m5d.4$Sol[,3]+0.5*m5d.4$VCV[,1])))  
```

This highlights one of the disadvantages of hurdle models. If
explanatory variables have been fitted that affect the expectation of
the Poisson process then this implies that the proportion of zero's
observed will also vary across these same explanatory variables, even in
the absence of zero-inflation. It may then be necessary to fit an
equally complicated model for both processes even though a single
parameter would suffice in a ZIP model. However, in the absence of
zero-inflation the intercept of the zero-inflation process in a ZIP
model is $-\infty$ on the logit scale causing numerical and inferential
problems. An alternative type of model are zero-altered models.

## Zero-altered Models {#ZAP}

Zero-altered Poisson (ZAP) models are identical to Poisson-hurdle models
except a complementary log-log link is used instead of the logit link
when modeling the proportion of zeros. However for reasons that will
become clearer below, the zero-altered process (`za`) is predicting
non-zeros as opposed to the ZIP and hurdle-Poisson models where it is
the number of zeros. The likelihood is:

$$\begin{array}{rl}
Pr(y=0) =& 1-\texttt{pexp}(\texttt{exp}(l_{2}))\\
Pr(y | y>0) =& \texttt{pexp}(\texttt{exp}(l_{2}))\ast \texttt{dpois}(y, \texttt{exp}(l_{1}))/(1-\texttt{ppois}(0, \texttt{exp}(l_{1})))\\
\end{array}$$

since the inverse of the complementary log-log transformation is the
distribution function of the extreme value (log-exponential)
distribution.\
It happens that
$\texttt{ppois}(0,\texttt{exp}(l)) = \texttt{dpois}(0,\texttt{exp}(l)) = 1-\texttt{pexp}(\texttt{exp}(l))$
so that if $l = l_{1} = l_{2}$ then the likelihood reduces to:

$$\begin{array}{rl}
Pr(y=0) =& \texttt{dpois}(0,\texttt{exp}(l))\\
Pr(y | y>0) =& \texttt{dpois}(y, \texttt{exp}(l))\\
\end{array}$$

which is equivalent to a standard Poisson model.\
We can then test for zero-flation by constraining the over-dispersion to
be the same for both process using a `trait` by `units` interaction in
the R-structure, and by setting up the contrasts so that the
zero-altered regression coefficients are expressed as differences from
the Poisson regression coefficients. When this difference is zero the
variable causes no zero-flation, when it is negative it causes
zero-inflation and when it is positive it causes zero-deflation:

``` {r echo=TRUE, cache=TRUE}
m5d.5<-MCMCglmm(art~trait*(fem +mar + kid5 + phd + ment), rcov=~trait:units, data=bioChemists, family="zapoisson", verbose=FALSE)
summary(m5d.5)
```

we can see from this that the more papers a mentor produces, the more
zero-deflation (or conversely the less papers a mentor produces, the
more zero-inflation).

[^1]: Earlier versions of the CourseNotes had `nu=1.002`. In versions
    $<$`<!-- -->`{=html}2.05 the marginal prior of a variance associated
    with an `idh` structure was inverse-Wishart with
    $\texttt{nu}^{\ast}=\texttt{nu}-1$ where $\texttt{nu}^{\ast}$ is the
    marginal degree of belief. In versions $>=$`<!-- -->`{=html}2.05 I
    changed this so that $\texttt{nu}^{\ast}=\texttt{nu}$ as it was
    leading to confusion.
