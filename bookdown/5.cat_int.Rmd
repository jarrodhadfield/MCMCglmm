# Categorical Random Interactions {#cat-int}

In Chapter \@ref(ranef) we looked at a simple random-effect specification where a single effect is associated with each level of a categorical predictor and the effects are assumed to be identically and independently distributed. In the next three chapters we will cover more complicated random-effect specifications. In this Chapter we will start by exploring models where an interaction is formed between two categorical predictors, one which (usually) has few levels with main effects fitted as fixed and the other which (usually) has many levels and would normally be fitted as random. We will also cover related models where the random effects associated with two or more sets of categorical predictors (including residual effects) are correlated. This includes multi-membership models as a special case. 

It is probably easiest to start with an example. The data set $\texttt{sigma}$ contains data on how well fruit flies (*Drosophila melanogaster*) transmit sigma virus to their offspring [@Carpenter.2012]. 

```{r }
data(sigma)
head(sigma)
```

The number of flies in a vial that are infected with the virus ($\texttt{infected}$) or not ($\texttt{not_infected}$) are recorded. The mean number of flies per vial is `r round(mean(sigma$infected+sigma$not_infected))` although the range is large (`r min(sigma$infected+sigma$not_infected)` - `r max(sigma$infected+sigma$not_infected)` ). All the flies in a vial are the offspring of parents infected with one of five strains ($\texttt{strain}$) of sigma virus, a virus that is transmitted to offspring through eggs and sperm. The parents belong to one of 67 genetically distinct lines ($\texttt{line}$).  Let's start with a simple binomial GLMM that we covered in Chapter \@ref(ranef), but to keep things manageable we'll only analyse data from three strains ($\texttt{France}$, $\texttt{Greece}$ and  $\texttt{Spain}$, named after their country of origin):

```{r sigma-1}
sigma_small<-subset(sigma, virus%in%c("France","Greece", "Spain"))

prior.sigma.1=list(R=list(V=1, nu=0.002), 
           G=list(
              G1=list(V=1, nu=1, aplha.mu=0, aplha.V=1000)
           ))

m.sigma.1<-MCMCglmm(cbind(infected, not_infected)~virus, random=~line, data=sigma_small, family="multinomial2", prior=prior.sigma.1)

summary(m.sigma.1)
```

For the base-line virus, $\texttt{France}$, the estimate of the median probability of infection is `r round(mean(plogis(m.sigma.1$Sol[,"(Intercept)"])),2)`, obtained by applying the `plogis` function to the intercept (Section \@ref(binom-pred-sec)). However, the viruses have substantially different infection rates with the median *odds* of infection for $\texttt{Greece}$ being only `r round(mean(exp(m.sigma.1$Sol[,"virusGreece"])),2)` times that of $\texttt{France}$, whereas $\texttt{Spain}$ has an odds of infection `r round(mean(exp(m.sigma.1$Sol[,"virusSpain"])),2)` times greater (obtained by exponentiating the coefficients). 

There is also substantial variation across lines in how well they transmit the virus (posterior mean for $\sigma^2_\texttt{line}$= `r round(mean(m.sigma.1$VCV[,"line"]), 2)`) and substantial overdispersion (posterior mean for $\sigma^2_\texttt{units}$= `r round(mean(m.sigma.1$VCV[,"units"]), 2)`). However, this model contains the implicit assumption that the increased odds of infection associated with a line is constant over viruses. For example, the first level of $\texttt{line}$ is $\texttt{101A}$. If this line's effect is $u^{(\texttt{line})}_1$ then the linear predictor for all observations of that line will include $u^{(\texttt{line})}_1$ and the odds of infection is expected to change by a factor $\textrm{exp}(u^{(\texttt{line})}_1)$. However, we may expect there to be genetic interactions between the virus and the fly such that the infectivity of a line differs depending on the virus.  We could add a simple interaction between line and variance and treat these ($67\times 3 = 201$) interaction effects as random: 


```{r sigma-2}
prior.sigma.2=list(R=list(V=1, nu=0.002), 
           G=list(
              G1=list(V=1, nu=1, aplha.mu=0, aplha.V=1000),
              G2=list(V=1, nu=1, aplha.mu=0, aplha.V=1000)
           ))

m.sigma.2<-MCMCglmm(cbind(infected, not_infected)~virus, random=~line+virus:line, data=sigma_small, family="multinomial2", prior=prior.sigma.2)

summary(m.sigma.2)
```

While the variance in the interaction effects is substantially smaller than the main $\texttt{line}$ effects (posterior mean $\sigma^2_\texttt{virus:line}=$ `r round(mean(m.sigma.2$VCV[,"virus:line"]),2)`), the lower 95\% credible interval is well removed from zero suggesting that interactions are present.  This model still has implicit assumptions baked into it about how lines vary in their infectivity for a given virus, and how lines covary in their infectivity between viruses. To avoid complications later, I will - with some abuse of notation - call the main $\texttt{line}$ effects in this model $u^{(\texttt{m-line})}$ and their variance $\sigma^2_\texttt{m-line}$, with posterior mean `r round(mean(m.sigma.2$VCV[,"line"]),2)`.  The *aggregate* effect of $\texttt{line}$ $i$ infected with $\texttt{virus}$ $j$ is then given by:


$$ u^{(\texttt{line})}_{ij}  = u^{(\texttt{m-line})}_i+u^{(\texttt{virus:line})}_{ij}$$ 

If we took the linear predictor for two observations made on the same line in the same virus they would both share the same two random effects and therefore have the same $u^{(\texttt{line})}_{ij}$. Consequently, the covariance in their linear predictor is the variance of $u^{(\texttt{line})}$: $\sigma^2_\texttt{m-line}+\sigma^2_\texttt{virus:line}$. Since these two variances are assumed constant with with respect to virus, we are implicitly assuming that the variance between-lines is the same for all viruses. If we took the linear predictor for two observations made on the same line but in different viruses they would both share the same $u^{(\texttt{m-line})}$ effect but have different interaction effects ($u^{(\texttt{virus:line})}$). Consequently, the covariance in their linear predictor is the variance of $u^{(\texttt{m-line})}$ only: $\sigma^2_\texttt{m-line}$. Again, $\sigma^2_\texttt{m-line}$ is assumed constant over viruses and so this implies that the between-line covariance is also constant: the covariance between lines infected with $\texttt{France}$ versus $\texttt{Spain}$ is the same as the covariance between lines infected with $\texttt{France}$ versus $\texttt{Greece}$ or $\texttt{Spain}$ versus $\texttt{Greece}$. This constancy is also reflected in the between-line correlation (which has a posterior mean of `r round(mean(m.sigma.2$VCV[,"line"]/rowSums(m.sigma.2$VCV[,c("virus:line", "line")])),2)`) :


$$r_{\texttt{line}}= \frac{\sigma^2_\texttt{m-line}}{\sigma^2_\texttt{m-line}+\sigma^2_\texttt{virus:line}}$$


Since $\sigma^2_\texttt{m-line}$ is a variance, and therefore constrained to be positive, we are also implicitly assuming that the covariance and correlation are both constant and positive: a line that shows greater infectivity with $\texttt{France}$ are not expected, on average, to have reduced infectivity with $\texttt{Spain}$. We can think of summarising this information in a $3\times 3$ between-line covariance matrix (the meaning of the colours will become apparent soon):


$${\bf V}_{{\color{red}{\texttt{line}}}}=
\left[
\begin{array}{ccc}
\sigma^{2}_{\color{blue}{\texttt{France}}}&\sigma_{\color{blue}{\texttt{France}, \texttt{Spain}}}&\sigma_{\color{blue}{\texttt{France}, \texttt{Greece}}}\\
\sigma_{\color{blue}{\texttt{France}, \texttt{Spain}}}&\sigma^{2}_{\color{blue}{\texttt{Spain}}}&\sigma_{\color{blue}{\texttt{Spain}, \texttt{Greece}}}\\
\sigma_{\color{blue}{\texttt{France}, \texttt{Greece}}}&\sigma_{\color{blue}{\texttt{Spain}, \texttt{Greece}}}&\sigma^{2}_{\color{blue}{\texttt{Greece}}}\\
\end{array}
\right]$$

The diagonal elements gives us the variance in $\texttt{line}$ effects for each virus and the off-diagonal elements gives us the covariance in $\texttt{line}$ effects between pairs of viruses. This covariance matrix has 6 (co)variances, but in model `m.sigma.2` we assumed that these (co)variances could be parametrised using only two parameters: all diagonal elements are equal to $\sigma^2_\texttt{m-line}+\sigma^2_\texttt{virus:line}$ and all off-diagonal elements are equal to $\sigma^2_\texttt{m-line}$. If we wish to relax this assumption we need to understand how the variance functions, such as `us()` and `idh()`, work.

## Variance Structures {#vstruct-sec}

We could refit our first model `m.sigma.2` using the random effect specifications:

``` {r echo=TRUE}
random=~us(1):line
```

or 

``` {r echo=TRUE}
random=~idh(1):line
```

and this would give exactly the same answer as the model specified by `~line`. The term inside the brackets is a model formula and is interpreted exactly how you would interpret any R formula expect the intercept is only fitted if it is explicitly defined (as here).  These formula are therefore fitting an intercept which is interacted with the random effects. We can get a representation of the interaction for the first few levels of `line` (`r levels(sigma$line)[1:5]`):

$$
\begin{array}{c|rrrrrc}
&{\color{red}{\texttt{101A}}}&{\color{red}{\texttt{109A}}}&{\color{red}{\texttt{129A}}}&{\color{red}{\texttt{142A}}}&{\color{red}{\texttt{153A}}}&\dots\\
\hline
{\color{blue}{\texttt{(1)}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{153A}}}&\dots\\
\end{array}
$$

Across the top, we have the original $\texttt{line}$ effects in red, and along the side we have the term defined by the variance structure formula (just the intercept in this case). The interaction forms a new set of factors. Although they have different names from the original $\texttt{line}$ effects, it is clear that there is a one to one mapping between the original and the new factor levels and the models are therefore equivalent. For more complex interactions this is not the case. For example, if we fit $\texttt{virus}$ in the variance structure model, (i.e. `us(virus):line` or `idh(virus):line`) we get[^4.1]:


$$\begin{array}{c|rrrrrc}
&{\color{red}{\texttt{101A}}}&{\color{red}{\texttt{109A}}}&{\color{red}{\texttt{129A}}}&{\color{red}{\texttt{142A}}}&{\color{red}{\texttt{153A}}}&\dots\\
\hline
{\color{blue}{\texttt{France}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{153A}}}&\dots\\
{\color{blue}{\texttt{Spain}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{153A}}}&\dots\\
{\color{blue}{\texttt{Greece}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{153A}}}&\dots\\
\end{array}$$


Across the top, we have the original $\texttt{line}$ effects in red, and along the side we have the term defined by the variance structure formula (just the intercept in this case). The interaction forms a new set of factors. Although they have different names from the original $\texttt{line}$ effects, it is clear that there is a one to one mapping between the original and the new factor levels and the models are therefore equivalent. For more complex interactions this is not the case. For example, if we fit $\texttt{virus}$ in the variance structure model, (i.e. `us(virus):line` or `idh(virus):line`) we get[^4.1]:

$$\begin{array}{c|rrrrrc}
&{\color{red}{\texttt{101A}}}&{\color{red}{\texttt{109A}}}&{\color{red}{\texttt{129A}}}&{\color{red}{\texttt{142A}}}&{\color{red}{\texttt{153A}}}&\dots\\
\hline
{\color{blue}{\texttt{France}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{153A}}}&\dots\\
{\color{blue}{\texttt{Spain}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{153A}}}&\dots\\
{\color{blue}{\texttt{Greece}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{153A}}}&\dots\\
\end{array}$$

which creates three times as many random effects, one associated with observations for each $\texttt{virus}$ for each each  $\texttt{line}$.

### $\texttt{idh}$ Variance Structure

The different variance functions make different assumptions about how the effects in associated with these different factors are distributed. First, we may want to allow the variance in the effects to be different for each row of factors; i.e. does $\texttt{line}$ explain different amounts of variation depending on the $\texttt{virus}. We can fit this model using the `idh` function:


```{r sigma-3}
prior.sigma.3=list(R=list(V=1, nu=0.002), 
           G=list(
              G1=list(V=diag(3), nu=1, aplha.mu=c(0,0,0), aplha.V=diag(3)*1000)
           ))

m.sigma.3<-MCMCglmm(cbind(infected, not_infected)~virus, random=~idh(virus):line, data=sigma_small, family="multinomial2", prior=prior.sigma.3)

```


In the simpler models we have fitted so far, each random effect term (terms separated by `+` in the `random` formula) specified a single variance (e.g. $\sigma^_{\texttt{virus:line}}$) and the prior specification was relatively simple and covered in Sections \@ref(Vprior-sec) ad \@ref(PXprior-sec). We discuss prior specifications for covariance matrices in Section \@ref(VCVprior-sec), but for now, note that the prior specification for the `idh(virus):line` involves $3\times 3$ matrices. If we take a look at the model summary:

```{r }
summary(m.sigma.3)
```

we see that three parameters are summarised for `~idh(virus):line`: the variance in line effects for each virus. While the $\texttt{idh}$ variance function allows the variances to be different across the viruses it assumes that line effects for one virus are independent of the line effects for a different virus. In terms of our coloured table of effects  $\texttt{idh}$ assumes effects in different rows are independently distributed, but it relaxes the assumption that they are identically distributed - they can have different variances.  We can represent this structure in terms of a $3\times3$ covariance matrix:

$${\bf V}_{{\color{red}{\texttt{line}}}}=
\left[
\begin{array}{ccc}
\sigma^{2}_{\color{blue}{\texttt{France}}}&0&0\\
0&\sigma^{2}_{\color{blue}{\texttt{Spain}}}&0\\
0&0&\sigma^{2}_{\color{blue}{\texttt{Greece}}}\\
\end{array}
\right]$$

We can extract the posterior means for each variance and place them into a matrix:

``` {r echo=TRUE, eval=TRUE}
Vline.idh<-diag(colMeans(m.sigma.3$VCV)[1:3])
colnames(Vline.idh)<-rownames(Vline.idh)<-c("France", "Spain", "Greece")
Vline.idh
```

Because the $\texttt{line}$ effects are assumed to be multivariate normal we can also represent their distribution in terms of an ellipsoid: 

```{r label=rgl, echo=TRUE, include=TRUE, webgl=TRUE, cache=FALSE}
plotsubspace(Vline.idh, axes.lab=TRUE)
```
Widget 4.1: Ellipsoid that circumscribes 95\% of the expected $\texttt{line}$ effects as estimated in model `m.sigma.3`. This can be thought of as a scatter plot of the $\texttt{line}$ effects between each virus$, if the $\texttt{line}$ effects could be directly measured.  Because the covariances of the $\texttt{line}$ effects between each virus were set to zero, and the variance of the line effects are quite similar for each virus, the ellipsoid is almost spherical.

Although we have allowed the variance in $\texttt{line}$ effects to be different across the viruses, we have used the default specification for the residual structure (`rcov=~units`). $\texttt{MCMCglmm}$ augments the original data-frame with an additional column called $\texttt{units}$ which is a factor that has unique levels for each row. This is covered in greater depth when discussing multi-response models (Chapter \@ref(multi)) but for the single-response model used here we can simply think of $\texttt{units}$ as indexing residuals. Having a constant residual variance for each virus but allowing the between-line variance to vary is somewhat dangerous: if the residual variances did vary then this may be reflected in the estimates of the between-line variances. Allowing the residual variances to also vary is straightforward:

```{r sigma-4}
prior.sigma.4=list(R=list(V=diag(3), nu=0.002), 
           G=list(
              G1=list(V=diag(3), nu=1, aplha.mu=c(0,0,0), aplha.V=diag(3)*1000)
           ))

m.sigma.4<-MCMCglmm(cbind(infected, not_infected)~virus, random=~idh(virus):line, rcov=~idh(virus):units, data=sigma_small, family="multinomial2", prior=prior.sigma.4)

summary(m.sigma.4)
```

### $\texttt{us}$ Variance Structure {#us-sec}

The $\texttt{idh}$ structure allowed the variance in $\texttt{line}$ effects to be different for different viruses, but it assumes that the effect of a line for one virus is uncorrelated with the effect of that line in another virus. However, the strong correlation (`r round(mean(m.sigma.2$VCV[,"line"]/rowSums(m.sigma.2$VCV[,c("virus:line", "line")])),2)`) in $\texttt{line}$ effects estimated from model `m.sigma.2` suggests that this assumption is likely to be unreasonable.  We can relax this assumption by using the `us` function which estimates the fully parameterised matrix seen earlier:

```{r sigma-5}
prior.sigma.5=list(R=list(V=diag(3), nu=0.002), 
           G=list(
              G1=list(V=diag(3), nu=3, aplha.mu=c(0,0,0), aplha.V=diag(3)*1000)
           ))

m.sigma.5<-MCMCglmm(cbind(infected, not_infected)~virus, random=~us(virus):line, rcov=~idh(virus):units, data=sigma_small, family="multinomial2", prior=prior.sigma.5)

summary(m.sigma.5)
```

The numbers rows displayed for the line structure correspond to the 9 elements of the $3\times 3$ covariance matrix (displayed column-wise). As before, we can arrange the posterior mean estimates in to a matrix 

``` {r echo=TRUE, eval=TRUE}
Vline.us<-matrix(colMeans(m.sigma.5$VCV)[1:9], 3, 3)
colnames(Vline.us)<-rownames(Vline.us)<-c("France", "Spain", "Greece")
Vline.us
```

We can see that the covariances are large, and like the variances, relatively constant. If we visualise this distribution we can see it is quite different from that obtained from model $\texttt{m.sigma.3}$ (or model $\texttt{m.sigma.3}$) where the covariances are assumed to be zero.

```{r label=rgl2, fig=-TRUE, echo=TRUE, eval=TRUE, webgl=TRUE, cache=FALSE}

plotsubspace(Vline.us, axes.lab=TRUE)
```
Widget 4.2: Ellipsoid that circumscribes 95% of the expected $\texttt{line}$ effects as estimated in model $\texttt{m.sigma.5}$. This can be thought of as a scatter plot of the $\texttt{line}$ effects between virus, if the $\texttt{line}$ effects could be directly measured.  The correlations of the $\texttt{line}$ effects between viruses are large, and the variance in $\texttt{line}$ effects are roughly equal in magnitude across viruses.  Consequently the orientation of the major axis of the ellipsoid is tending towards $45^{o}$ relative to the figure axes.


We can use the function `posterior.cor` to convert the posterior samples of the (co)variance matrix into posterior samples of the correlation matrix. Doing so we can see that the correlation is indeed large and relatively invariant.

``` {r echo=TRUE, eval=TRUE}
rline.us<-matrix(colMeans(posterior.cor(m.sigma.5$VCV[,1:9])), 3, 3)
colnames(rline.us)<-rownames(rline.us)<-c("France", "Spain", "Greece")
rline.us
```

In this particular example, it looks like the simpler two-parameter model implemented in model `m.sigma.2` may do a good job of representing the covariance matrix: 

``` {r echo=TRUE, eval=TRUE}
Vline.2<-matrix(mean(m.sigma.2$VCV[,"line"]), 3, 3)
diag(Vline.2)<-diag(Vline.2)+mean(m.sigma.2$VCV[,"virus:line"])
colnames(Vline.2)<-rownames(Vline.2)<-c("France", "Spain", "Greece")
Vline.2
```

and we can see this if we visualise the two distributions simultaneously:


```{r label=rgl3, fig=-TRUE, echo=TRUE, eval=TRUE, webgl=TRUE, cache=FALSE}
plotsubspace(Vline.us, Vline.2, axes.lab=TRUE, shadeCA=FALSE)
```
Widget 4.3: Ellipsoids that circumscribes 95% of the expected $\texttt{line}$ effects as estimated in model $\texttt{m.sigma.5}$ where all (co)variance parameters are estimated (red wireframe) and model $\texttt{m.sigma.2}$ where the variances across viruses, and the correlations between viruses, are assumed constant (solid blue).

Note that in this model we retained the $\texttt{idh}$ structure for the residuals. Since the flies in each vial (indexed by $\texttt{unit}$ since they correspond to a row of the data-frame) are all exposed to the same virus the residual covariances between viruses cannot be estimated and so can be set to zero. In Chapter \@ref(multi) we will look at multi-response models where this generally isn't the case. 

### Other Variance Structures

In some cases, additional restrictions need to be placed on a covariance matrix. In Section \@ref(bernoulli-sec) we used the prior argument $\texttt{fix}$ to force the residual variance to be fixed at a specified value in a Bernoulli GLM. When a variance structure defines a (co)variance matrix, $\texttt{fix}$ specifies a diagonal element of the matrix that splits the matrix into four quadrants. For example, imagine we have the $5\times 5$ matrix specified in the $\texttt{V}$ element of the prior:

$$\texttt{V}=
\left[
\begin{array}{cc|ccc}
1&0.5&0.5&0&0\\
0.5&1&0.5&0&0\\
\hline
0.5&0.5&2&0&0\\
0&0&0&2&1\\
0&0&0&1&3\\
\end{array}
\right]$$

If $\texttt{fix}=3$ then the matrix is split into four quardants with the lower right quadrant starting on the 3rd diagonal element. This quadrant will be fixed at the value specified in $\texttt{V}$ but all remaining (co)variance parameters will be estimated (if a $\texttt{us}$ structure is used). Sometimes constraints are required that can not be achieved by using $\texttt{fix}$ and cannot be expressed in terms of the (re)parameterisations discussed above. Table \@ref(tab:vfunction) provides a summary of available variance functions and $\texttt{ante}$ structures - which give the greatest flexibility - are covered in Section \@ref(ante-sec)).

``` {r echo=FALSE, results='asis'}
df <- data.frame(
  MCMCglmm = c("`line`", "`us(virus):line`", "`virus:line`", "`line+virus:line`", "`idh(virus):line`", "`corg(virus):line`", "`corgh(virus):line`", "`cors(virus):line`"),
  lmer = c("`(1|line)`", "`(virus|line)`", "`(1|virus:line)`", "`(1|line)+(1|virus:line)`", "", "", "", ""),
  nparameters = c(1,6,1,2,3,3,3, 4),
  Variance      = c("$\\left[\\begin{array}{ccc}V&V&V\\\\V&V&V\\\\V&V&V\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V_{1,1}&C_{1,2}&C_{1,3}\\\\C_{2,1}&V_{2,2}&C_{2,3}\\\\C_{3,1}&C_{3,2}&V_{3,3}\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V&{\\color{red}{0}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&V&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{0}}&V\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V_1+V_2&V_1&V_1\\\\V_1&V_1+V_2&V_1\\\\V_1&V_1&V_1+V_2\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V_{1,1}&{\\color{red}{0}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&V_{2,2}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{0}}&V_{3,3}\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}{\\color{red}{V_{1,1}}}&r_{1,2}{\\color{red}{\\sqrt{V_{1,1}V_{2,2}}}}&r_{1,3}{\\color{red}{\\sqrt{V_{1,1}V_{3,3}}}}\\\\r_{2,1}{\\color{red}{\\sqrt{V_{2,2}V_{1,1}}}}&{\\color{red}{V_{2,2}}}&r_{2,3}{\\color{red}{\\sqrt{V_{2,2}V_{3,3}}}}\\\\r_{3,1}{\\color{red}{\\sqrt{V_{3,3}V_{1,1}}}}&r_{3,2}{\\color{red}{\\sqrt{V_{3,3}V_{2,2}}}}&{\\color{red}{V_{3,3}}}\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V_{1,1}&C_{1,2}&C_{1,3}\\\\C_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\C_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$"
                    ),
  Correlation      = c("$\\left[\\begin{array}{ccc}{\\color{red}{1}}&{\\color{red}{1}}&{\\color{red}{1}}\\\\{\\color{red}{1}}&{\\color{red}{1}}&{\\color{red}{1}}\\\\{\\color{red}{1}}&{\\color{red}{1}}&{\\color{red}{1}}\\\\ \\end{array}\\right]$", 
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&{\\color{red}{0}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{1}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{0}}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r&r\\\\r&{\\color{red}{1}}&r\\\\r&r&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&{\\color{red}{0}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{1}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{0}}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$"
                      ),
  stringsAsFactors = FALSE
)

kable(df, "html", escape = FALSE, caption = "Different random effect specifications in $\\texttt{MCMCglmm}$ (with equivalent $\\texttt{lmer}$ syntax when possible) with fixed parameters in black and estimated parameters in red. $\\texttt{virus}$ is a factor with three levels so the resulting covariance and correlation matrices are $3\\times3$. For the `cors(virus):line` structure, the prior specification had `fix=2` which forces the last diagonal block (starting from position 2,2) to be a correlation matrix rather than a covariance matrix. `fix` can also be specified for other structures but then it fixes the last diagonal block to what ever is specified in the `V` element of the prior.  `ante` structures are not covered here but in Section \\@ref(ante-sec)", label="vfunction") |>
  kable_styling(full_width = FALSE)%>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```

## Linking Functions

In some models we would like to estimate covariances between sets of random effects (or random effects and residuals) in a way that is conceptually very similar to that discussed in Section \@ref(vstruct-sec) using variance structures. However, the way that we would like the (co)variances to be set up cannot be achieved by forming interactions. This happens when the predictors associated with the two or more sets of random effects are in different columns of the data frame, despite sharing the same set of levels, and we need some way to link them. Again, this is perhaps best illustrated through an example. 

### $\texttt{str}$: covariances between random terms 

My mum is a great mum. She's also a super grandmother to my children, and I wonder whether these two things are correlated[^cant_int.4].  Let's say we could measure the well-being for many children over several generations, and importantly we have multiple children from the same mother so we can estimate $\texttt{mother}$ effects. As importantly, some of these mothers go onto to be grandmothers of their children's children. If we also have measures of well-being for these children we can also estimate $\texttt{grandmother}$ effects. However, it's not clear how we could specify a covariance between these effects in the same way that we did using variance structures. Our hypothetical data-frame may look like this:

```{r echo=FALSE}
n<-500
mother<-as.factor(sample(letters, n, replace=TRUE))
grandmother<-as.factor(sapply(mother, function(x){sample(setdiff(letters, x), 1)}))

V<-matrix(0.5, 2, 2)
diag(V)<-c(2,1)

u<-MASS::mvrnorm(26, c(0,0), V)

y<-u[mother,1]+u[grandmother,2]+rnorm(n, 0, sqrt(4))

data.nana<-data.frame(y=y, mother=mother, grandmother=grandmother)
```

```{r echo=FALSE}
head(data.nana)
```

I have ensured that the $\texttt{mother}$ and $\texttt{grandmother}$ columns have the same factor levels using the $\texttt{levels}$ argument of the function $\texttt{as.factor}$. It is not necessary that *all* mothers appear as grandmothers, or vice versa, but of course *some* mothers must appear as grandmothers if we wish to estimate the correlation in their effects on a child's well-being. The linking function $\texttt{str}$ links two or more sets of random effects and assumes a fully unstructured covariance matrix for their distribution, as seen with a $\texttt{us}$ structure  (Section \@ref(us-sec)).  


```{r }
prior.str=list(R=list(V=1, nu=0.002), 
           G=list(
              G1=list(V=diag(2), nu=2, aplha.mu=c(0,0), aplha.V=diag(2)*1000)
           ))

m.str<-MCMCglmm(y~1, random=~str(mother+grandmother), data=data.nana, prior=prior.str)
```

The model output shows that a $2\times 2$ covariance matrix has been estimated for the $\texttt{mother}$/$\texttt{grandmother}$ effects.

```{r }
summary(m.str)
```

When simulating the data I assumed the variance in mother effects was 2, the variance in grandmother effects was 1 and the covariance between them was 0.5 (giving a correlation of `r round(0.5/sqrt(2), 2)`). The data set contained few (grand)mothers and so the estimates are uncertain, but the credible intervals overlap the true values. 

### $\texttt{mm}$: Multi-membership Models

Using $\texttt{str}$ allowed us to link random effects associated with categorical predictors appearing in different columns of our data frame. The effects defined by different columns were allowed to have different variances (the variance in mother effects was different from the variance in grandmother effects) and the correlation was estimated. In some cases we would like to treat the effects defined by the different columns as the same effects when their levels agree. In the $\texttt{str}$ example we did not want to do this because being a mother is not the same thing as being a grandmother. However, imagine that each of our children has five friends:


```{r echo=FALSE}
n<-500

y<-rnorm(n, 0, sqrt(2))

friend<-t(sapply(1:n, FUN=function(x){sample(LETTERS, 5)}))

data.friends<-data.frame(y=y, friend=friend)

u<-rnorm(26,0, sqrt(0.1))


for(i in 1:5){
data.friends[,i+1]<-factor(data.friends[,i+1], levels=LETTERS)
data.friends$y<-data.friends$y+u[as.numeric(data.friends[,i+1])]
}


```

```{r echo=FALSE}
head(data.friends)
```

Friend `r names(which.max(table(unlist(data.friends[1:6,-1]))))` appears as a friend to multiple children and we would lie to the model that friend's effect on the focal child's well-being irrespective of whether `r names(which.max(table(unlist(data.friends[1:6,-1]))))` appears as $\texttt{friend.1}$, $\texttt{friend.2}$ ... or $\texttt{friend.5}$ as the numbering is arbitrary.  The linking function $\texttt{mm}$ allows us to specify this:


```{r }
prior.mm=list(R=list(V=1, nu=0.002), 
           G=list(
              G1=list(V=1, nu=1, aplha.mu=0, aplha.V=1000)
           ))

m.mm<-MCMCglmm(y~1, random=~mm(friend.1+friend.2+friend.3+friend.4+friend.5), data=data.friends, prior=prior.mm)
```

```{r }
summary(m.mm)
```

The variance in $\texttt{friend}$ effects was set to 0.1 in the simulated data. While small compared to the simulated residual variance (2) the aggregate effect of all friends if five times this: 0.5. Note that if children varied in how many friends they had, the number of 'friend' columns can be set to the maximum and children with fewer friends than the maximum can have $\texttt{NA}$ in any redundant rows.   


#### Bradley-Terry Models

Bradley-Terry Models have similarities with multi-membership models and were designed to analyse the outcomes of dyadic competitions such as chess games. 



### $\texttt{covu}$: covariances between random and residual terms 

Occasionally, we need to link a set of random effects with a set of residuals. This most commonly occurs in multi-response models (Chapter \@ref(multi)) when we have repeated measures for one response but only single measures for another response, and so I will leave the main discussion of this to then (Section \@ref(covu-sec)). However, as a brief overview, we can link the set of random effects appearing in the final random term of the `random` specification with the residuals appearing in the first residual term of the `rcov` specification.  The linking is specified by adding a `covu=TRUE` to the prior specification for the first residual term. For example, imagine the random effect model specification:

```{r }
random=~...+...+us(at.level(response, "repeat")):individual
```

where $\dots$ indicate additional random terms and the rather complicated final random term is simply fitting $\texttt{individual}$ effects but for the repeat-measure response only. We could then specify the residual model as:

```{r }
rcov=~us(at.level(response, "single")):individual+us(at.level(response, "repeat")):units`
```

The first term, which again looks rather complicated, is simply fitting residuals for the single-measure response only. This can be achieved by using $\texttt{individual}$ effects since each individual only has one observation for this response. The second term fits residuals for the repeat-measure response and here we use $\texttt{units}$ to specify a residual for each of an individual's repeat measure observations. Note we could also have used $\texttt{units}$ in place of $\texttt{individual}$ for the first term, but then the levels of $\texttt{units}$ would not agree with the levels of $\texttt{individual}$ appearing in the random term and so, as with $\texttt{str}$ linking functions, the two sets of effects could not be linked. The prior specification for the residual term would then look something like:

```{r }
R=list(R1=list(V=diag(2), nu=2, alpha.mu=c(0,0), alpha.V=diag(2)*1000, covu=TRUE), 
       R2=list(V=1, nu=1, alpha.mu=0, alpha.V=1000)
```

The first prior specifies a $2\times 2$ covariance matrix for the two sets of $\texttt{individual}$ effects. Since the prior for the last set of random effects is absorbed into the residual prior specification, it should be omitted from the random-effect (G-structure) prior specification. The covariance matrix inherits the variance function of the first residual term, in this case $\texttt{us}$.   


### $\texttt{theta_scale}$: scaled linear predictor 









## Priors for Covariance Matrices {#VCVprior-sec}

Priors for covariance matrices are tricky. What maybe non-informative for a covariance may be informative for a correlation and *vice versa*.

### Priors for `us` structures

A useful result is that the marginal distribution of a variance is also inverse-Wishart distributed:

$$\sigma^{2}_{1} \sim IW\left(\texttt{nu}^{\ast}\texttt{=nu-dim(V)+1},\ \texttt{V}^{\ast}=\frac{\texttt{nu}}{\texttt{nu}^{\ast}}\texttt{V[1,1]}\right)$$

using the first variance as an example, and indicating the new parameters with an asterisk.

An uninformative prior for the correlations is an improper prior with `V=diag(dim(V))`$\ast$`0` and `nu=dim(V)+1`. For the $3\times3$ virus by line covariance matrix in model `m3a.4` we used a proper prior with `V=diag(3)`$\ast$`0.02` and `nu=4` in the hope that this would be relatively uninformative for the correlations. We can plot the marginal density of the variances for this distribution as we did in Chapter \@ref(bayesian):

``` {r label=NIc, echo=TRUE, include=TRUE, fig.cap="Marginal prior distribution of a variance using an inverse Wishart prior for the covariance matrix with \\texttt{V=diag(3)*0.02} and \\texttt{nu=4}.", fig.width=7, fig.height=5}
nu.ast<-prior.m3a.4$G$G1$nu-dim(prior.m3a.4$G$G1$V)[1]+1
V.ast<-prior.m3a.4$G$G1$V[1,1]*(prior.m3a.4$G$G1$nu/nu.ast)
xv<-seq(1e-16,1,length=100)
dv<-dgamma(1/xv, shape=nu.ast/2, rate=(nu.ast*V.ast)/2)/(xv^2)
plot(dv~xv, type="l")
```

In Chapter \@ref(bayesian) we saw that a non-informative prior for a variance component was `V=0` and `nu=-2`. This result generalises to covariance matrices where the improper prior `V=diag(dim(V))`$\ast$`0` and `nu=dim(V)-3` is non-informative for the variances and covariances. This can be verified for the variances using the results derived above for the marginal distribution:

$$
\begin{array}{rl}
\sigma^{2}_{1} \sim& IW\left(\texttt{nu}^{\ast}\texttt{=dim(V)-3-dim(V)+1},\ \texttt{V}^{\ast}=\frac{\texttt{nu}}{\texttt{nu}^{\ast}}\texttt{0}\right)\\
               \sim& IW\left(\texttt{nu}^{\ast}\texttt{=-2},\ \texttt{V}^{\ast}=\texttt{0}\right)\\

\end{array}
$$

### Priors for `idh` structures

For `idh` the diagonal elements of the matrix are independent and each variance is distributed as[^4.2]:

$$\sigma^{2}_{1} \sim IW\left(\texttt{nu}^{\ast}\texttt{=nu},\ \texttt{V}^{\ast}=\texttt{V[1,1]}\right)$$

### Priors for `corg` and `corgh` structures

For `corg` and `corgh` structures[^4.3] the diagonals of `V` define the fixed variances (`corgh`) or are ignored and the variances set to one (`corg`). I use the prior specification in @Barnard.2000 where `nu` controls how much the correlation matrix approaches an identity matrix. The marginal distribution of individual correlations ($r$) is given by
@Barnard.2000 [and @Box.1973]:

$$\begin{array}{lr}
Pr(r) \propto (1-r^{2})^\frac{\texttt{nu-dim(V)-1}}{\texttt{2}}, & |r|<1\\
\end{array}$$

and as shown above setting `nu` =`dim(V)+1` results in marginal correlations that are uniform on the interval \[-1,1\].

In most cases correlation matrices do not have known form and so cannot be directly Gibbs sampled. $\texttt{MCMCglmm}$ uses a method proposed by @Liu.2006 with the target prior as in @Barnard.2000. Generally this algorithm is very efficient as the Metropolis-Hastings acceptance probability only depends on the degree to which the candidate prior and the target prior (the prior you specify) conflict. The candidate prior is equivalent to the prior in @Barnard.2000 with `nu=0` so as long as a diffuse prior is set, mixing is generally not a problem. If `nu=0` is set (the default) then the Metropolis-Hastings steps are always accepted resulting in Gibbs sampling. However, a prior of this form puts high density on extreme correlations which can cause problems if the data give support to correlations in this region.



[^4.1]: Remember that a global intercept is not fitted by default for variance structure models, and the model formula is essentially `~virus-1`. To add the global intercept, `us(1+virus):line` could be fitted but this can be harder to interpret because the effects are then `France`, `Spain-France` and `Greece-France`. If a $\texttt{us}$ structure is fitted, the two models are equivalent reparameterisations of each other although the priors have to be modified accordingly. This is not the case if the variance function is $\texttt{idh}$. In this case the virus-specific variances are allowed to vary as before, but a constant covariance equal to $\sigma^{2}_{\color{blue}{\texttt{France}}}$ is also assumed

[^cant_int.4]: If anyone has an actual data set that could serve as an example, please get in touch. The code was developed for estimating a direct-maternal genetic correlation in an animal model (Chapter \@ref(pedigree)) but as an example it is too complicated.

[^4.2]: IMPORTANT: In versions $<$ 2.05 priors on each variance of an $\texttt{idh}$ structure were distributed as $IW\left(\texttt{nu}^{\ast}\texttt{=nu-dim(V)+1},\ \texttt{V}^{\ast}=\texttt{V[1,1]}\right)$ but this was a source of confusion and was changed.

[^4.3]: In versions $<2.18$ `cor` fitted what is now a `corg` structure. The reason for the change is to keep the `asreml` and $\texttt{MCMCglmm}$ syntax equivalent. However, the `corgh` structure in asreml is a reparameterised `us` structure whereas in $\texttt{MCMCglmm}$ the variances are fixed in the prior.
