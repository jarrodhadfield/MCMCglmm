# Categorical Random Interactions {#cat-int}

In Chapter \@ref(ranef) we looked at a simple random-effect specification where a single effect is associated with each level of a categorical predictor and the effects are assumed to be identically and independently distributed. In the next three chapters we will cover more complicated random-effect specifications. In this Chapter we will start by exploring models where an interaction is formed between two categorical predictors, one which (usually) has few levels with main effects fitted as fixed and the other which (usually) has many levels and would normally be fitted as random. We will also cover related models where the random effects associated with two or more sets of categorical predictors (including residual effects) are correlated. This includes multi-membership models as a special case. 

It is probably easiest to start with an example. The data set $\texttt{sigma}$ contains data on how well fruit flies (*Drosophila melanogaster*) transmit sigma virus to their offspring [@Carpenter.2012]. 

```{r }
data(sigma)
head(sigma)
```

The number of flies in a vial that are infected with the virus ($\texttt{infected}$) or not ($\texttt{not_infected}$) are recorded. The mean number of flies per vial is `r round(mean(sigma$infected+sigma$not_infected))` although the range is large (`r min(sigma$infected+sigma$not_infected)` - `r max(sigma$infected+sigma$not_infected)` ). All the flies in a vial are the offspring of parents infected with one of five strains ($\texttt{strain}$) of sigma virus, a virus that is transmitted to offspring through eggs and sperm. The parents belong to one of 67 genetically distinct lines ($\texttt{line}$).  Let's start with a simple binomial GLMM that we covered in Chapter \@ref(ranef), but to keep things manageable we'll only analyse data from three strains ($\texttt{France}$, $\texttt{Greece}$ and  $\texttt{Spain}$, named after their country of origin). Note that we will specify the prior using prior generator functions (Section \@ref(Vprior-gen-sec)) and so this specification can be recycled in all models. See Section \@ref(VCVprior-r--sec) for why we have specified a scaled-$F_{1,2}$ distribution rather than the scaled-$F_{1,1}$ distribution used when estimating scalar variances (Section \@ref(PXprior-sec)).

```{r sigma-1}
sigma_small<-subset(sigma, virus%in%c("France","Greece", "Spain"))

prior.sigma=list(R=IW(1, 0.002), G=F(2, 1000))

m.sigma.1<-MCMCglmm(cbind(infected, not_infected)~virus, random=~line, data=sigma_small, family="binomial", prior=prior.sigma)

summary(m.sigma.1)
```

For the base-line virus, $\texttt{France}$, the estimate of the median probability of infection is `r round(mean(plogis(m.sigma.1$Sol[,"(Intercept)"])),2)`, obtained by applying the `plogis` function to the intercept (Section \@ref(binom-pred-sec)). However, the viruses have substantially different infection rates with the median *odds* of infection for $\texttt{Greece}$ being only `r round(mean(exp(m.sigma.1$Sol[,"virusGreece"])),2)` times that of $\texttt{France}$, whereas $\texttt{Spain}$ has an odds of infection `r round(mean(exp(m.sigma.1$Sol[,"virusSpain"])),2)` times greater (obtained by exponentiating the coefficients). 

There is also substantial variation across lines in how well they transmit the virus (posterior mean for $\sigma^2_\texttt{line}$= `r round(mean(m.sigma.1$VCV[,"line"]), 2)`) and substantial overdispersion ($\sigma^2_\texttt{units}$= `r round(mean(m.sigma.1$VCV[,"units"]), 2)`). However, this model contains the implicit assumption that the increased odds of infection associated with a line is constant over viruses. For example, the first level of $\texttt{line}$ is $\texttt{101A}$. If this line's effect is $u^{(\texttt{line})}_1$ then the linear predictor for all observations of that line will include $u^{(\texttt{line})}_1$ and the odds of infection is expected to change by a factor $\textrm{exp}(u^{(\texttt{line})}_1)$ irrespective of the virus. However, we may expect there to be genetic interactions between the virus and the fly such that the infectivity of a line differs depending on the virus.  We could add a simple interaction between line and variance and treat these ($67\times 3 = 201$) interaction effects as random: 


```{r sigma-2}
m.sigma.2<-MCMCglmm(cbind(infected, not_infected)~virus, random=~line+virus:line, data=sigma_small, family="binomial", prior=prior.sigma)

summary(m.sigma.2)
```

While the variance in the interaction effects ($\sigma^2_\texttt{virus:line}=$ `r round(mean(m.sigma.2$VCV[,"virus:line"]),2)`) is substantially smaller than the main $\texttt{line}$ effects, the lower 95\% credible interval is well removed from zero suggesting that interactions are present.  This model still has implicit assumptions baked into it about how lines vary in their infectivity for a given virus, and how lines covary in their infectivity between viruses. To avoid complications later, I will - with some abuse of notation - call the main $\texttt{line}$ effects in this model $u^{(\texttt{m-line})}$ and their variance $\sigma^2_\texttt{m-line}$, with a posterior mean of `r round(mean(m.sigma.2$VCV[,"line"]),2)`.  The *aggregate* effect of $\texttt{line}$ $i$ infected with $\texttt{virus}$ $j$ is then given by:


$$ u^{(\texttt{line})}_{ij}  = u^{(\texttt{m-line})}_i+u^{(\texttt{virus:line})}_{ij}$$ 

If we took the linear predictor for two observations made on the same line in the same virus they would both share the same two random effects and therefore have the same $u^{(\texttt{line})}_{ij}$. Consequently, the covariance in their linear predictor is the variance of $u^{(\texttt{line})}$: $\sigma^2_\texttt{m-line}+\sigma^2_\texttt{virus:line}$. Since these two variances are assumed constant over viruses, we are implicitly assuming that the variance between-lines is the same for all viruses. If we took the linear predictor for two observations made on the same line but in different viruses they would both share the same $u^{(\texttt{m-line})}$ effect but have different interaction effects ($u^{(\texttt{virus:line})}$). Consequently, the covariance in their linear predictor is the variance of $u^{(\texttt{m-line})}$ only: $\sigma^2_\texttt{m-line}$. Again, $\sigma^2_\texttt{m-line}$ is assumed constant over viruses and so this implies that the between-line covariance is also constant: the covariance between lines infected with $\texttt{France}$ versus $\texttt{Spain}$ is the same as the covariance between lines infected with $\texttt{France}$ versus $\texttt{Greece}$ or $\texttt{Spain}$ versus $\texttt{Greece}$. This constancy is also reflected in the between-line correlation:


$$r_{\texttt{line}}= \frac{\sigma^2_\texttt{m-line}}{\sigma^2_\texttt{m-line}+\sigma^2_\texttt{virus:line}}$$

which has a posterior mean of `r round(mean(m.sigma.2$VCV[,"line"]/rowSums(m.sigma.2$VCV[,c("virus:line", "line")])),2)` and a relatively tight 95\% credible interval of `r round(HPDinterval(m.sigma.2$VCV[,"line"]/rowSums(m.sigma.2$VCV[,c("virus:line", "line")]))[1],2)` - `r round(HPDinterval(m.sigma.2$VCV[,"line"]/rowSums(m.sigma.2$VCV[,c("virus:line", "line")]))[2],2)`. Since $\sigma^2_\texttt{m-line}$ is a variance, and therefore constrained to be positive, we are also implicitly assuming that the covariance and correlation are both constant and positive: a line that shows greater infectivity with $\texttt{France}$ is not expected, on average, to have reduced infectivity with $\texttt{Spain}$. 

We can think of summarising this information in a $3\times 3$ between-line covariance matrix (the meaning of the colours will become apparent soon):


$${\bf V}_{{\color{red}{\texttt{line}}}}=
\left[
\begin{array}{ccc}
\sigma^{2}_{\color{blue}{\texttt{France}}}&\sigma_{\color{blue}{\texttt{France}, \texttt{Spain}}}&\sigma_{\color{blue}{\texttt{France}, \texttt{Greece}}}\\
\sigma_{\color{blue}{\texttt{France}, \texttt{Spain}}}&\sigma^{2}_{\color{blue}{\texttt{Spain}}}&\sigma_{\color{blue}{\texttt{Spain}, \texttt{Greece}}}\\
\sigma_{\color{blue}{\texttt{France}, \texttt{Greece}}}&\sigma_{\color{blue}{\texttt{Spain}, \texttt{Greece}}}&\sigma^{2}_{\color{blue}{\texttt{Greece}}}\\
\end{array}
\right]$$

The diagonal elements gives us the variance in $\texttt{line}$ effects for each virus and the off-diagonal elements gives us the covariance in $\texttt{line}$ effects between pairs of viruses. This covariance matrix has 6 (co)variances, but in model `m.sigma.2` we assumed that these (co)variances could be parametrised using only two parameters: all diagonal elements are equal to $\sigma^2_\texttt{m-line}+\sigma^2_\texttt{virus:line}$ and all off-diagonal elements are equal to $\sigma^2_\texttt{m-line}$. If we wish to relax this assumption we need to understand how variance functions, such as $\texttt{us}$ and $\texttt{idh}$, work.

## Variance Structures {#vstruct-sec}

We could refit our first model `m.sigma.2` using the random effect specifications:

```{r echo=TRUE}
random=~us(1):line
```

or 

```{r echo=TRUE}
random=~idh(1):line
```

and this would give exactly the same answer as the model specified by `~line`. The term inside the brackets is a model formula and is interpreted exactly how you would interpret any R formula expect the intercept is only fitted if it is explicitly defined (as here - remember a 1 in a formula stands in for 'intercept').  These formula are therefore fitting an intercept which is interacted with the random effects. We can get a representation of the interaction for the first few levels of `line` (`r levels(sigma$line)[1:5]`):

$$
\begin{array}{c|rrrrrc}
&{\color{red}{\texttt{101A}}}&{\color{red}{\texttt{109A}}}&{\color{red}{\texttt{129A}}}&{\color{red}{\texttt{142A}}}&{\color{red}{\texttt{153A}}}&\dots\\
\hline
{\color{blue}{\texttt{(1)}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{(1)}}}.{\color{red}{\texttt{153A}}}&\dots\\
\end{array}
$$

Across the top, we have the original $\texttt{line}$ effects in red, and along the side we have the term defined by the variance structure formula (just the intercept in this case). The interaction forms a new set of factors. Although they have different names from the original $\texttt{line}$ effects, it is clear that there is a one to one mapping between the original and the new factor levels and the models are therefore equivalent. For more complex interactions this is not the case. For example, if we fit $\texttt{virus}$ in the variance structure model (i.e. `us(virus):line` or `idh(virus):line`) we get[^4.1]:


$$\begin{array}{c|rrrrrc}
&{\color{red}{\texttt{101A}}}&{\color{red}{\texttt{109A}}}&{\color{red}{\texttt{129A}}}&{\color{red}{\texttt{142A}}}&{\color{red}{\texttt{153A}}}&\dots\\
\hline
{\color{blue}{\texttt{France}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{France}}}.{\color{red}{\texttt{153A}}}&\dots\\
{\color{blue}{\texttt{Spain}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{Spain}}}.{\color{red}{\texttt{153A}}}&\dots\\
{\color{blue}{\texttt{Greece}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{101A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{109A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{129A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{142A}}}&{\color{blue}{\texttt{Greece}}}.{\color{red}{\texttt{153A}}}&\dots\\
\end{array}$$

which creates three times as many random effects, one associated with observations for each $\texttt{virus}$ for each each  $\texttt{line}$.

### $\texttt{idh}$ Variance Structure {#idh-sec}

The different variance functions make different assumptions about how the effects are distributed within and across rows. First, we may want to allow the variance in the effects to be different for each row of factors; i.e. does $\texttt{line}$ explain different amounts of variation depending on the $\texttt{virus}$. We can fit this model using the `idh` function:


```{r sigma-3}
m.sigma.3<-MCMCglmm(cbind(infected, not_infected)~virus, random=~idh(virus):line, data=sigma_small, family="binomial", prior=prior.sigma)

```


In the simpler models we have fitted so far, each random effect term (terms separated by `+` in the `random` formula) specified a single variance (e.g. $\sigma^2_\texttt{virus:line}$) and the prior specification was relatively simple and covered in Sections \@ref(Vprior-sec) and \@ref(PXprior-sec). Prior specifications for covariance matrices are much trickier and are covered in Section \@ref(VCVprior-sec). For now, simply note that we have recycled the same prior generator function that we used in the simpler models, despite the the prior specification requiring $3\times 3$ matrices. If we take a look at the model summary:

```{r }
summary(m.sigma.3)
```

we see that three parameters are summarised for `~idh(virus):line`: the variance in line effects for each virus. While the $\texttt{idh}$ variance function allows the variances to be different across the viruses it assumes that $\texttt{line}$ effects for one virus are independent of the $\texttt{line}$ effects for a different virus. In terms of our coloured table of effects  $\texttt{idh}$ assumes effects in different rows are independently distributed, but it relaxes the assumption that they are identically distributed - they can have different variances.  We can represent this structure in terms of a $3\times3$ covariance matrix:

$${\bf V}_{{\color{red}{\texttt{line}}}}=
\left[
\begin{array}{ccc}
\sigma^{2}_{\color{blue}{\texttt{France}}}&0&0\\
0&\sigma^{2}_{\color{blue}{\texttt{Spain}}}&0\\
0&0&\sigma^{2}_{\color{blue}{\texttt{Greece}}}\\
\end{array}
\right]$$

We can extract the posterior means for each variance and place them into a matrix:

```{r echo=TRUE, eval=TRUE}
Vline.idh<-diag(colMeans(m.sigma.3$VCV)[1:3])
colnames(Vline.idh)<-rownames(Vline.idh)<-c("France", "Spain", "Greece")
Vline.idh
```

Because the $\texttt{line}$ effects are assumed to be multivariate normal we can also represent their distribution in terms of an ellipsoid (you can rotate the image): 

```{r label=rgl, echo=TRUE, include=TRUE, webgl=TRUE, cache=FALSE}
plotsubspace(Vline.idh, axes.lab=TRUE)
```
Widget 5.1: Ellipsoid that circumscribes 95\% of the expected $\texttt{line}$ effects as estimated in model `m.sigma.3`. This can be thought of as a scatter plot of the $\texttt{line}$ effects between each virus, if the $\texttt{line}$ effects could be directly measured.  Because the covariances of the $\texttt{line}$ effects between each virus were set to zero, and the variance of the $\texttt{line}$ effects are quite similar for each virus, the ellipsoid is almost spherical.

Although we have allowed the variance in $\texttt{line}$ effects to be different across the viruses, we have used the default specification for the residual structure (`rcov=~units`). $\texttt{MCMCglmm}$ augments the original data-frame with a column called $\texttt{units}$ which is a factor that has unique levels for each row. This is covered in greater depth when discussing multi-response models (Chapter \@ref(multi)) but for the single-response model used here we can simply think of $\texttt{units}$ as indexing residuals. Having a constant residual variance for each virus but allowing the between-line variance to vary is somewhat dangerous: if the residual variances did vary then this may be reflected to some degree in the estimates of the between-line variances. Allowing the residual variances to also vary overs viruses is straightforward:

```{r sigma-4}
m.sigma.4<-MCMCglmm(cbind(infected, not_infected)~virus, random=~idh(virus):line, rcov=~idh(virus):units, data=sigma_small, family="binomial", prior=prior.sigma)

summary(m.sigma.4)
```

As with the variance in $\texttt{line}$ effects, the residual variances don't appear to be dramatically different across viruses.

### $\texttt{us}$ Variance Structure {#us-sec}

The $\texttt{idh}$ structure allowed the variance in $\texttt{line}$ effects to be different for different viruses, but it assumed that the effect of a line for one virus is uncorrelated with the effect of that line in another virus. However, the strong correlation (`r round(mean(m.sigma.2$VCV[,"line"]/rowSums(m.sigma.2$VCV[,c("virus:line", "line")])),2)`) in $\texttt{line}$ effects estimated from model `m.sigma.2` suggests that this assumption is unlikely to be reasonable.  We can relax this assumption by using the `us` function which estimates the fully parameterised matrix seen earlier:

```{r sigma-5}
m.sigma.5<-MCMCglmm(cbind(infected, not_infected)~virus, random=~us(virus):line, rcov=~idh(virus):units, data=sigma_small, family="binomial", prior=prior.sigma)

summary(m.sigma.5)
```

The rows displayed for the G-structure correspond to the 9 elements of the $3\times 3$ covariance matrix (displayed column-wise). As before, we can arrange the posterior mean estimates in to a matrix 

```{r echo=TRUE, eval=TRUE}
Vline.us<-matrix(colMeans(m.sigma.5$VCV)[1:9], 3, 3)
colnames(Vline.us)<-rownames(Vline.us)<-c("France", "Spain", "Greece")
Vline.us
```

We can see that the covariances are large, and like the variances, relatively constant. If we visualise this distribution we can see it is quite different from that obtained from model $\texttt{m.sigma.4}$ (or model $\texttt{m.sigma.3}$) where the covariances are assumed to be zero.

```{r label=rgl2, fig=-TRUE, echo=TRUE, eval=TRUE, webgl=TRUE, cache=FALSE}
plotsubspace(Vline.us, axes.lab=TRUE)
```
Widget 5.2: Ellipsoid that circumscribes 95% of the expected $\texttt{line}$ effects as estimated in model $\texttt{m.sigma.5}$. This can be thought of as a scatter plot of the $\texttt{line}$ effects between virus, if the $\texttt{line}$ effects could be directly measured.  The correlations of the $\texttt{line}$ effects between viruses are large, and the variance in $\texttt{line}$ effects are roughly equal in magnitude across viruses.  Consequently the orientation of the major axis of the ellipsoid is tending towards $45^{o}$ relative to the figure axes.


We can use the function `posterior.cor` to convert the posterior samples of the (co)variance matrix into posterior samples of the correlation matrix. Doing so we can see that the estimated correlations are indeed large and relatively invariant.

```{r echo=TRUE, eval=TRUE}
rline.us<-matrix(colMeans(posterior.cor(m.sigma.5$VCV[,1:9])), 3, 3)
colnames(rline.us)<-rownames(rline.us)<-c("France", "Spain", "Greece")
rline.us
```

In this particular example, it looks like the simpler two-parameter model implemented in model `m.sigma.2` may do a good job of representing the covariance matrix: 

```{r echo=TRUE, eval=TRUE}
Vline.2<-matrix(mean(m.sigma.2$VCV[,"line"]), 3, 3)
diag(Vline.2)<-diag(Vline.2)+mean(m.sigma.2$VCV[,"virus:line"])
colnames(Vline.2)<-rownames(Vline.2)<-c("France", "Spain", "Greece")
Vline.2
```

and we can see this if we visualise the two distributions simultaneously:


```{r label=rgl3, fig=-TRUE, echo=TRUE, eval=TRUE, webgl=TRUE, cache=FALSE}
plotsubspace(Vline.us, Vline.2, axes.lab=TRUE, shadeCA=FALSE)
```
Widget 5.3: Ellipsoids that circumscribes 95% of the expected $\texttt{line}$ effects as estimated in model $\texttt{m.sigma.5}$ where all (co)variance parameters are estimated (red wireframe) and model $\texttt{m.sigma.2}$ where the variances across viruses, and the correlations between viruses, are assumed constant (solid blue).

Note that when using a $\texttt{us}$ structure for the $\texttt{line}$ effects in model $\texttt{m.sigma.6}$ we retained the $\texttt{idh}$ structure for the residuals. Since the flies in each vial (indexed by $\texttt{units}$ since they correspond to a row of the data-frame) are all exposed to the same virus, the residual covariances between viruses cannot be estimated and so can be set to zero. In Chapter \@ref(multi) we will look at multi-response models where this generally isn't the case. 

### Other Variance Structures

In some cases, additional restrictions need to be placed on a covariance matrix. In Section \@ref(bernoulli-sec) we used the prior argument $\texttt{fix}$ to force the residual variance to be fixed at a specified value in a Bernoulli GLM. When a variance structure defines a (co)variance matrix, $\texttt{fix}$ specifies a diagonal element of the matrix that splits the matrix into four quadrants. For example, imagine we have the $5\times 5$ matrix specified in the $\texttt{V}$ element of the prior:

$$\texttt{V}=
\left[
\begin{array}{cc|ccc}
1&0.5&0.5&0&0\\
0.5&1&0.5&0&0\\
\hline
0.5&0.5&2&0&0\\
0&0&0&2&1\\
0&0&0&1&3\\
\end{array}
\right]$$

If $\texttt{fix}=3$ then the matrix is split into four quardants with the lower right quadrant starting on the 3rd diagonal element. This quadrant will be fixed at the value specified in $\texttt{V}$ but all remaining (co)variance parameters will be estimated (if a $\texttt{us}$ structure is used). 

Sometimes constraints are required that can not be achieved by using $\texttt{fix}$ and cannot be expressed in terms of the (re)parameterisations discussed above. Table \@ref(tab:vfunction) provides a summary of available variance functions and $\texttt{ante}$ structures - which give the greatest flexibility - are covered in Section \@ref(ante-sec)).

```{r echo=FALSE, results='asis'}
df <- data.frame(
  MCMCglmm = c("`line`", "`us(virus):line`", "`virus:line`", "`line+virus:line`", "`idh(virus):line`", "`corg(virus):line`", "`corgh(virus):line`", "`cors(virus):line`"),
  lmer = c("`(1|line)`", "`(virus|line)`", "`(1|virus:line)`", "`(1|line)+(1|virus:line)`", "", "", "", ""),
  nparameters = c(1,6,1,2,3,3,3, 4),
  Variance      = c("$\\left[\\begin{array}{ccc}V&V&V\\\\V&V&V\\\\V&V&V\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V_{1,1}&C_{1,2}&C_{1,3}\\\\C_{2,1}&V_{2,2}&C_{2,3}\\\\C_{3,1}&C_{3,2}&V_{3,3}\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V&{\\color{red}{0}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&V&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{0}}&V\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V_1+V_2&V_1&V_1\\\\V_1&V_1+V_2&V_1\\\\V_1&V_1&V_1+V_2\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V_{1,1}&{\\color{red}{0}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&V_{2,2}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{0}}&V_{3,3}\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}{\\color{red}{V_{1,1}}}&r_{1,2}{\\color{red}{\\sqrt{V_{1,1}V_{2,2}}}}&r_{1,3}{\\color{red}{\\sqrt{V_{1,1}V_{3,3}}}}\\\\r_{2,1}{\\color{red}{\\sqrt{V_{2,2}V_{1,1}}}}&{\\color{red}{V_{2,2}}}&r_{2,3}{\\color{red}{\\sqrt{V_{2,2}V_{3,3}}}}\\\\r_{3,1}{\\color{red}{\\sqrt{V_{3,3}V_{1,1}}}}&r_{3,2}{\\color{red}{\\sqrt{V_{3,3}V_{2,2}}}}&{\\color{red}{V_{3,3}}}\\\\ \\end{array}\\right]$",
                    "$\\left[\\begin{array}{ccc}V_{1,1}&C_{1,2}&C_{1,3}\\\\C_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\C_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$"
                    ),
  Correlation      = c("$\\left[\\begin{array}{ccc}{\\color{red}{1}}&{\\color{red}{1}}&{\\color{red}{1}}\\\\{\\color{red}{1}}&{\\color{red}{1}}&{\\color{red}{1}}\\\\{\\color{red}{1}}&{\\color{red}{1}}&{\\color{red}{1}}\\\\ \\end{array}\\right]$", 
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&{\\color{red}{0}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{1}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{0}}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r&r\\\\r&{\\color{red}{1}}&r\\\\r&r&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&{\\color{red}{0}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{1}}&{\\color{red}{0}}\\\\{\\color{red}{0}}&{\\color{red}{0}}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$",
                      "$\\left[\\begin{array}{ccc}{\\color{red}{1}}&r_{1,2}&r_{1,3}\\\\r_{2,1}&{\\color{red}{1}}&r_{2,3}\\\\r_{3,1}&r_{3,2}&{\\color{red}{1}}\\\\ \\end{array}\\right]$"
                      ),
  stringsAsFactors = FALSE
)

kable(df, "html", escape = FALSE, caption = "Different random effect specifications in $\\texttt{MCMCglmm}$ (with equivalent $\\texttt{lmer}$ syntax when possible) with fixed parameters in red and estimated parameters in black. $\\texttt{virus}$ is a factor with three levels so the resulting covariance and correlation matrices are $3\\times 3$. For the `cors(virus):line` structure, the prior specification had `fix=2` which forces the last diagonal block (starting from position 2,2) to be a correlation matrix rather than a covariance matrix. $\\texttt{fix}$ can also be specified for other structures but then it fixes the last diagonal block to what ever is specified in the $\\texttt{V}$ element of the prior.  $\\texttt{ante}$ structures are not covered here but in Section \\@ref(ante-sec)", label="vfunction") |>
  kable_styling(full_width = FALSE)%>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```

## Linking Functions

In some models we would like to estimate covariances between sets of random effects (or random effects and residuals) in a way that is conceptually very similar to that discussed in Section \@ref(vstruct-sec) using variance structures. However, the way that we would like the (co)variances to be set up cannot be achieved by forming interactions. This happens when the predictors associated with the two or more sets of random effects are in different columns of the data frame, despite sharing the same set of levels, and we need some way to link them. Again, this is perhaps best illustrated through an example. 

### $\texttt{str}$: covariances between random terms 

My mum is a great mum. She's also a super grandmother to my children, and I wonder whether these two things are correlated?[^cant_int.4]  Let's say we could measure the well-being for many children over several generations, and importantly we have multiple children from the same mother so we can estimate $\texttt{mother}$ effects. As importantly, some of these mothers go onto to be grandmothers of their children's children. If we also have measures of well-being for these children we can also estimate $\texttt{grandmother}$ effects. However, it's not clear how we could specify a covariance between these effects in the same way that we did using variance structures. Our hypothetical data-frame may look like this:

```{r echo=FALSE}
n<-500
mother<-as.factor(sample(letters, n, replace=TRUE))
grandmother<-as.factor(sapply(mother, function(x){sample(setdiff(letters, x), 1)}))

V<-matrix(0.5, 2, 2)
diag(V)<-c(2,1)

u<-MASS::mvrnorm(26, c(0,0), V)

y<-u[mother,1]+u[grandmother,2]+rnorm(n, 0, sqrt(4))

data.nana<-data.frame(y=y, mother=mother, grandmother=grandmother)
```

```{r echo=FALSE}
head(data.nana)
```

with $\texttt{y}$ being the children's well-being score and the letters being the names of the child's mother and grandmother. I have ensured that the $\texttt{mother}$ and $\texttt{grandmother}$ columns have the same factor levels using the $\texttt{levels}$ argument of the function $\texttt{as.factor}$. It is not necessary that *all* mothers appear as grandmothers, or vice versa, but of course *some* mothers must appear as grandmothers if we wish to estimate the correlation in their effects on a child's well-being. The linking function $\texttt{str}$ links two or more sets of random effects and assumes a fully unstructured covariance matrix for their distribution. It may help to visualise the random effects that the term `str(mother+grandmother)` is specifying:


$$\begin{array}{c|rrrrc}
&{\color{red}{\texttt{a}}}&{\color{red}{\texttt{b}}}&{\color{red}{\texttt{c}}}&{\color{red}{\texttt{d}}}&\dots\\
\hline
{\color{blue}{\texttt{mother}}}&{\color{blue}{\texttt{mother}}}.{\color{red}{\texttt{a}}}&{\color{blue}{\texttt{mother}}}.{\color{red}{\texttt{b}}}&{\color{blue}{\texttt{mother}}}.{\color{red}{\texttt{c}}}&{\color{blue}{\texttt{mother}}}.{\color{red}{\texttt{d}}}&\dots\\
{\color{blue}{\texttt{grandmother}}}&{\color{blue}{\texttt{grandmother}}}.{\color{red}{\texttt{a}}}&{\color{blue}{\texttt{grandmother}}}.{\color{red}{\texttt{b}}}&{\color{blue}{\texttt{grandmother}}}.{\color{red}{\texttt{c}}}&{\color{blue}{\texttt{grandmother}}}.{\color{red}{\texttt{d}}}&\dots\\
\end{array}$$

We can seen that this is conceptually identical to our sigma virus example but rather than specifying the classifying variable inside $\texttt{idh()}$ or $\texttt{us()}$, as we could for $\texttt{virus}$, the classifying variable is actually the column names of the terms specified in $\texttt{str()}$ .


```{r }
prior.str=list(R=IW(1, 0.002), G=F(1,1000))

m.str<-MCMCglmm(y~1, random=~str(mother+grandmother), data=data.nana, prior=prior.str)
```

The model output shows that a $2\times 2$ covariance matrix has been estimated for the $\texttt{mother}$/$\texttt{grandmother}$ effects.

```{r }
summary(m.str)
```

When simulating the data I assumed the variance in mother effects was 2, the variance in grandmother effects was 1 and the covariance between them was 0.5 (giving a correlation of `r round(0.5/sqrt(2), 2)`). The data set contained few (grand)mothers and so the estimates are uncertain, but the credible intervals overlap the true values. 

### $\texttt{mm}$: multi-membership models {#multim-sec}

Using $\texttt{str}$ allowed us to link random effects associated with categorical predictors appearing in different columns of our data frame. The effects defined by different columns were allowed to have different variances (the variance in mother effects was different from the variance in grandmother effects) and the correlation was estimated. In some cases we would like to treat the effects defined by the different columns as the same effects when their levels agree. In the $\texttt{str}$ example we did not want to do this because being a mother is not the same thing as being a grandmother. However, imagine that each of our children has five friends:


```{r echo=FALSE}
n<-500

y<-rnorm(n, 0, sqrt(2))

friend<-t(sapply(1:n, FUN=function(x){sample(LETTERS, 5)}))

data.friends<-data.frame(y=y, friend=friend)

u<-rnorm(26,0, sqrt(0.1))


for(i in 1:5){
data.friends[,i+1]<-factor(data.friends[,i+1], levels=LETTERS)
data.friends$y<-data.friends$y+u[as.numeric(data.friends[,i+1])]
}


```

```{r echo=FALSE}
head(data.friends)
```

Friend `r names(which.max(table(unlist(data.friends[1:6,-1]))))` appears as a friend to multiple children and we would like to model that friend's effect on the focal child's well-being irrespective of whether `r names(which.max(table(unlist(data.friends[1:6,-1]))))` appears as $\texttt{friend.1}$, $\texttt{friend.2}$ ... or $\texttt{friend.5}$ as the numbering is arbitrary.  The linking function $\texttt{mm}$ allows us to specify this. 

```{r }
prior.mm=list(R=IW(1, 0.002), G=F(1,1000))

m.mm<-MCMCglmm(y~1, random=~mm(friend.1+friend.2+friend.3+friend.4+friend.5), data=data.friends, prior=prior.mm)
```

```{r }
summary(m.mm)
```

The variance in $\texttt{friend}$ effects was set to 0.1 in the simulated data. While small compared to the simulated residual variance (2) the aggregate effect of all friends is five times this: 0.5. Note that if children varied in how many friends they had, the number of $\texttt{friend.#}$ columns can be set to the maximum and children with fewer friends than the maximum can have $\texttt{NA}$ in any redundant columns. For example, lets say the second child in our data-frame only had four friends - $\texttt{friend}$ `r data.friends[2,5]` is not, in fact, a friend of this child. We would then just set this cell to $\texttt{NA}$:

```{r echo=FALSE}
data.friends[2,5]<-NA
```

```{r }
head(data.friends)
```

When fitting the model, $\texttt{MCMCglmm}$ will issue the warning:

``` r
missing values in random predictors
```

but this can be ignored.


### $\texttt{covu}$: covariances between random and residual terms 

Occasionally, we need to link a set of random effects with a set of residuals. This most commonly occurs in multi-response models (Chapter \@ref(multi)) when we have repeated measures for one response but only single measures for another response. I will leave the main discussion of this topic to then (Section \@ref(covu-sec)). However, to give some idea of how this works, imagine that the final random term in the `random` formula defines the set of random effects $[{\bf u}^{(1)}, {\bf u}^{(2)},  {\bf u}^{(3)}]$ (for example the $\texttt{virus}$ by $\texttt{line}$ effects discussed above). Imagine also that the first residual term in the `rcov` formula defines the residuals $[{\bf e}^{(1)}, {\bf e}^{(2)}]$. By specifying $\texttt{covu=TRUE}$ in the prior for the first residual term a $5\times 5$ covariance matrix is estimated rather than a $3\times 3$ covariance matrix for the random effects and a $2\times 2$ covariance matrix for the residuals.
 
### $\texttt{theta_scale}$: scaled linear predictor 

Occasionally, we need to force a common relationship between two sets of (multiple) random effects. As with $\texttt{covu}$ models, this most commonly occurs in multi-response models (Chapter \@ref(multi)) and so will be covered in detail there (Section \@ref(theta-scale-sec)). For now, we can express the idea compactly by imagining two sets random effects $\left\{{\bf u}^{(1)}, {\bf u}^{(2)}\right\}$ and $\left\{{\bf u}^{(3)}, {\bf u}^{(4)}\right\}$. The $\texttt{theta_scale}$ argument allows the user to force the constraint $\left\{{\bf u}^{(3)}, {\bf u}^{(4)}\right\}$=$\left\{b{\bf u}^{(1)}, b{\bf u}^{(2)}\right\}$ with $b$ estimated. Fixed effects can also be added to the set to be scaled. for example, $\left\{{\bf \beta}^{(B)}, {\bf u}^{(3)}, {\bf u}^{(4)}\right\}$=$\left\{b{\bf \beta}^{(A)}, b{\bf u}^{(1)}, b{\bf u}^{(2)}\right\}$.


## Priors for Covariance Matrices {#VCVprior-sec}


When fitting a $k$ dimensional (co)variance matrix, $\texttt{V}$ and $\texttt{alpha.V}$ must specify $k$ dimensional (co)variance matrices and $\texttt{alpha.mu}$ must specify a vector of length $k$. The degrees-of-freedom, $\texttt{nu}$, however, remains scalar. By using prior generators to specify the prior (Section \@ref(Vprior-gen-sec)) we hid this complexity. If we want to understand exactly what the prior generators are specifying in this context we cannot avoid this complexity. In addition, if we want to add arguments such as $\texttt{fix}$ or $\texttt{covu}$ to the prior, prior generators cannot be used.

What follows is important, but also hard. For those short of time, patience or expertise, I would recommend using the prior generator `F(2,1000)` for random-effect covariance matrices. However, you may need to adjust the scale if the standard deviation of the data differs greatly from one (Section \@ref(PXprior-sec)).    

### Marginal Priors for Variances

The joint distribution of all $k(k-1)/2$ elements of the covariance matrix is hard to characterise or visualise [@Tokuda.2025]. We will start by exploring a simpler problem - given some prior specification for a $k$ dimensional (co)variance matrix what is the marginal prior distribution of each variance? Luckily, the marginal distributions have the same form as they have in the scalar case but with some rescaling of parameters. We will designate the parameters of the marginal distribution using asterisks. The marginal distribution is characterised by the *scalar* parameters $\texttt{V}^{\ast}$ and $\texttt{nu}^{\ast}$, and if $F$ priors are used, then $\texttt{alpha.mu}^{\ast}$ and $\texttt{alpha.V}^{\ast}$. The properties of these (marginal) distributions are fully covered in Sections \@ref(Vprior-sec) and \@ref(PXprior-sec), respectively.


Before continuing, it should be pointed out that what follows in the next paragraph does **not** hold when an $\texttt{idh}$ structure is used. $\texttt{idh}$ structures set all correlations to zero and simply estimate a set of $k$ variances, where $k$ is the number of parameters specified by the $\texttt{idh}$ model. For example, in the sigma virus example we used `idh(virus):line` and $k=3$ since there were three levels of $\texttt{virus}$. The prior specification still requires $k\times k$ ($3\times 3$) matrices for $\texttt{V}$ and $\texttt{alpha.V}$ but the off-diagonal elements are ignored, and the prior for the $i^\textrm{th}$ variance is simply $\texttt{V}^{\ast}=\texttt{V[i,i]}$, $\texttt{nu}^\ast=\texttt{nu}$,  $\texttt{alpha.mu}^\ast=\texttt{alpha.mu[i]}$ and $\texttt{alpha.V}^\ast=\texttt{alpha.V[i,i]}$[^4.2]. Forcing all variances to have the same degrees-of-freedom might be too restrictive, but at least with $\texttt{idh}$ structures this can be relaxed using an alternative parameterisation[^4.3]. This luxury isn't afforded to other variance structures for which covariances or correlation are estimated.

For structures where a full covariance matrix is to be inferred (for example by using a $\texttt{us}$ structure) a useful result is that the prior for the $i^\textrm{th}$ variance has parameters $\texttt{V}^{\ast}=\frac{\texttt{nu}}{\texttt{nu}^{\ast}}\texttt{V[i,i]}$, $\texttt{nu}^{\ast}=\texttt{nu}-k+1$, $\texttt{alpha.mu}^\ast=\texttt{alpha.mu[i]}$ and $\texttt{alpha.V}^\ast=\texttt{alpha.V[i,i]}$. Consequently, we can choose a $\texttt{V}$ and $\texttt{nu}$ that allows us to specify one of the priors discussed in Sections \@ref(Vprior-sec) and \@ref(PXprior-sec) for a single variance.  For example, in Section \@ref(Vprior-sec) we saw that an inverse-gamma prior with a shape and scale of 0.001 is equivalent to the scalar inverse-Wishart with $\texttt{V}^{\ast}=1$ and $\texttt{nu}^{\ast}=0.002$. Consequently, we can place this prior on the $i^\textrm{th}$ variance by setting $\texttt{nu}=k-1+0.002$ such that $\texttt{nu}^{\ast}=0.002$. Having $\texttt{V[i,i]}=\texttt{nu}^{\ast}/\texttt{nu}=0.002/(k-1+0.002)$ then results in  $\texttt{V}^{\ast}=1$. An improper prior that is flat for the variance requires $\texttt{V}^{\ast}=0$ and $\texttt{nu}^{\ast}=-2$ which can be achieved by setting $\texttt{nu}=k-3$ and $\texttt{V[i,i]}=0$. In Section \@ref(PXprior-sec) we noted that scaled $F$ priors for variances (scaled folded-$t$ priors for standard deviations) can have better properties than the inverse-Wishart. As in Section \@ref(PXprior-sec) we can set $\texttt{alpha.mu}$ to be a vector of zeros and work with the scaled central $F$ and scaled half-$t$. The marginal distribution for the $i^\textrm{th}$ variance then follows a $F_{1, \texttt{nu}^{\ast}}$ distribution with scale equal to $\texttt{V}^{\ast}*\texttt{alpha.V[i,i]}$. We can therefore chose $\texttt{V}$, $\texttt{nu}$ and $\texttt{alpha.V}$ to specify one of the priors discussed in Section \@ref(Vprior-sec) for a single variance or standard deviation. For example, by setting $\texttt{nu}=k$ we have $\texttt{nu}^{\ast}=1$ generating a scaled $F_{1,1}$ prior for the variance (or half-Cauchy for the standard deviation). We noted in Section \@ref(PXprior-sec) that the prior specification in $\texttt{MCMCglmm}$ is redundant and we can set the scale by setting $\texttt{V}$ to one and controlling the scale through $\texttt{alpha.V}$. This can be achieved in the multivariate case by setting $\texttt{V[i,i]}=1/k$ such that $\texttt{V}^{\ast}=1$ and then altering $\texttt{alpha.V[i,i]}$ as before.

The rules for rescaling can be hard to remember and fiddly to implement. In order to simplify the process of prior specification, priors can also be specified using the prior generator functions detailed in Section \@ref(Vprior-gen-sec). When the random term defines a (co)variance matrix, these prior generators specify a prior for which the *marginal* prior distributions for the variance have the specified distribution. In all cases the off-diagonal elements of $\texttt{V}$ and $\texttt{alpha.V}$ are set to zero. If we want to see the prior specification generated by a prior generator we can use the function `resolve_prior` which also requires the dimension of the (co)variance matrix ($\texttt{k}$) and the type of variance structure ($\texttt{vtype}$). For example, to have scaled-$F_{1,1}$ marginal priors for the variances with a scale of 1,000, the specification for a $3\times 3$ unstructured (co)variance matrix is:

```{r }
resolve_prior(F(1,1000), k=3, vtype="us")
``` 

### Marginal Priors for Covariances and Correlations {#VCVprior-r-sec}

Above, we looked at the prior specifications that resulted in marginal priors for the variances which are well understood. What do the marginal priors for the covariances and correlations look like under these prior specifications? While the marginal distribution for a covariance does not have an easy form, the marginal distribution of a correlations for an inverse-Wishart with diagonal $\texttt{V}$ is a beta distribution transformed to the interval $[-1, 1]$ [@Box.1973; @Barnard.2000].  The shape and scale of the beta are equal to $(\texttt{nu}-k+1)/2$ which is the marginal degrees-of-freedom $\texttt{nu}^{\ast}$ used in the prior generator function[^corg-prior].  The beta is flat when  $\texttt{nu}^{\ast}=2$ since the shape=scale=1. As $\texttt{nu}^{\ast}$ becomes smaller the distribution becomes more and more U-shaped with a lot of prior density close to -1 and 1. As $\texttt{nu}^{\ast}$ increases beyond $2$ this distribution becomes more constrained around zero (Figure \@ref(fig:prior-cor)). Since the working parameters in parameter expansion cancel for the correlation, the distribution of the correlations is the same as that under the inverse-Wishart (although the distribution of the covariances will differ). 


```{r prior-cor, echo=FALSE, include=TRUE, fig.cap="Prior marginal density for a correlation when $\\texttt{V}$ is diagonal. The density is a beta distribution over the interval $[-1,1]$ with shape=scale=$\\texttt{nu}^{\\ast}/2$. $\\texttt{nu}^{\ast}/2$ is the marginal degrees of freedom for a variance: $\\texttt{nu}-k+1$.", fig.width=7, fig.height=5}

r<-seq(0, 1, length=1000)
plot(I(dbeta(r, 1/2, 1/2)/2)~I(r*2-1), ylab="Prior density", xlab="Correlation", type="l", lwd=1.5, ylim=c(dbeta(0.01, 4/2, 4/2)/2, dbeta(0.01, 1/2, 1/2)/2))
lines(I(dbeta(r, 2/2, 2/2)/2)~I(r*2-1), ylab="Prior density", xlab="Correlation", type="l", lwd=1.5)
lines(I(dbeta(r, 3/2, 3/2)/2)~I(r*2-1), ylab="Prior density", xlab="Correlation", type="l", lwd=1.5)
lines(I(dbeta(r, 4/2, 4/2)/2)~I(r*2-1), ylab="Prior density", xlab="Correlation", type="l", lwd=1.5)
lines(I(dbeta(r, 10/2, 10/2)/2)~I(r*2-1), ylab="Prior density", xlab="Correlation", type="l", lwd=1.5)


text(0, dbeta(0.5, 1/2, 1/2)/2-0.05, expression("nu"^{'*'}==1))
text(0, dbeta(0.5, 2/2, 2/2)/2-0.05, expression("nu"^{'*'}==2))
text(0, dbeta(0.5, 3/2, 3/2)/2-0.05, expression("nu"^{'*'}==3))
text(0, dbeta(0.5, 4/2, 4/2)/2-0.05, expression("nu"^{'*'}==4))
text(0, dbeta(0.5, 10/2, 10/2)/2+0.05, expression("nu"^{'*'}==10))
```

With simple random effects models, where only variances need to be estimated, I usually use a scaled $F_{1,1}$ prior with large scale. In the past, I also specified priors for covariance matrices that had a marginal $F_{1,1}$ prior. However, since this implies $\texttt{nu}^{\ast}$=1 (and $\texttt{nu}=k$) this may push the posterior correlation away from zero, especially if the data have support for correlations that are large in magnitude. Using an $F_{1,2}$ marginal prior for the variance is flat for the correlation but it does mean we've switched from a half-Cauchy (half-$t_1$) to a half-$t_2$ for the standard deviation. However, with a large scale ($\sqrt{1000}=$ `r round(sqrt(1000),1)` for the standard deviation), this is unlikely to have a large effect (Figure \@ref(fig:prior-compare-t)). 

```{r prior-compare-t, echo=FALSE, include=TRUE, fig.cap=paste0("Marginal prior densities for the standard deviation using the prior generator functions $\\texttt{F(1,1000)}$, $\\texttt{F(2,1000)}$ and $\\texttt{F(3,1000)}$. $\\texttt{F(2,1000)}$ results in a flat prior for the correlation. Note that the legend refers to these in terms of the marginal density for the standard deviations (ie. half-$t$) and that the half-$t_1$ is the Cauchy. Also, while the x-axis runs from zero to ", round(max(sqrt(mschool.1$VCV[,"school"]))), ", the posterior distributions for the standard deviation in $\\texttt{line}$ effects lie in the range ", round(min(sqrt(m.sigma.5$VCV[,c(1,5,9)])),1), "-", round(max(sqrt(m.sigma.5$VCV[,c(1,5,9)])),1), ". By doing this, the figure is directly comparable to Figure \\@ref(fig:prior-compare)."), fig.width=7, fig.height=5}

sd.x<-seq(1e-18, max(sqrt(mschool.1$VCV[,"school"])), length=1000)

dsd.x<-as.list(1:4)

t1.prior<-resolve_prior(F(1, 1000), k=1, vtype="us")
t2.prior<-resolve_prior(F(2, 1000), k=1, vtype="us")
t3.prior<-resolve_prior(F(3, 1000), k=1, vtype="us")
t10.prior<-resolve_prior(F(10, 1000), k=1, vtype="us")

dsd.x[[1]]<-dprior(sd.x, t1.prior, sd=TRUE)
dsd.x[[2]]<-dprior(sd.x, t2.prior, sd=TRUE)
dsd.x[[3]]<-dprior(sd.x, t3.prior, sd=TRUE)
dsd.x[[4]]<-dprior(sd.x, t10.prior, sd=TRUE)

cols    <- palette()[1:4] 

plot(dsd.x[[1]]~sd.x,                     
     main = "",
     xlab = expression(sigma[line]), ylab = "Prior density",
     col  = cols[1], lwd = 2, type="l", ylim=c(0, max(unlist(dsd.x))))

for (i in 2:length(dsd.x))                  
  lines(dsd.x[[i]]~sd.x, col = cols[i], lwd = 2)

model.names<-c(expression("half-"~t[1]~": "~sqrt(1000)), expression("half-"~t[2]~": "~sqrt(1000)), expression("half-"~t[3]~": "~sqrt(1000)), expression("half-"~t[10]~":"~sqrt(1000)))

legend("bottomleft", lwd = 2, col = cols,
       legend = model.names, bty = "n")
```

Let's rerun model `m.sigma.5` with an $F_{1,1}$ prior of the same scale to see what happens:

```{r sigma-6}
prior.sigma.6=list(R=IW(1, 0.002), G=F(1, 1000))

m.sigma.6<-MCMCglmm(cbind(infected, not_infected)~virus, random=~us(virus):line, rcov=~idh(virus):units, data=sigma_small, family="binomial", prior=prior.sigma.6)
```

The variance in $\texttt{line}$ effects over viruses remain largely unchanged

```{r line-vcompare, echo=FALSE, include=TRUE, fig.cap="MCMC traces for the variance in $\\texttt{line}$ effects for the three viruses when the marginal prior for the variances are scaled (1000) $F_{1,2}$ (black - model $\\texttt{m.sigma.5}$) or $F_{1,1}$ (red- model $\\texttt{m.sigma.6}$) distributions.  The $F_{1,2}$ prior is flat for the correlation in $\\texttt{line}$ effects across viruses.", fig.width=7, fig.height=8}

plot(mcmc.list(m.sigma.5$VCV[,c(1,5,9)], m.sigma.6$VCV[,c(1,5,9)]), density=FALSE)
```

The correlations in $\texttt{line}$ effects between viruses are nudged a little further from zero under the new prior but the effect is rather subtle (there's also some auto-correlation in chains so they should be run for longer than the number of default iterations). 

```{r line-rcompare, echo=FALSE, include=TRUE, fig.cap="MCMC traces for the correlation in $\\texttt{line}$ effects between the three viruses when the marginal prior for the variances are scaled (1000) $$F_{1,2}$ (black - model $\\texttt{m.sigma.5}$) or $F_{1,1}$ (red- model $\\texttt{m.sigma.6}$) distributions.  The $F_{1,2}$ prior is flat for the correlation in $\\texttt{line}$ effects across viruses.", fig.width=7, fig.height=8}
plot(mcmc.list(posterior.cor(m.sigma.5$VCV[,1:9])[,c(2:3,6)], posterior.cor(m.sigma.6$VCV[,1:9])[,c(2:3,6)]), density=FALSE)
```

In contrast, altering the degrees-of-freedom in the inverse-Wishart (ie. not using parameter-expanded $F$-priors) to achieve a better prior distribution for the correlation can have a much stronger influence on the variances and standard deviations \@ref(fig:prior-compare-iw).


```{r prior-compare-iw, echo=FALSE, include=TRUE, fig.cap=paste0("Marginal prior densities for the standard deviation using the prior generator functions $\\texttt{IW(1,0.002)}$, $\\texttt{IW(1,1)}$ and $\\texttt{IW(1,2)}$, $\\texttt{IW(1,3)}$ and $\\texttt{IW(1,10)}$. $\\texttt{IW(1,2)}$ results in a flat prior for the correlation. Note that the legend refers to these in terms of the marginal density for the variance (ie. inverse-gamma). Also, while the x-axis runs from zero to ", round(max(sqrt(mschool.1$VCV[,"school"]))), ", the posterior distributions for the standard deviation in $\\texttt{line}$ effects lie in the range ", round(min(sqrt(m.sigma.5$VCV[,c(1,5,9)])),1), "-", round(max(sqrt(m.sigma.5$VCV[,c(1,5,9)])),1), ". By doing this, the figure is directly comparable to Figure \\@ref(fig:prior-compare)."), fig.width=7, fig.height=5}

sd.x<-seq(1e-18, max(sqrt(mschool.1$VCV[,"school"])), length=1000)

dsd.x<-as.list(1:5)

iw0.prior<-resolve_prior(IW(1, 0.002), k=1, vtype="us")
iw1.prior<-resolve_prior(IW(1, 1), k=1, vtype="us")
iw2.prior<-resolve_prior(IW(1, 2), k=1, vtype="us")
iw3.prior<-resolve_prior(IW(1, 3), k=1, vtype="us")
iw10.prior<-resolve_prior(IW(1, 10), k=1, vtype="us")

dsd.x[[1]]<-dprior(sd.x, iw0.prior, sd=TRUE)
dsd.x[[2]]<-dprior(sd.x, iw1.prior, sd=TRUE)
dsd.x[[3]]<-dprior(sd.x, iw2.prior, sd=TRUE)
dsd.x[[4]]<-dprior(sd.x, iw3.prior, sd=TRUE)
dsd.x[[5]]<-dprior(sd.x, iw10.prior, sd=TRUE)

cols    <- palette()[1:5] 

plot(dsd.x[[1]]~sd.x,                     
     main = "",
     xlab = expression(sigma[line]), ylab = "Prior density",
     col  = cols[1], lwd = 2, type="l", ylim=c(0, max(unlist(dsd.x))))

for (i in 2:length(dsd.x))                  
  lines(dsd.x[[i]]~sd.x, col = cols[i], lwd = 2)

model.names<-c("inverse-gamma(0.001, 0.001)", "inverse-gamma(0.5, 0.5)", "inverse-gamma(1, 1)", "inverse-gamma(1.5, 1.5)", "inverse-gamma(5, 5)")

legend("topright", lwd = 2, col = cols,
       legend = model.names, bty = "n")
```

### Full joint prior

We have concentrated on the marginal priors, since these are relatively easy to understand and visualise. However, by concentrating on the marginal distributions we may miss aspects of the joint prior distribution that is important. Characterising the joint distribution is hard, particular under parameter expansion where the full probability density function *probably* doesn't have a closed form solution. However, we can simulate from these priors using the function `rprior` and use the visualisation tools in the library $\texttt{VisCov}$ [@Tokuda.2025]. 

```{r viscov, include=TRUE, fig.cap="Visualisation of the prior distribution used for the $3\\times 3$ (co)variance matrix of $\\texttt{line}$ effects from model $\\texttt{m.sigma.5}$ where a parameter expanded prior was used with marginal $F{1,2}$ priors on the variances with a scale of a 1,000 (half-$t_2$ prior on the standard deviation with scale $\\sqrt{1000}\\approx 32$). The prior is flat for the correlations. See @Tokuda.2025 for discussion of $\\texttt{VisCov}$ plots.", fig.width=7, fig.height=8}

prior.line<-resolve_prior(prior.sigma$G, k=3, vtype="us")

VCV<-rprior(2000, prior=prior.line)
# simulate from prior for line (co)variance matrix 

mat<- apply(VCV, 1, matrix, nrow=3, simplify=FALSE)
# coerce to list

CovPlotData = VisCov("User defined distribution", param=list(prob = 0.5, mat = mat))
```

We can also visualise the inverse-Wishart prior with $\texttt{nu}=4$ which is flat for the correlations since $\texttt{nu}^{\ast}=2$. We can see the dependencies in the posterior are stronger, particularly between the correlations and the standard deviations (Figure \@ref(fig:viscov2)).

```{r viscov2, include=TRUE, fig.cap="Visualisation of the prior distribution for a $3\\times 3$ (co)variance matrix using an inverse-Wishart distribution with $\\texttt{n}=4$ and $\\texttt{V}={\\bf I}\\frac{1}{2}$. This prior is flat for the correlations ($\\texttt{nu}^{\\ast}=2$) but the marginal prior for the variances is inverse-gamma with shape and scale equal to one. See @Tokuda.2025 for discussion of $\\texttt{VisCov}$ plots.", fig.width=7, fig.height=8}

prior.IW<-resolve_prior(IW(1,2), k=3, vtype="us")
# inverse-Wishart prior with marginal inverse-gamma(1,1) prior on variances

VCV<-rprior(2000, prior=prior.IW)
# simulate from prior for line (co)variance matrix 

mat<- apply(VCV, 1, matrix, nrow=3, simplify=FALSE)
# coerce to list

CovPlotData = VisCov("User defined distribution", param=list(prob = 0.5, mat = mat))
```

[^4.1]: Remember that a global intercept is not fitted by default for variance structure models, and the model formula is essentially `~virus-1`. To add the global intercept, `us(1+virus):line` could be fitted but this can be harder to interpret because the effects are then `France`, `Spain-France` and `Greece-France`. If a $\texttt{us}$ structure is fitted, the two models are equivalent reparameterisations of each other although the priors have to be modified accordingly. This is not the case if the variance function is $\texttt{idh}$. In this case the virus-specific variances are allowed to vary as before, but a constant covariance equal to $\sigma^{2}_{\color{blue}{\texttt{France}}}$ is also assumed

[^cant_int.4]: If anyone has an actual data set that could serve as an example, please get in touch. The code was developed for estimating a direct-maternal genetic correlation in an animal model (Chapter \@ref(pedigree)) but as an example it is too complicated.

[^4.2]: In versions $<$ 2.05 priors on each variance of an $\texttt{idh}$ structure were distributed as $IW\left(\texttt{nu}^{\ast}\texttt{=nu-dim(V)+1},\ \texttt{V}^{\ast}=\texttt{V[1,1]}\right)$ but this was neither intuitive or completely consistent with the marginal prior from a full-covariance matrix (since $\texttt{V}^{\ast}$ should be $\texttt{V[1,1]}\frac{\texttt{nu}}{\texttt{nu}^{\ast}}$). 

[^4.3]: In models `m.sigma.3` (and `m.sigma.4`) we fitted the $\texttt{idh}$ term `idh(virus):line`. We could also have set this model up as the much more ugly `idh(at.level(virus, "France")):line+idh(at.level(virus, "Spain")):line+idh(at.level(virus, "Greece")):line`. The function `at.level` returns a vector with ones where the first term ($\texttt{virus}$) is equal to the second term (e.g. $\texttt{"France"}$) and zero otherwise. In this context, it essentially just fits $\texttt{line}$ effects on observations made on the $\texttt{France}$ virus. The prior specification would require a prior on each of the three variances and a different $\texttt{nu}$ could be specified for each.

[^corg-prior]: When correlation matrices are estimated using $\texttt{corg}$ and $\texttt{corgh}$ structures only $\texttt{nu}$ determines the prior density  - $\texttt{V}$ is ignored. The marginal density for the correlations are the same as those given for the $inverse-Wishart$ and marginal $F$-priors. 


