\newif\ifalone
\alonefalse
\ifalone
\documentclass{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{Sweave}
\usepackage{lscape}
\usepackage{makeidx}
\usepackage{longtable}



\title{Appendices}
\author{Jarrod Hadfield (\texttt{j.hadfield@ed.ac.uk})}

\begin{document}
\maketitle
\else
\chapter{Technical Details}
\label{chap7}
\fi




\section{Model Form}
The probability of the $i^{th}$ data point is represented by:

\begin{equation}
f_{i}(y_{i} | l_{i})
\label{pyl-Eq}
\end{equation}

where $f_{i}$ is the probability density function associated with $y_{i}$. For example, if $y_{i}$ was assumed to be Poisson distributed and we used the canonical log link function, then Equation \ref{pyl-Eq} would have the form:

\begin{equation}
f_{P}\left(y_{i} | \lambda = \textrm{exp}(l_{i})\right)
\label{pyl2-Eq}
\end{equation}

where $\lambda$ is the canonical parameter of the Poisson denisty function $f_{P}$. Table \ref{dist-tab} has a full list of supported distributions and link functions.\\

The vector of latent variables follow the linear model

\begin{equation}
{\bf l}  = {\bf X}{\bm \beta}+{\bf Z}{\bf u}+{\bf e}
\label{l-Eq}
\end{equation}
 
where ${\bf X}$ is a design matrix relating fixed predictors to the data, and ${\bf Z}$ is a design matrix relating random predictors to the data.  These predictors have associated parameter vectors ${\bm \beta}$ and ${\bf u}$, and ${\bf e}$ is a vector of residuals.  In the Poisson case these residuals deal with any over-dispersion in the data after accounting for fixed and random sources of variation.\\

The location effects (${\bm \beta}$ and ${\bf u}$), and the residuals (${\bf e}$) are assumed to come from a multivariate normal distribution:

\begin{equation}
\left[
\begin{array}{c}
{\bm \beta}\\
{\bf u}\\
{\bf e}
\end{array}
\right]
 \sim N\left(
\left[
\begin{array}{c}
{\bm \beta}_{0}\\
{\bf 0}\\
{\bf 0}\\
\end{array}
\right]
, 
\left[
\begin{array}{ccc}
{\bf B}&{\bf 0}&{\bf 0}\\
{\bf 0}&{\bf G}&{\bf 0}\\
{\bf 0}&{\bf 0}&{\bf R}\\
\end{array}
\right]
\right)
\label{V-Eq}
\end{equation}

where ${\bm \beta}_{0}$ is a vector of prior means for the fixed effects with prior (co)variance ${\bf B}$, and ${\bf G}$ and ${\bf R}$ are the expected (co)variances of the random effects and residuals respectively.  The zero off-diagonal matrices imply \emph{a priori} independence between fixed effects, random effects, and residuals.  Generally, ${\bf G}$ and ${\bf R}$ are large square matrices with dimensions equal to the number of random effects or residuals. Typically they are unknown, and must be estimated from the data, usually by assuming they are structured in a way that they can be parameterised by few parameters. Below we will focus on the structure of ${\bf G}$, but the same logic can be applied to ${\bf R}$.\\ 

At its most general, {\bf MCMCglmm} allows variance structures of the form:

\begin{equation}
{\bf G}= \left({\bf V}_{1}\otimes{\bf A}_{1}\right) \oplus \left({\bf V}_{2}\otimes{\bf A}_{2}\right) \oplus \ldots
\label{G3-Eq}
\end{equation}

where the parameter (co)variance matrices (${\bf V}$) are usually low-dimensional and are to be estimated, and the structured matrices (${\bf A}$) are usually high dimensional and treated as known.\\

In the case of ordinal probit models with $>2$ categories (i.e. \texttt{"threshold"} or \texttt{"ordinal"} models), $f_{T}/f_{O}$ depends on an extra set of parameters in addition to the latent variable: the $\textrm{max}(y)+1$ cutpoints ${\bm \gamma}$. The probability of $y_{i}$ is then: 

\begin{equation}
f_{T}(y_{i} | l_{i}, {\bm \gamma}) = 1\ \textrm{if}\ \gamma_{y_{i}+1} <  l_{i} < \gamma_{y_{i}}
\end{equation}

and

\begin{equation}
f_{O}(y_{i} | l_{i}, {\bm \gamma}) = F_{N}(\gamma_{y_{i}} | l_{i}, 1)-F_{N}(\gamma_{y_{i}+1} | l_{i},1)
\end{equation}  

where $F_{N}$ is the cumulative density function for the normal. Note that the two models can be made equivalent.

\section{MCMC Sampling Schemes}
\label{MCMC-app}

\subsection[Updating the latent variables]{Updating the latent variables ${\bf l}$}

The conditional density of $l$ is given by:

\begin{equation}
Pr(l_{i}| {\bf y}, {\bm \theta}, {\bf R}, {\bf G}) \propto  f_{i}(y_{i} | l_{i})f_{N}(e_{i}|{\bf r}_{i}{\bf R}_{/i}^{-1}{\bf e}_{/i}, r_{i}-{\bf r}_{i}{\bf R}_{/i}^{-1}{\bf r}^{'}_{i})
\label{pcl-Eq}
\end{equation}

where $f_{N}$ indicates a Multivariate normal density with specified mean vector and covariance matrix.  Equation \ref{pcl-Eq} is the probability of the data point $y_{i}$ from distribution $f_{i}$ with latent varaible $l_{i}$, multiplied by the probability of the linear predictor residual. The linear predictor residual follows a conditional normal distribution where the conditioning is on the residuals associated with data points other than $i$. Vectors and matrices with the row and/or column associated with $i$ removed are denoted $/i$.  Three special cases exist for which we sample directly from Equation \ref{pcl-Eq}: i) When $y_{i}$ is normal $f_{i}(y_{i} | l_{i})=1$ if $y_{i}=l_{i}$ and zero otherwise so $l_{i}=y_{i}$ with out the need for updating, ii) when $y_{i}$ is discrete and modelled using \texttt{family="threshold"} then Equation \label{pcl-Eq} defines a truncated normal distribution and can be slice sampled \citep{Robert.1995} and iii) when $y_{i}$ is missing $f_{i}(y_{i} | l_{i})$ is not defined and samples can drawn directly from the normal.
\\

In practice, the conditional distribution in Equation \ref{pcl-Eq} only involves other residuals which are expected to show some form of residual covariation, as defined by the ${\bf R}$ structure.  Because of this we actually update latent variables in blocks, where the block is defined as groups of residuals which are expected to be correlated:

\begin{equation}
Pr({\bf l}_{j}|{\bf y}, {\bm \theta}, {\bf R}, {\bf G}) \propto   \prod_{i \in j}{p}_{i}({y}_{i} | l_{i})f_{N}({\bf e}_{j}|{\bf 0}, {\bf R}_{j})
\label{pcl2-Eq}
\end{equation}

where $j$ indexes blocks of latent variables that have non-zero residual covariances. For response variables that are neither Gaussian nor threshold, the density in equation \ref{pcl2-Eq} is in non-standard form and so Metropolis-Hastings updates are employed. We use adaptive methods during the burn-in phase to determine an efficient multivariate normal proposal distribution centered at the previous value of ${\bf l}_{j}$ with covariance matrix $m{\bf M}$. For computational efficiency we use the same ${\bf M}$ for each block $j$, where ${\bf M}$ is the average posterior (co)variance of ${\bf l}_{j}$ within blocks and is updated each iteration of the burn-in period  \citet{Haario.2001}. The scalar $m$ is chosen using the method of \citet{Ovaskainen.2008} so that the proportion of successful jumps is optimal, with a rate of 0.44 when ${\bf l}_{j}$ is a scalar declining to 0.23 when ${\bf l}_{j}$ is high dimensional \citep{Gelman.2004}.\\

 A special case arises for multi-parameter distributions in which each parameter is  associated with a linear predictor. For example, in the zero-inflated Poisson two linear predictors are used to model the same data point, one to predict zero-inflation, and one to predict the Poisson variable. In this case the two linear predictors are updated in a single block even when the residual covariance between them is set to zero, because the first probability in Equation \ref{pcl2-Eq} cannot be factored:

\begin{equation}
Pr({\bf l}_{j}|{\bf y}, {\bm \theta}, {\bf R}, {\bf G}) \propto    {p}_{i}({y}_{i} | {\bf l}_{j})({\bf e}_{j}|{\bf 0}, {\bf R}_{j})
\label{pcl3-Eq}
\end{equation}
  
When the block size is one (i.e. a univariate analysis) then the latent variables can be slice sampled for two-category \texttt{ordinal} and \texttt{categorical} models if \texttt{slice=TRUE} is passed to \texttt{MCMCglmm}.\\

\subsection[Updating the location vector]{Updating the location vector ${\bm \theta} =  \left[{\bm \beta}^{'}\; {\bf u}^{'}\right]^{'}$}

\citet{Garcia-Cortes.2001} provide a method for sampling ${\bm \theta}$ as a complete block that involves solving the sparse linear system:

\begin{equation}
\tilde{\bm \theta} = {\bf C}^{-1}{\bf W}^{'}{\bf R}^{-1}({\bf l} - {\bf W}{\bm \theta}_{\star}-{\bf e}_{\star})
\label{sMME-Eq}
\end{equation}

where ${\bf C}$ is the mixed model coefficient matrix:

\begin{equation}
{\bf C} = {\bf W}^{'}{\bf R}^{-1}{\bf W}+
\left[
\begin{array}{c c}
{\bf B}^{-1}&{\bf 0}\\
{\bf 0}&{\bf G}^{-1}\\
\end{array}
\right]
\end{equation}

and ${\bf W} = \left[{\bf X}\; {\bf Z}\right]$, and ${\bf B}$ is the prior (co)variance matrix for the fixed effects.\\

${\bm \theta}_{\star}$ and ${\bf e}_{\star}$ are random draws from the multivariate normal distributions:

\begin{equation}
{\bm \theta}_{\star} \sim N\left(
\left[
\begin{array}{c}
{\bm \beta_{0}}\\
{\bf 0}\\
\end{array}
\right]
,
\left[
\begin{array}{c c}
{\bf B}&{\bf 0}\\
{\bf 0}&{\bf G}\\
\end{array}
\right]
\right)
\end{equation}

and 

\begin{equation}
{\bf e}_{\star} \sim N\left({\bf 0},{\bf R}\right)
\end{equation}

$\tilde{\bm  \theta} + {\bm \theta}_{\star}$ gives a realisation from the required probability distribution:

\begin{equation}
Pr({\bm \theta} | {\bf l}, {\bf W}, {\bf R}, {\bf G})
\end{equation}

Equation \ref{sMME-Eq} is solved using Cholesky factorisation. Because ${\bf C}$ is sparse and the pattern of non-zero elements fixed, an initial symbolic Cholesky factorisation of ${\bf P}{\bf C}{\bf P}^{'}$ is preformed where ${\bf P}$ is a fill-reducing permutation matrix  \citep{Davis.2006}. Numerical factorisation must be performed each iteration but the fill-reducing permutation (found via a minimum degree ordering of ${\bf C}+{\bf C}^{'}$) reduces the computational burden dramatically compared to a direct factorisation of ${\bf C}$  \citep{Davis.2006}.\\

Forming the inverse of the variance structures is usually simpler because they can be expressed as a series of direct sums and Kronecker products:

\begin{equation}
{\bf G}= \left({\bf V}_{1}\otimes{\bf A}_{1}\right) \oplus \left({\bf V}_{2}\otimes{\bf A}_{2}\right) \oplus \ldots
\label{G3-Eq}
\end{equation}

and the inverse of such a structure has the form 

\begin{equation}
{\bf G}^{-1} = \left({\bf V}^{-1}_{1}\otimes{\bf A}^{-1}_{1}\right) \oplus \left({\bf V}^{-1}_{2}\otimes{\bf A}^{-1}_{2}\right) \oplus \ldots\\
\end{equation}

which involves inverting the parameter (co)variance matrices (${\bf V}$), which are usually of low dimension, and inverting ${\bf A}$. For many problems ${\bf A}$ is actually an identity matrix and so inversion is not required.  When ${\bf A}$ is a relationship matrix associated with a pedigree, \citet{Henderson.1976, Meuwissen.1992} give efficient recursive algorithms for obtaining the inverse, and \citet{Hadfield.2010b} derive a similar procedure for phylogenies.
 
\subsection[Updating the variance structures]{Updating the variance structures ${\bf G}$ and ${\bf R}$}

Components of the direct sum used to construct the desired variance structures are conditionally independent.  The sum of squares matrix associated with each component term has the form:

\begin{equation}
{\bf S} = {\bf U}^{'}{\bf A}^{-1}{\bf U}
\end{equation}

where ${\bf U}$ is a matrix of random effects where each column is associated with the relevant row/column of ${\bf V}$ and each row associated with the relevant row/column of ${\bf A}$. The parameter (co)variance matrix can then be sampled from the inverse Wishart distribution:


\begin{equation}
{\bf V} \sim IW(({\bf S}_{p}+{\bf S})^{-1},\ n_{p}+n)
\label{pIW-Eq}
\end{equation}

where $n$ is the number of rows in ${\bf U}$, and ${\bf S}_{p}$ and $n_{p}$ are the prior sum of squares and prior degree's of freedom, respectively.\\

In some models, some elements of a parameter (co)variance matrix cannot be estimated from the data and all the information comes from the prior. In these cases it can be advantageous to fix these elements at some value and \citet{Korsgaard.1999} provide a strategy for sampling from a conditional inverse-Wishart distribution which is appropriate when the rows/columns of the parameter matrix can be permuted so that the conditioning occurs on some diagonal sub-matrix. When this is not possible Metropolis-Hastings updates can be made. 

\subsection{Ordinal Models}

For ordinal models it is necessary to update the cutpoints which define the bin boundaries for latent variables associated with each category of the outcome.  To achieve good mixing we used the method developed by \citep{Cowles.1996} that allows the latent variables and cutpoints to be updated simultaneously using a Hastings-with-Gibbs update.

\subsection{Path Analyses}

Elements of the response vector can be regressed on each other using the \texttt{sir} and \texttt{path} functions. Using the matrix notation of \citet{Gianola.2004}, Equation \ref{l-Eq} can be rewritten as:

\begin{equation}
{\bm \Lambda}{\bf l}  = {\bf X}{\bm \beta}+{\bf Z}{\bf u}+{\bf e}
\label{rs-Eq1}
\end{equation}

where ${\bm \Lambda}$ is a square matrix of the form:

\begin{equation}
\begin{array}{rl}
{\bm \Lambda} =& {\bf I}-\sum_{l}{\bm \Psi}^{(l)}\lambda_{l}\\
\end{array}
\label{rs-Eq2}
\end{equation}

This sets up a regression where the $i^{th}$ element of the response vector acts as a weighted (by $\Psi^{(l)}_{i,j}$) predictor for the $j^{th}$ element of the response vector with associated regression parameter $\lambda_{l}$. Often ${\bm \Psi}^{(l)}$ is an incidence matrix with the patterns of ones determining which elements of the response are regressed on each other.\\

Conditional on the vector of regression coefficients ${\bm \lambda}$, the location effects and variance structures can be updated as before by simply substituting ${\bf l}$ for ${\bm \Lambda}{\bf l}$ in the necessary equations. \citet{Gianola.2004} provide a simple scheme for updating ${\bm \lambda}$. Note that Equation \ref{rs-Eq1} can be rewritten as:

\begin{equation}
\begin{array}{rl}
{\bf l} - {\bf X}{\bm \beta} - {\bf Z}{\bf u} =& {\bf e}+\sum_{l}{\bm \Psi}^{(l)}{\bf l}\lambda_{l}\\
                                              =& {\bf e}+{\bf L}{\bm \lambda}\\
\end{array}
\end{equation}

where ${\bf L}$ is the design matrix $\left[{\bm \Psi}^{(1)}{\bf l}, {\bm \Psi}^{(2)}{\bf l} \dots {\bm \Psi}^{(L)}{\bf l}\right]$ for the $L$ path coefficients. Conditional on ${\bm \beta}$ and ${\bm u}$, ${\bm \lambda}$ can then be sampled using the method of \citet{Garcia-Cortes.2001} with ${\bf l} - {\bf X}{\bm \beta} - {\bf Z}{\bf u}$ as response and ${\bf L}$ as predictor. However, only in a fully recursive system (there exists a row/column permutation by which all ${\bm \Psi}$'s are triangular) are the resulting draws from the appropriate conditional distribution, which requires multiplication by the Jacobian of the transform: $|{\bm \Lambda}|$. An extra Metropolis Hastings step is used to accept/reject the proposed draw when $|{\bm \Lambda}|\neq 1$.\\

When the response vector is Gaussian and fully observed, the latent variable does not need updating. For non-Gaussian data, or with missing responses, updating the latent variable is difficult because Equation \ref{pcl-Eq} becomes:

\begin{equation}
Pr(l_{i}| {\bf y}, {\bm \theta}, {\bf R}, {\bf G}, {\bm \lambda}) \propto  f_{i}(y_{i} | l_{i})f_{N}(({\bm \Lambda}^{-1}{\bf e})_{i}|{\bf q}_{i}{\bf Q}_{/i}^{-1}{\bf e}_{/i}, q_{i}-{\bf q}_{i}{\bf Q}_{/i}^{-1}{\bf q}^{'}_{i})
\end{equation}

where ${\bf Q} = {\bm \Lambda}^{-1}{\bf R}{\bm \Lambda}^{-\top}$. In the general case ${\bf Q}$ will not have block diagonal structure like ${\bf R}$ and so the scheme for updating latent variables within residual blocks (i.e. Equation \ref{pcl2-Eq}) is not possible. However, in some cases ${\bm \Lambda}$ may have the form where all non-zero elements correspond to elements of the response vector that are in the same residual block. In such cases updating the latent variables remains relatively simple:

\begin{equation}
Pr({\bf l}_{j}|{\bf y}, {\bm \theta}, {\bf R}, {\bf G}) \propto    {p}_{i}({y}_{i} | {\bf l}_{j})({\bm \Lambda}^{-1}_{j}{\bf e}_{j}|{\bf 0}, {\bm \Lambda}^{-1}_{j}{\bf R}_{j}{\bm \Lambda}^{-\top}_{j})
\end{equation}




\subsection{Deviance and DIC}

The deviance $D$ is defined as:

\begin{equation}
D = -2\textrm{log}(\Pr({\bf y} | {\bm \Omega}))
\end{equation}

where ${\bm \Omega}$ is some parameter set of the model.  The deviance can be calculated in different ways depending on what is in `focus', and MCMCglmm calculates this probability for the lowest level of the hierarchy \citep{Spiegelhalter.2002}. For fully-observed Gaussian response variables in the likelihood is the density:

\begin{equation}
f_{N}({\bf y} | {\bf W}{\bm \theta},\ {\bf R}) 
\end{equation}

where ${\bm \Omega} = \left\{{\bm \theta},\ {\bf R}\right\}$.  For discrete response variables in univariate analyses modeled using \texttt{family="threshold"} the density is

\begin{equation}
\prod_{i} F_{N}(\gamma_{y_{i}} | {\bf w}_{i}{\bm \theta}, \ r_{ii})-F_{N}(\gamma_{y_{i}+1} | {\bf w}_{i}{\bm \theta}, \ r_{ii})
\end{equation}

where ${\bm \Omega} = \left\{{\bm \gamma},\ {\bm \theta},\ {\bf R}\right\}$. For other response variables variables (including discrete response variables modeled using \texttt{family="ordinal"}) it is the product:

\begin{equation}
\prod_{i}f_{i}(y_{i} | l_{i})
\label{LLikL}
\end{equation}

with ${\bm \Omega} = {\bf l}$.\\

For multivariate models with mixtures of Gaussian (g), threshold (t) and other non-Gaussian (n) data (including missing data) we can define the deviance in terms of three conditional densities:  

\begin{equation}
\begin{array}{rl}
Pr({\bf y} | {\bm \Omega}) =& \Pr({\bf y}_{g}, {\bf y}_{t}, {\bf y}_{n} | {\bm \gamma}, {\bm \theta}_{g}, {\bm \theta}_{t}, {\bm l}_{n}, {\bm R})\\
                           =& \Pr({\bf y}_{t} | {\bm \gamma}, {\bm \theta}_{t}, {\bf y}_{g}, {\bm l}_{n}, {\bm R})\Pr({\bf y}_{g} | {\bm \theta}_{g}, {\bm l}_{n}, {\bm R})\Pr({\bf y}_{n} | {\bm l}_{n})\\
\label{Eq-MVdeviance}
\end{array}
\end{equation}

with ${\bm \Omega} = \left\{{\bm \gamma},\ {\bm \theta}_{/n},\ {\bm l}_{n}\ {\bf R}\right\}$.  Have $({\bm W}{\bm \theta})_{a|b}={\bm W}_{a}{\bm \theta}_{a}+{\bf R}_{a,b}{\bf R}^{-1}_{b,b}({\bf l}_{b}-{\bm W}_{b}{\bm \theta}_{b})$ and ${\bf R}_{a |b} = {\bf R}_{a,a}-{\bf R}_{a,b}{\bf R}^{-1}_{b,b}{\bf R}_{a,b}$ where the subscripts denote rows of the data vector/design matrices or rows/columns of the ${\bf R}$-structure.
Then, the conditional density of ${\bf y}_{g}$ in Equation \ref{Eq-MVdeviance} is:

\begin{equation}
f_{N}\left({\bf y}_{g} | ({\bm W}{\bm \theta})_{g|n},\ {\bf R}_{g|n}\right) 
\end{equation}

The conditional density of ${\bf y}_{n}$ in Equation \ref{Eq-MVdeviance} is identical to that given in Equation \ref{LLikL},  and for a single \texttt{"threshold"} trait  


\begin{equation}
\prod_{i} F_{N}(\gamma_{y_{i}} | ({\bm W}{\bm \theta})_{ti|g,n}, \ r_{ti|g, n})-F_{N}(\gamma_{y_{i}+1} | ({\bm W}{\bm \theta})_{ti|g,n}, \ r_{ti|g, n})
\label{Eq-cpmvnorm}
\end{equation}

is the conditional density for ${\bf y}_{t}$ in Equation \ref{Eq-MVdeviance}, where $({\bm W}{\bm \theta})_{ti|g,n}$ is the $i^{\textrm{th}}$ element of $({\bm W}{\bm \theta})_{t|g,n}$. Currently the deviance (and hence the DIC) will not be returned if there is more than one threshold trait.\\

The deviance is calculated at each iteration if \texttt{DIC=TRUE} and stored each \texttt{thin}$^{th}$ iteration after burn-in.  However, for computational reasons the deviance is calculated mid-iteration such that the deviance returned at iteration $i$  uses ${\bm \Omega}_{i} = \left\{{\bm \gamma}_{i},\ {\bm \theta}_{/n, i},\ {\bm l}_{n, i-1}\ {\bf R}_{i}\right\}$. The mean deviance ($\bar{D}$) is calculated over all iterations, as is the mean of the latent variables (${\bf l}$) the ${\bf R}$-structure and the vector of predictors (${\bf W}{\bm \theta}$).  The deviance is calculated at the mean estimate of the parameters ($D(\bar{\bm \Omega})$) and the deviance information criterion calculated as:

\begin{equation}
\textrm{DIC} = 2\bar{D}-D(\bar{\bm \Omega})
\end{equation}

\begin{landscape}
\LTcapwidth=6.2in
\begin{longtable}{cccrl}
%\begin{center}
%\small
%\begin{tabular}{cccrl}
\hline
Distribution   &    No. Data       &         No. latent        &  Density & function \\
   type        &      columns      &           columns            &          &           \\   
\hline\\
   \texttt{"gaussian"}        &  1  &   1  &         $Pr(y) =$&$f_{N}({\bf w}{\bm \theta},\sigma^{2}_{e})$\\   
&&&&\\
   \texttt{"poisson"}        &  1  &   1 &               $Pr(y) =$&$ f_{P}(\textrm{exp}(l))$\\   
&&&&\\
   \texttt{"categorical"}        &  1  &   $J$-1  &     $Pr(y=k | k\neq1) =$&$ \frac{\textrm{exp}(l_{k})}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}$ \\   
           &   &     &                                  $Pr(y=1) =$&$ \frac{1}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}$ \\   
&&&&\\
   \texttt{"multinomial$J$"}  &  $J$    &  $J$-1  &     $Pr(y_{k}=n_{k}| k\neq J) =$&$ \left(\frac{\textrm{exp}(l_{k})}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}\right)^{n_{k}}$ \\   
    &      &   &     $Pr(y_{k}=n_{k} | k=J) =$&$ \left(\frac{1}{1+\sum^{J-1}_{j=1}\textrm{exp}(l_{j})}\right)^{n_{k}}$ \\   
&&&&\\ 
   \texttt{"ordinal"}  &  1    &  1     &              $Pr(y=k) =$&$ F_{N}(\gamma_{k} | l,1)-F_{N}(\gamma_{k+1} | l,1)$ \\   
&&&&\\
   \texttt{"threshold"}  &  1    &  1     &              $Pr(y=k) =$&$ F_{N}(\gamma_{k} | {\bf w}{\bm \theta}, \sigma^{2}_{e})-F_{N}(\gamma_{k+1} | {\bf w}{\bm \theta}, \sigma^{2}_{e})$ \\   
&&&&\\


   \texttt{"exponential"}         &  1  &   1  &         $Pr(y)=$&$ f_{E}(\textrm{exp}(-l))$\\      
&&&&\\
   \texttt{"geometric"}         &  1  &   1  &         $Pr(y)=$&$ f_{G}(\frac{\textrm{exp}(l)}{1+\textrm{exp}(l)})$\\      
&&&&\\
   \texttt{"cengaussian"}        &  2 &   1  &        $Pr(y_{1}>y>y_{2}) =$&$ F_{N}(y_{2} | {\bf w}{\bm \theta},\sigma^{2}_{e})-F_{N}( y_{1} | {\bf w}{\bm \theta},\sigma^{2}_{e})$\\
&&&&\\
   \texttt{"cenpoisson"}        &  2  &   1  &        $Pr(y_{1}>y>y_{2}) =$&$ F_{P}(y_{2} | l)-F_{P}(y_{1} | l)$\\
&&&&\\
   \texttt{"cenexponential"}     &2    &  1  &          $Pr(y_{1}>y>y_{2}) =$&$ F_{E}(y_{2} | l)-F_{E}(y_{1} | l)$\\  
&&&&\\  
   \texttt{"zipoisson"}        &  1  &   2  &     $Pr(y=0) =$&$ \frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}+\left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)f_{P}(y|\textrm{exp}(l_{1}))$\\     
                                              &    &       & $Pr(y | y>0) =$&$ \left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)f_{P}(y |\textrm{exp}(l_{1}))$\\      
&&&&\\
   \texttt{"ztpoisson"}         &  1  &   1  &         $Pr(y)=$&$ \frac{f_{P}(y |\textrm{exp}(l))}{1-f_{P}(0 |\textrm{exp}(l))}$\\      
&&&&\\
   \texttt{"hupoisson"}        &  1  &   2  &     $Pr(y=0) =$&$ \frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}$\\     
                               &    &       & $Pr(y | y>0) =$&$\left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)\frac{f_{P}(y |\textrm{exp}(l_{1}))}{1-f_{P}(0 |\textrm{exp}(l_{1}))}$\\      
&&&&\\
   \texttt{"zapoisson"}        &  1  &   2  &     $Pr(y=0) =$&$1-\textrm{exp}(\textrm{exp}(l_{2}))$\\     
                               &    &       & $Pr(y | y>0) =$&$ \textrm{exp}(\textrm{exp}(l_{2}))\frac{f_{P}(y |\textrm{exp}(l_{1}))}{1-f_{P}(0 |\textrm{exp}(l_{1}))}$\\      
&&&&\\
   \texttt{"zibinomial"}        &  2  &   2  &     $Pr(y_{1}=0) =$&$ \frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}+\left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)f_{B}(0, n=y_{1}+y_{2}|\frac{\textrm{exp}(l_{1})}{1+\textrm{exp}(l_{1})})$\\     
                               &    &       & $Pr(y_{1} | y_{1}>0) =$&$ \left(1-\frac{\textrm{exp}(l_{2})}{1+\textrm{exp}(l_{2})}\right)f_{B}(y_{1}, n=y_{1}+y_{2}|\frac{\textrm{exp}(l_{1})}{1+\textrm{exp}(l_{1})})$\\      
&&&&\\
\hline\\
%\end{tabular}
\caption{\footnotesize{Distribution types that can fitted using \texttt{MCMCglmm}.  The prefixes \texttt{"zi"}, \texttt{"zt"}, \texttt{"hu"} and \texttt{"za"} stand for zero-inflated, zero-truncated, hurdle and zero-altered respectively. The prefix \texttt{"cen"} standards for censored where $y_{1}$  and $y_{2}$ are the upper and lower bounds for the unobserved datum $y$. $J$ stands for the number of categories in the multinomial/categorical distributions and this must be specified in the family argument for the multinomial distribution. The density function is for a single datum in a univariate model with ${\bf w}$ being a row vector of ${\bf W}$.  $f$ and $F$ are the density and distribution functions for the subscripted distribution ($N$=Normal, $P$=Poisson, $E$=Exponential, $G$=Geometric, $B$=Binomial). The $J-1$ $\gamma$'s in the ordinal models are the cutpoints, with $\gamma_{1}$ set to zero.\label{dist-tab}}}
%\end{center}
\end{longtable}
\end{landscape}


\ifalone
\end{document}
\else
\fi

