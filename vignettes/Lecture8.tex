\newif\ifalone
\alonefalse
\ifalone
\documentclass{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{Sweave}
\usepackage{lscape}
\usepackage{makeidx}



\title{Parameter Expansion}
\author{Jarrod Hadfield (\texttt{j.hadfield@ed.ac.uk})}

\begin{document}
\maketitle
\else
\chapter{Parameter Expansion}
\label{secPX}
\fi


As the covariance matrix approaches a singularity the mixing of the chain becomes notoriously slow. This problem is often encountered in single-response models when a variance component is small and the chain becomes stuck at values close to zero.  Similar problems occur for the EM algorithm and \citet{Liu.1998} introduced parameter expansion to speed up the rate of convergence. The idea was quickly applied to Gibbs sampling problems \citep{Liu.1999} and has now been extensively used to develop more efficient mixed-model samplers \citep[e.g.][]{vanDyk.2001, Gelman.2008b, Browne.2009}.\\

The columns of the design matrix (${\bf W}$) can be multiplied by the non-identified working parameters ${\bm \alpha} = \left[1,\ \alpha_{1},\ \alpha_{2},\ \dots \alpha_{k}\right]^{'}$:

\begin{equation} 
{\bf W}_{\alpha} = \left[{\bf X}\ {\bf Z}_{1}\alpha_{1}\ {\bf Z}_{2}\alpha_{2}\ \dots\ {\bf Z}_{k}\alpha_{k}\right]
\label{wstar}
\end{equation}

where the indices denote submatrices of ${\bf Z}$ which pertain to effects associated with the same variance component. Replacing  ${\bf W}$ with ${\bf W}_{\alpha}$ we can sample the new location effects  ${\bm \theta}_{\alpha}$ as described above, and rescale them to obtain ${\bm \theta}$:

\begin{equation} 
{\bm \theta} = ({\bf I}_{\beta}\oplus_{i=1}^{k}{\bf I}_{u_{i}}\ \alpha_{i}){\bm \theta}_{\alpha}
\end{equation}

where the identity matrices are of dimension equal to the length of the subscripted parameter vectors.\\ 


Likewise, the (co)variance matrices can be rescaled by the set of $\alpha$'s associated with the variances of a particular variance structure component (${\bm \alpha}_{\mathcal{V}}$):

\begin{equation} 
{\bf V} = Diag({\bm \alpha}_{\mathcal{V}}){\bf V}_{\alpha}Diag({\bm \alpha}_{\mathcal{V}})
\end{equation} 


The working parameters are not identifiable in the likelihood, but do have a proper conditional distribution. Defining the $n\times(k+1)$ design matrix ${\bf X}_{\alpha}$ with each column equal to the submatrices in Equation \ref{wstar} postmultiplied by the relevant subvectors of ${\bm \theta}_{\alpha}$, we can see that ${\bm \alpha}$ is a vector of regression coefficients: 

\begin{equation}
\begin{array}{rl}
\bf{l} =& {\bf X}_{\alpha}{\bm \alpha}+\bf{e}\\
\end{array}
\end{equation}

and so the methods described above can be used to update them. 

\subsection{Variances close to zero}

To use parameter exapnsion in \texttt{MCMCglmm} it is necessary to specify a prior covariance matrix for ${\bm \alpha}$ which is non-null. In section \ref{secPX-p} I discuss what this prior means in the context of posterior inference but for now we will specify two models, one parameter expanded and the other not. To illustrate I will fit a model that estimates the between mother variation in offsping sex ratio using parameter expansions:

\begin{Schunk}
\begin{Sinput}
> BTdata$sex[which(BTdata$sex == "UNK")] <- NA
> BTdata$sex <- gdata::drop.levels(BTdata$sex)
> prior1b = list(R = list(V = 1, fix = 1), G = list(G1 = list(V = 1, 
+     nu = 1, alpha.mu = 0, alpha.V = 1000)))
> m7b.1 <- MCMCglmm(sex ~ 1, random = ~dam, data = BTdata, 
+     family = "categorical", prior = prior1b, verbose = FALSE)
\end{Sinput}
\end{Schunk}

and fit a model that does not use parameter expansion:

\begin{Schunk}
\begin{Sinput}
> prior2b = list(R = list(V = 1, fix = 1), G = list(G1 = list(V = 1e-10, 
+     nu = -1)))
> m7b.2 <- MCMCglmm(sex ~ 1, random = ~dam, data = BTdata, 
+     family = "categorical", prior = prior2b, verbose = FALSE)
\end{Sinput}
\end{Schunk}

The prior densities in the two models are very similar across the range of variances with reasonable posterior support, and running the models for long enough will verify that they are sampling from very similar posterior densities. However, the mixing properties of the two chains are very different, with the non-parameter expanded chain (in red) getting stuck at values close to zero (Figure \ref{sexratio-fig}).\\



\begin{figure}[!h]
\begin{center}
\includegraphics{Lecture8-005}
\end{center}
\caption{Traces of the sampled posterior distribution for between female variance in sex ratio. The black trace is from a parameter expanded model, and the red trace from a non-parameter expanded model.}
\label{sexratio-fig}
\end{figure}

The parameter expanded model is 25\% slower per iteration but the effective sample size is 3.049 times greater:

\begin{Schunk}
\begin{Sinput}
> effectiveSize(m7b.1$VCV[, 1])
\end{Sinput}
\begin{Soutput}
    var1 
194.9393 
\end{Soutput}
\begin{Sinput}
> effectiveSize(m7b.2$VCV[, 1])
\end{Sinput}
\begin{Soutput}
    var1 
63.93423 
\end{Soutput}
\end{Schunk}

\subsection{Parameter expanded priors}
\label{secPX-p}

The original aim of applying parameter expanded methods to Gibbs sampling was to speed up the convergence and mixing properties of the chain. They achieve this by introducing parameters that are not identified in the likelihood, and for which all information comes from the prior distribution. By placing priors on these parameters we can induce different prior distributions for the variance components. These priors are all from the non-central scaled F-distribution, which implies the prior for the standard deviation is a non-central folded scaled t-distribution \citep{Gelman.2006}. To use parameter expansion it is necessary to specify the prior means (\texttt{alpha.mu}) and prior covariance matrix (\texttt{alpha.V}) in the prior. Without loss of generality \texttt{V} can be set to one, so that the prior for the variance (\texttt{v}) has density function:


\begin{Schunk}
\begin{Sinput}
> df(v/alpha.V, df1 = 1, df2 = nu, ncp = (alpha.mu^2)/alpha.V)
\end{Sinput}
\end{Schunk}

and the prior for the standard deviation:

\begin{Schunk}
\begin{Sinput}
> 2 * dt(sqrt(v)/sqrt(alpha.V), df = nu, ncp = alpha.mu/sqrt(alpha.V))
\end{Sinput}
\end{Schunk}

where \texttt{v}$>0$.\\

To illustrate I'll use the original Schools example from \citep{Gelman.2006}

\begin{Schunk}
\begin{Sinput}
> data(schools)
> head(schools)
\end{Sinput}
\begin{Soutput}
  school estimate   sd
1      A    28.39 14.9
2      B     7.94 10.2
3      C    -2.75 16.3
4      D     6.82 11.0
5      E    -0.64  9.4
6      F     0.63 11.4
\end{Soutput}
\end{Schunk}

The response variable \texttt{estimate} is the relative effect of Scholastic Aptitude Test coaching programs in 8 \texttt{schools}, and \texttt{sd} are the standard errors of the estimate. In the original example Gelman focused on the standard deviation of the between school effects and so we will place an improper flat prior on the standard deviation:  

\begin{Schunk}
\begin{Sinput}
> prior1 <- list(R = list(V = diag(schools$sd^2), 
+     fix = 1), G = list(G1 = list(V = 1e-10, nu = -1)))
> m7a.1 <- MCMCglmm(estimate ~ 1, random = ~school, 
+     rcov = ~idh(school):units, data = schools, 
+     prior = prior1, verbose = FALSE)
\end{Sinput}
\end{Schunk}

In this example there is information on the between school variance although we only have a single estimate for each school. This is possible because the within school variance was available for each school and we were able to fix the residual variance for each school at this value (See Section \ref{meta-sec}). The posterior distribution of the between school standard deviation is shown in Figure \ref{school1-fig} with the flat prior shown as a solid line.\\ 


\begin{figure}[!h]
\begin{center}
\includegraphics{Lecture8-012}
\end{center}
\caption{Between school standard deviation in educational test scores, with an improper uniform prior}
\label{school1-fig}
\end{figure}

We can also use the inverse-gamma prior with scale and shape equal to 0.001:

\begin{Schunk}
\begin{Sinput}
> prior2 <- list(R = list(V = diag(schools$sd^2), 
+     fix = 1), G = list(G1 = list(V = 1, nu = 0.002)))
> m7a.2 <- MCMCglmm(estimate ~ 1, random = ~school, 
+     rcov = ~idh(school):units, data = schools, 
+     prior = prior2, verbose = FALSE)
\end{Sinput}
\end{Schunk}

but Figure \ref{school2-fig} indicates that such a prior in this context may put too much density and values close to zero.\\


\begin{figure}[!h]
\begin{center}
\includegraphics{Lecture8-015}
\end{center}
\caption{Between school standard deviation in educational test scores, with an inverse-gamma prior with shape and scale set to 0.001}
\label{school2-fig}
\end{figure}


For the final prior we have \texttt{V=1}, \texttt{nu=1}, \texttt{alpha.mu=0} which is equivalent to a proper Cauchy prior for the standard deviation with scale equal to $\sqrt{\texttt{alpha.V}}$. Following \texttt{Gelman.2006} we use a scale of 25: 

\begin{Schunk}
\begin{Sinput}
> prior3 <- list(R = list(V = diag(schools$sd^2), 
+     fix = 1), G = list(G1 = list(V = 1, nu = 1, 
+     alpha.mu = 0, alpha.V = 25^2)))
> m7a.3 <- MCMCglmm(estimate ~ 1, random = ~school, 
+     rcov = ~idh(school):units, data = schools, 
+     prior = prior3, verbose = FALSE)
\end{Sinput}
\end{Schunk}

and Figure \ref{school3-fig} shows that the prior may have better properties than the inverse-gamma, and that the posterior is less distorted.


\begin{figure}[!h]
\begin{center}
\includegraphics{Lecture8-018}
\end{center}
\caption{Between school standard deviation in educational test scores, with a Cauchy prior with a scale of 25.}
\label{school3-fig}
\end{figure}

\subsection{Binary response models}

When analysing binary responses the residual variance is not identified in the likelihood and without a prior the posterior is improper.  If a weak prior is placed on the residual variance then the chain appears to mix poorly and the MCMC output often looks terrible. However, this poor mixing is in some ways superficial. As discussed in section \ref{cat-sec} we can rescale the location effects and variances by the estimated residual variance to obtain the posterior distribution for some fixed value of the actual residual variance. For example, we can refit the sex ratio model using a residual variance fixed at ten rather than one:

\begin{Schunk}
\begin{Sinput}
> prior3b = list(R = list(V = 10, fix = 1), G = list(G1 = list(V = 1, 
+     nu = 1, alpha.mu = 0, alpha.V = 1000)))
> m7b.3 <- MCMCglmm(sex ~ 1, random = ~dam, data = BTdata, 
+     family = "categorical", prior = prior3b, verbose = FALSE)
\end{Sinput}
\end{Schunk}

The two models appear to give completely different posteriors (Figure \ref{sexratio2})

\begin{Schunk}
\begin{Sinput}
> plot(mcmc.list(m7b.1$VCV[, 1], m7b.3$VCV[, 1]))
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Lecture8-021}
\end{center}
\caption{Between mother variation in sex ratio with the residual variance fixed at 1 (black trace) and 10 (red trace).}
\label{sexratio2}
\end{figure}

but rescaling indicates that they are very similar: 

\begin{Schunk}
\begin{Sinput}
> c2 <- (16 * sqrt(3)/(15 * pi))^2
> plot(mcmc.list(m7b.1$VCV[, 1]/(1 + c2 * m7b.1$VCV[, 
+     "units"]), m7b.3$VCV[, 1]/(1 + c2 * m7b.3$VCV[, 
+     "units"])))
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Lecture8-023}
\end{center}
\caption{Between mother variation in sex ratio with the residual variance fixed at 1 (black trace) and 10 (red trace) but with both estimates rescaled to what would be observed under no residual variance.}
\label{sexratio2}
\end{figure}


The prior specification for the between mother variance is different in the two models but Figure \ref{sexratio2} suggests that the difference has little influence. However, the mixing properties of the second chain are much better \citep{vanDyk.2001}:

\begin{Schunk}
\begin{Sinput}
> effectiveSize(m7b.1$VCV[, 1]/(1 + c2 * m7b.1$VCV[, 
+     "units"]))
\end{Sinput}
\begin{Soutput}
    var1 
194.9393 
\end{Soutput}
\begin{Sinput}
> effectiveSize(m7b.3$VCV[, 1]/(1 + c2 * m7b.3$VCV[, 
+     "units"]))
\end{Sinput}
\begin{Soutput}
    var1 
636.7686 
\end{Soutput}
\end{Schunk}

Although the chain mixes faster as the residual variance is set to be larger, numerical problem are often encountered because the latent variables can take on extreme values. For most models a variance of 1 is safe, but care needs to be taken so that the absolute value of the latent variable is less than 20 in the case of the logit link and less than 7 for the probit link. If the residual variance is not fixed but has an alternative proper prior placed on it then the Metropolis-Hastings proposal distribution for the latent variables may not be well suited to the local properties of the conditional distribution and the acceptance ratio may fluctuate widely around the optimal 0.44. This can be fixed by using the slice sampling methods outlined in \citet{Damien.1999} by passing \texttt{slice=TRUE} to \texttt{MCMCglmm}. Slice sampling can also be more efficient even if the prior is fixed at some value: 
 
\begin{Schunk}
\begin{Sinput}
> m7b.4 <- MCMCglmm(sex ~ 1, random = ~dam, data = BTdata, 
+     family = "categorical", prior = prior3b, verbose = FALSE, 
+     slice = TRUE)
> effectiveSize(m7b.4$VCV[, 1]/(1 + c2 * m7b.3$VCV[, 
+     "units"]))
\end{Sinput}
\begin{Soutput}
    var1 
625.4758 
\end{Soutput}
\end{Schunk}

\ifalone
\end{document}
\else
\fi

